
* Air-gapped MAAS
Many MAAS users maintain their data centres in an air-gapped environment that does not have an external Internet connection. MAAS runs well in this configuration, though keeping MAAS supplied with updates and images requires a bit of extra effort.

There are essentially four things that must be available to an air-gapped MAAS for smooth operation:

1. Snap updates (via the snap proxy)
2. Packages (via a local repo, possibly with a transparent proxy)
3. MAAS-maintained images (via  local mirror, possibly with a transparent proxy)
4. Other OS images (various methods)

There is at least one way to make each of these things available in an air-gapped environment.  Some of these can be set up to use a transparent proxy, which minimises changes to other components of the MAAS environment.

This article will help you learn:

- [About using the snap proxy](#heading--about-the-snap-proxy)
- [About air-gapped package updates](#heading--about-air-gapped-package-updates)
- [About local image mirroring](#heading--about-local-image-mirroring)
- [About non-MAAS images](#heading--about-non-maas-images)
- [How to use user_data to access non-MAAS-maintained images](#heading--other-os-user-data)
- [About transparent proxies](#heading--about-transparent-proxies)

<a href="#heading--About the snap proxy">** About the snap proxy

Using snaps in an air-gapped environment is possible with the Snap Store Proxy, which can be deployed in networks that are disconnected from the Internet.  Currently, the features required to use this proxy in an [air-gapped](https://docs.ubuntu.com/snap-store-proxy/en/airgap)`↗` mode are part of a password-protected internal Beta.

Client devices connect to the air-gapped proxy and never contact the general Snap Store nor the Internet.  Proxy operators will need to side-load all needed snaps and updates into the proxy.  To use this proxy, you must:

1. Register an offline Snap Store Proxy on an Internet-connected machine.
2. Set up HTTPS access to ensure adequate security.
3. Fetch the necessary snaps as needed by your MAAS environment (on the Internet-connected machine).

This proxy requires a properly configured PostgreSQL database -- see the [setup instructions](https://docs.ubuntu.com/snap-store-proxy/en/airgap)`↗` for the Snap Store Proxy for more details.

** About air-gapped package updates

The simplest way to use local package repos is via the [reprepro](https://manpages.ubuntu.com/manpages/focal/man1/reprepro.1.html)`↗` command.  There is an older command, `apt-mirror`, which is no longer maintained; it's not recommended.

The `reprepro` command manages a local repository of Debian packages.  You can add files manually or download them from some other repository.  It does not require an external database.  This command also handles signatures of mirrored repos, and can create signatures for the generated package indices, if desired.

You may wish to create a [transparent proxy](#heading--transparent-proxy) to make using your local repo easier.

** About local image mirroring
 
MAAS has an [established process](/t/how-to-mirror-images-locally/5927) for mirroring images locally.  In general, you must:

1. Install the `simplestreams` package.
2. Define some variables to simplify CLI usage.
3. Create the desired mirrors, specifying where you want your images stored.
4. Set up a new boot source on your local server, referring to the local mirror.

See the [local image mirror](/t/how-to-mirror-images-locally/5927) for details.  Note that you can use the menu at the top of that page to switch to specific instructions for the version, build-type, and interface you prefer.

** About non-MAAS images

MAAS allows you to deploy many types of OSes, and, once deployed, install specific software.  MAAS can configure a user specified repository for Ubuntu, so a user can mirror the Ubuntu apt repositories and point MAAS at those repos. When Ubuntu deploys apt will automatically be configured to use the user defined apt mirrors. 

MAAS only does this for Ubuntu, not CentOS or RHEL. If you deploy CentOS or RHEL with MAAS, the repos that built the image will be deployed.  But this won't work in an air \-gapped environment. To access non-MAAS-maintained images in an air-gapped environment, you will need to use one of two methods:

- Use `user_data`. A user can create custom `user_data` which will configure CentOS or RHEL to use a specific mirror.  Check out the [machine customisation](/t/how-to-customise-machines/5108) page for details on how to make this work.

- Create custom images and store them in your local mirror.  You can also [create custom images](/t/how-to-build-custom-images/5104) and store them in your local mirror.  Once you have the image built, consult the page on [local image mirrors](/t/how-to-mirror-images-locally/5927) to see how to incorporate your newly-built image into the local stash.

** About transparent proxies

If you don't wish to disturb the default configurations for Ubuntu and MAAS, you can create a transparent proxy for Debian packages and images, via the following general steps:

1. Configure Ubuntu to get packages via HTTP.
2. Configure MAAS to get packages via HTTP.
3. Create a local mirror repo for `archive.ubuntu.com`.
4. Create a local image mirror for `images.maas.io`.
5. Configure DNS to point to the local mirrors for both of those URLs.

This avoids any need to change the default settings for MAAS or Ubuntu.

* Annotations
Annotations are descriptive, searchable phrases that apply only to machines.  There are two types of annotations: static (always present in any machine state), and dynamic (only present in allocated or deployed states).  Annotations help you identify, characterise, and inform others about your machines.


* Ansible

[Ansible playbooks](https://github.com/maas/MAAS-ansible-playbook) are now available for MAAS. These extended YAML files automate various routine aspects of MAAS setup and configuration.  Automate the drudgery of installing and configuring MAAS with Ansible.  

With MAAS 3.3, playbooks are available to install and configure MAAS, including regions and racks.  There is also a set of groups that will automate setting up specific sections of MAAS. For example, there is a PostgreSQL group that sets up the primary and secondary PostgreSQL roles, bypassing the need to run both playbooks individually.

After installing Ansible, running each of the playbooks on a blank machine will have a fresh install of MAAS ready to go. For example, running the region+rack will setup a region+rack on the host machine.

** Ansible basics

A user should probably have a solid grasp of the Ansible standard terminology:

- Playbooks / plays
- Hosts and groups
- Inventory

[Ansible](https://www.redhat.com/en/technologies/management/ansible/what-is-ansible)`↗` is a sophisticated IT automation tool that allows users to set up [playbooks](https://docs.ansible.com/ansible/latest/getting_started/get_started_playbook.html)`↗`, which automate complex, repetitive (or error-prone)`↗` setup activities.  While we won't provide a detailed tutorial on Ansible here, there is a bit of terminology you should master before trying to use Ansible with MAAS:

- **[Modules](https://docs.ansible.com/ansible/latest/network/getting_started/basic_concepts.html#modules)`↗`** are binaries (or even pieces of code)`↗` that Ansible can run on a managed node.  These modules can be grouped into named collections.
- **[Tasks](https://docs.ansible.com/ansible/latest/network/getting_started/basic_concepts.html#tasks)`↗`** are individual operations with one or more modules; each task generally accomplishes some otherwise-human-driven function, such as "partition and format /sda".
- **[Plays](https://docs.ansible.com/ansible/latest/network/getting_started/basic_concepts.html#plays)`↗`** are sequences of tasks that Ansible will execute to accomplish larger operations, e.g., "install the OS" ==> "partition and format /sda", "install binary x.7.iso", etc.
- **[Playbooks](https://docs.ansible.com/ansible/latest/getting_started/get_started_playbook.html)`↗`** are YAML files that run plays in a specific order; for example, "install the OS", "install MAAS", "create a region controller", "sync images", etc.
- **[Inventory](https://docs.ansible.com/ansible/latest/network/getting_started/basic_concepts.html#id3)`↗`** is a source(s)`↗` of managed nodes; also called a "hostfile".

These simple descriptions do not fully explain the terms, so it is worthwhile to consult the referenced links, if necessary, before proceeding.  You will also want to understand how Ansible uses the terms "[hosts and groups](https://docs.ansible.com/ansible/latest/user_guide/intro_patterns.html)`↗`", since these are applied somewhat differently than we use them in MAAS.

** MAAS Ansible playbooks

Playbooks are available to automate the setup for:

- [MAAS region controllers](#heading--MAAS-region-controller): install and configure a MAAS region on a targeted host; running the playbook on hosts with a MAAS region will upgrade it.

- [MAAS rack controllers](#heading--MAAS-rack-controller): install and configure a MAAS rack.

- [MAAS de-installation](#heading--MAAS-de-installation): remove MAAS from a targeted host.

- [MAAS high availability](#heading--MAAS-high-availability): install and configure the HA proxy.

- [PostgreSQL primary role](#heading--PostgreSQL-primary-role): setup the postgres primary role.

- [PostgreSQL secondary role](#heading--PostgreSQL-secondary-role): setup the postgres secondary role.

- [Firewall rules](#heading--Firewall-rules): setup firewall rules.

MAAS Playbooks will eventually be available through Ansible Galaxy.

There is also a set of groups that will automate setting up specific sections of MAAS.  For example, there is a PostgreSQL group that sets up the primary and secondary PostgreSQL roles, bypassing the need to run both playbooks individually.  These groups include:

- [PostgreSQL role bundling scripts](#heading--PostgreSQL-role-bundling-scripts)
- ?? (there must be more, huh?)

After installing ansible, running each of the playbooks on a blank machine will have a fresh install of MAAS ready to go. For example, running the region+rack will setup a region+rack on the host machine.

** Running a MAAS Ansible playbook

In general terms, you can run any of the MAAS Ansible playbookss with a command of this form:

```nohighlight
ansible-playbook -i hosts \
--extra_vars \
"maas_version=$MAAS_VERSION 
maas_postgres_password=$MAAS_PG_PASSWORD 
maas_postgres_replication_password=$MAAS_PG_REP_PASSWORD 
maas_installation_type=<deb|snap> 
maas_url=$MAAS_URL" \
./site.yaml
```

A command of this form will run all of the plays below (i.e., the entire playbook).  If you want to run the tasks for one particular role (or roles), you can use the form  `--tags <target role(s)>` to limit which parts of the MAAS Ansible playbook run.  Consult the Ansible documentation for more details on additional options and command structure.

* API authentication reference
The MAAS API uses [OAuth](http://en.wikipedia.org/wiki/OAuth)`↗` as its authentication mechanism. This isn't third-party (3-legged) OAuth, so the process used is what's commonly referred to as 0-legged OAuth: the consumer accesses protected resources by submitting OAuth signed requests.

Note that some API endpoints support unauthenticated requests (i.e. anonymous access). This article will help you learn:

- [How to perform authenticated requests in Python](#heading--python)
- [How to perform authenticated requests in Ruby](#heading--ruby)

Here are two examples on how to perform an authenticated GET request to retrieve the list of nodes. The &lt;consumer_key&gt;, &lt;consumer_token&gt;, &lt;secret&gt; tokens are the three elements that compose the API key (API key = '&lt;consumer_key&gt;:&lt;consumer_token&gt;:&lt;secret&gt;').

** How to perform authenticated requests in Python

Note: the below example uses [fades](https://fades.readthedocs.io/)`↗`, but you can also install the `requests_oauthlib` ([pypi link](https://pypi.org/project/requests-oauthlib/)`↗`) and `oauthlib` ([pypi link](https://pypi.org/project/oauthlib/)`↗`) packages with `pip`. Replace `<MAAS_SERVER_IP>` with your server's IP address, and `<API-KEY>` with your API key.

``` python
from oauthlib.oauth1 import SIGNATURE_PLAINTEXT # fades
from requests_oauthlib import OAuth1Session # fades

MAAS_HOST = "http://<MAAS_SERVER_IP>:5240/MAAS"
CONSUMER_KEY, CONSUMER_TOKEN, SECRET = "<API-KEY>".split(":")

maas = OAuth1Session(CONSUMER_KEY, resource_owner_key=CONSUMER_TOKEN, resource_owner_secret=SECRET, signature_method=SIGNATURE_PLAINTEXT)

nodes = maas.get(f"{MAAS_HOST}/api/2.0/machines/", params={"op": "list_allocated"})
nodes.raise_for_status()

print(nodes.json())
```

** How to perform authenticated requests in Ruby

``` ruby
require 'oauth'
require 'oauth/signature/plaintext'

def perform_API_request(site, uri, key, secret, consumer_key)
    consumer = OAuth::Consumer.new(
        consumer_key, "",
        { :site => "http://localhost:5240/MAAS/api/2.0",
          :scheme => :header, :signature_method => "PLAINTEXT"})
    access_token = OAuth::AccessToken.new(consumer, key, secret)
    return access_token.request(:get, "/nodes/?op=list")
end

# API key = "<consumer_key>:<key>:<secret>"
response = perform_API_request(
     "http://server:5240/MAAS/api/2.0", "/nodes/?op=list", "<key>", "<secret>",
     "consumer_key>")
```

** How to perform authenticated requests using cURL

    curl --header "Authorization: OAuth oauth_version=1.0, oauth_signature_method=PLAINTEXT, oauth_consumer_key=$API_KEY[1], oauth_token=$API_KEY[2], oauth_signature=&$API_KEY[3], oauth_nonce=$(uuidgen), oauth_timestamp=$(date +%s)" \
    $MAAS_URL/MAAS/api/2.0/users/


** How to perform authenticated requests using HTTPie and fish

    set API_KEY (string split : $API_KEY)

    http $MAAS_URL/api/2.0/users/ \
    Authorization:"OAuth oauth_version=1.0, oauth_signature_method=PLAINTEXT, oauth_consumer_key=$API_KEY[1], oauth_token=$API_KEY[2], oauth_signature=&$API_KEY[3], oauth_nonce=$(uuidgen), oauth_timestamp=$(date +%s)"
	

* Audit event logs reference
Audit events are a subset of the MAAS event logs.  This article will provide reference material for those who want to review and report on events designated as MAAS audit events.

** About MAAS audit events

MAAS audit events can be viewed using the CLI with a command similar to the following:

```nohighlight
maas $PROFILE events query level=AUDIT
```

Such a command would produce JSON output like this:

```nohighlight
Machine-readable output follows:
{
    "count": 14,
    "events": [
        {
            "username": "admin",
            "node": "e86c7h",
            "hostname": "valued-moth",
            "id": 12729,
            "level": "AUDIT",
            "created": "Mon, 25 Apr. 2022 21:51:23",
            "type": "Node",
            "description": "Started deploying 'valued-moth'."
        },
        {
            "username": "admin",
            "node": "e86c7h",
            "hostname": "valued-moth",
            "id": 12725,
            "level": "AUDIT",
            "created": "Mon, 25 Apr. 2022 21:51:18",
            "type": "Node",
            "description": "Acquired 'valued-moth'."
        },
        {
            "username": "admin",
            "node": null,
            "hostname": "valued-moth",
            "id": 12502,
            "level": "AUDIT",
            "created": "Mon, 25 Apr. 2022 21:44:51",
            "type": "Node",
            "description": "Aborted 'commissioning' on 'valued-moth'."
        },
        {
            "username": "admin",
            "node": null,
            "hostname": "valued-moth",
            "id": 12497,
            "level": "AUDIT",
            "created": "Mon, 25 Apr. 2022 21:41:52",
            "type": "Node",
            "description": "Started commissioning on 'valued-moth'."
        },
        {
            "username": "admin",
            "node": null,
            "hostname": "valued-moth",
            "id": 12493,
            "level": "AUDIT",
            "created": "Mon, 25 Apr. 2022 21:41:18",
            "type": "Node",
            "description": "Started releasing 'valued-moth'."
        },
        {
            "username": "admin",
            "node": null,
            "hostname": "valued-moth",
            "id": 12486,
            "level": "AUDIT",
            "created": "Mon, 25 Apr. 2022 21:40:42",
            "type": "Node",
            "description": "Acquired 'valued-moth'."
        },
        {
            "username": "admin",
            "node": null,
            "hostname": "valued-moth",
            "id": 12479,
            "level": "AUDIT",
            "created": "Mon, 25 Apr. 2022 21:40:34",
            "type": "Node",
            "description": "Started releasing 'valued-moth'."
        },
        {
            "username": "admin",
            "node": null,
            "hostname": "valued-moth",
            "id": 134,
            "level": "AUDIT",
            "created": "Thu, 21 Apr. 2022 19:36:48",
            "type": "Node",
            "description": "Started deploying 'valued-moth'."
        },
        {
            "username": "admin",
            "node": null,
            "hostname": "valued-moth",
            "id": 130,
            "level": "AUDIT",
            "created": "Thu, 21 Apr. 2022 19:36:21",
            "type": "Node",
            "description": "Acquired 'valued-moth'."
        },
        {
            "username": "admin",
            "node": null,
            "hostname": "unknown",
            "id": 18,
            "level": "AUDIT",
            "created": "Thu, 21 Apr. 2022 19:21:46",
            "type": "Settings",
            "description": "Updated configuration setting 'completed_intro' to 'True'."
        },
        {
            "username": "admin",
            "node": null,
            "hostname": "unknown",
            "id": 14,
            "level": "AUDIT",
            "created": "Thu, 21 Apr. 2022 19:20:49",
            "type": "Settings",
            "description": "Updated configuration setting 'upstream_dns' to '8.8.8.8'."
        },
        {
            "username": "admin",
            "node": null,
            "hostname": "unknown",
            "id": 13,
            "level": "AUDIT",
            "created": "Thu, 21 Apr. 2022 19:20:49",
            "type": "Settings",
            "description": "Updated configuration setting 'maas_name' to 'neuromancer'."
        },
        {
            "username": "admin",
            "node": null,
            "hostname": "unknown",
            "id": 12,
            "level": "AUDIT",
            "created": "Thu, 21 Apr. 2022 19:20:47",
            "type": "Settings",
            "description": "Updated configuration setting 'http_proxy' to ''."
        },
        {
            "username": "admin",
            "node": null,
            "hostname": "unknown",
            "id": 11,
            "level": "AUDIT",
            "created": "Thu, 21 Apr. 2022 19:20:24",
            "type": "Authorisation",
            "description": "Logged in admin."
        }
    ],
    "next_uri": "/MAAS/api/2.0/events/?op=query&level=AUDIT&owner=admin&after=12729",
    "prev_uri": "/MAAS/api/2.0/events/?op=query&level=AUDIT&owner=admin&before=11"
}
```

These MAAS audit events consist of the following information:

- **username**: the name of the user whose actions triggered the event.  This field is frequently blank, since many recordable events are triggered by MAAS and not by a specific user.
- **node**: this is the `$SYSTEM_ID` frequently used in the CLI to reference node.  This field is filled if a particular node participated in the event, even if the node did not trigger that event.  
- **hostname**: this is the node which triggered the event.  Generally, this will be the name of the region controller, the name of a machine, or blank.  Blank entries are events triggered by MAAS itself, such as `Starting rack boot image import`, which are not triggered by node. 
- **id**: a unique ID number assigned to table records as a primary key.
- **level**: the level of event, such as AUDIT, DEBUG, etc.
- **created**: the timestamp when this event entry was created.
- **description**: a long text description of what took place. This field is almost always populated; this is the primary information used for auditing MAAS events.
- **type**: this is the type of event that occurred, as shown in the following table.

|               name               |                 description                |
|---------------------------------|----------------------------------------------|
| AUTHORISATION | Authorisation |
| IMAGES | Images |
| NETWORKING | Networking |
| NODE | Node |
| NODE_HARDWARE_SYNC_BLOCK_DEVICE | Node Block Device hardware sync state change |
| NODE_HARDWARE_SYNC_BMC | Node BMC hardware sync state change |
| NODE_HARDWARE_SYNC_CPU | Node CPU hardware sync state change |
| NODE_HARDWARE_SYNC_INTERFACE | Node Interface hardware sync state change |
| NODE_HARDWARE_SYNC_MEMORY | Node Memory hardware sync state change |
| NODE_HARDWARE_SYNC_PCI_DEVICE | Node PCI Device hardware sync state change |
| NODE_HARDWARE_SYNC_USB_DEVICE | Node USB Device hardware sync state chage |
| POD | Pod |
| SETTINGS | Settings |
| TAG | Tag |
| ZONES | Zones |

For information on how to use these audit events to answer specific questions, see [How to work with audit event logs](/t/how-to-work-with-audit-event-logs/5987).

* Bootstrap MAAS
An evolving example may be useful to introduce you to MAAS, and it doesn't have to be comprehensive --  just coherent and plausible.  For this example, we'll use the latest MAAS snap from the UI.

** Installation

Begin by installing (but not initialising) the MAAS snap:

```nohighlight
sudo snap install maas
maas (3.2/stable) <some-build-string> from Canonical installed
```

The MAAS initialisation mode "region+rack" will do fine for this install.  No need to add the complexity of separate rack controllers just yet.  It's not quite time to initialise, though; we need to choose production vs. proof-of-concept. For now, let's go with the production configuration, since there's more to see and do.

A production setup starts with a local PostgreSQL install, from packages.  And, like most Debian installs, that starts with an update, to grab any packages that might be needed for the install to succeed:

```nohighlight
sudo apt update -y

[sudo] password for stormrider: 
Hit:1 http://dl.google.com/linux/chrome/deb stable InRelease
Hit:2 http://us.archive.ubuntu.com/ubuntu focal InRelease                                      
Get:3 http://security.ubuntu.com/ubuntu focal-security InRelease [107 kB]
Get:4 http://us.archive.ubuntu.com/ubuntu focal-updates InRelease [111 kB]
Get:5 http://us.archive.ubuntu.com/ubuntu focal-backports InRelease [98.3 kB]
Get:6 http://us.archive.ubuntu.com/ubuntu focal-updates/main amd64 Packages [310 kB]
Get:7 http://security.ubuntu.com/ubuntu focal-security/main amd64 DEP-11 Metadata [21.2 kB]
Get:8 http://us.archive.ubuntu.com/ubuntu focal-updates/main i386 Packages [187 kB]     
Get:9 http://us.archive.ubuntu.com/ubuntu focal-updates/main amd64 DEP-11 Metadata [196 kB]
Get:10 http://us.archive.ubuntu.com/ubuntu focal-updates/universe amd64 Packages [142 kB]
Get:11 http://us.archive.ubuntu.com/ubuntu focal-updates/universe i386 Packages [77.6 kB]
Get:12 http://security.ubuntu.com/ubuntu focal-security/universe amd64 DEP-11 Metadata [35.8 kB]
Get:13 http://us.archive.ubuntu.com/ubuntu focal-updates/universe Translation-en [71.7 kB]
Get:14 http://us.archive.ubuntu.com/ubuntu focal-updates/universe amd64 DEP-11 Metadata [176 kB]
Get:15 http://us.archive.ubuntu.com/ubuntu focal-updates/multiverse amd64 DEP-11 Metadata [2,468 B]
Get:16 http://us.archive.ubuntu.com/ubuntu focal-backports/universe amd64 DEP-11 Metadata [1,972 B]
Fetched 1,538 kB in 2s (827 kB/s)                                             
Reading package lists... Done
Building dependency tree       
Reading state information... Done
325 packages can be upgraded. Run 'apt list --upgradable' to see them.
```

Then I can install PostgreSQL, probably version 12:

```nohighlight
sudo apt install -y postgresql

Reading package lists... Done
Building dependency tree       
Reading state information... Done
The following packages were automatically installed and are no longer required:
enchant geoip-database gir1.2-mutter-5 gsfonts libbind9-161 libcroco3 libdns-export1107
libdns1107 libdns1109 libenchant1c2a libfprint0 libgeoip1 libgnome-desktop-3-18 libirs161
libisc-export1104 libisc1104 libisc1105 libisccc161 libisccfg163 liblwres161 libmicrodns0
libmutter-5-0 liboauth0 libpoppler90 libpython3.7 libpython3.7-minimal libpython3.7-stdlib
linux-image-5.3.0-40-generic linux-modules-5.3.0-40-generic
linux-modules-extra-5.3.0-40-generic ubuntu-software ubuntu-system-service
Use 'sudo apt autoremove' to remove them.
Suggested packages:
postgresql-doc
The following NEW packages will be installed:
postgresql
0 upgraded, 1 newly installed, 0 to remove and 325 not upgraded.
Need to get 4,004 B of archives.
After this operation, 67.6 kB of additional disk space will be used.
Get:1 http://us.archive.ubuntu.com/ubuntu focal/main amd64 postgresql all 12+214 [4,004 B]
Fetched 4,004 B in 0s (13.2 kB/s)     
Selecting previously unselected package postgresql.
(Reading database ... 227326 files and directories currently installed.)
Preparing to unpack .../postgresql_12+214_all.deb ...
Unpacking postgresql (12+214) ...
Setting up postgresql (12+214) ...
```

** Initialisation

Yep, version 12.  Now we need to set up a PostgreSQL user:

```nohighlight
sudo -u postgres psql -c "CREATE USER \"maascli\" WITH ENCRYPTED PASSWORD 'maascli'"
CREATE ROLE
```

We also need a suitable MAAS database:

```nohighlight
sudo -u postgres createdb -O "maascli" "maasclidb"
```

Note that there's no system response (the old UNIX rule of "no news is good news").  Next, we need to add the database to the PostgreSQL HBA configuration, by editing `/etc/postgres/12/main/pg_hba.conf`, adding a line to the bottom of the file:

```nohighlight
sudo vi /etc/postgresql/12/main/pg_hba.conf
host    maasclidb       maascli         0/0                     md5
```

Finally, we can initialise MAAS, like this:

```nohighlight
sudo maas init region+rack --database-uri "postgres://maascli:maascli@localhost/maasclidb"
MAAS URL [default=http://192.168.43.251:5240/MAAS]:
```

This command offers me a bit of important feedback, the MAAS URL, which will be needed for the CLI login.  That's followed by a running commentary on the steps MAAS is taking to start up.

It all ends with the following admonition:

```nohighlight
MAAS has been set up.

If you want to configure external authentication or use
MAAS with Canonical RBAC, please run

sudo maas configauth

To create admins when not using external authentication, run

sudo maas createadmin
```

*** Creating an admin user

Well, that's an easy call.  Let's just run "createadmin" real quick:


```nohighlight
sudo maas createadmin
[sudo] password for stormrider: 
Username: admin
Password: 
Again: 
Email: admin@admin.com
Import SSH keys [] (lp:user-id or gh:user-id): xxxxxxxxxxx
```

** What we are trying to achieve

So imagine that you're the IT administrator for a new, 100-bed hospital that's under construction, intended to serve a suburban community of 5,000 people.  Call it "Metaphorical General Hospital" (MGH).   Your job is to design a flexible data centre for this facility.  You've decided to start with MAAS as your tool of choice, and for this planning exercise, you'll use VMs in a VM host.  You're trying to get to this setup:

<a href="https://discourse.maas.io/uploads/default/original/1X/18456dbd3fbfec14eddd044816fd0719692282da.jpeg" target = "_blank"><img src="https://discourse.maas.io/uploads/default/original/1X/18456dbd3fbfec14eddd044816fd0719692282da.jpeg"></a> 

First, there's some planning work do to.

** Machines

You'll need to start with a little network thinking (and design).  Talking through requirements with the staff, you come up with a random list of functions:
<table width="100%">
<tr><td>Charts</td><td>Provider orders</td><td>Provider documentation</td></tr>
<tr><td>Pharmacy</td><td>Narcotics control</td><td>Insurance collections</td></tr>
<tr><td>Housekeeping</td><td>Nursing orders</td><td>Med reconciliation</td></tr>
<tr><td>Timeclock</td><td>Patient collections</td><td>Med/surgical supplies</td></tr>
<tr><td>Office supplies</td><td>Patient registration</td><td>Insurance reconciliation</td></tr>
<tr><td>Payroll</td><td>Medication admin</td><td>Continuing education</td></tr>
<tr><td>Food service</td><td>Instrumentation</td><td>Information technology</td></tr>
</table>

You can handle this lowest level with individual [machines](/t/how-to-deploy-physical-machines/6192).  With MAAS, you'll be able to modify how many machines are performing which functions, somewhat on-the-fly, but let's assume that you start by creating (at least) one VM for each function.  Since you can reassign machines at will, you aren't going to name them for their functions; instead, you're just going to use the MAC address of each machine to uniquely identify it.

<details>
<summary>
<em>Try it!</em>
</summary>

*** Creating some sample VMs

Assuming you've [installed libvirt](https://help.ubuntu.com/lts/serverguide/libvirt.html)`↗` on the machine where you'll be running MAAS, you can create virtual machines like this:

Open the Virtual Machine Manager application.  You'll see a screen that looks something like this:

<a href="https://discourse.maas.io/uploads/default/original/1X/f66940a21313a27734bcaef6c539d36a720a6834.jpeg" target = "_blank"><img src="https://discourse.maas.io/uploads/default/original/1X/f66940a21313a27734bcaef6c539d36a720a6834.jpeg"></a> 

Click on "New Virtual Machine," which brings you to a corresponding dialog:

<a href="https://discourse.maas.io/uploads/default/original/1X/0702d9f2ab4c3659d13be553449093548a9e2f10.jpeg" target = "_blank"><img src="https://discourse.maas.io/uploads/default/original/1X/0702d9f2ab4c3659d13be553449093548a9e2f10.jpeg"></a> 

Select the "Network Boot (PXE)" option and click the "Forward" button:

<a href="https://discourse.maas.io/uploads/default/original/1X/0000fb5f072f2b3668465753ae6a713859d8a444.jpeg" target = "_blank"><img src="https://discourse.maas.io/uploads/default/original/1X/0000fb5f072f2b3668465753ae6a713859d8a444.jpeg"></a>

Choose the "Generic..." operating system by typing the first letters of "Generic" in the text box and selecting the relevant choice when it becomes available, then go Forward:

<a href="https://discourse.maas.io/uploads/default/original/1X/041914a0718633fce685ac7919e2478da0e62c1b.jpeg" target = "_blank"><img src="https://discourse.maas.io/uploads/default/original/1X/041914a0718633fce685ac7919e2478da0e62c1b.jpeg"></a> 

For CPU and memory, you can usually accept the defaults:

<a href="https://discourse.maas.io/uploads/default/original/1X/5a46262e3573aae7252951b3331ac9e3f3ef69c4.jpeg" target = "_blank"><img src="https://discourse.maas.io/uploads/default/original/1X/5a46262e3573aae7252951b3331ac9e3f3ef69c4.jpeg"></a> 

The storage values have a noticeable effect on local disk usage, so note that, generally, a VM only requires about 5.0 GiB, given an example exercise like this:

<a href="https://discourse.maas.io/uploads/default/original/1X/15f5e344c03bd1469c00333d466027e403c00ee8.jpeg" target = "_blank"><img src="https://discourse.maas.io/uploads/default/original/1X/15f5e344c03bd1469c00333d466027e403c00ee8.jpeg"></a> 

In the next screen, you'll have the chance to set a name; here, we've used a pseudo-MAC address, although you can name the machine whatever you want (and then return later to set the name to match the MAC address, if desired):

<a href="https://discourse.maas.io/uploads/default/original/1X/d4191b100d963032d47fed1f198aea76e8de273e.jpeg" target = "_blank"><img src="https://discourse.maas.io/uploads/default/original/1X/d4191b100d963032d47fed1f198aea76e8de273e.jpeg"></a> 

Selecting "Finish" will create the virtual machine and attempt to boot it -- which will fail, since no device currently knows about this VM (and hence can't boot it).  Not to worry; you're not done yet:

<a href="https://discourse.maas.io/uploads/default/original/1X/09b4e50049c2a251d100113e50a241d0c4a06f51.jpeg" target = "_blank"><img src="https://discourse.maas.io/uploads/default/original/1X/09b4e50049c2a251d100113e50a241d0c4a06f51.jpeg"></a> 

Select the "information" button (blue circle, white lowercase "i") to switch to the VM configuration screens, then select the "Boot Options" choice from the left-hand menu:

<a href="https://discourse.maas.io/uploads/default/original/1X/7b6cd37f7663db53571845da0159977092898fa4.jpeg" target = "_blank"><img src="https://discourse.maas.io/uploads/default/original/1X/7b6cd37f7663db53571845da0159977092898fa4.jpeg"></a> 

Turn off the "IDE" item under "Boot device order:"

<a href="https://discourse.maas.io/uploads/default/original/1X/54a8d6a77d9660e13aa1c0e278048ed1c751d65e.jpeg" target = "_blank"><img src="https://discourse.maas.io/uploads/default/original/1X/54a8d6a77d9660e13aa1c0e278048ed1c751d65e.jpeg"></a> 

When you select "Apply," a dialog will pop up to remind you that you need to restart this VM for changes to take effect:

<a href="https://discourse.maas.io/uploads/default/original/1X/6f4ab26216cc2951a202851869f7c7efc5691129.jpeg" target = "_blank"><img src="https://discourse.maas.io/uploads/default/original/1X/6f4ab26216cc2951a202851869f7c7efc5691129.jpeg"></a> 

Switch to the "NIC..." option and set the "Network source" and "Device model" as shown, then select "Apply" and respond to the dialog:

<a href="https://discourse.maas.io/uploads/default/original/1X/26fe981020c03e46c81e2bceed840bea7b2f14d6.jpeg" target = "_blank"><img src="https://discourse.maas.io/uploads/default/original/1X/26fe981020c03e46c81e2bceed840bea7b2f14d6.jpeg"></a> 

You'll next select the drop-down arrow next to the "on/off" menu bar option and select "Force reset," then answer the prompt in the affirmative:

<a href="https://discourse.maas.io/uploads/default/original/1X/537a485f0ff014aeb82afc71bc09b2988bf5cb56.jpeg" target = "_blank"><img src="https://discourse.maas.io/uploads/default/original/1X/537a485f0ff014aeb82afc71bc09b2988bf5cb56.jpeg"></a> 

You now have a VM that you can add to MAAS.  If you want more than one, you can simply right-click on the one you've just created and select "Clone:"

[note]
**Pro Tip**: Cloned VMs tend to use considerably less host disk space than newly-created ones.
[/note]

<a href="https://discourse.maas.io/uploads/default/original/1X/2348efd7dbf17ba445e3c4e6b3926fdc8cfbc888.jpeg" target = "_blank"><img src="https://discourse.maas.io/uploads/default/original/1X/2348efd7dbf17ba445e3c4e6b3926fdc8cfbc888.jpeg"></a> 

Another VM will instantiate, using the name of the cloned VM with an added "-clone" suffix:

<a href="https://discourse.maas.io/uploads/default/original/1X/a14b17602c2ad2465197a77c302080ca2eb59fc8.jpeg" target = "_blank"><img src="https://discourse.maas.io/uploads/default/original/1X/a14b17602c2ad2465197a77c302080ca2eb59fc8.jpeg"></a> 

You can create VMs as desired, remembering to mind your overall disk usage on your host system.

</details>

Let's assume that once you're done adding VMs, you have around 20 up and ready, all named after their assigned MAC address:

<a href="https://discourse.maas.io/uploads/default/original/1X/f9f302d8de9344908758a433dae9abfada0b0db3.jpeg" target = "_blank"><img src="https://discourse.maas.io/uploads/default/original/1X/f9f302d8de9344908758a433dae9abfada0b0db3.jpeg"></a> 

No need to create a lot of VMs for this example (unless you just want to do so).  

*** Manually adding machines

Once you've created the necessary VMs, you'll want to [manually add machines](/t/how-to-make-machines-available/5160#heading--how-to-add-a-machine-manually) to MAAS that correspond to your VMs.

<a href="https://discourse.maas.io/uploads/default/original/1X/91679cd615868eda4654541a68e59de57328ddfa.jpeg" target = "_blank"><img src="https://discourse.maas.io/uploads/default/original/1X/91679cd615868eda4654541a68e59de57328ddfa.jpeg"></a> 

<details>
<summary><em>Try it!</em></summary>

Creating a machine from a VM requires about a dozen pieces of information, most of which you can gather from the VM itself:

<a href="https://discourse.maas.io/uploads/default/original/1X/bc6c18c0fd31367bd4a9909fb7d954dc06f15c40.jpeg" target = "_blank"><img src="https://discourse.maas.io/uploads/default/original/1X/bc6c18c0fd31367bd4a9909fb7d954dc06f15c40.jpeg"></a> 

In the left column, you're only required to enter a machine name and the machine's MAC address:  

<a href="https://discourse.maas.io/uploads/default/original/1X/1de8d7afae996292d71e9787641bf0317b2327c9.jpeg" target = "_blank"><img src="https://discourse.maas.io/uploads/default/original/1X/1de8d7afae996292d71e9787641bf0317b2327c9.jpeg"></a> 

Here, we've assigned a variant of the MAC address as the machine name.  Note that the machine name cannot include colons (":"), we've substituted dashes.  In the right column, it's necessary to choose the power type.  When enlisting VMs, the correct power type is "Virsh," as shown below:

<a href="https://discourse.maas.io/uploads/default/original/1X/aa076ee437ce481808bb5f41320a45e60f3676de.jpeg" target = "_blank"><img src="https://discourse.maas.io/uploads/default/original/1X/aa076ee437ce481808bb5f41320a45e60f3676de.jpeg"></a> 

For default configurations, the Virsh Address is "qemu+ssh://[your-login-id]@192.168.122.1/system;" replace "[your-login-id]" with your username or login ID on the machine where you're hosting MAAS and the Virtual Machine Manager.  Likewise, the password is your normal login password for the same host.  Finally, you can retrieve the Virsh VM ID from the "Overview" screen of the VM itself:

<a href="https://discourse.maas.io/uploads/default/original/1X/79e135e48576bb6f455dd42fd7a09a2c7448d221.jpeg" target = "_blank"><img src="https://discourse.maas.io/uploads/default/original/1X/79e135e48576bb6f455dd42fd7a09a2c7448d221.jpeg"></a> 
</details>

As you add machines, they automatically commission:

<a href="https://discourse.maas.io/uploads/default/original/1X/37f1df9e4072b29c7183d4ae8ec1768504c4f66f.jpeg" target = "_blank"><img src="https://discourse.maas.io/uploads/default/original/1X/37f1df9e4072b29c7183d4ae8ec1768504c4f66f.jpeg"></a> 

When finished, the commissioned machines with be at the "Ready" state.

** Tags

Assigning machines to specific functions is something you can do after you [commission](/t/how-to-put-machines-to-work/5112#heading--how-to-commission-a-machine) and [deploy](/t/how-to-put-machines-to-work/5112) them.  (Later on, we'll discuss ways to load user apps and data onto the machines using the MAAS API.) Once you've got machines running apps, you want to keep up-to-date about which machine is doing what, when you're looking at the machine list.  You'll want to assign [tags](/t/how-to-tag-machines/6200) to machines.  

<a href="https://discourse.maas.io/uploads/default/original/1X/2ea0827b9ef327b59ad722215d556969218cc22f.jpeg" target = "_blank"><img src="https://discourse.maas.io/uploads/default/original/1X/2ea0827b9ef327b59ad722215d556969218cc22f.jpeg"></a> 

<details>
<summary><em>Try it!</em></summary>
Adding a tag to a machine is simple.  Just decide which machine you want to tag:

<a href="https://discourse.maas.io/uploads/default/original/1X/4f32fb8105ecee30afd0f3ca226b265dffe6e11b.jpeg" target = "_blank"><img src="https://discourse.maas.io/uploads/default/original/1X/4f32fb8105ecee30afd0f3ca226b265dffe6e11b.jpeg"></a> 

You'll want to click on the machine name (in this case, the MAC address), and then choose "Configuration" on the next screen that comes up.  This will bring you to a screen from which you can edit some parameters about the machine:

<a href="https://discourse.maas.io/uploads/default/original/1X/c31a50cebf68c8c5fbfbbe0115bb5c1daeb84ae8.jpeg" target = "_blank"><img src="https://discourse.maas.io/uploads/default/original/1X/c31a50cebf68c8c5fbfbbe0115bb5c1daeb84ae8.jpeg"></a> 

Click on "Edit," and then add a tag name to the "Tags" field.  Tags are automatically remembered by MAAS, so the next time you want to enter the same tag, an auto-complete field will appear, as shown below:

<a href="https://discourse.maas.io/uploads/default/original/1X/39a0e2f01ba7f3dc141bcf57c09b4e62f737525d.jpeg" target = "_blank"><img src="https://discourse.maas.io/uploads/default/original/1X/39a0e2f01ba7f3dc141bcf57c09b4e62f737525d.jpeg"></a> 

Select "Save changes" to add the tag(s) to the machine.  When you return to the machine list, you'll note that the tag is now associated with that machine:

<a href="https://discourse.maas.io/uploads/default/original/1X/8a21ca291aa800440d9074270ab9d9108cff9be1.jpeg" target = "_blank"><img src="https://discourse.maas.io/uploads/default/original/1X/8a21ca291aa800440d9074270ab9d9108cff9be1.jpeg"></a> 

</details>

Tags can will help you keep up with which machine(s) are covering which functions as you apply your apps.  You can search and filter by tags, and you can utilise tags from within the API, as well.

** Resource pools

As you look at the list of functions you've created, and talk more with the staff, you discover that some of these functions fit together more closely than others.  With some effort, you work out the following update to your network design:

<table width="100%">
<tr><td><strong>Provider services</strong></td><td></td><td></td></tr>
<tr><td>Charts</td><td>Provider orders</td><td>Provider documentation</td></tr>
<tr><td><strong>Nursing services</strong></td><td></td><td></td></tr>
<tr><td>Nursing orders</td><td>Continuing education</td><td></td></tr>
<tr><td><strong>Nursing meds</strong></td><td></td><td></td></tr>
<tr><td>Medication administration</td><td>Narcotics control</td><td></td></tr>
<tr><td><strong>Prescriber controls</strong></td><td></td><td></td></tr>
<tr><td>Pharmacy</td><td>Narcotics control</td><td>Medication reconciliation</td></tr>
<tr><td><strong>Staff compensation</strong></td><td></td><td></td></tr>
<tr><td>Timeclock</td><td>Payroll</td><td></td></tr>
<tr><td><strong>Supplies & services</strong></td><td></td><td></td></tr>
<tr><td>Medical and surgical supplies</td><td>Office and general supplies</td><td></td></tr>
<tr><td><strong>Business office</strong></td><td></td><td></td></tr>
<tr><td>Patient registration</td><td>Insurance reconciliation</td><td></td></tr>
<tr><td><strong>Collections</strong></td><td></td><td></td></tr>
<tr><td>Patient collections</td><td>Insurance collections</td><td></td></tr>
<tr><td><strong>Patient support</strong></td><td></td><td></td></tr>
<tr><td>Housekeeping</td><td>Food service</td><td></td></tr>
<tr><td><strong>Staff support</strong></td><td></td><td></td></tr>
<tr><td>Instrumentation</td><td>Information technology</td><td></td></tr>
</table>

You're aware that the number of machines you'll need use for each of the individual functions with vary according to real-world events in the hospital.  Still, you'd prefer to budget machines for these different functions, so that you know you can meet the needs of each.  The easiest way to handle this?  Creating [resource pools](/t/how-to-deploy-physical-machines/6193#heading--about-resource-pools) and naming them after the (new) top-level headings in your outline.  That way, you can reserve some number of machines for those functions, learning over time the right number of machines to allocate to each activity.

<details>
<summary><em>Try it!</em></summary>

Notice at the top of the machine list, there is a tab labelled, "Resource pools:"

<a href="https://discourse.maas.io/uploads/default/original/1X/f7d4c52a176f53f29a0c1ac3190e7abb563dc993.jpeg" target = "_blank"><img src="https://discourse.maas.io/uploads/default/original/1X/f7d4c52a176f53f29a0c1ac3190e7abb563dc993.jpeg"></a> 

In this example, there are already some resource pools defined to match the different functions above, except for one: Provider services.  Click the "Resource pools" tab to go there:

<a href="https://discourse.maas.io/uploads/default/original/1X/c05804c1f1bba45439d8894698b4dcefd64e7a5a.jpeg" target = "_blank"><img src="https://discourse.maas.io/uploads/default/original/1X/c05804c1f1bba45439d8894698b4dcefd64e7a5a.jpeg"></a> 

To add the "Provider services" (ProServ) pool, click on "Add pool:"

<a href="https://discourse.maas.io/uploads/default/original/1X/bebf192974683dde6cb21407f6db299f1e407925.jpeg" target = "_blank"><img src="https://discourse.maas.io/uploads/default/original/1X/bebf192974683dde6cb21407f6db299f1e407925.jpeg"></a> 

Fill in the fields for "Name" (which is a required field, with no spaces), and for "Description."  In this case, we've filled them in with "ProServ" and "Provider services:"

<a href="https://discourse.maas.io/uploads/default/original/1X/9da1e1c703818ac133db81082e1f3b01a72fb3e9.jpeg" target = "_blank"><img src="https://discourse.maas.io/uploads/default/original/1X/9da1e1c703818ac133db81082e1f3b01a72fb3e9.jpeg"></a> 

Click on "Add pool" to add this resource pool to the list, then click on "Machines" to return to the machine list.  Once there, it's simple to add machines to a particular pool.   In the column marked "POOL/NOTE," you'll see that your machines are in the "default" pool when created.  If you click on "default" there, you'll bring up a drop-down of already-created resource pools:

<a href="https://discourse.maas.io/uploads/default/original/1X/f373606dcd50c96a35af932379830f101d4a77e0.jpeg" target = "_blank"><img src="https://discourse.maas.io/uploads/default/original/1X/f373606dcd50c96a35af932379830f101d4a77e0.jpeg"></a> 

Just choose the one you want for this machine (in our example, ProServ) and you're done:

<a href="https://discourse.maas.io/uploads/default/original/1X/0cff1cf26f28236dbabc89b14a92c69435934933.jpeg" target = "_blank"><img src="https://discourse.maas.io/uploads/default/original/1X/0cff1cf26f28236dbabc89b14a92c69435934933.jpeg"></a> 

</details>

Here's a snippet of the updated machine list, with all machines added to the appropriate resource pool:

<a href="https://discourse.maas.io/uploads/default/original/1X/704b6d1603f6f90fca42891d98c3bb418458b94a.jpeg" target = "_blank"><img src="https://discourse.maas.io/uploads/default/original/1X/704b6d1603f6f90fca42891d98c3bb418458b94a.jpeg"></a> 

Resource pools are mostly for your use, helping you to budget servers within a given category.  Untagged servers can be in a pool, so if you've got five servers in the "Prescriber controls" resource pool, you can tag them with "Pharmacy," "Medication reconciliation," etc., as you use them.  It will also be obvious when you're running low on servers for that pool, and need to either provision more or move some unused ones from another pool.

** Notes

Another optional identifier for machines is the "Note" field.  While it can be long, a portion of it shows up on the machine list, which makes it useful for adding special identifiers or groupings.  In this example, we've added a vague identifier which might help an IT admin remember server locations or access rights.

<a href="https://discourse.maas.io/uploads/default/original/1X/8724395dfe9fc4d3f4a10a05687c33c6a3dded07.jpeg" target = "_blank"><img src="https://discourse.maas.io/uploads/default/original/1X/8724395dfe9fc4d3f4a10a05687c33c6a3dded07.jpeg"></a> 

<details>
<summary><em>Try it!</em></summary>

You can edit notes by clicking on a machine name in the machine list, switching to the "Configuration" tab, and selecting the "Edit" button.  These choices will bring you to a screen like this one:

<a href="https://discourse.maas.io/uploads/default/original/1X/a9d61f28a4ada7d97ff6f896d2f1e8e719ad680b.jpeg" target = "_blank"><img src="https://discourse.maas.io/uploads/default/original/1X/a9d61f28a4ada7d97ff6f896d2f1e8e719ad680b.jpeg"></a> 

From here, you can add free-form text into the "Note" field:

<a href="https://discourse.maas.io/uploads/default/original/1X/f8d647daffa9b3210fb99d440107a58e539a6c35.jpeg" target = "_blank"><img src="https://discourse.maas.io/uploads/default/original/1X/f8d647daffa9b3210fb99d440107a58e539a6c35.jpeg"></a> 

When you save the changes and return to the machine list, you'll notice that the NOTE field for that machine now contains your changes: 

<a href="https://discourse.maas.io/uploads/default/original/1X/46cf42808ef44829f1c610e479d6dfb62af2d898.jpeg" target = "_blank"><img src="https://discourse.maas.io/uploads/default/original/1X/46cf42808ef44829f1c610e479d6dfb62af2d898.jpeg"></a> 
</details>

** VLANs

Looking over your design, you notice that some of these resource pools must have their network traffic "fire-walled" from others -- for example, Provider services and Nursing services shouldn't be readily visible to Staff compensation or Food service.  Likewise, the relevant monitoring agencies require that facilities manage medications as a separate activity. The traditional way to separate these networks (other than creating entirely *separate* networks) would be a VLAN.  Luckily, MAAS supports multiple VLANS.  Adding one higher level to your design, you find yourself with this updated network topology:

<table width="100%">
<tr><td><strong>Caregiver services</strong></td><td></td></tr>
<tr><td>Provider services</td><td>Nursing services</td></tr>
<tr><td><strong>Medication management</strong></td><td></td></tr>
<tr><td>Nursing meds</td><td>Prescriber controls</td></tr>
<tr><td><strong>Accounts payable</strong></td><td></td></tr>
<tr><td>Staff compensation</td><td>Supplies & services</td></tr>
<tr><td><strong>Accounts receivable</strong></td><td></td></tr>
<tr><td>Business office</td><td>Collections</td></tr>
<tr><td><strong>Patient support</strong></td><td></td></tr>
<tr><td>Housekeeping</td><td>Food service</td></tr>
<tr><td><strong>Staff support</strong></td><td></td></tr>
<tr><td>Instrumentation</td><td>Information technology</td></tr>
</table>

Each of these higher-level groupings is ideal for a VLAN, so you create six of them, one for each division:

<a href="https://discourse.maas.io/uploads/default/original/1X/7245ed378ce0b9000aaf6f15b16ea16dbde2fccf.jpeg" target = "_blank"><img src="https://discourse.maas.io/uploads/default/original/1X/7245ed378ce0b9000aaf6f15b16ea16dbde2fccf.jpeg"></a> 

<details>
<summary><em>Try it!</em></summary>

Adding a functional VLAN requires some additional (common) networking aspects, which we'll cover later.  In the meantime, though, here's the short version of adding and naming the VLAN itself.  

From anywhere on the MAAS page, select "Subnets" from the top menu-bar, which brings you to this screen:

<a href="https://discourse.maas.io/uploads/default/original/1X/befd3a3eb5987d412477d0a076d16a50e81dae30.jpeg" target = "_blank"><img src="https://discourse.maas.io/uploads/default/original/1X/befd3a3eb5987d412477d0a076d16a50e81dae30.jpeg"></a> 

Using the "Add" drop-down, select "VLAN:"

<a href="https://discourse.maas.io/uploads/default/original/1X/dbdea7bec608d14e89da82cfdea87df3f93855dd.jpeg" target = "_blank"><img src="https://discourse.maas.io/uploads/default/original/1X/dbdea7bec608d14e89da82cfdea87df3f93855dd.jpeg"></a> 

You'll arrive at this screen, which allows you to specify the VLAN:

<a href="https://discourse.maas.io/uploads/default/original/1X/e371011171ba18839f96788fefa40a04af3e79bb.jpeg" target = "_blank"><img src="https://discourse.maas.io/uploads/default/original/1X/e371011171ba18839f96788fefa40a04af3e79bb.jpeg"></a> 

Enter the Name and ID of the VLAN, and select the fabric to enclose it (in this case, the "default" fabric):

<a href="https://discourse.maas.io/uploads/default/original/1X/961d5cae7119db1c3fb7e8d6ae6ce7015d9263d1.jpeg" target = "_blank"><img src="https://discourse.maas.io/uploads/default/original/1X/961d5cae7119db1c3fb7e8d6ae6ce7015d9263d1.jpeg"></a> 

When you're satisfied with your choices, select "Add VLAN" to complete the operation.

</details>

Ignoring the networking aspects (for now), these VLANs should help isolate major functions and provide a level of data integrity and access control for your new hospital network.

** Fabrics

Considering your network design so far, you notice that some of the VLANs need to be able to communicate with each other some of the time.  In fact, you decide on three pairs of VLANs to cover this new networking situation:

<table>
<tr><td><strong>Patient management</strong></td><td></td></tr>
<tr><td>Caregiver services</td><td>Medication management</td></tr>
<tr><td><strong>Accounting</strong></td><td></td></tr>
<tr><td>Accounts payable</td><td>Accounts receivable</td></tr>
<tr><td><strong>Facilities</strong></td><td></td></tr>
<tr><td>Patient support</td><td>Staff support</td></tr>
</table>

You want to incorporate these highest-level groupings into your network, but how?  MAAS provides the answer with fabrics.  A fabric is a set of interconnected VLANs that can communicate, so you simply create three fabrics, each covering one of these top-level categories.

<details>
<summary><em>Try it!</em></summary>

You can add a fabric by selecting the "Subnets" tab, clicking on the "Add" drop-down, and choosing "Fabric:"

<a href="https://discourse.maas.io/uploads/default/original/1X/509e9696919e69cfc57602a6228425a472b3ac1d.jpeg" target = "_blank"><img src="https://discourse.maas.io/uploads/default/original/1X/509e9696919e69cfc57602a6228425a472b3ac1d.jpeg"></a> 

You'll see the "Add fabric" dialog appear.  Enter the desired fabric name and click "Add fabric:"

<a href="https://discourse.maas.io/uploads/default/original/1X/7873e6a97212673ab08c8c3c33f9d63d7069b8e8.jpeg" target = "_blank"><img src="https://discourse.maas.io/uploads/default/original/1X/7873e6a97212673ab08c8c3c33f9d63d7069b8e8.jpeg"></a> 

Here you'll notice three new fabrics, one for each of the top-level groupings in your example network design:

<a href="https://discourse.maas.io/uploads/default/original/1X/4f787bc5d57c7f811641e32b42c96bb2a2792356.jpeg" target = "_blank"><img src="https://discourse.maas.io/uploads/default/original/1X/4f787bc5d57c7f811641e32b42c96bb2a2792356.jpeg"></a> 

Next, you'll want to assign your VLANs to this fabric.  Begin by clicking on any VLAN you want to move, which will bring you to a summary screen for that VLAN:

<a href="https://discourse.maas.io/uploads/default/original/1X/ecca590663b90106b144c003851732a16acd5220.jpeg" target = "_blank"><img src="https://discourse.maas.io/uploads/default/original/1X/ecca590663b90106b144c003851732a16acd5220.jpeg"></a> 

You can click "Edit" and choose the desired fabric from the drop-down list:

<a href="https://discourse.maas.io/uploads/default/original/1X/6f6e2bff0d67dc02d33800e5cc1d60db24fb398a.jpeg" target = "_blank"><img src="https://discourse.maas.io/uploads/default/original/1X/6f6e2bff0d67dc02d33800e5cc1d60db24fb398a.jpeg"></a> 

Finally, click "Save summary" to move this VLAN to the desired fabric.  The end result of assigning our example VLANs to the three fabrics is shown below.

</details>

<a href="https://discourse.maas.io/uploads/default/original/1X/23c214cd6836dd783347f050f2cdba04da7bcaa1.jpeg" target = "_blank"><img src="https://discourse.maas.io/uploads/default/original/1X/23c214cd6836dd783347f050f2cdba04da7bcaa1.jpeg"></a>

* Cloud networking

Cloud network architectures deviate significantly from the architecture of the Internet infrastructure.  These deviations are driven mostly by economics, simplicity, and scalability.  This article will help you learn:

- [About old and new network architectures](#heading--clos-architecture)
- [Problems with the AAG architecture](#heading--aag-problems)
- [Disaggregating the cloud](#heading--disaggregating-the-cloud)
- [Routing still rules](#heading--routing-still-rules)

** About old and new network architectures

Before there were networks, monolithic applications ran on a mainframe with hardwired I/O devices.  As CPUs proliferated in separate enclosures, LANs like [Banyan Vines](https://en.wikipedia.org/wiki/Banyan_VINES)`↗` grew up.  Proprietary mismatch led to the [OSI model](https://maas.io/docs/about-tcp-ip-networks#heading--about-the-osi-model)`↗`.  Next came the Web, which distributed processing to client devices.  Now, we have the idea of generic switches and servers in cloud and bare-metal clusters.  Servers have shifted from dedicated applications to being completely [virtualised](https://en.wikipedia.org/wiki/Virtualization)`↗`.

A traditional AAC architecture looks like this:

<a href="https://discourse.maas.io/uploads/default/original/2X/e/e15a35da43b2788883ec014efb1832b8f641e872.jpeg" target="_blank"><img src="https://discourse.maas.io/uploads/default/original/2X/e/e15a35da43b2788883ec014efb1832b8f641e872.jpeg"></a>

It’s heavy on network hardware; that is, the radio of switches to servers is (too) high.  Switch-driven networks used hardware packet switching ([merchant silicon](https://etherealmind.com/analysis-merchant-custom-silicon)`↗`) to serve proprietary network configurations.  In theory, bridges needed no configuration, although [congestion](https://en.wikipedia.org/wiki/Network_congestion)`↗` and [mistaken identity](https://en.wikipedia.org/wiki/IP_address#Addressing_conflicts)`↗` led to the need for [STP](https://en.wikipedia.org/wiki/Spanning_Tree_Protocol)`↗`, [per-VLAN trees](https://networklessons.com/spanning-tree/per-vlan-spanning-tree-pvst)`↗`, and IP address redundancy management techniques.

A cloud architecture simplifies these networks:

<a href="https://discourse.maas.io/uploads/default/original/2X/f/fd86954e48538ce9ba8fc6e02df23b0a2337ef12.jpeg" target="_blank"><img src="https://discourse.maas.io/uploads/default/original/2X/f/fd86954e48538ce9ba8fc6e02df23b0a2337ef12.jpeg"></a>

Cloud architecture is a simple [spine-and-leaf network](https://networklessons.com/cisco/ccna-200-301/spine-and-leaf-architecture)`↗`, built from cheap, identical switches connecting cheap, identical servers.  Every switch has a path to every other switch, mitigating congestion.  Any switch can route any traffic anywhere, and any server can do any job (at any time)`↗`.  This architecture moves the developer’s focus from metal to actual computing problems.

** Problems with the AAG architecture

Modern applications generate much more server-to-server traffic than the original Access-Aggregation-Core networks envisioned.  As a result, the AAG architecture presents several problems under load:

- **VLAN shorting**: At cloud scale, 12 bits of VLAN ID don't provide nearly enough VLAN instances.  Even with Multi-instance/STP, there simply aren't enough virtual LAN networks to accommodate the build-out of even a modest cloud configuration.  Tenant machines come and go rapidly, and STP hello messages simply can't keep up.
- **STP switch-count**: STP is only designed for two aggregation switches, which throttles bandwidth and causes massive, constant congestion in an extended cloud network.
- **Million-plus-packet floods**: Broadcast-and-learn bridges can't keep up with a million-plus MAC addresses, and even if they do, when those cached addresses start to time out, the periodic re-learning floods edge hosts, incapacitating them on a regular basis.
- **Unintentional VLAN partitioning**: Large VLANs plus many devices creates a high-probability of a configuration mistake or an ID refresh miss; these issues cause the network to be invisibly partitioned, making troubleshooting all but impossible.
- **ARP storms**: Hundreds of virtual machines bound to a couple of aggregation switches translates to a network-saturating flurry of ARP refreshes.  There's no easy fix for this without changing the network topology, or setting the refresh timer so high that DHCP hosts will fall off the DNS periodically.

There are other issues usually cited, but these are most concrete.

** Disaggregating the cloud

An effective cloud architecture breaks routers and bridges into separate hardware and software, sometimes called "switch disaggregation".  A merchant-silicon chassis (the hardware) can be combined with someone else's NOS (network operating system, the software) to produce more standard and cost-effective network switch-gear.  In practice, few cloud providers actually create their own NOS, choosing instead to either integrate merchant silicon with a separately purchased NOS, or, more likely, to buy bundled units from switch providers that have broader integration experience.

Either way, the effect is to have switch-gear which is (1) less likely to be proprietary, and (2) easier to upgrade as the hardware advances.  Switch disaggregation effectively commoditizes the switching elements, effectively eliminating them from network throughput calculations.  In other words, the bridges and routers in a modern cloud network are essentially invisible, uninteresting, and inexpensive, which facilitates the cloud-network build-out model.

** Routing still rules

Regardless of your network vintage, packet routing is still the fundamental algorithm that gates throughput.  If you're familiar with routing, this section can be skipped. If you're trying to level up your cloud networking knowledge, this section should help quite a bit.

Routing is fundamentally simple: you're trying to get a packet from a source to a destination, using the destination IP address embedded in that packet.  In practice, routing is more complicated.  Most of the routers in a network path are not forwarding directly to the destination.  Instead, they're sending the packet to the next hop, that is, another, reachable router that is closer to the intended destination.

In [the TCP/IP tutorial](#heading--borrowed-from-ma-bell), we talked about telephone-line repeaters that refreshed the signal to keep it crystal clear over long runs of wire.  The signals didn't have to make it to the intended recipient, they just had to make it to the next repeater, which boosted the signal, cleaned up the noise, and sent the call on down the line to another repeater.  Eventually the call would make it to the subscriber.

Routing works very much the same way: most of the routers in the loop understand how to find another router which is close to the intended destination.  That "next hop" will take care of repackaging the packets and sending them further down the line.  What matters in high-traffic cloud networks is not routing, per se, but how the packets are routed.  The choice of routing protocols can make all the difference in whether or not your network lags.  For this reason, we're going to take a more extended look at routing.

*** Multicast routing

One helpful tool for modern cloud networks is the concept of multicast routing.  Stated simply, multicast routing allows one packet to be received by many servers, but only if those servers are interested in receiving it.  Multicast receivers -- which have to subscribe in order to receive the packets -- support a much larger (and more flat) network.  This layout more easily scales to the cloud architecture shown above. Multicasting is generally faster and more efficient for certain payloads; unlike broadcast packets, multicast packets are not examined by every NIC in the packet's path.

For example, all IPv6 communications are multicast, so there's no ARP.  Instead, a process called [neighbour discovery](https://en.wikipedia.org/wiki/Neighbor_Discovery_Protocol)`↗` (NDP) is used.  But IPv4 networks can also handle multicast transactions.  Both IPv4 and IPv6 protocols support [multicast address blocks](https://en.wikipedia.org/wiki/Multicast_address)`↗`, which enable multicasting.

A multicasting server sends one packet, and the network handles replication and addressing (multiplexing) of the packets to the subscribed servers.  Multicast is a one-way protocol: any responses have to be sent by other protocols.  For larger server farms, multicast is a good way to handle things like a software update or a database refresh.  Two special protocols handle subscriptions: [IGMP](https://en.wikipedia.org/wiki/Internet_Group_Management_Protocol)`↗`, used by individual IPv4 receivers, and [PIM](https://en.wikipedia.org/wiki/Protocol_Independent_Multicast)`↗`, which is used by L3 devices (like routers) to manage multicast "trees" across a network or subnet.

* Commissioning logs reference
Commissioning logs contain a list of commissioning scripts with a timestamp and result.  You can view the detailed logs for each of these scripts in the UI by clicking on the "Commissioning" tab for a specific machine.  

You will be presented with a status table of commissioning scripts.  Each of the items in the "NAME" column is the name of a [commissioning script](/t/how-to-put-machines-to-work/5112#heading--how-to-commission-a-machine).  Each entry gives a timestamp and a result (e.g. passed, failed, ...). At the end of each line is a link to view the log. It's worth taking a look at each of the script logs and their typical output.

You can also use the MAAS CLI to retrieve the verbatim logs of commissioning script runs, including those that are currently in progress.

```nohighlight
maas $PROFILE node-script-result read $SYSTEM_ID $RESULTS
```

If you only want to see the latest or currently-running result, you can use `current-commissioning`, `current-testing`, or `current-installation` instead of `$SYSTEM_ID`.  You can also limit which results are returned by type (commissioning, testing, or installation), script name, or script run:

```nohighlight
maas $PROFILE node-script-results read \
 $SYSTEM_ID type=$SCRIPT_TYPE filters=$SCRIPT_NAME,$TAGS
```

You can also suppress failed results, which is useful if you want to ignore a known failure:

```nohighlight
maas $PROFILE node-script-results update \
 $SYSTEM_ID type=$SCRIPT_TYPE filters=$SCRIPT_NAME,$TAGS suppressed=$SUPPRESSED
```

where `$SUPPRESSED` is either `True` or `False`. The JSON formatted output to the above command will include 'results' dictionary with an entry for `suppressed`:

```nohighlight
"results": [
    {
        "id": 21,
        "created": "Tue, 02 Apr 2019 17:00:36 -0000",
        "updated": "Tue, 02 Apr 2019 20:56:41 -0000",
        "name": "smartctl-validate",
        "status": 5,
        "status_name": "Aborted",
        "exit_status": null,
        "started": "Tue, 02 Apr 2019 20:56:41 -0000",
        "ended": "Tue, 02 Apr 2019 20:56:41 -0000",
        "runtime": "0:00:00",
        "starttime": 1554238601.765214,
        "endtime": 1554238601.765214,
        "estimated_runtime": "0:00:00",
        "parameters": {
            "storage": {
                "argument_format": "{path}",
                "type": "storage",
                "value": {
                    "id_path": "/dev/vda",
                    "model": "",
                    "name": "sda",
                    "physical_blockdevice_id": 1,
                    "serial": ""
                }
            }
        },
        "script_id": 1,
        "script_revision_id": null,
        "suppressed": true
    }
]
```

Finally, results can be downloaded, either to stdout, stderr, as combined output or as a tar.xz:

```nohighlight
maas $PROFILE node-script-result download $SYSTEM_ID $RUN_ID output=all \
 filetype=tar.xz > $LOCAL_FILENAME
```

[note]
**$RUN_ID** is labelled `id` in the verbose result output.
[/note]

** maas-support-info

MAAS gathers information that helps to identify and characterise the machine for debugging purposes, such as the kernel, versioning of various components, etc.  This script gathers this information, mostly as a bundle to be provided to a support specialist to help get the baseline for the machine in question.  This script runs in parallel with other scripts to speed commissioning. 

Here's a quick breakdown of the fields you may see in the script output:

- **KERNEL INFO:** this is the output of `uname -a` -- the MAC address is the machine name in this case.

- **KERNEL COMMAND LINE:** these are the kernel command line parameters, directed at various modules built into the kernel.  The function `modprobe` parses this kernel command line and collects the relevant module parameters when it loads a module.  Note that this command line could also be used to pull in loadable modules.

- **CLOUD CONFIG QUERY:** a cloud-init query is used to retrieve cloud instance metadata used by cloud-init when booting an instance.  This section shows the specific metadata retrieved during cloud-init query for this machine.

- **CPU CORE COUNT AND MODEL:** the data produced here is similar to the output you could retrieve by running `nproc` and then attempting a `cat /sys/devices/cpu/caps/pmu_name` -- and so on.  There several ways to retrieve this info, but all can produce the number and type of CPU(s) available.

- **PCI INFO:** the devices, real or virtual, that are connected to the machine via PCI (Peripheral Component Interconnect) bus.

- **USB INFO:** the devices, real or virtual, that are connected to the machine via USB bus.

- **MODALIASES:** a modalias is a sysfs technique to capture the information that a hardware item exposes to the kernel, with the file basically providing a template or structure for this information.  Each of the entries in this list describe one particular part of the machine's (real or virtual) hardware, down to the level of alarm timers, framebuffers, and even speakers.  In the event of a bug, this information can help your support engineer (or yourself) understand exactly what hardware is configured for this machine.

- **SERIAL PORTS:** this section just lists the serial devices made available on this machine.

- **NETWORK INTERFACES:** summarises the network interfaces available on this machine -- essentially an abbreviated version of the output from some form of an `ip` command.

- **BLOCK DEVICE SUMMARY:** a thumbnail sketch of the block devices (usually storage) available on this machine.

- **#dmidecode...:** this section presents the basic DMI data, including the BIOS type, extent, size, and table location.

- **DMI DATA:** the raw (undecoded) DMI table for this machine, presented for verification of the following DMI data sections, if desired.

- **DMI KEYPAIRS:** the individual machine specifications, as decoded from the DMI table.  The manpage **dmidecode (8)** gives more details on each of these keypairs.

** maas-lshw

This script pulls system BIOS and vendor info, and generates user-defined tags for later use.  `maas-lshw` runs in parallel with other scripts to speed up the commissioning process.  This output is roughly equivalent to the output of `lshw -xml` on the machine in question.  

There are many available references to decode this information, so for now, here is a short glossary of most the terms (essentially, the tags) that typically appear in a listing like this:

- **businfo** - the bus information for this device.

- **capacity** - the maximum capacity reported by the device.

- **class** - the device's class.

- **clock** - the bus clock of the device (in Hz).

- **description** - a human-readable description of the hardware node.

- **dev** - the device number (major.minor).

- **id** - the internal identifier used by `lshw`.

- **logicalname** - the logical node name used by the system.

- **physid** - the physical id of the device.

- **product** - the specific product name of the device.

- **serial** - the serial number of the device.

- **size** - the actual size of the device.

- **slot** - location of the physical (or virtual) connection.

- **vendor** - the name of the vendor or manufacturer of the device.

- **version** - the version or release information associated with the device.

- **width** - the address width of the device (32/64 bits).

Some additional fields may be present in this output.  These will be identified and described as necessary for specific instances and situations.

** 20-maas-01-install-lldpd

This script installs the link layer discovery protocol (LLDP) daemon, which will later capture networking information about the machine.  This logs the basic install process to the commissioning logs in real time.

** maas-list-modaliases

This script figures out what hardware modules are loaded, providing a way to autorun certain scripts based on which modules are loaded.  `maas-list-modaliases` runs in parallel with other scripts to speed up the commissioning process.

** 20-maas-02-dhcp-unconfigured-ifaces

MAAS will want to know all the ways the machine is connected to the network. Only PXE comes online during boot; this script brings all the other networks online so they can be recognised.  This script logs the discovery of these networks to the commissioning logs.

** maas-get-fruid-api-data

This script gathers information for the Facebook wedge power type, and it runs in parallel with other scripts to improve commissioning speed. You will note that this output does not actually contain any specific information, but rather the echoed commands of a script to gather the information.

** maas-serial-ports

This script lists what serial ports are available on the machine; it runs in parallel to speed up commissioning.  Log output from a normal, successful run is simply a list of serial ports.  The output of this script may be useful when you're trying to verify that a particular serial interface is active and available on your machine.

** 40-maas-01-network-interfaces

In MAAS 2.9 and below, this script is just used to get the IP address, which can then be associated with a VLAN/subnet.  The results are simply the output of a command similar to `ip a`.  This script is not used in MAAS 3.0 and higher.

** 50-maas-01-commissioning

This script is the main MAAS tool, gathering information on machine resources, such as storage, network devices, CPU, RAM, etc. We currently pull this data using lxd: We use a Go binary built from lxd source that just contains the minimum source to gather the resource information we need.

This output represents a catalogue of the resources available on this machine, in a format readable by both humans and machines.  You can use this to verify that your configuration is what you expected.

** maas-capture-lldp

This script gathers LLDP network information to be presented on the logs page; this data is not used by MAAS at all.  The script runs in parallel with other scripts for speed. Note that the log output from a successful run is uninteresting.

** maas-kernel-cmdline

This script is used to update the boot devices; it double-checks that the right boot interface is selected.  Successful output is the specific command that boots the machine kernel, something like this:

```nohighlight
nomodeset ro root=squash:http://192.168.122.2:5248/images/ubuntu/amd64/generic/bionic/daily/squashfs ip=::::52-54-00-0b-6d-8c:BOOTIF ip6=off overlayroot=tmpfs overlayroot_cfgdisk=disabled cc:{'datasource_list': ['MAAS']}end_cc cloud-config-url=http://192-168-122-0--24.maas-internal:5248/MAAS/metadata/latest/by-id/pb6833/?op=get_preseed apparmor=0 log_host=192.168.122.2 log_port=5247 BOOTIF=01-52:54:00:0b:6d:8c
```
These are the kernel command line parameters, which control the invocation of various modules built into the kernel.  The function `modprobe` parses this kernel command line and collects the relevant module parameters when it loads a module.  Note that this command line could also be used to pull in loadable modules.

* Commissioning scripts

MAAS runs scripts during enlistment, commissioning and testing to collect data about nodes. Both enlistment and commissioning run all builtin commissioning scripts, though enlistment runs only built-ins. Commissioning also runs any user-uploaded commissioning scripts by default, unless the user manually provides a list of scripts to run. MAAS uses these commissioning scripts to configure hardware and perform other tasks during commissioning, such as updating the firmware. Similarly, MAAS employs hardware testing scripts to evaluate system hardware and report its status.

Scripts can be selected to run from web UI during commissioning, by testing hardware,  or from the command line. Note that MAAS only runs built-in commissioning scripts during enlistment. Custom scripts can be run when you explicitly choose to commission a machine.  A typical administrator workflow (with machine states), using customised commissioning scripts, can be represented as:

Add machine -&gt; Enlistment (runs built-in commissioning scripts MAAS) -&gt; New -&gt; Commission (runs built-in and custom commissioning scripts) -&gt; Ready -&gt; Deploy

NOTE: Scripts are run in alphabetical order in an ephemeral environment.  We recommend running your scripts after any MAAS built-in scripts.  This can be done by naming your scripts 99-z*.  It is possible to reboot the system during commissioning using a script, however, as the environment is ephemeral, any changes to the environment will be destroyed upon reboot (barring, of course, firmware type updates).

When a machine boots, MAAS first instructs it to run cloud-init to set up SSH keys (during commissioning only), set up NTP, and execute a script that runs other commissioning scripts.  Currently, the sequence of MAAS-provided commissioning scripts proceeds like this:

- **maas-support-info:** MAAS gathers information that helps to identify and characterise the machine for debugging purposes, such as the kernel, versioning of various components, etc.  **Runs in parallel with other scripts.**

- **maas-lshw:** this script pulls system BIOS and vendor info, and generates user-defined tags for later use.  **Runs in parallel with other scripts.**

- **20-maas-01-install-lldpd:** this script installs the link layer discovery protocol (LLDP) daemon, which will later capture networking information about the machine.  This script provides some extensive logging.

- **maas-list-modaliases:** this script figures out what hardware modules are loaded, providing a way to autorun certain scripts based on which modules are loaded.  **Runs in parallel with other scripts.**

- **20-maas-02-dhcp-unconfigured-ifaces:** MAAS will want to know all the ways the machine is connected to the network.  Only PXE comes online during boot; this script brings all the other networks online so they can be recognised.  This script provides extensive logging.

- **maas-get-fruid-api-data:** this script gathers information for the Facebook wedge power type.  **Runs in parallel with other scripts.**

- **maas-serial-ports:** this script lists what serial ports are available on the machine.  **Runs in parallel with other scripts.**

[tabs]
[tab version="v3.4 Snap,v3.4 Packages,v3.3 Snap,v3.3 Packages,v3.2 Snap,v3.2 Packages,v3.1 Snap,v3.1 Packages,v3.0 Snap,v3.0 Packages"]
[note]
As of MAAS version 3.0, **40-maas-01-network-interfaces** is no longer used by MAAS.
[/note]
[/tab]
[tab version="v2.9 Snap,v2.9 Packages"]
- **40-maas-01-network-interfaces:** this script is just used to get the IP address, which can then be associated with a VLAN/subnet.
[/tab]
[/tabs]

[tabs]
[tab version="v3.4 Snap,v3.4 Packages,v3.3 Snap,v3.3 Packages,v3.2 Snap,v3.2 Packages,v3.1 Snap,v3.1 Packages,v3.0 Snap,v3.0 Packages"]
- **50-maas-01-commissioning:** this script is the main MAAS tool, gathering information on machine resources, such as storage, network devices, CPU, RAM, details about attached USB and PCI devices, etc.  We currently pull this data using lxd: We use a Go binary built from lxd source that just contains the minimum source to gather the resource information we need.  This script also checks whether the machine being commissioning is a virtual machine, which may affect how MAAS interacts with it.
[/tab]
[tab version="v2.9 Snap,v2.9 Packages"]
- **50-maas-01-commissioning:** this script is the main MAAS tool, gathering information on machine resources, such as storage, network devices, CPU, RAM, etc.  We currently pull this data using lxd: We use a Go binary built from lxd source that just contains the minimum source to gather the resource information we need. This script also checks whether the machine being commissioning is a virtual machine, which may affect how MAAS interacts with it.
[/tab]
[/tabs]

- **maas-capture-lldp:** this script gathers LLDP network information to be presented on the logs page; this data is not used by MAAS at all.  **Runs in parallel with other scripts.**

- **maas-kernel-cmdline:** this script is used to update the boot devices; it double-checks that the right boot interface is selected.

Commissioning runs the same dozen or so scripts as enlistment, gathering all the same information, but with these caveats:

- Commissioning also runs user-supplied commissioning scripts, if present.  Be aware that these scripts run as root, so they can execute any system command.

- Commissioning runs test scripts which are not run during enlistment.

- Commissioning scripts can send BMC configuration data, and can be used to configure BMC data.

- The environment variable BMC_CONFIG_PATH is passed to serially run commissioning scripts; these scripts may write BMC power credentials to BMC_CONFIG_PATH in YAML format, where each key is a power parameter.  The first script to write BMC_CONFIG_PATH is the only script allowed to configure the BMC, allowing you to override MAAS' built-in BMC detection.  If the script returns 0, that value will be send to MAAS.

- All built-in commissioning scripts have been migrated into the database.

- `maas-run-remote-scripts` is capable of enlisting machines, so enlistment `user-data` scripts have been removed.

- The metadata endpoints `http://<MAAS>:5240/<latest or 2012-03-01>/` and `http://<MAAS>:5240/<latest or 2012-03-01>/meta-data/` are now available anonymously for use during enlistment.

In both enlistment and commissioning, MAAS uses either the MAC address or the UUID to identify machines.  Currently, because some machine types encountered by MAAS do **not** use unique MAC addresses, we are trending toward using the UUID.

[note]
To commission a node, it must have a status of "New".
[/note]

You have the option of setting some parameters to change how commissioning runs:

- `enable_ssh`: Optional integer. Controls whether to enable SSH for the commissioning environment using the user's SSH key(s). '1' == True, '0' == False. Roughly equivalent to the **Allow SSH access and prevent machine powering off** in the web UI.

- `skip_bmc_config`: Optional integer.  Controls whether to skip re-configuration of the BMC for IPMI based machines. '1' == True, '0' == False.

- `skip_networking`: Optional integer.  Controls whether to skip re-configuring the networking on the machine after the commissioning has completed. '1' == True, '0' == False. Roughly equivalent to **Retain network configuration** in the web UI.

- `skip_storage`: Optional integer.  Controls whether to skip re-configuring the storage on the machine after the commissioning has completed. '1' == True, '0' == False.  Roughly equivalent to **Retain storage configuration** in the web UI.

- `commissioning_scripts`: Optional string.  A comma separated list of commissioning script names and tags to be run. By default all custom commissioning scripts are run. Built-in commissioning scripts always run. Selecting `update_firmware` or `configure_hba` will run firmware updates or configure HBA's on matching machines.

- `testing_scripts`: Optional string.  A comma separated list of testing script names and tags to be run. By default all tests tagged `commissioning` will be run. Set to `none` to disable running tests.

- `parameters`: Optional string.  Scripts selected to run may define their own parameters. These parameters may be passed using the parameter name. Optionally a parameter may have the script name prepended to have that parameter only apply to that specific script.

* Commissioning scripts reference
This document provides technical details about commissioning scripts.  For the corresponding details about test scripts, see [Hardware test scripts reference](/t/hardware-test-scripts-reference/5392).

** Metadata field reference

Metadata fields tell MAAS when to use the script, how it should run, and what information it's gathering. A script can employ some combination of the following fields:

| Field | Description |
|:------|:------------|
| `name`| The name of the script.|
| `title`| Human-friendly descriptive version of the name, used within the web UI.|
| `description`| Brief outline of what the script does.|
| `tags`| List of tags associated with the script.|
| `type`| Either commissioning or testing.|
| `timeout`| Length of time before MAAS automatically fails and kills execution of the script. The time may be specified in seconds or using the HH:MM:SS format. |
| `destructive`| True or False, depending on whether the script will overwrite system data. You can't run destructive tests on a deployed machine. |
| `comment`| Describes changes made in this revision of the script.  A comment can be passed via the API when uploading the script.  MAAS doesn’t look at the script metadata for this field. |
| `hardware_type`| Defines the type of hardware the script configures or tests. If the script returns results, hardware types associate the results with specific hardware. The following types are valid: |
| | `node`: Not associated with any hardware type; this is the default. |
| | `cpu`: Configures or tests the CPUs on the machine. |
| | `memory`: Configures or tests memory on the machine. |
| | `storage`: Configures or tests storage on the machine. |
| | `network`: Configures or tests network on the machine. |
|   `parallel`| Enables scripts to be run in parallel and can be one of the following: |
| | `disabled`: The script will run serially on its own. |
| | `instance`: Runs in parallel only with other instances of the same script. |
| | `any`: Runs in parallel alongside any other scripts with parallel set to any. |
| `parameters`| What [parameters](#heading--parameters) the script accepts. |
| `results`| What [results](#heading--results) the script will return. |
| `packages`| List of packages to be installed or extracted before running the script. Packages must be specified as a dictionary. For example, `packages: {apt: stress-ng}`, would ask `apt` to install stress-ng. Package sources can be any of the following: |
| | `apt`: Use the Ubuntu apt repositories as configured by MAAS to install a package. |
| | `snap`: Installs packages using [snap][snapcraft]. May also be a list of dictionaries. The dictionary must define the name of the  package to be installed, and optionally, the `channel`, `mode` and `revision`.|
| | `url`: The archive will be downloaded and, if possible, extracted or installed when a Debian package or [snap][snapcraft]. |
|   `for_hardware`| Specifies the hardware that must be on the machine for the script to run. May be a single string or list of strings of the following:
| | `modalias`: Starts with 'modalias:' may optionally contain wild cards. |
| | `PCI ID`: Must be in the format of 'pci:VVVV:PPPP' where VVVV is the vendor ID, and PPPP is the product ID. |
| | `USB ID`: Must be in the format of 'usb:VVVV:PPPP' where VVVV is the vendor ID, and PPPP is the product ID. |
| | `System Vendor`: Starts with 'system_vendor:'. |
| | `System Product`: Starts with 'system_product:'. |
| | `System Version`: Starts with 'system_version:'. |
| | `Mainboard Vendor`: Starts with 'mainboard_vendor:'. |
| | `Mainboard Product`: Starts with 'mainboard_product:'. |
|   `may_reboot` |When True, indicates to MAAS that the script may reboot the machine. MAAS will allow up to 20 minutes between heartbeats while running a script with `may_reboot` set to True. |
| `recommission`| After all commissioning scripts have finished running rerun |
|   `script_type` | commissioning or test. Indicates whether the script should run during commissioning or hardware testing. |

** Parameter reference

Scripts can accept the following types of parameters:

1. storage
2. interface
3. URL

The values of these parameters are strictly checked against existing disks (storage), working interfaces (interface), and valid URLs (URL)  No other types of information can be passed as parameters; they are not configured to pass user-specified data.

Parameters are automatically switched by MAAS to match the device being tested, to allow one test to be run against multiple devices at the same time while keeping separate logs.  For this reason, you may only specify parameters within the embedded YAML of the script, and they must take the form of a dictionary of dictionaries.

The key of the dictionary must be a string, and it's this string that's used by the UI and API when users are setting parameter values during commissioning or testing.  The value is a dictionary with the following fields:

| Field | Description |
|:------|:------------|
|   `type`: Every parameter must contain a type field, which describes what the parameter may accept and its default values. It may be one of the following: |
| |   `storage`: Allows the selection of a storage device on the currently running machine.|
| |   `interface`: Allows the selection of an interface on the currently running machine.|
| |   `url`: Allows the the passing of a valid URL.|
| |   `runtime`: Specifies how long the script may run. This overrides the timeout value. It is currently only passed as the environment variable RUNTIME.|
|   `title`: The title of the parameter field when displayed in the UI. The following types have the following default values:|
| |   `storage`: storage device.|
| |   `interface`: interface specifier.|
| |   `url`: valid URL.|
|   `argument-format`: Specifies how the argument should be passed to the script. Input is described as `{input}`. The storage type may also use `{name}`, `{path}`, `{model}` or `{serial}`. MAAS will look up the values of path, model, and serial based on user selection. For storage, `{input}` is synonymous with `{path}`. The interface type may also use `{name}`, `{mac_address}`, `{product}`, or `{vendor}`. For interface `{input}` is synonymous with `{name}`. The following types have the following default values:|
| |   `storage`: `--storage={path}`|
| |   `interface`: `--interface={name}`|
| |   `url`: `--url={input}`|
|   `default`: The default value of the parameter. The following types have the following default values. Setting these to '' or None will override these values:|
| |   `storage`: all.|
| |   `interface`: all.|
|   `required`: Whether or not user input is required. A value of false sets no default, and no user input will mean no parameters passed to the script. Defaults to `true`.|
|   `results`: What results the script will return on completion. You can only define this parameter within the embedded YAML of the script. Results may be a list of strings or a dictionary of dictionaries.|

Example script using default values:

``` python
#!/usr/bin/env python3

# --- Start MAAS 1.0 script metadata ---
# name: example
# parallel: instance
# parameters:
#   storage: {type: storage}
# --- End MAAS 1.0 script metadata ---

import argparse

parser = argparse.ArgumentParser(description='')
parser.add_argument(
    '--storage', dest='storage', required=True,
    help='path to storage device you want to test. e.g. /dev/sda')
args = parser.parse_args()

print("Testing: %s" % args.storage)
```

Example script using customised parameters:

``` bash
#!/bin/bash

# --- Start MAAS 1.0 script metadata ---
# name: example
# parallel: instance
# parameters:
#   storage:
#     type: storage
#     argument-format: '{model}' '{serial}'
# --- End MAAS 1.0 script metadata ---

echo "Model: $1"
echo "Serial: $2"
```

** Environment variable reference

The following environment variables are available when a script runs within the MAAS environment:

1.   `OUTPUT_STDOUT_PATH`: The path to the log of STDOUT from the script.
2.   `OUTPUT_STDERR_PATH`: The path to the log of STDERR from the script.
3.   `OUTPUT_COMBINED_PATH`: The path to the log of the combined STDOUT and STDERR from the script.
4.   `RESULT_PATH`: Path for the script to write a result YAML.
5.   `DOWNLOAD_PATH`: The path where MAAS will download all files.
6.   `RUNTIME`: The amount of time the script has to run in seconds.
7.   `HAS_STARTED`: When 'True', MAAS has run the script once before but not to completion. Indicates the machine has rebooted.

** Commissioning script example: Configure HPA

Below is a sample script to configure an Intel C610/X99 HPA controller on an HP system. The script will only run on systems with an Intel C610/X99 controller identified by the PCI ID 8086:8d06.

Before the script runs, MAAS will download and install the [HP RESTful Interface Tool](https://downloads.linux.hpe.com/SDR/project/hprest/)`↗` package from HP. After the script completes, the built-in commissioning scripts will be re-run to capture the new configuration.

```nohighlight
#!/bin/bash -ex
# --- Start MAAS 1.0 script metadata ---
# name: hp_c610_x99_ahci
# title: Configure Intel C610/X99 controller on HP systems
# description: Configure Intel C610/X99 controller on HP systems to AHCI
# script_type: commissioning
# tags: configure_hpa
# packages:
#  url: http://downloads.linux.hpe.com/SDR/repo/hprest/pool/non-free/hprest-1.5-26_amd64.deb
# for_hardware: pci:8086:8d06
# recommission: True
# --- End MAAS 1.0 script metadata ---
output=$(sudo hprest get EmbeddedSata --selector HpBios.)
echo $output
if [ $(echo $output | grep -c 'EmbeddedSata=Raid') ]; then
    echo "Server is in Dynamic Smart Array RAID mode. Changing to SATA AHCI support mode."
    sudo hprest set EmbeddedSata=Ahci --selector HpBios. --commit
else:
    echo "No changes made to the system, Server is Already in AHCI Mode"
fi
```

** Commissioning script example: Update firmware

Below is a sample script to update the mainboard firmware on an ASUS P8P67 Pro using a vendor-provided tool. The tool is automatically downloaded and extracted by MAAS. The script reboots the system to complete the update. The system will boot back into the MAAS ephemeral environment to finish commissioning and (optionally) testing.

[note]
Currently, MAAS does not support vendor tools which use UEFI boot capsules or need to store resource files on disk while rebooting.
[/note]

```nohighlight
#!/bin/bash -ex
# --- Start MAAS 1.0 script metadata ---
# name: update_asus_p8p67_firmware
# title: Firmware update for the ASUS P8P67 mainboard
# description: Firmware update for the ASUS P8P67 mainboard
# script_type: commissioning
# tags: update_firmware
# packages:
#  url: http://example.com/firmware.tar.gz
# for_hardware: mainboard_product:P8P67 PRO
# may_reboot: True
# --- End MAAS 1.0 script metadata ---
$DOWNLOAD_PATH/update_firmware
reboot
```

* Controllers
Controllers are the backbone of MAAS.  Depending on your network configuration and your machine count, you may want to [adjust your controllers](/t/how-to-adjust-your-controllers/5172) a bit at this point.  You might even want to set up a [high availability](/t/how-to-enable-high-availability/5120) configuration.

Most of the functionality of MAAS is contained in a series of controllers.  There are two basic types: a region controller and one or more rack controllers. The region controller deals with operator requests, while the rack controller(s) provides high-bandwidth services to the individual machines.  In essence, the region controller interacts with the user, while the rack controllers manage the bare metal.

** About region controllers

A region controller consists of the following components:

- REST API server (TCP port 5240)
- PostgreSQL database
- DNS
- caching HTTP proxy
- web UI

Region controllers are responsible for either a data centre or a single region. Multiple fabrics are used by MAAS to accommodate subdivisions within a single region, such as multiple floors in a data centre.  A region controller typically manages activities like the following:

- initiates and offers RPC endpoints to rack controllers
- intercepting and routing API requests
- handling the OpenAPI interface
- getting image indexes, downloads, and attributes, via simplestreams
- handling Macaroon authentication
- interfacing with a running Prometheus instance
- authenticating users and managing any user-initiated settings changes
- handling certificates for VM hosts
- keeps DNS up to date (listens for DNS changes, marks DNS for updates, and restarts `bind9`)
- keeps the MAAS proxy up to date (listens for changes to the proxy, marks it for updates, and restarts the proxy)
- keeps RBAC updated with any changes
- manages access to secrets when Vault is in use

Region controllers perform many other functions, but these are the most conspicuous.

** About rack controllers

A rack controller provides four services:

- DHCP
- TFTP
- HTTP (for images)
- power management

A rack controller is attached to each "fabric". As the name implies, a typical setup is to have a rack controller in each data centre server rack. The rack controller will cache large items for performance, such as operating system install images, but maintains no independent state other than the credentials required to talk to the region controller.

****# Tell me about fabrics

A fabric is simply a way of linking [VLANs](/t/maas-glossary/5416#heading--vlans) (Virtual LANs) together.  If you're familiar with a VLAN, you know that it's designed to limit network traffic to specific ports (e.g., on a [switch](/t/maas-glossary/5416#heading--switch)) or by evaluating labels called "tags" (unrelated to MAAS tags).  By definition, this would mean that two VLANs can't communicate with each other -- it would defeat the purpose of the VLAN -- unless you implement some extraordinary measures.

For example, let's say that your [hospital](/t/how-to-get-started-with-maas/5092) has three key functions: Patient management, Accounting, and Facilities, each on their own VLAN.  Let's say that there are some situations in which you need to share data between all three of these functions.  To accomplish this, you can create a fabric that joins these three VLANS.  Since this fabric just makes it possible for these VLANs to communicate, you can manage the cross-VLAN access with additional software, or permissions, depending on your application software architecture.

You can learn more about fabrics in the [Concepts and terms](/t/maas-glossary/5416#heading--fabrics) section of this documentation.

** About controller communication

MAAS communication happens in a strict hierarchy, flowing from the UI/API through the region controller, to the rack controller, to the machines (and back).  While [high availability](/t/how-to-enable-high-availability/5120) (HA) may add controllers, it does not change the flow of communication through the MAAS system.  Understanding this message flow may help you with the machine topics which follow.

*** How machines communicate with the rack controller

All machine communication with MAAS is proxied through rack controllers, including HTTP metadata, DNS, syslog and APT (cache-and-forward proxies via Squid). 

MAAS creates an internal DNS domain, not manageable by the user, and a unique DNS resource for each subnet that is managed by MAAS. Each subnet includes all rack controllers that have an IP on that subnet. Booting machines use the subnet DNS resource to resolve the rack controller available for communication. If multiple rack controllers belong to the same subnet, MAAS uses a round-robin algorithm to balance the load across numerous rack controllers. This arrangement ensures that machines always have a rack controller.

Machines use this internal domain for HTTP metadata queries, APT (proxying via Squid), and Syslog. DNS queries, PXE booting, and NTP polls use IP addresses.

The rack controller installs and configures `bind` as a forwarder. All machines communicate via the rack controller directly.

[note]
Zone management and maintenance still happen within the region controller.
[/note]

<a href="https://discourse.maas.io/uploads/default/original/1X/02a7ca58b989c67c74421b9d5e0c8b32907a2de1.jpeg" target = "_blank"><img src="https://discourse.maas.io/uploads/default/original/1X/02a7ca58b989c67c74421b9d5e0c8b32907a2de1.jpeg"></a>

*** How region and rack controllers communicate

The MAAS region and rack controllers interact in a number of different ways, depending upon the operation you've requested.  Consider the process of commissioning a machine, that is, taking over the machine and gathering information on its available resources, including CPU, RAM, storage, and MIB information (obtainable via LLDP requests).  Here's a rough idea of what that sequence looks like -- a sequence that is representative of the communication between rack and region controllers:

1. An operator makes a request of MAAS, either via the Web UI or the API.  
2. MAAS translates this to an API request to the region controller.
3. The region controller locates the rack controller that has BMC access to the machine in question, that is, the rack controller that can power on that machine.
4. That same rack controller powers on the machine via IPMI request.
5. The rack controller tasked with providing DHCP handles assigning an IP address to the machine via the [DORA](/t/maas-glossary/5416#heading--dhcp) sequence (Discover, Offer, Request, Acknowledge).  **Note** that this rack controller doesn't have to be the same one that powers on the machine.
6. The DHCP-managing rack controller inserts itself as the DHCP "next-server" and requests a network boot.
7. (Still) the same rack controller RPCs the region controller to get what's needed to boot an ephemeral Ubuntu kernel, namely the kernel, any kernel parameters, an initrd daemon, and a squashfs load.
8. That same rack controller transforms the RPC response from the region controller into a valid PXE config and tells the machine to come get its files.
9. The booting machine loads the kernel and initrd, boots with that initrd, and then loads the squashfs, eventually making its way up to an ephemeral Ubuntu instance.
10. The booted machine pulls cloud-init metadata from the region controller, proxying through the rackd.
11. cloud-init uses this metadata to gather resource information about the machine and pass it back to the region controller, again proxied by the rackd.
12. The region controller (regiond or "region daemon") stores this machine information in a postgres database that is accessible only to the regiond, making MAAS truly stateless with respect to machines.

Again, this list doesn't represent every interaction between the controllers and machines, but it gives you a good idea of how MAAS works.

<details><summary>Tell me about the DHCP "next-server" statement</summary>

The `next-server` directive is used to specify the host address from which an initial boot file is to be loaded, usually a TFTP server.  In the case of MAAS, the rack controller providing DHCP actually inserts itself, since it can proxy (broker) the delivery of boot bits to the machine in question.
</details>

* Create a custom image
When we talk about creating custom OS images for MAAS, it feels like something that's about to get really complex.  But with packer-maas, that's often not the case.

Let's see if we can't create and deploy a custom Ubuntu image with packer, just to see how easy it can be.

** First, install MAAS

If we're going to create custom images for MAAS, then first, we'll need MAAS!  You can use [this tutorial](https://maas.io/docs/try-out-the-maas-cli)`↗` to accomplish that, and get a feel for MAAS while you're at it.

** Install packer

The next step is to install the tool `packer`, which will do the heavy lifting for us.  Packer is easily installed from its Debian package:

```nohighlight
sudo apt install packer
```

This should install with no additional prompts.

** Install dependencies

We're going to create a "custom" Ubuntu image for this build, so we'll want to install a few dependencies.  Enter the following commands.  Don't worry about any output between commands:

```nohighlight
sudo apt install qemu-utils
sudo apt install qemu-system
sudo apt install ovmf
sudo apt install cloud-image-utils
```

** Get the packer templates

Packer uses "templates", which are very much like scripts that build your custom image.  You can obtain the packer templates by cloning the [packer-maas github repository](https://github.com/canonical/packer-maas.git)`↗`, like this:

```nohighlight
cd ~
mkdir -p tmp/git
cd tmp/git
git clone https://github.com/canonical/packer-maas.git
```

Make sure to pay attention to where the repository is cloned, in this case, `tmp/git` in your home directory.  The packer template in this cloned repository creates a Ubuntu AMD64 image for use with MAAS.

** Build an Ubuntu image

Now that we have that, let's build a custom Ubuntu image that we can deploy with MAAS.  Use these commands to do that:

```nohighlight
cd ~/tmp/git/packer-maas/ubuntu
make custom-ubuntu-lvm.dd.gz
```

This `make` will run for a couple of minutes before attempting to boot the image.  While waiting for the image to boot, you will see terminal messages similar to this one for upwards of three to five minutes:

```nohighlight
2022/05/09 15:50:46 packer-builder-qemu plugin: [DEBUG] handshaking with SSH
2022/05/09 15:50:50 packer-builder-qemu plugin: [DEBUG] SSH handshake err: ssh: handshake failed: ssh: unable to authenticate, attempted methods [none password], no supported methods remain
2022/05/09 15:50:50 packer-builder-qemu plugin: [DEBUG] Detected authentication error. Increasing handshake attempts.
```

That's expected.  Eventually, you should see a successful SSH connection:

```nohighlight
2022/05/09 15:50:57 packer-builder-qemu plugin: [INFO] Attempting SSH connection to 127.0.0.1:2351...
2022/05/09 15:50:57 packer-builder-qemu plugin: [DEBUG] reconnecting to TCP connection for SSH
2022/05/09 15:50:57 packer-builder-qemu plugin: [DEBUG] handshaking with SSH
2022/05/09 15:51:17 packer-builder-qemu plugin: [DEBUG] handshake complete!
```

After this, a few more commands will run.  Eventually the terminal screen will clear and show just one line, as follows:

```nohighlight
rm OVMF_VARS.fd
```

That means you've successfully built the image!  Just to prove it to ourselves, let's take a couple of additional steps.

** Validate the build

You can check the validity of the operation with a simple `ls` command in `~/tmp/git/packer-maas/ubuntu` (where you ran the `make` command), like this:

```nohighlight
stormrider@neuromancer:~/mnt/Dropbox/src/git/packer-maas/ubuntu$ ls
custom-ubuntu-lvm.dd.gz  packages      seeds-lvm.iso     user-data-lvm
http                     packer_cache  ubuntu-flat.json
Makefile                 README.md     ubuntu-lvm.json
meta-data                scripts       user-data-flat
```

See the `custom-ubuntu-lvm.dd.gz` file?  That's our image, ready to try out.

** Upload the image to MAAS

You can upload your newly-created image with the following command:

```nohighlight
$ maas admin boot-resources create \
    name='custom/ubuntu-raw' \
    title='Ubuntu Custom RAW' \
    architecture='amd64/generic' \
    filetype='ddgz' \
    content@=custom-ubuntu-lvm.dd.gz
```

** Deploy the image in MAAS

What good is an image if we can't deploy it?  Pick one of the VMs you created in the [last tutorial](/t/try-out-the-maas-cli/5236) and deploy your new OS image to it.  Then use a command like this one to see it running:

```nohighlight
maas admin machines read | jq -r '(["HOSTNAME","SYSID","POWER","STATUS",
"OWNER", "OS", "DISTRO"] | (., map(length*"-"))),
(.[] | [.hostname, .system_id, .power_state, .status_name, .owner // "-",
.osystem, .distro_series]) | @tsv' | column -t

HOSTNAME     SYSID   POWER  STATUS    OWNER  OS      DISTRO
--------     -----   -----  ------    -----  --      ------
valued-moth  e86c7h  on     Deployed  admin  ubuntu  focal
open-gannet  nk7x8y  on     Deployed  admin  custom  ubuntu-raw
```

It's the machine named `open-gannet` in the listing above, but your machine name and $SYSID will be unique to your instance.

** That's all there is to it!

In a few simple steps, you've used packer to create a custom Ubuntu image, upload it to a running MAAS, and deploy it.  There are many different custom images that can be deployed with MAAS -- check [this guide](/t/how-to-build-custom-images/5104) to learn more.

* Custom images

MAAS is much more useful when you can upload images that aren't gathered from [the MAAS image repository](http://images.maas.io/)`↗`, deploy them to MAAS-managed machines, and count on them to work properly. But there's a problem: the typical, off-the-shelf ISO image can't just be uploaded to MAAS and deployed to a machine.  For one thing, the machines couldn't write the image to their disks or boot the images once they're there.  For another, any non-standard configuration items (networking, storage, users, added software) wouldn't be loaded.

We can help guide you in preparing ISO images to run on MAAS machines. Usable MAAS images need both a `curtin` hook script (to write and boot the image), and some `cloud-init` meta-data (to configure the image beyond the out-of-the-box experience).  As long as a prepared image meets these requirements, you can successfully upload it to MAAS, deploy it to a machine, and expect it to run properly on that machine.

This article explains a little more about how MAAS images differ from a standard ISO, and what has to happen to make those off-the-shelf ISOs deployable and usable by MAAS.

** About transforming an ISO

When it comes to creating images for MAAS machines, you can hand-build images, as long as they meet the `curtin` and `cloud-init` requirements; or you can use a third-party tool called  [packer](https://www.packer.io)`↗` to prepare special versions of these images that will work with MAAS.  There are also static Ubuntu images targeted at older MAAS versions (<3.1).  Beyond providing a bit of technical detail here, we won't shepherd you through hand-building images: you're pretty much on your own there.  We will try to help you understand how to create and customise MAAS-friendly images, mostly focusing on packer templates.

We maintain a [git repo](https://github.com/canonical/packer-maas)`↗` of packer templates for a few popular operating systems.  You can check out this graphic of a real, running lab MAAS instance to get an idea:

<a href="https://discourse.maas.io/uploads/default/original/2X/a/a80ed5eb191a798d049cb82fade4ee117f5128fd.png" target = "_blank"><img src="https://discourse.maas.io/uploads/default/original/2X/a/a80ed5eb191a798d049cb82fade4ee117f5128fd.png"></a>

Packer uses templates (built in HCL2) to run different build, provisioning, and post-processing tools that produce an image MAAS can deploy -- one that you can successfully access and use. These tools might be as simple as a shell command, or as specialised as the RedHat `anaconda` installer.  It really just depends on what's needed to prepare an image so that MAAS can deploy it.

We encourage and document custom images -- and help informally as much as we can -- but we're really not able to offer much support.  After all, other OS images are built from code we don't own, and licensed in ways that may or may not be compatible with a MAAS deployment.  For those reasons, among others, we recommend you customise machines using `cloud-init` user_data and/or `curtin` preseed data, whenever you can, instead of creating a custom image.

[note]
That warning bears repeating: While it may be possible to deploy a certain image with MAAS, the particular use case may not be supported by that image’s vendor due to licensing or technical reasons. Canonical recommends that, whenever possible, you should customise machines using `cloud-init` user_data or `curtin` preseed data, instead of creating a custom image.
[/note]

There are two types of custom images we'll explain here: static Ubuntu images (just below) and [packer images](#heading--about-packer).

** About static Ubuntu images

MAAS provides the capability for you to build a static Ubuntu OS image to deploy with MAAS, using any image-building method you choose.  You can create the image once, with a fixed configuration, and deploy it to many machines.  This fixed configuration can consist of anything that a normal image would contain: users, packages, etc.  This capability is really targeted at older versions of MAAS, but it should work with MAAS of any vintage.

There are five things that we should explain about static Ubuntu images: 

- [About uploading hand-built Ubuntu images](#heading--about-uploading-hand-built-ubuntu-images)
- [How MAAS handles static Ubuntu images](#heading--about-how-maas-handles-these-images)
- [How MAAS boots static Ubuntu images](#heading--about-how-maas-boots-these-images)
- [About configuring deployed machine networking](#heading--about-configuring-deployed-machine-networking)
- [About configuring deployed machine storage](#heading--about-configuring-deployed-machine-storage)
- [About static image metrics](#heading--about-static-image-metrics)

If you're using newer versions of MAAS (>3.0), we recommend choosing packer, since the packer-maas repository already has a built-in Ubuntu image you can customise -- but the choice is yours.

*** About uploading hand-built Ubuntu images

You can upload hand-built Ubuntu images, containing a kernel, bootloader, and a fixed configuration, for deployment to multiple machines.  The image can be built via a tool, such as [packer](https://www.packer.io)`↗`, or build with scripts. You can upload these images to the boot-resources endpoint, where it will then be available for deployment to machines.

At a minimum, this image must contain a kernel, a bootloader, and a `/curtin/curtin-hooks` script that configures the network. A sample can be found in the [packer-maas repos](https://github.com/canonical/packer-maas/tree/master/ubuntu/scripts)`↗`. The image must be in raw img file format, since that is the format MAAS accepts for upload.  This is the most portable format, and the format most builders support. Upon completing the image build, you will upload this img file to the boot-resources endpoint, specifying the architecture for the image.

*** How MAAS handles static Ubuntu images

MAAS will save the image -- in the same way it would save a `tar.gz` file -- in the database.  MAAS can differentiate between custom Ubuntu images and custom non-Ubuntu images, generating appropriate pre-seed configurations for each image type.

MAAS will also recognise the base Ubuntu version, so it can apply the correct ephemeral OS version for installation.  Custom images are always deployed with the ephemeral operating system. The base_image field is used to select the appropriate version of the ephemeral OS to avoid errors. This ensures a smooth deployment later.

*** How MAAS boots static Ubuntu images

When you decide to deploy a machine with your uploaded, custom image, MAAS ensures that the machine receives the kernel, bootloader and root file system provided in the image. The initial boot loader takes over, and boots an ephemeral OS of the same Ubuntu version as the custom image, to reduce the chances of incompatibilities.  Curtin then writes your entire custom image to disk.  Once the custom image is written to disk, it is not modified by MAAS.

Note that custom non-Ubuntu images still use a standard Ubuntu ephemeral OS to boot, prior to installing the non-Ubuntu OS.

*** About configuring deployed machine networking

If you deploy a machine with a custom Ubuntu image, MAAS allows you to configure the deployed machine's networks just like you would for any other MAAS machine.  If you create an interface and assign it to a subnet or static address, this will be reflected in the deployed machine.

For this reason, MAAS also does some initial diagnostics while installing the custom image.  MAAS will detect when a network configuration is not present and abort the installation with a warning.  Essentially, MAAS checks to be sure that `cloud-init` and `netplan` are present in the images written by `curtin`.  If not, MAAS won't deploy the machine with the image.

*** About configuring deployed machine storage

If you deploy a machine with a custom Ubuntu image, you will also want to be able to configure storage, just like you would do with any other machine.  MAAS facilitates changes to the storage configuration.  You can resize the root partition, as well as attaching and formatting any additional block devices you may desire.

*** About static image metrics

As a user, you want to keep track of how many static images are being used, and how many deployed machines are using static images.  The standard MAAS dashboard reflects both of these metrics.

** About packer

- [About packer dependencies](#heading--about-packer-dependencies)
- [About packer templates](#heading--about-packer-templates)
- [About the image installation process](#heading--about-the-image-installation-process)
- [About packer-created images](#heading--about-packer-created-images)

The [packer documentation](https://www.packer.io/docs)`↗` has an excellent, in-depth discussion of what packer does, how it works, and where it is limited.  Simply put, packer creates OS images that can be uploaded and deployed using MAAS. We can summarise packer with the following linearised flowchart:

<a href="https://discourse.maas.io/uploads/default/original/2X/4/47cb177f4ee2f52ac00c877449770a23cfa0c9b4.jpeg" target = "_blank"><img src="https://discourse.maas.io/uploads/default/original/2X/4/47cb177f4ee2f52ac00c877449770a23cfa0c9b4.jpeg"></a>

We can walk through packer operation like this:

 - A template is created or obtained which drives the packer build.  The [packer-maas](https://github.com/canonical/packer-maas)`↗` repository uses HCL2 templates.

 - The template specifies packer commands and data sources.

 - The template specifies a builder, which creates the MAAS-consumable images.

 - Multiple builds can run in parallel.  Within the MAAS domain, we typically don't set templates up that way, but it is possible to do so.

 - Provisioners spin up a running version of the image and add things that make it usable, like `curtin` hooks, `cloud-init` meta-data to install custom packages, and so on.

 - Post-processors do things to the built image to make it usable, e.g., compressing the file into a `tar.gz` image.

 - Because packer creates a wide-range of load packages, the results are called "artefacts" in packer terminology.  MAAS simply refers to these as "images".

Note that we said this flow is linearised.  You can see that provisioners might need to run before a post-processor creates an uploadable `tar.gz` image.  The actual flow depends on the template, which depends on the OS being customised into an image.  In the parlance of packer, all of these components -- builders, post-processors, provisioners -- are sometimes referred to collectively as "plugins".
    
** About packer dependencies

Depending upon which image you are building, packer-maas may require various dependencies.  For example, when customising an Ubuntu image, you'd need to install the following dependencies:

 - qemu-utils
 - qemu-system
 - ovmf
 - cloud-image-utils

These dependencies -- and the functionality they provide -- will be explained in the specific image sections which follow.

** About packer templates

A [packer template](https://www.packer.io/docs/templates)`↗` could just as easily be called a "packer script".  It contains declarations and commands that sequence and configure plugins.  Templates also have built-in functions to help you customise your artefacts. Our packer-maas templates are implemented in HCL2.

Templates are run by the packer `build` command.  Within packer-maas, packer commands (like `build`) are collected into makefiles that prevent you from having to know a lot about how packer works.  Even so, it's beneficial to take a quick tour of how a typical packer template works.  Let's use the [ubuntu-cloudimg](https://github.com/canonical/packer-maas/blob/master/ubuntu/ubuntu-cloudimg.pkr.hcl)`↗` template as a simple example.

[note]
Building workable templates can be extremely difficult. This section is intended to familiarise you with templates and their components so that you can possibly pinpoint bugs in community-provided templates.  If you want to build your own template, you should rely on the [packer documentation](https://www.packer.io/docs)`↗` as your guide.
[/note]

This template builds a customised Ubuntu image with packer:

```nohighlight
packer {
  required_version = ">= 1.7.0"
  required_plugins {
    qemu = {
      version = "~> 1.0"
      source  = "github.com/hashicorp/qemu"
    }
  }
}

variable "ubuntu_series" {
  type        = string
  default     = "focal"
  description = "The codename of the Ubuntu series to build."
}

variable "filename" {
  type        = string
  default     = "custom-cloudimg.tar.gz"
  description = "The filename of the tarball to produce"
}

variable "kernel" {
  type        = string
  default     = ""
  description = "The package name of the kernel to install. May include version string, e.g linux-image-generic-hwe-22.04=5.15.0.41.43"
}

variable "customize_script" {
  type        = string
  description = "The filename of the script that will run in the VM to customize the image."
}

variable "architecture" {
  type        = string
  default     = "amd64"
  description = "The architecture to build the image for (amd64 or arm64)"
}

variable "headless" {
  type        = bool
  default     = true
  description = "Whether VNC viewer should not be launched."
}

variable "http_directory" {
  type        = string
  default     = "http"
  description = "Directory for files to be accessed over http in the VM."
}

variable "http_proxy" {
  type        = string
  default     = "${env("http_proxy")}"
  description = "HTTP proxy to use when customizing the image inside the VM. The http_proxy enviroment is set, and apt is configured to use the http proxy"
}

variable "https_proxy" {
  type        = string
  default     = "${env("https_proxy")}"
  description = "HTTPS proxy to use when customizing the image inside the VM. The https_proxy enviroment is set, and apt is configured to use the https proxy"
}

variable "no_proxy" {
  type        = string
  default     = "${env("no_proxy")}"
  description = "NO_PROXY environment to use when customizing the image inside the VM."
}

variable "ssh_password" {
  type        = string
  default     = "ubuntu"
  description = "SSH password to use to connect to the VM to customize the image. Needs to match the hashed password in user-data-cloudimg."
}

variable "ssh_username" {
  type        = string
  default     = "root"
  description = "SSH user to use to connect to the VM to customize the image. Needs to match the user in user-data-cloudimg."
}

locals {
  qemu_arch = {
    "amd64" = "x86_64"
    "arm64" = "aarch64"
  }
  uefi_imp = {
    "amd64" = "OVMF"
    "arm64" = "AAVMF"
  }
  qemu_machine = {
    "amd64" = "ubuntu,accel=kvm"
    "arm64" = "virt"
  }
  qemu_cpu = {
    "amd64" = "host"
    "arm64" = "cortex-a57"
  }

  proxy_env = [
    "http_proxy=${var.http_proxy}",
    "https_proxy=${var.https_proxy}",
    "no_proxy=${var.https_proxy}",
  ]
}


source "qemu" "cloudimg" {
  boot_wait      = "2s"
  cpus           = 2
  disk_image     = true
  disk_size      = "4G"
  format         = "qcow2"
  headless       = var.headless
  http_directory = var.http_directory
  iso_checksum   = "file:https://cloud-images.ubuntu.com/${var.ubuntu_series}/current/SHA256SUMS"
  iso_url        = "https://cloud-images.ubuntu.com/${var.ubuntu_series}/current/${var.ubuntu_series}-server-cloudimg-${var.architecture}.img"
  memory         = 2048
  qemu_binary    = "qemu-system-${lookup(local.qemu_arch, var.architecture, "")}"
  qemu_img_args {
    create = ["-F", "qcow2"]
  }
  qemuargs = [
    ["-machine", "${lookup(local.qemu_machine, var.architecture, "")}"],
    ["-cpu", "${lookup(local.qemu_cpu, var.architecture, "")}"],
    ["-device", "virtio-gpu-pci"],
    ["-drive", "if=pflash,format=raw,id=ovmf_code,readonly=on,file=/usr/share/${lookup(local.uefi_imp, var.architecture, "")}/${lookup(local.uefi_imp, var.architecture, "")}_CODE.fd"],
    ["-drive", "if=pflash,format=raw,id=ovmf_vars,readonly=on,file=/usr/share/${lookup(local.uefi_imp, var.architecture, "")}/${lookup(local.uefi_imp, var.architecture, "")}_VARS.fd"],
    ["-drive", "file=output-qemu/packer-qemu,format=qcow2"],
    ["-drive", "file=seeds-cloudimg.iso,format=raw"]
  ]
  shutdown_command       = "sudo -S shutdown -P now"
  ssh_handshake_attempts = 500
  ssh_password           = var.ssh_password
  ssh_timeout            = "45m"
  ssh_username           = var.ssh_username
  ssh_wait_timeout       = "45m"
  use_backing_file       = true
}

build {
  sources = ["source.qemu.cloudimg"]

  provisioner "shell" {
    environment_vars = concat(local.proxy_env, ["DEBIAN_FRONTEND=noninteractive"])
    scripts          = ["${path.root}/scripts/cloudimg/setup-boot.sh"]
  }


  provisioner "shell" {
    environment_vars  = concat(local.proxy_env, ["DEBIAN_FRONTEND=noninteractive"])
    expect_disconnect = true
    scripts           = [var.customize_script]
  }

  provisioner "shell" {
    environment_vars = [
      "CLOUDIMG_CUSTOM_KERNEL=${var.kernel}",
      "DEBIAN_FRONTEND=noninteractive"
    ]
    scripts = ["${path.root}/scripts/cloudimg/install-custom-kernel.sh"]
  }

  provisioner "file" {
    destination = "/tmp/"
    sources     = ["${path.root}/scripts/cloudimg/curtin-hooks"]
  }

  provisioner "shell" {
    environment_vars = ["CLOUDIMG_CUSTOM_KERNEL=${var.kernel}"]
    scripts          = ["${path.root}/scripts/cloudimg/setup-curtin.sh"]
  }

  provisioner "shell" {
    environment_vars = ["DEBIAN_FRONTEND=noninteractive"]
    scripts          = ["${path.root}/scripts/cloudimg/cleanup.sh"]
  }

  post-processor "shell-local" {
    inline = [
      "IMG_FMT=qcow2",
      "source ../scripts/setup-nbd",
      "OUTPUT=$${OUTPUT:-${var.filename}}",
      "source ./scripts/cloudimg/tar-rootfs"
    ]
    inline_shebang = "/bin/bash -e"
  }
}
```

You can see that the sections match the typical structure of a `packer` HCL2 template: declarations (variables); a source declaration; and build tools.  We can deconstruct these briefly to understand what the template is doing.  This will help explain the image creation process.

*** Variables (declaration section)

The variables section of this template looks like this:

```nohighlight
variable "ubuntu_series" {
  type        = string
  default     = "focal"
  description = "The codename of the Ubuntu series to build."
}

variable "filename" {
  type        = string
  default     = "custom-cloudimg.tar.gz"
  description = "The filename of the tarball to produce"
}

variable "kernel" {
  type        = string
  default     = ""
  description = "The package name of the kernel to install. May include version string, e.g linux-image-generic-hwe-22.04=5.15.0.41.43"
}

variable "customize_script" {
  type        = string
  description = "The filename of the script that will run in the VM to customize the image."
}

variable "architecture" {
  type        = string
  default     = "amd64"
  description = "The architecture to build the image for (amd64 or arm64)"
}

variable "headless" {
  type        = bool
  default     = true
  description = "Whether VNC viewer should not be launched."
}

variable "http_directory" {
  type        = string
  default     = "http"
  description = "Directory for files to be accessed over http in the VM."
}

variable "http_proxy" {
  type        = string
  default     = "${env("http_proxy")}"
  description = "HTTP proxy to use when customizing the image inside the VM. The http_proxy enviroment is set, and apt is configured to use the http proxy"
}

variable "https_proxy" {
  type        = string
  default     = "${env("https_proxy")}"
  description = "HTTPS proxy to use when customizing the image inside the VM. The https_proxy enviroment is set, and apt is configured to use the https proxy"
}

variable "no_proxy" {
  type        = string
  default     = "${env("no_proxy")}"
  description = "NO_PROXY environment to use when customizing the image inside the VM."
}

variable "ssh_password" {
  type        = string
  default     = "ubuntu"
  description = "SSH password to use to connect to the VM to customize the image. Needs to match the hashed password in user-data-cloudimg."
}

variable "ssh_username" {
  type        = string
  default     = "root"
  description = "SSH user to use to connect to the VM to customize the image. Needs to match the user in user-data-cloudimg."
}

locals {
  qemu_arch = {
    "amd64" = "x86_64"
    "arm64" = "aarch64"
  }
  uefi_imp = {
    "amd64" = "OVMF"
    "arm64" = "AAVMF"
  }
  qemu_machine = {
    "amd64" = "ubuntu,accel=kvm"
    "arm64" = "virt"
  }
  qemu_cpu = {
    "amd64" = "host"
    "arm64" = "cortex-a57"
  }

  proxy_env = [
    "http_proxy=${var.http_proxy}",
    "https_proxy=${var.https_proxy}",
    "no_proxy=${var.https_proxy}",
  ]
}
```

Most of this is straightforward.  We're going to use a base image of Ubuntu 20.04, keeping the HTTP files in directory `http` and making three possible proxy options available: HTTP, HTTPS, or no proxy.  The produced image will have an SSH username and password of "ubuntu".  It's that simple.

The really complicated "builders" section of the old JSON version is replaced by a "source" section that is much cleaner.  Here's the source section of this HCL2 template, with a few comments added for clarity:

```nohighlight
source "qemu" "cloudimg" {
  boot_wait      = "2s"
# SETS UP THE IMAGE FOR A TWO-CPU VIRTUAL/MACHINE:
  cpus           = 2
  disk_image     = true
# SETS UP THE IMAGE TO EXPECT A 4GB DISK:
  disk_size      = "4G"
  format         = "qcow2"
# WHETHER OR NOT THE IMAGE EXPECTS TO RUN HEADLESS, THAT IS, WITHOUT A CONSOLE:
  headless       = var.headless
# THE HTTP DIRECTORY WILL (HOPEFULLY) BE THE USER'S HTTP DIRECTORY:
  http_directory = var.http_directory
# THE CHECKSUM FOR THE ISO IMAGE WILL BE FOUND HERE:
  iso_checksum   = "file:https://cloud-images.ubuntu.com/${var.ubuntu_series}/current/SHA256SUMS"
# THE ISO IMAGE ITSELF WILL BE FOUND AT THIS URL:
  iso_url        = "https://cloud-images.ubuntu.com/${var.ubuntu_series}/current/${var.ubuntu_series}-server-cloudimg-${var.architecture}.img"
# THE IMAGE SHOULD EXPECT THIS MUCH MEMORY:
  memory         = 2048
  qemu_binary    = "qemu-system-${lookup(local.qemu_arch, var.architecture, "")}"
  qemu_img_args {
    create = ["-F", "qcow2"]
  }
# IF YOU STUDY THE QEMU DOCUMENTATION, IT'S FAIRLY EASY TO SEE WHAT THESE ARGS DO:
  qemuargs = [
    ["-machine", "${lookup(local.qemu_machine, var.architecture, "")}"],
    ["-cpu", "${lookup(local.qemu_cpu, var.architecture, "")}"],
    ["-device", "virtio-gpu-pci"],
    ["-drive", "if=pflash,format=raw,id=ovmf_code,readonly=on,file=/usr/share/${lookup(local.uefi_imp, var.architecture, "")}/${lookup(local.uefi_imp, var.architecture, "")}_CODE.fd"],
    ["-drive", "if=pflash,format=raw,id=ovmf_vars,readonly=on,file=/usr/share/${lookup(local.uefi_imp, var.architecture, "")}/${lookup(local.uefi_imp, var.architecture, "")}_VARS.fd"],
    ["-drive", "file=output-qemu/packer-qemu,format=qcow2"],
    ["-drive", "file=seeds-cloudimg.iso,format=raw"]
  ]
# HERE'S THE SHUTDOWN COMMAND TO USE:
  shutdown_command       = "sudo -S shutdown -P now"
# HERE'S HOW MANY TIMES YOU TRY SSH:
  ssh_handshake_attempts = 500
# HERE'S HOW YOU GATHER THE SSH PASSWORD:
  ssh_password           = var.ssh_password
# USE A REALLY LONG SSH WAIT TIMEOUT:
  ssh_timeout            = "45m"
# HERE'S HOW YOU GATHER THE SSH USERNAME:
  ssh_username           = var.ssh_username
# USE A REALLY LONG SSH TIMEOUT, TOO:
  ssh_wait_timeout       = "45m"
  use_backing_file       = true
}
```

The high number of SSH handshake attempts -- and the really long timeouts -- have to do with trying to catch the system after it has successfully booted.  Because of the way packer works, it has no direct way to be informed that the system has booted.  As a consequence, to finish the build and run provisioners and post-processors, packer has to keep trying for a while until an SSH connection is successful.  In practice, this should only take 2-3 minutes, but this template uses very long values, just to be sure.

*** Build section

The build section of this template lays out the tools that will build the packed image:

```nohighlight
build {
  sources = ["source.qemu.cloudimg"]

  provisioner "shell" {
    environment_vars = concat(local.proxy_env, ["DEBIAN_FRONTEND=noninteractive"])
    scripts          = ["${path.root}/scripts/cloudimg/setup-boot.sh"]
  }


  provisioner "shell" {
    environment_vars  = concat(local.proxy_env, ["DEBIAN_FRONTEND=noninteractive"])
    expect_disconnect = true
    scripts           = [var.customize_script]
  }

  provisioner "shell" {
    environment_vars = [
      "CLOUDIMG_CUSTOM_KERNEL=${var.kernel}",
      "DEBIAN_FRONTEND=noninteractive"
    ]
    scripts = ["${path.root}/scripts/cloudimg/install-custom-kernel.sh"]
  }

  provisioner "file" {
    destination = "/tmp/"
    sources     = ["${path.root}/scripts/cloudimg/curtin-hooks"]
  }

  provisioner "shell" {
    environment_vars = ["CLOUDIMG_CUSTOM_KERNEL=${var.kernel}"]
    scripts          = ["${path.root}/scripts/cloudimg/setup-curtin.sh"]
  }

  provisioner "shell" {
    environment_vars = ["DEBIAN_FRONTEND=noninteractive"]
    scripts          = ["${path.root}/scripts/cloudimg/cleanup.sh"]
  }

  post-processor "shell-local" {
    inline = [
      "IMG_FMT=qcow2",
      "source ../scripts/setup-nbd",
      "OUTPUT=$${OUTPUT:-${var.filename}}",
      "source ./scripts/cloudimg/tar-rootfs"
    ]
    inline_shebang = "/bin/bash -e"
  }
}
```

Rather than walking through each of these lines individually, we can just note that this HCL2 causes packer to:

 - retrieve scripts that set up the bootloader, configure curtin hooks, and install custom packages from a named gzip source.
 - set the homedir and proxy options for the image.
 - set up curtin, networking, and maybe storage for the image.
 - clean up the image prior to post-processing.

The post-processing section of this template prepares the image for use:

```nohighlight
  post-processor "shell-local" {
    inline = [
      "IMG_FMT=qcow2",
      "source ../scripts/setup-nbd",
      "OUTPUT=$${OUTPUT:-${var.filename}}",
      "source ./scripts/cloudimg/tar-rootfs"
    ]
    inline_shebang = "/bin/bash -e"
  }
```

You can see right away that this template has one post-processor (only one `post-processor` entry).  This post-processor is a local shell, invoked with the `-e` option, which causes the shell to terminate if there's an error (rather than continuing with the next command).  In this case, we can see that the shell runs four commands:

 - sets `$IMG_FMT` to "qcow2"
 - runs the script `setup-nbd`
 - sets $OUTPUT to "<name of image>-custom-cloudimg.tar.gz"
 - runs the script `tar-rootfs`

In this case, it's worth a quick look at the two scripts to see what this post-processor does. First, let's glance at `setup-nbd`:

```highlight
#!/bin/bash -e
#
# setup-nbd - Bind Packer qemu output to a free /dev/nbd device.
#
# Author: Lee Trager <lee.trager@canonical.com>
#
# Copyright (C) 2020 Canonical
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as
# published by the Free Software Foundation, either version 3 of the
# License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with this program.  If not, see <http://www.gnu.org/licenses/>.

if [ $UID -ne 0 ]; then
    echo "ERROR: Must be run as root!" >&2
    exit 1
fi

if [ ! -f output-qemu/packer-qemu ]; then
    echo "ERROR: Not in the same path as template!" >&2
    exit
fi

echo 'Loading nbd...'
shopt -s extglob
modprobe nbd
for nbd in /sys/class/block/nbd+([0-9]); do
    if [ "$(cat ${nbd}/size)" -eq 0 ]; then
	nbd="/dev/$(basename $nbd)"
	echo "Using $nbd"
	break
    fi
done

if [ -z "${nbd}" ] || ! echo $nbd | grep -q "/dev"; then
    echo "ERROR: Unable to find nbd device to mount image!" >&2
    exit 1
fi

echo "Binding image to $nbd..."
qemu-nbd -d $nbd
if [ -n "$IMG_FMT" ]; then
    qemu-nbd -c $nbd -f "$IMG_FMT" -n output-qemu/packer-qemu
else
    qemu-nbd -c $nbd -n output-qemu/packer-qemu
fi
echo 'Waiting for partitions to be created...'
tries=0
while [ ! -e "${nbd}p1" -a $tries -lt 60 ]; do
    sleep 1
    tries=$((tries+1))
done
```

As you can see, this is just a well-structured script to export a QEMU image as a Network Block Device, binding it to a `/dev/nbd` directory.  This is first step in creating MAAS-loadable Ubuntu image.  The second step comes in `tar-rootfs`:

```highlight
#!/bin/bash -e
#
# tar-rootfs - Create a tar.gz from a binded /dev/nbd device
#
# Author: Alexsander de Souza <alexsander.souza@canonical.com>
#
# Copyright (C) 2021 Canonical
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as
# published by the Free Software Foundation, either version 3 of the
# License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with this program.  If not, see <http://www.gnu.org/licenses/>.

cleanup() {
    qemu-nbd -d "$nbd"
    [ -d "${TMP_DIR}" ] && rm -rf "${TMP_DIR}"
}
trap cleanup EXIT

if [ ${UID} -ne 0 ]; then
    echo "ERROR: Must be run as root!" >&2
    exit 1
fi

TMP_DIR=$(mktemp -d /tmp/packer-maas-XXXX)

echo 'Mounting root partition...'
mount "${nbd}p2" "${TMP_DIR}"
mount "${nbd}p1" "${TMP_DIR}/boot/efi"

echo "Creating MAAS image $OUTPUT..."
tar -Sczpf "$OUTPUT" --acls --selinux --xattrs -C "${TMP_DIR}" .

echo 'Unmounting image...'
umount "${TMP_DIR}/boot/efi"
umount "${TMP_DIR}"
```

This script just creates a `.tar.gz` from a bound `/dev/nbd` device (where the QEMU image was initially stored by the last script.

As you can see, the process of creating a customised packer image is not overly complex.  Nevertheless, it's a difficult process to get right, hence our community-contributed templates.

** About the image installation process

Installing a packer-created image is highly dependent on the application.  In the case of MAAS, we use the CLI `boot-resources` command to upload the image to MAAS, something like this:

```nohighlight
$ maas admin boot-resources create \
    name='custom/ubuntu-tgz' \
    title='Ubuntu Custom TGZ' \
    architecture='amd64/generic' \
    filetype='tgz' \
    content@=custom-ubuntu.tar.gz
```

At this point, the image shows up in MAAS, synced to the controller, the same as any other image.

** About packer-created images

If you're more interested in the anatomy of a packer-created image, for example, an ISO image, you can use `isoinfo` to explore the image file.  The image should be found in the packer git repository, under `<imagename>/packer-cache`.  Ideally, it shouldn't differ too much from any other customised ISO image.  You can explore with a few of the `isoinfo` commands.  For example, you can read the primary volume descriptor like this:

```nohighlight
stormrider@neuromancer:~/mnt/Dropbox/src/git/packer-maas/ubuntu/packer_cache$ isoinfo -d -i ubuntu.iso | more                                                         
CD-ROM is in ISO 9660 format
System id: 
Volume id: Ubuntu-Server 20.04.4 LTS amd64
Volume set id: 
Publisher id: 
Data preparer id: XORRISO-1.2.4 2012.07.20.130001, LIBISOBURN-1.2.4, LIBISOFS-1.2.4, LIBBURN-1.2.4
Application id: 
Copyright File id: 
Abstract File id: 
Bibliographic File id: 
Volume set size is: 1
Volume set sequence number is: 1
Logical block size is: 2048
Volume size is: 650240
El Torito VD version 1 found, boot catalog is in sector 250
Joliet with UCS level 3 found
Rock Ridge signatures version 1 found
Eltorito validation header:
    Hid 1
    Arch 0 (x86)
    ID ''
    Key 55 AA
    Eltorito defaultboot header:
        Bootid 88 (bootable)
        Boot media 0 (No Emulation Boot)
        Load segment 0
        Sys type 0
        Nsect 4
        Bootoff 8EC04 584708
```

You could also generate an exhaustive directory listing with `isoinfo -f -i <isoname>`, and possibly pipe that through `grep` to ensure that your desired packages have been added to the image.  Or, if you prefer to sweep the image directories manually, you can use `isoinfo -l -i <isoname>`.  The larger point, of course, is that a packer-generated image is essentially identical to any prepared ISO image, including, of course, any customisations (e.g., extra software) that the template loads before finalising the image.

* Device labelling reference
** Tag definition reference examples

Here are some examples of tag definitions -- [more examples are available](https://github.com/canonical/mxt)`↗`.

Commonly used Xpath functions usually include:

- contains
- starts-with
- ends-with

**# Example 1

This definition will identify machines with proper CPU tags, cores, and RAM, and tag them as a hypervisor:

<a href="https://discourse.maas.io/uploads/default/original/2X/d/d1c8e2674445045ee9c8c9f1d14f3fa413af9be8.png" target = "_blank"><img src="https://discourse.maas.io/uploads/default/original/2X/d/d1c8e2674445045ee9c8c9f1d14f3fa413af9be8.png"></a>

This example looks for at least 40 cores and 256 GB of RAM and has all the required CPU features for being a hypervisor for both Intel and AMD.

[note]
Don't forget to adjust cores and RAM (in bytes) to suit your particular needs and available resources.
[/note]

You can also define this tag with the CLI:

```nohighlight
maas ${MAAS_PROFILE} tags create name=hypervisor \
definition='//node[@id="memory"]/size >= "274877906944" and \
//node[@id="cpu"]/configuration/setting/id="cores" >= 40 and \
//node[@id="cpu"]//capabilities/capability/@id = "vmx" or @id="svm" and \
//node[@id="cpu"]//capabilities/capability/@id = "aes" and 
//node[@id="cpu"]//capabilities/capability/@id = "flexpriority" and 
//node[@id="cpu"]//capabilities/capability/@id = "tpr_shadow" and 
//node[@id="cpu"]//capabilities/capability/@id = "ept" and 
//node[@id="cpu"]//capabilities/capability/@id = "vpid" and 
//node[@id="cpu"]//capabilities/capability/@id = "vnmi"'
```

**# Example 2

This will tag UEFI enabled KVM VMs running on AMD-based servers:

<a href="https://discourse.maas.io/uploads/default/original/2X/a/adde5f51e396a3a2d2f70daad7787fe087723664.png" target = "_blank"><img src="https://discourse.maas.io/uploads/default/original/2X/a/adde5f51e396a3a2d2f70daad7787fe087723664.png"></a>

You can also do this with the CLI:

```nohighlight
maas ${MAAS_PROFILE} tags create \
name=kvm-amd-uefi \
definition='//node[@class="system"]/vendor = "QEMU" and //node[@id="firmware"]/capabilities/capability/@id = "virtualmachine" and //node[@id="firmware"]/capabilities/capability/@id = "uefi" and //node[@class="processor"]/vendor[starts-with(.,"Advanced Micro Devices")]' \
kernel_opts='nomodeset console=tty0 console=ttyS0,115200n8 amd_iommu=on kvm-amd.nested=1 kvm-amd.enable_apicv=n kvm.ignore_msrs=1' \
comment='Tag for automatically identifying AMD-based KVM vms (UEFI BIOS)'
```

**# Example 3

This will automatically tag servers that have NVME controllers:

<a href="https://discourse.maas.io/uploads/default/original/2X/1/166cd775669610ba454b5f2883e7729b79770bd0.png" target = "_blank"><img src="https://discourse.maas.io/uploads/default/original/2X/1/166cd775669610ba454b5f2883e7729b79770bd0.png"></a>

To accomplish the same thing in the CLI:

```nohighlight
maas ${MAAS_PROFILE} tags create name=NVME comment="xpath tag for automatically tagging servers that have NVME controllers" definition='//node[@id="storage" and @class="storage"]/description = "Non-Volatile memory controller"'
```

**# Example 4

This will tag servers with Mellanox ConnectX-5 NICs:

<a href="https://discourse.maas.io/uploads/default/original/2X/3/34ed75cf40ded49ac5eb8d76467817b5618b11a9.png" target = "_blank"><img src="https://discourse.maas.io/uploads/default/original/2X/3/34ed75cf40ded49ac5eb8d76467817b5618b11a9.png"></a>

And you can also do this in the CLI:

```nohighlight
maas ${MAAS_PROFILE} tags create \
name=connectx-5 \
definition='//node[@class="network"]/vendor[starts-with(.,"Mellanox")] and //node[@class="network"]/product[contains(.,"ConnectX-5")]' \
comment='Tag for automatically identifying servers with Mellanox Technologies ConnectX-5 cards'
```

**# Example 5

This will enable GPU passthrough for Nvidia Quadro K series GPUs on AMD:

<a href="https://discourse.maas.io/uploads/default/original/2X/3/3f258d7e98c0adc7b605b8d2846b76737d46a27e.png" target = "_blank"><img src="https://discourse.maas.io/uploads/default/original/2X/3/3f258d7e98c0adc7b605b8d2846b76737d46a27e.png"></a>

You can also duplicate this example in the CLI:

```nohighlight
maas ${MAAS_PROFILE} tags create \
name=gpgpu-quadro-k-a \
comment="Enable passthrough for Nvidia Quadro K series GPUs on AMD" \
definition='//node[@id="cpu:0"]/capabilities/capability/@id = "svm" and //node[@id="display"]/vendor[contains(.,"NVIDIA")] and //node[@id="display"]/description[contains(.,"3D")] and //node[@id="display"]/product[contains(.,"Quadro K")]'
```

* Device labels
When you're working with a half-dozen machines, a lot of labelling isn't necessary.  As your constellation of machines grows, though, you reach a point where you can't keep a compact picture in your head.  This is where tags, annotations, and filtering come in.  

Two of the more useful attributes of machines are tags and annotations.  These can be used not only to identify machines, but to customise them (e.g., kernel options) when they are commissioned and deployed. You can [tag machines](/t/how-to-tag-machines/5928), [annotate machines](/t/how-to-annotate-machines/5929), and use tags to mark [machines](/t/how-to-use-machine-tags/5224), [storage](/t/how-to-use-storage-tags/5232), [controllers](/t/how-to-use-controller-tags/5216), and [network interfaces](/t/how-to-use-network-tags/5228).  

You can also use [filtering](/t/how-to-find-machines/5192) to find a given subset of machines.  There isn't a lot of theory, per se, on filtering, since it's just a utility function of the user interface.

The rest of this article contains a little background information on tags and annotations.

** About tags

Tags are short, descriptive, searchable words that can be applied to various MAAS objects, including:

- machines (physical and virtual)
- VM hosts
- controllers (rack and region)
- storage (virtual and physical; block devices or partitions)
- network interfaces
- devices
- nodes (in the CLI only)

Tags serve to help you identify, group, and find objects easily, especially when you routinely deploy hundreds of machines.


** About annotations

Annotations are descriptive, searchable phrases that apply only to machines.  There are two types of annotations: static (always present in any machine state), and dynamic (only present in allocated or deployed states).  Annotations help you identify, characterise, and inform others about your machines.


** About tags and scripts

As with general tag management, tags make scripts easier to manage; grouping scripts together for commissioning and testing, for example:

``` bash
maas $PROFILE node-script add-tag $SCRIPT_NAME tag=$TAG
maas $PROFILE node-script remove-tag $SCRIPT_NAME tag=$TAG
```

MAAS runs all commissioning scripts by default. However, you can select which custom scripts to run during commissioning by name or tag:

``` bash
maas $PROFILE machine commission \
commissioning_scripts=$SCRIPT_NAME,$SCRIPT_TAG
```

You can also select which testing scripts to run by name or tag:

``` bash
maas $PROFILE machine commission \
testing_scripts=$SCRIPT_NAME,$SCRIPT_TAG
```

Any testing scripts tagged with commissioning will also run during commissioning.

** About automatic tags

[tabs]
[tab version="v3.4 Snap,v3.4 Packages,v3.3 Snap,v3.3 Packages,v3.2 Snap,v3.2 Packages"]
Automatic tags are tags with a definition. The definition allows you to auto-apply tags to machines that match with an [XPath expression](#heading--xpath-expressions) you created. Setting up an automatic tag will help you recognise special hardware characteristics and settings. For instance, we can configure the gpu passthrough by creating an XPath expression that recognises a prospective GPU, as shown in the example below.  

In the MAAS REST API, a tag has 4 attributes namely, name, definition, kernel options, and a comment. When this tag is created, the MAAS REST API will try to match all machines with this definition and automatically apply the tag to those machines. Every time a new machine is discovered in your MAAS, if new machines match this definition, they will be automatically tagged as well.

** About XPath expressions

MAAS automatic tags accept XPath expressions in the definition attribute of the tag. XPath expressions are evaluated against `lshw`'s XML output; they are used to locate elements or attributes of the XML document for use in configuring automatic tags. You can use the lshw output in the hardware configuration details of a machine in MAAS and use that to create an XPath expression. 

** About node capabilities

Capabilities are used to report features of a given node. The exact meaning of each feature depends on the type of node. It can be the presence of an arithmetical co-processor for a CPU, the ability to run at 1GB/s for a network interface, etc. In most cases, capabilities reported by lshw are auto-documented.

To see the capabilities of a specific machine, you can download your HW configuration information, as described above.

** About Kernel options

You can add kernel options when creating both manual and automatic tags. Kernel options will be automatically applied at boot time or when the machine with that tag is commissioned or deployed. 

When updating kernel options on a tag that matches Deployed machines, be aware that the new kernel option will be applied during boot time, so you will need to release and re-deploy them to pick up the change. Otherwise, these deployed machines will have the old kernel options.

If there are multiple tags associated with a machine, the kernel options will be concatenated from all the tags combined, sorted alphabetically .
[/tab]
[tab version="v3.1 Snap,v3.1 Packages,v3.0 Snap,v3.0 Packages,v2.9 Snap,v2.9 Packages"]

[note]
Tag management UI is available starting in MAAS v3.2.
[/note]
[/tab]
[/tabs]




* DHCP and MAAS

The Dynamic Host Control Protocol or DHCP is a key part of how MAAS is able to manage bare-metal servers.  Many issues with MAAS revolve around misunderstanding -- or unintentional misuse -- of DHCP, so it's worth it to take an in-depth look.  This section will help you learn:

- [About DORA](#heading--about-dora)
- [About DHCP traffic](#heading--about-dhcp-traffic)
- [About DHCP standard message types](#heading--about-dhcp-standard-message-types)
- [About DHCP address allocation](#heading--about-dhcp-address-allocation)
- [About multiple DHCP servers serving different IP ranges](#heading--about-multiple-dhcp-servers-diff-ips)
- [About multiple DHCP servers serving overlapping IP ranges](#heading--about-multiple-dhcp-servers-overlapping-ranges)
- [About DHCP relays](#heading--about-dhcp-relays)

** About DORA

The DHCP negotiation process is known as DORA: Discover, Offer, Request, Acknowledge. Just like the exchange above, it’s carried out by the network equivalent of shouting, that is, broadcast exchanges. While the payloads for the messages carry unique addressing information (after the first DISCOVER message), the Destination IP (DIP) is always 255.255.255.255 — broadcast mode. Every machine on the network will see the packet, but they will ignore it when they look at the payload.

By the way, there are several other possible responses at various points in the exchange, such as NACK (from the DHCP server, when the client has waited too long to make the request) and DECLINE (e.g., when the IP address configuration offered isn’t usable by the client).

The network version looks something like this:

<a href="https://discourse.maas.io/uploads/default/original/2X/a/a8c1756c1f0a9309fa01f1f5ccc0573e33e436fa.jpeg" target="_blank"><img src="https://discourse.maas.io/uploads/default/original/2X/a/a8c1756c1f0a9309fa01f1f5ccc0573e33e436fa.jpeg"></a>

Message 1 ("DISCOVER") is the only one that carries no destination info in its payload.  The
entire exchange takes place in broadcasts, rather than addressed packets. Said differently, the DIP (Destination IP address) is always 255.255.255.255, the broadcast address. The exchange becomes semi-private with the first OFFER, via the use of message payloads.

[note]
The MAAS-provided DHCP server uses the `next-server` parameter to tell machines where to request their Network Bootstrap Program (NBP); `next-server` will be set to the IP address of a TFTP or HTTP boot server that can supply the NBP.  We recommend allowing MAAS to provide DHCP for your machines.
[/note]

** About DHCP traffic

In fact, it’s worthwhile to (very briefly) consider a DHCP packet exchange. Typically, it looks something like this:

``` nohighlight
# DHCP Discover
Ethernet Header: DA=FF-FF-FF-FF-FF-FF, SA=<client MAC>
IP Header: SIP=0.0.0.0, DIP=255.255.255.255
DHCP Payload: Client MAC=<client MAC>

# DHCP Offer
Ethernet Header: DA=FF-FF-FF-FF-FF-FF, SA=<DHCP server MAC>
IP Header: SIP=<DHCP server IP address>, DIP=255.255.255.255
DHCP Payload: Offered IP=<offered IP>, Client MAC=<client MAC>,
Subnet Mask=<Subnet mask>, Router IP=<router IP>, 
DNS=<DNS server1 IP, DNS server2 IP>, IP Lease Time=<time>s,
DHCP Server Identifier=<DHCP server IP address>

# DHCP Request
Ethernet Header: DA=FF-FF-FF-FF-FF-FF, SA=<client MAC>
IP Header: SIP=0.0.0.0, DIP=255.255.255.255
DHCP Payload: Client MAC=<client MAC>, 
Requested IP Address=<offered IP>, 
DHCP Server Identifier=<DHCP server IP address>

# DHCP Ack
Ethernet Header: DA=FF-FF-FF-FF-FF-FF, SA=<DHCP server MAC>
IP Header: SIP=<DHCP server IP address>, DIP=255.255.255.255
DHCP Payload: Offered IP=<offered IP>, Client MAC=<client MAC>,
Subnet Mask=<Subnet mask>, Router IP=<router IP>, 
DNS=<DNS server1 IP, DNS server2 IP>, IP Lease Time=<time>s,
DHCP Server Identifier=<DHCP server IP address>
```

A couple of clarifications can come in handy here:

- The DHCP server provides all the needed configuration parameters in both the OFFER and the ACK messages. Nothing is left out: the client receives an IP address, a subnet mask, a router location, the IPs of both DNS servers, the lease time limit, and the identity of the DHCP server. Nothing else is necessary for the requesting client to begin operating independently on the network.

- The requesting client does not start using the offered IP address until it has received the ACK message. Note that in the DHCP Request, above, the SIP (Source IP) is still 0.0.0.0, even though the client knows the offered IP (it sends it back in the request to confirm).

With that very basic view of DHCP, we can now talk about how multiple DHCP servers and multiple requesting clients can keep things straight.

** About DHCP standard message types

DHCP is, technically, a network management protocol. In other words, it’s part of a collection of hardware and software tools that help to manage network traffic. DHCP is designed to automatically assign IP addresses and other communication parameters, such as default gateway, domain name, name server IPs, or time server IPs to clients. 

There are (at least) two participants in a DHCP transaction: a server and a client, but the client has to meet some requirements to participate. Specifically, the client has to implement an instance of the DHCP protocol stack; without that, it has no idea how to formulate Discovery and Request packets, nor can it recognise Offers or Acknowledgements (or NAKs, for that matter).

For what it’s worth, the “DHCP protocol stack” just means that a device can handle at least the following standard message types:

- **DHCPDiscover**: a broadcast message sent in the hopes of finding a DHCP server.  Note that clients that don’t get a DHCP response may be able to assign themselves an Automatic Private IPv4 address (APIPA), which should always be in the range 169.254.0.0/16. This is good to know, because you want to pretty much always leave that scope (that range of IP addresses) unused by anything else in your system.

- **DHCPOffer**: also a broadcast message, one that offers an IPv4 address lease; the lease is more than just an IP address, as we saw in the last DHCP blog.

- **DHCPRequest**: If you haven’t noticed by now, DHCP exchanges are little like rolling snowballs: they pick up more protocol information as they go and keep it for the duration of the transaction, sending it back and forth. In this case, the client sends back everything the DHCP server sent, along with a request to actually take the offered lease.

- **DHCPAcknowlegement**: If everything matches up when the DHCP server gets the Request, it responds with an Acknowledgement, which basically says, “Okay, you can lease this IP address for a set period of time.”

- **DHCPNak**: If the client waits too long to Request an Offer (generally, if a different server has already claimed the offered IP address), the DHCP server may respond with a Nak. This requires the client to start over again at Discover.

- **DHCPDecline**: If the client determines that, for some reason, the Offer has a configuration that won’t work for it, it can Decline the offer — that this also means it has to start again at Discover.

- **DHCPRelease**: When a client is done with an IP address, it can send a Release message to cancel the rest of the lease and return the IP address to the server’s available pool.

- **DHCPInform**: This is a relatively new message, which allows a client that already has an IP address to easily get other configuration parameters (related to that IP address) from a DHCP server.

Note that, shortly before a lease expires, most DHCP clients will renew the lease, often with a shortened form of the exchange (Request/Acknowledge) which does not require a full DORA exchange. Also, this renewal exchange takes place directly between the client and the DHCP server, rather than being broadcast across the entire network.

** About DHCP address allocation

There are (at least) three ways that a DHCP server can assign addresses to requesting clients:

- **Manual or static allocation** essentially means that the client receives a specifically-chosen IP address, or, at a minimum, keeps the first one that it’s assigned until the client decides to release it.

- **Dynamic allocation** means that a DHCP server assigns IP addresses from an available pool (scope) of addresses, which can change to another available address in that scope at any time, depending on the network dynamics.

- **Automatic allocation** is sort of a cross between the other two types. The DHCP server assigns an address from its defined scope, but then remembers which client got what address, and re-assigns that address to the same client when a new request is made.

Regardless of the allocation method, the DHCP server’s scope — its range of IP addresses that it controls (and can assign) — is something that must be user-configured.

DHCP is “connectionless,” meaning that basically everything takes place via UDP, usually by broadcast packets — that is, packets not overtly addressed to a specific device. The messages become targeted pretty quickly, using the payload to specify the IP address of the DHCP server and the MAC address of the requesting client, to avoid requiring every other device on the network to completely decode every DHCP message. Note that it is possible to target a UDP packet at a specific server by choosing a unicast message type.

A DHCP client can request its previous IP address, if it had one, but whether it gets that address or not depends on four things: scope, allocation, topology, and authority. Specifically:

 - The larger the DHCP server’s scope of addresses, the more likely it is that the requested address will be available again.

 - The chances of getting the same IP address again also depend on how the server is allocating addresses (see above). Static allocation guarantees the same address; automatic allocation makes it very likely; with dynamic allocation, it’s impossible to predict.

- Topology also plays into this process: if the DHCP server is using one or more DHCP relays to get some or all of its addresses, the chances of re-using the same IP address go down.

- Authority also affects the probability. An authoritative DHCP server will definitely answer any unanswered DHCPDiscover message, but that server is pulling only from its own scope.

** About multiple DHCP servers serving different IP ranges

It’s possible to have more than one DHCP server on the same network segment and still have everything work right, with no conflicts and no dropped packets or IP requests. There are three possible scopes for IP ranges to consider:

- **Adjacent scopes**: In this configuration, IP addresses are assigned from portions of the same subnet. For example, one server might control scope 192.168.14.2 – 192.168.14.187, and another server might manage scope 192.168.14.200 – 192.168.14.247. This is the most common (and most reliable) setup for multiple DHCP servers.

- **Heterogeneous scopes**: This arrangement basically has DHCP servers on different subnets, such as 192.168.14.2 – .253 for one server, and 10.17.22.3 – .98 for the other. This can be made to work, but it’s extremely difficult to set up and not so easy to manage. 

- **Overlapping scopes**: In this situation, more than one server can offer the same IP address. There is a way to make this work, by setting up the DHCP servers to talk to one another, but for most applications, this configuration can be avoided. 

Adjacent and heterogeneous scopes are really the same thing. The two servers do not work together; they may not ever be aware of one another. The servers and clients operate independently on a first-come, first-served basis, serving from their specific pool of IP addresses.

A client makes a DHCPRequest. One or both of the servers may answer, depending on load and spare IP addresses. It’s also possible that neither will answer, because they’re both out of IP addresses, but with good network planning — and making one of those servers authoritative — those situations will be kept to a minimum or eliminated entirely.

** About multiple DHCP servers serving overlapping IP ranges

Some DHCP implementations offer a feature called server conflict detection or SCD. In short, DHCP SCD uses ICMP Echo messages (pings) — with an appropriate wait time — to see if an IP address is in use before trying to lease it to a client. If all the DHCP servers on a given subnet have SCD enabled, you don’t have to worry about whether the DHCP server scopes overlap. You can assign whatever set of IP addresses you want to whichever DHCP server -- even identical IP ranges -- and they will work together without causing any IP conflict errors.

Oddly, SCD is assumed by the creators of DHCP.  In RFC 2131, ping checks are recommended on both ends, in all cases, by the DHCP server and the client:

"As a consistency check, the allocating server SHOULD probe the reused address before allocating the address, e.g., with an ICMP echo request, and the client SHOULD probe the newly received address, e.g., with ARP."

The capital letters there came from the spec itself. Essentially, DHCP servers really should check to make sure the addresses they send out aren’t already in use — and clients that get them should make sure they’re actually free before they use them.

From an architectural perspective, it might make more sense for DHCP servers to be enabled to talk to each other and coordinate assignment of IP addresses. It is possible to build and configure such DHCP servers, but that type of coordination usually isn't possible in a MAAS networking environment.  Usually, in MAAS networks, there is an external source of DHCP outside the control of the MAAS administrator, one which can't be isolated from the MAAS network.  

As a protocol, DHCP is designed to be loosely coupled. Specifically, any client that has the DHCP protocol stack can discover any DHCP server or servers; any server can make an offer; and a client can take whichever offer it wants (though it’s typically coded to take the first DHCP offer that it can accept). Keeping that loosely-coupled architecture intact means letting DHCP servers check to see if the address they’re sending is in use before offering it, and letting clients check to see if an IP address is in use before they request to accept the offer.

There’s no exact count, but it’s fair to say that a non-trivial number of MAAS installation and configuration issues revolve around competing DHCP servers, that is, multiple DHCP servers on the same subnet, using the same scope (or overlapping scopes), colliding with each other and preventing machines from getting IP addresses. This collision usually shows up as an ability to power on a machine, but not to commission it, since it can’t manage to complete the process of getting an IP address via DHCP.

MAAS already has some conflict detection built in.  If MAAS manages a subnet that is not empty -- which could result in MAAS assigning a duplicate IP address -- MAAS is capable of detecting IPs in use on a subnet. Be aware that there are two caveats

1. If a previously-assigned NIC is in a quiescent state or turned off, MAAS may not detect it before duplicating an IP address.

2. At least one rack controller must have access to the IP-assigned machine in order for this feature to work.

MAAS also recognises when the subnet ARP cache is full, so that it can re-check the oldest IPs added to the cache to search for free IP addresses.

If you want your configuration to run more smoothly, it’s useful to enable SCD on every DHCP provider on your network, if you can. It doesn’t hurt anything, and it really doesn’t cost that much (besides a little extra delay when assigning addresses). There are plenty of network issues associated with a large, bare-metal network. There’s no reason why DHCP conflicts need to be one of those issues.

** About DHCP relays

A DHCP relay is really just a specialised router.  Like all routers, it replaces source and destination addresses of packets crossing its domain so that every server gets the intended messages.  

The only substantial difference is that the DHCP relay knows the IP address of the DHCP server.  When a DHCPRequest reaches the relay from a requesting server, for example, the relay absorbs the broadcast packet and creates a routed unicast packet, bound directly for the DHCP server.  On the way back, the relay converts the DHCPOffer back to a broadcast packet.

* Disk erasure
Disk erasure pertains to the erasing of data on each of a machine's disks when the machine has been released (see [Release action](/t/maas-glossary/5416#heading--release)) back into the pool of available machines. The user can choose from among three erasure types before confirming the Release action, and a default erasure configuration can also be set.  This section will help you learn:

- [About disk erasure types](#heading--about-disk-erasure-types)
- [About standard erasure](#heading--about-standard-erase)
- [About secure erasure](#heading--about-secure-erasure)
- [About quick erasure](#heading--about-quick-erasure)
- [About erasure order of preference](#heading--about-erasure-order-of-preference)

** About disk erasure types

The three disk erasure types are:

1.   Standard erasure
2.   Secure erasure
3.   Quick erasure

Each of these are explained below.


** About standard erasure

Overwrites all data with zeros.


** About secure erasure

Although effectively equivalent to Standard erase, Secure erase is much faster because the disk's firmware performs the operation. Because of this, however, some disks may not be able to perform this erasure type (SCSI, SAS, and FC disks in particular).


** About quick erasure

Same as Standard erase but only targets the first 1 MB and the last 1 MB of each disk. This removes the partition tables and/or superblock from the disk, making data recovery difficult but not impossible.


** About erasure order of preference

If all three options are checked when the machine is released, the following order of preference is applied:

1.  Use 'secure erase' if the disk supports it
2.  If it does not, then use 'quick erase'

* Event logs reference
The events log tracks state changes and the execution of basic configuration steps, serving as a timeline for your MAAS machines.  A simple view of the events log might look something like this:

```
  Time 	                        Event
  Sun, 04 Oct. 2020 23:12:35 	Ready
  Sun, 04 Oct. 2020 23:12:31 	Running test - smartctl-validate on sda
  Sun, 04 Oct. 2020 23:10:37 	Gathering information
  Sun, 04 Oct. 2020 23:10:30 	Loading ephemeral
  Sun, 04 Oct. 2020 23:10:15 	Performing PXE boot
  Sun, 04 Oct. 2020 23:09:54 	Powering on
  Sun, 04 Oct. 2020 23:09:53 	Commissioning
```

To view the Events log in the UI -- for a particular machine -- select a machine from the machine list and choose the "Events" tab at the top of the screen. You can also see a more detailed view by selecting "View full history" in near the upper right of the log output.

To view the raw Events log in the CLI, enter the following command:

```
maas $PROFILE events query
```

You can tabulate the results, sorted by machine, with the following command:

```
maas admin events query | jq -r '(["HOSTNAME","TIMESTAMP","TYPE","DESCRIPTION"] | (., map(length*"-"))),
(.events[] | [.hostname, .created, .type, .description // "-"]) | @tsv' | column -t -s $'\t'
```

** Table of event logs

Non-AUDIT MAAS events can be divided into four categories:

- INFO: informational only; no failure or potential failure state detected.
- WARNING: warnings; a potential failure state was detected, or a failure may occur in the future if this condition is not corrected.
- ERROR: errors; a failure state was observed.  The intended operation likely did not complete successfully.
- DEBUG: debugging messages; internal informational messages, used to help characterize failure states.

These four categories are summarized below.  Both the internal (code) representation and the corresponding external message are shown, as MAAS sometimes throws exceptions which use the internal representation.

*** INFO events

| Internal representation | External message |
|:---|:----|
| ABORTED_COMMISSIONING  | Aborted commissioning |
| ABORTED_DEPLOYMENT  | Aborted deployment |
| ABORTED_DISK_ERASING  | Aborted disk erasing |
| ABORTED_TESTING  | Aborted testing |
| COMMISSIONING  | Commissioning |
| CONFIGURING_OS  | Configuring OS |
| CONFIGURING_STORAGE  | Configuring storage |
| ENTERING_RESCUE_MODE  | Entering rescue mode |
| EXITED_RESCUE_MODE  | Exited rescue mode |
| FAILED_COMMISSIONING  | Failed commissioning |
| FAILED_EXITING_RESCUE_MODE  | Failed exiting rescue mode |
| FAILED_TESTING  | Failed testing |
| GATHERING_INFO  | Gathering information |
| INSTALLING_OS  | Installing OS |
| LOADING_EPHEMERAL  | Loading ephemeral |
| NODE_POWER_CYCLE_STARTING  | Power cycling |
| NODE_POWER_OFF_STARTING  | Powering off |
| NODE_POWER_ON_STARTING  | Powering on |
| PERFORMING_PXE_BOOT  | Performing PXE boot |
| RESCUE_MODE  | Rescue mode |
| RUNNING_TEST  | Running test |
| SCRIPT_DID_NOT_COMPLETE  | Script |

*** WARNING events

| Internal representation | External message |
|:---|:----|
| NODE_POWER_QUERY_FAILED  | Failed to query node's BMC |
| RACK_IMPORT_WARNING  | Rack import warning |
| REGION_IMPORT_WARNING  | Region import warning |

*** ERROR events

| Internal representation | External message |
|:---|:----|
| NODE_COMMISSIONING_EVENT_FAILED  | Node commissioning failure |
| NODE_ENTERING_RESCUE_MODE_EVENT_FAILED  | Node entering rescue mode failure |
| NODE_EXITING_RESCUE_MODE_EVENT_FAILED  | Node exiting rescue mode failure |
| NODE_INSTALL_EVENT_FAILED  | Node installation failure |
| NODE_POST_INSTALL_EVENT_FAILED  | Node post-installation failure |
| NODE_POWER_CYCLE_FAILED  | Failed to power cycle node |
| NODE_POWER_OFF_FAILED  | Failed to power off node |
| NODE_POWER_ON_FAILED  | Failed to power on node |
| RACK_IMPORT_ERROR  | Rack import error |
| REGION_IMPORT_ERROR  | Region import error |
| REQUEST_NODE_MARK_BROKEN_SYSTEM  | Marking node broken |
| REQUEST_NODE_MARK_FAILED_SYSTEM  | Marking node failed |
| SCRIPT_RESULT_ERROR  | Script result lookup or storage error |

*** DEBUG events

| Internal representation | External message |
|:---|:----|
| NODE_CHANGED_STATUS  | Node changed status |
| NODE_COMMISSIONING_EVENT  | Node commissioning |
| NODE_ENTERING_RESCUE_MODE_EVENT  | Node entering rescue mode |
| NODE_EXITING_RESCUE_MODE_EVENT  | Node exiting rescue mode |
| NODE_HTTP_REQUEST  | HTTP Request |
| NODE_INSTALLATION_FINISHED  | Installation complete |
| NODE_INSTALL_EVENT  | Node installation |
| NODE_POWERED_OFF  | Node powered off |
| NODE_POWERED_ON  | Node powered on |
| NODE_PXE_REQUEST  | PXE Request |
| NODE_STATUS_EVENT  | Node status event |
| NODE_TFTP_REQUEST  | TFTP Request |
| RACK_IMPORT_INFO  | Rack import info |
| REGION_IMPORT_INFO  | Region import info |
| REQUEST_CONTROLLER_REFRESH  | Starting refresh of controller hardware and networking information |
| REQUEST_NODE_ABORT_COMMISSIONING  | User aborting node commissioning |
| REQUEST_NODE_ABORT_DEPLOYMENT  | User aborting deployment |
| REQUEST_NODE_ABORT_ERASE_DISK  | User aborting disk erase |
| REQUEST_NODE_ABORT_TESTING  | User aborting node testing |
| REQUEST_NODE_ACQUIRE  | User acquiring node |
| REQUEST_NODE_ERASE_DISK  | User erasing disk |
| REQUEST_NODE_LOCK  | User locking node |
| REQUEST_NODE_MARK_BROKEN  | User marking node broken |
| REQUEST_NODE_MARK_FAILED  | User marking node failed |
| REQUEST_NODE_MARK_FIXED  | User marking node fixed |
| REQUEST_NODE_MARK_FIXED_SYSTEM  | Marking node fixed |
| REQUEST_NODE_OVERRIDE_FAILED_TESTING  | User overrode 'Failed testing' status |
| REQUEST_NODE_RELEASE  | User releasing node |
| REQUEST_NODE_START  | User powering up node |
| REQUEST_NODE_START_COMMISSIONING  | User starting node commissioning |
| REQUEST_NODE_START_DEPLOYMENT  | User starting deployment |
| REQUEST_NODE_START_RESCUE_MODE  | User starting rescue mode |
| REQUEST_NODE_START_TESTING  | User starting node testing |
| REQUEST_NODE_STOP  | User powering down node |
| REQUEST_NODE_STOP_RESCUE_MODE  | User stopping rescue mode |
| REQUEST_NODE_UNLOCK  | User unlocking node |
| REQUEST_RACK_CONTROLLER_ADD_CHASSIS  | Querying chassis and enlisting all machines |
| SCRIPT_RESULT_CHANGED_STATUS  | Script result |

* Events and event logs
Event logs display a list of timestamped status updates for events and actions performed on the machine.  Events are state changes that happen to MAAS elements, such as controllers, networks, or machines.  These state changes can be caused by MAAS itself, some external agent (such as an external DHCP server), or by users (such as when commissioning a machine).  Being able to review events is often essential to debugging or verifying your MAAS system.

Events can be seen in the MAAS logs, in the UI event log, and in output from the CLI `events query` command.  These three sources provide analogous (but somewhat different information). For example, consider the following log listing, obtained by doing a `grep "fun-zebra" *.log | grep "transition from"` in the MAAS log directory:

```nohighlight
maas.log:2022-09-29T15:04:07.795515-05:00 neuromancer maas.node: [info] fun-zebra: Status transition from COMMISSIONING to TESTING
maas.log:2022-09-29T15:04:17.288763-05:00 neuromancer maas.node: [info] fun-zebra: Status transition from TESTING to READY
```

This information appears this way when events are queried from the CLI:

```nohighlight
        {
            "username": "unknown",
            "node": "bk7mg8",
            "hostname": "fun-zebra",
            "id": 170,
            "level": "INFO",
            "created": "Thu, 29 Sep. 2022 20:04:17",
            "type": "Ready",
            "description": ""
        },
        {
            "username": "unknown",
            "node": "bk7mg8",
            "hostname": "fun-zebra",
            "id": 167,
            "level": "INFO",
            "created": "Thu, 29 Sep. 2022 20:04:07",
            "type": "Running test",
            "description": "smartctl-validate on sda"
        },
```

And it appears like this in the UI events log:

| Time	| Event |
|---|---|
|**Thu, 29 Sep. 2022 20:04:17**	|**Node changed status - From 'Testing' to 'Ready'** |
|Thu, 29 Sep. 2022 20:04:17	|Ready |
|Thu, 29 Sep. 2022 20:04:17	|Script result - smartctl-validate on sda changed status from 'Running' to 'Skipped' |
|Thu, 29 Sep. 2022 20:04:16	|Script result - smartctl-validate on sda changed status from 'Installing dependencies' to 'Running' |
|Thu, 29 Sep. 2022 20:04:07	|Running test - smartctl-validate on sda |
|**Thu, 29 Sep. 2022 20:04:07**	|**Node changed status - From 'Commissioning' to 'Testing'** |

You can see that all three outputs are sources of truth, but the messages are somewhat different, include different information, and contain different levels of detail.

** Using the logs directly

By the way, if you're interested in reading the logs, and you're using snaps, you'll find what you need here:

- /var/snap/maas/common/log/maas.log
- /var/snap/maas/common/log/regiond.log
- /var/snap/maas/common/log/rackd.log
- /var/snap/maas/common/log/rsyslog/$MACHINE_NAME/$RELEVANT_DATE/messages

If you’re using packages, you’ll find the log files in these locations:

- /var/log/maas/maas.log
- /var/log/maas/regiond.log
- /var/log/maas/rackd.log
- /var/log/maas/rsyslog/$MACHINE_NAME/$RELEVANT_DATE/messages

These logs can be very large and hard to search, and the web UI does not separate events by type. For instance, commissioning a simple VM produces logging information like this:

```nohighlight
maas.log:2022-09-29T15:00:16.461402-05:00 neuromancer maas.interface: [info] eth0 (physical) on fun-zebra: IP address automatically unlinked: None:type=AUTO
maas.log:2022-09-29T15:00:16.553396-05:00 neuromancer maas.node: [info] fun-zebra: Status transition from NEW to COMMISSIONING
maas.log:2022-09-29T15:00:16.754265-05:00 neuromancer maas.power: [info] Changing power state (on) of node: fun-zebra (bk7mg8)
maas.log:2022-09-29T15:00:16.759676-05:00 neuromancer maas.node: [info] fun-zebra: Commissioning started
maas.log:2022-09-29T15:00:18.039441-05:00 neuromancer maas.power: [info] Changed power state (on) of node: fun-zebra (bk7mg8)
maas.log:2022-09-29T15:03:26.946507-05:00 neuromancer maas.node: [info] fun-zebra: Storage layout was set to flat.
maas.log:2022-09-29T15:04:07.795515-05:00 neuromancer maas.node: [info] fun-zebra: Status transition from COMMISSIONING to TESTING
maas.log:2022-09-29T15:04:17.288763-05:00 neuromancer maas.node: [info] fun-zebra: Status transition from TESTING to READY
regiond.log:2022-09-29 20:00:16 maasserver.models.signals.interfaces: [info] eth0 (physical) on fun-zebra: deleted IP addresses due to VLAN update (5001 -> 5002).
regiond.log:2022-09-29 20:00:32 maasserver.region_controller: [info] Reloaded DNS configuration; ip 10.103.114.192 connected to fun-zebra on eth0
regiond.log:2022-09-29 20:01:09 maasserver.rpc.leases: [info] Lease update: commit for 10.103.114.192 on 0:16:3e:a2:73:5c at 2022-09-29 20:01:09 (lease time: 600s) (hostname: fun-zebra)
regiond.log:2022-09-29 20:01:14 maasserver.region_controller: [info] Reloaded DNS configuration; ip 10.103.114.192 connected to fun-zebra on eth0
regiond.log:     * node fun-zebra renamed interface eth0 to enp5s0
regiond.log:     * ip 10.103.114.192 connected to fun-zebra on eth0
regiond.log:     * ip 10.103.114.192 disconnected from fun-zebra on eth0
```

Not all of this output is relevant, nor does it all trigger a recorded MAAS event.  Interpreting MAAS logs is a matter of practice with known events in a controlled environment.

** MAAS CLI events query command

In fact, probably the best way to review events is via the CLI sub-command, `events query`. This sub-command can help you filter and summarise events.  Let's take a look at how this tool works.

*** Basic queries

MAAS events can be queried with the simple CLI command:

```nohighlight
maas $PROFILE events query
```

where `$PROFILE` is your login name for your MAAS CLI.  This command produces a very long JSON listing, something like this:

```nohighlight
Success.
Machine-readable output follows:
{
    "count": 100,
    "events": [
        {
            "username": "unknown",
            "node": "ebd7dc",
            "hostname": "new-name",
            "id": 588448,
            "level": "WARNING",
            "created": "Tue, 27 Sep. 2022 20:49:37",
            "type": "Failed to query node's BMC",
            "description": "Failed to login to virsh console."
        },
        {
            "username": "unknown",
            "node": "mm3tc8",
            "hostname": "fair-marten",
            "id": 588447,
            "level": "WARNING",
            "created": "Tue, 27 Sep. 2022 20:49:37",
            "type": "Failed to query node's BMC",
            "description": "Failed to login to virsh console."
        },
		[... goes on for 100 events, by default ...]
        {
            "username": "unknown",
            "node": "ebd7dc",
            "hostname": "new-name",
            "id": 588442,
            "level": "WARNING",
            "created": "Tue, 27 Sep. 2022 20:39:22",
            "type": "Failed to query node's BMC",
            "description": "Failed to login to virsh console."
        }
    ],
    "next_uri": "/MAAS/api/2.0/events/?op=query&limit=5&after=588448",
    "prev_uri": "/MAAS/api/2.0/events/?op=query&limit=5&before=588442"
}
```

These listings can be very long and very hard to read.  You'll also notice that this particular MAAS has over 500,000 events, so parsing these logs by hand is certainly not practical.  There are two things you should do to make events easier to interpret:

- use the `jq` command, with some invocations we'll give you, to make neat tables out of your event lists.

- use the various filters -- supplied as part of the `events query` command -- to limit your output.

Let's explore both of these things in turn.

*** Using jq with events

We offer a [more complete tutorial on jq](/t/using-jq-with-the-maas-cli/6027) in this documentation set, but for now, we can give you some invocations that will make events much easier to read.  Let's take our example command above and add some `jq` to it to make the output more readable:

```nohighlight
maas $PROFILE events query limit=20 \
| jq -r '(["USERNAME","NODE","HOSTNAME","LEVEL","DATE","TYPE","EVENT"] | 
(., map(length*"-"))),
(.events[] | [.username,.node,.hostname,.level,.created,.type,.description]) 
| @tsv' | column -t -s$'\t'
```

This might give us output something like this:

```nohighlight
USERNAME  NODE    HOSTNAME       LEVEL    DATE                        TYPE                        EVENT
--------  ----    --------       -----    ----                        ----                        -----
unknown   ebd7dc  new-name       WARNING  Thu, 10 Mar. 2022 21:59:52  Failed to query node's BMC  Failed to login to virsh console.
unknown   mm3tc8  fair-marten    WARNING  Thu, 10 Mar. 2022 21:59:52  Failed to query node's BMC  Failed to login to virsh console.
unknown   pbpncx  ruling-bobcat  WARNING  Thu, 10 Mar. 2022 21:59:22  Failed to query node's BMC  Pod pbpncx: Failed to connect to the LXD REST API.
unknown   ebd7dc  new-name       WARNING  Thu, 10 Mar. 2022 21:54:34  Failed to query node's BMC  Failed to login to virsh console.
unknown   mm3tc8  fair-marten    WARNING  Thu, 10 Mar. 2022 21:54:34  Failed to query node's BMC  Failed to login to virsh console.
unknown   pbpncx  ruling-bobcat  WARNING  Thu, 10 Mar. 2022 21:54:05  Failed to query node's BMC  Pod pbpncx: Failed to connect to the LXD REST API.
unknown   ebd7dc  new-name       WARNING  Thu, 10 Mar. 2022 21:49:21  Failed to query node's BMC  Failed to login to virsh console.
unknown   mm3tc8  fair-marten    WARNING  Thu, 10 Mar. 2022 21:49:19  Failed to query node's BMC  Failed to login to virsh console.
unknown   pbpncx  ruling-bobcat  WARNING  Thu, 10 Mar. 2022 21:48:49  Failed to query node's BMC  Pod pbpncx: Failed to connect to the LXD REST API.
unknown   mm3tc8  fair-marten    WARNING  Thu, 10 Mar. 2022 21:44:08  Failed to query node's BMC  Failed to login to virsh console.
admin     ebd7dc  new-name       AUDIT    Thu, 09 Jun. 2022 21:39:54  Node                        Tagging 'new-name'.
unknown   pbpncx  contr-105      ERROR    Rack import error           Unable to import boot images: ('Connection broken: IncompleteRead(4096 bytes read)', IncompleteRead(4096 bytes read))
unknown   mm3tc8  fair-marten    WARNING  Thu, 10 Mar. 2022 21:38:50  Failed to query node's BMC  Failed to login to virsh console.
unknown   ebd7dc  contr-105      DEBUG    Thu, 12 May. 2022 21:38:26  Rack import info            Starting rack boot image import
unknown   pbpncx  ruling-bobcat  WARNING  Thu, 10 Mar. 2022 21:38:21  Failed to query node's BMC  Pod pbpncx: Failed to connect to the LXD REST API.
admin     pbpncx  ruling-bobcat  AUDIT    Thu, 16 Jun. 2022 21:35:16  Node                        Started commissioning on 'ruling-bobcat'.
unknown   mm3tc8  fair-marten    WARNING  Thu, 10 Mar. 2022 21:33:44  Failed to query node's BMC  Failed to login to virsh console.
unknown   pbpncx  ruling-bobcat  WARNING  Thu, 10 Mar. 2022 21:33:16  Failed to query node's BMC  Pod pbpncx: Failed to connect to the LXD REST API.
unknown   knpge8  bolla          INFO     Thu, 10 Mar. 2022 20:21:41  Ready                         
unknown   pbpncx  ruling-bobcat  WARNING  Thu, 10 Mar. 2022 18:01:47  Failed to query node's BMC  <LXDAPIException instance at 0x7f0b53e21dc0 with str error:\n Traceback (most recent call last):\n  File "/snap/maas/19076/usr/lib/python3/dist-packages/twisted/python/reflect.py", line 448, in safe_str\n    return str(o)\n  File "/snap/maas/19076/usr/lib/python3/dist-packages/pylxd/exceptions.py", line 18, in __str__\n    if self.response.status_code == 200:  # Operation failure\nAttributeError: 'LXDAPIException' object has no attribute 'status_code'\n>
```

You'll notice, in this listing, we have a mix of event types and responses.  In one case, the log even recorded a code exception.  You can probably see from this listing that events can be very helpful in tracking behaviours and resolving issues with your MAAS instance.  Even limited to 20 records, though, this output is still hard to parse, so let's explore ways to filter this table.

*** Filter parameters

The `events query` command accepts several different filters, all of them optional:

- *hostname*: Only events relating to the node with the matching hostname will be returned. This can be specified multiple times to get events relating to more than one node.

- *mac_address*: Only nodes with matching MAC addresses will be returned. Note that MAC address is not part of the standard output, so you'd need to look it up elsewhere.

- *id*: Only nodes with matching system IDs will be returned.  This corresponds to the `node` parameter in the JSON listing, not the `id` parameter there, which is a serial event number.

- *zone*: Only nodes in the zone will be returned.  Note that zones are not part of the standard output, so you'd need to look these up elsewhere.

- *level*: The event level to capture.  You can choose from AUDIT, CRITICAL, DEBUG, ERROR, INFO, or WARNING.  The default is INFO.

- *limit*: Number of events to return. The default is 100, the maximum in one command is 1000.

- *before*: Defines an event id to start returning older events.  This is the "id" part of the JSON, not the system ID or "node".  Note that `before` and `after` cannot be used together, as the results are unpredictable.

- *after*: Defines an event id to start returning newer events.  This is the "id" part of the JSON, not the system ID or "node".  Note that `before` and `after` cannot be used together, as the results are unpredictable.

This list of filters gives us a few different ways to simplify the output.  Let's try some of these combinations on the sample data, above.

*** Hostname, system ID, and MAC address filters

We can limit the hostname to, say, "new-name" by entering the following:

```nohighlight
maas $PROFILE events query limit=5 hostname=new-name\
| jq -r '(["USERNAME","NODE","HOSTNAME","LEVEL","DATE","TYPE","EVENT"] | 
(., map(length*"-"))),
(.events[] | [.username,.node,.hostname,.level,.created,.type,.description]) 
| @tsv' | column -t -s$'\t'
```

This might give us the following output:

```nohighlight
USERNAME  NODE    HOSTNAME  LEVEL    DATE                        TYPE                        EVENT
--------  ----    --------  -----    ----                        ----                        -----
unknown   ebd7dc  new-name  WARNING  Tue, 27 Sep. 2022 21:41:37  Failed to query node's BMC  Failed to login to virsh console.
unknown   ebd7dc  new-name  WARNING  Tue, 27 Sep. 2022 21:36:22  Failed to query node's BMC  Failed to login to virsh console.
unknown   ebd7dc  new-name  WARNING  Tue, 27 Sep. 2022 21:31:22  Failed to query node's BMC  Failed to login to virsh console.
unknown   ebd7dc  new-name  WARNING  Tue, 27 Sep. 2022 21:26:07  Failed to query node's BMC  Failed to login to virsh console.
unknown   ebd7dc  new-name  WARNING  Tue, 27 Sep. 2022 21:21:07  Failed to query node's BMC  Failed to login to virsh console.
```

We would get similar results with this command, using the "id" filter:

```nohighlight
maas $PROFILE events query limit=5 id=ebd7dc\
| jq -r '(["USERNAME","NODE","HOSTNAME","LEVEL","DATE","TYPE","EVENT"] | 
(., map(length*"-"))),
(.events[] | [.username,.node,.hostname,.level,.created,.type,.description]) 
| @tsv' | column -t -s$'\t'
```

We can also get similar results by looking up this machine's MAC address (52:54:00:32:8b:ea) and filtering by that parameter instead:

```nohighlight
maas $PROFILE events query limit=5 mac_address=52:54:00:32:8b:ea\
| jq -r '(["USERNAME","NODE","HOSTNAME","LEVEL","DATE","TYPE","EVENT"] | 
(., map(length*"-"))),
(.events[] | [.username,.node,.hostname,.level,.created,.type,.description]) 
| @tsv' | column -t -s$'\t'
```

In this particular case, all three would yield identical outputs.

*** Zone filter

We can look up one of the zones (using the Web UI or other CLI commands), and formulate a filter like this:

```nohighlight
maas $PROFILE events query limit=5 zone=twilight\
| jq -r '(["USERNAME","NODE","HOSTNAME","LEVEL","DATE","TYPE","EVENT"] | 
(., map(length*"-"))),
(.events[] | [.username,.node,.hostname,.level,.created,.type,.description]) 
| @tsv' | column -t -s$'\t'
```

Note that this query yields slightly different records:

```nohighlight
USERNAME  NODE    HOSTNAME     LEVEL    DATE                        TYPE                        EVENT
--------  ----    --------     -----    ----                        ----                        -----
unknown   mm3tc8  fair-marten  WARNING  Tue, 27 Sep. 2022 21:52:07  Failed to query node's BMC  Failed to login to virsh console.
unknown   mm3tc8  fair-marten  WARNING  Tue, 27 Sep. 2022 21:46:52  Failed to query node's BMC  Failed to login to virsh console.
unknown   mm3tc8  fair-marten  WARNING  Tue, 27 Sep. 2022 21:41:37  Failed to query node's BMC  Failed to login to virsh console.
unknown   mm3tc8  fair-marten  WARNING  Tue, 27 Sep. 2022 21:36:22  Failed to query node's BMC  Failed to login to virsh console.
unknown   mm3tc8  fair-marten  WARNING  Tue, 27 Sep. 2022 21:31:22  Failed to query node's BMC  Failed to login to virsh console.
```

*** Level filter

We can choose to look at specific events that match a logging level.  For example, we can repeat this command with `level=AUDIT`:

```nohighlight
maas $PROFILE events query limit=5 level=AUDIT\
| jq -r '(["USERNAME","NODE","HOSTNAME","LEVEL","DATE","TYPE","EVENT"] | 
(., map(length*"-"))),
(.events[] | [.username,.node,.hostname,.level,.created,.type,.description]) 
| @tsv' | column -t -s$'\t'
```

This will yield dramatically different results:

```nohighlight
USERNAME  NODE    HOSTNAME       LEVEL  DATE                        TYPE  EVENT
--------  ----    --------       -----  ----                        ----  -----
admin     ebd7dc  new-name       AUDIT  Thu, 22 Sep. 2022 15:25:55  Node  Overrode failed testing on 'new-name'.
admin     87pnsc  solid-tick     AUDIT  Thu, 22 Sep. 2022 15:22:33  Node  Aborted 'commissioning' on 'solid-tick'.
admin     pbpncx  ruling-bobcat  AUDIT  Thu, 22 Sep. 2022 15:19:00  Node  Started commissioning on 'ruling-bobcat'.
admin     87pnsc  solid-tick     AUDIT  Thu, 22 Sep. 2022 15:18:59  Node  Started commissioning on 'solid-tick'.
zorkobob  mm3tc8  fair-marten    AUDIT  Wed, 21 Sep. 2022 14:21:25  Node  Tagging 'fair-marten'.
zorkobob  mm3tc8  fair-marten    AUDIT  Wed, 21 Sep. 2022 14:10:38  Node  Untagging 'fair-marten'.
```

In fact, there are several different levels associated with MAAS events:

- INFO: the default, used if no `level=` is specified; shows `INFO` and `ERROR` events.  A typical `INFO` event is "Ready", indicating that a machine has reached the "Ready" state.
- CRITICAL: critical MAAS failures; shows only `CRITICAL` events.  These events usually represent severe error conditions that should be immediately remedied.
- ERROR: MAAS errors; shows only `ERROR` events. Typical `ERROR` events include such things as power on/off failures, commissioning timeouts, and image import failures.
- WARNING: failures which may or may not affect MAAS performance; shows `WARNING` and `ERROR` events.  A typical warning event, for example, might include the inability to find and boot a machine.
- DEBUG: information which would help debug MAAS behaviour; shows `DEBUG` and `INFO` events.  Typical `DEBUG` events involve routine image import activities, for example.
- AUDIT: information which helps determine settings and user actions in MAAS; shows only `AUDIT` events.  They are [covered in more detail elsewhere](/t/understanding-maas-audit-events/6372).

*** Combining filters

We can combine the `level` parameter with the `zone` parameter:

```nohighlight
maas $PROFILE events query limit=5 level=AUDIT zone=twilight\
| jq -r '(["USERNAME","NODE","HOSTNAME","LEVEL","DATE","TYPE","EVENT"] | 
(., map(length*"-"))),
(.events[] | [.username,.node,.hostname,.level,.created,.type,.description]) 
| @tsv' | column -t -s$'\t'
```

This combination gives us a very different output:

```nohighlight
USERNAME     NODE    HOSTNAME     LEVEL  DATE                        TYPE  EVENT
--------     ----    --------     -----  ----                        ----  -----
bobslidell   7h3cw7  polong       AUDIT  Tue, 27 Sep. 2022 21:53:38  Node  Set the zone to 'twilight' on 'polong'.
theotherbob  8r6pw7  karura       AUDIT  Tue, 27 Sep. 2022 21:53:34  Node  Set the zone to 'twilight' on 'karura'.
miltwaddams  mm3tc8  fair-marten  AUDIT  Wed, 21 Sep. 2022 14:21:25  Node  Tagging 'fair-marten'.
mikebolton   mm3tc8  fair-marten  AUDIT  Wed, 21 Sep. 2022 14:10:38  Node  Untagging 'fair-marten'.
admin        8r6pw7  karura       AUDIT  Wed, 21 Sep. 2022 14:00:47  Node  Untagging 'karura'.
```

These various filters can be combined, and even repeated as necessary:

```nohighlight
$ maas $PROFILE events query level=AUDIT hostname=karura hostname=polong limit=5 \
| jq -r '(["USERNAME","NODE","HOSTNAME","LEVEL","DATE","TYPE","EVENT"] | 
(., map(length*"-"))),
(.events[] | [.username,.node,.hostname,.level,.created,.type,.description]) 
| @tsv' | column -t -s$'\t'
```

Again, this combination gives us a different view of the event data:

```nohighlight
USERNAME    NODE    HOSTNAME  LEVEL  DATE                        TYPE  EVENT
--------    ----    --------  -----  ----                        ----  -----
stormrider  7h3cw7  polong    AUDIT  Tue, 27 Sep. 2022 21:53:38  Node  Set the zone to 'twilight' on 'polong'.
stormrider  8r6pw7  karura    AUDIT  Tue, 27 Sep. 2022 21:53:34  Node  Set the zone to 'twilight' on 'karura'.
admin       8r6pw7  karura    AUDIT  Wed, 21 Sep. 2022 14:00:47  Node  Untagging 'karura'.
admin       8r6pw7  karura    AUDIT  Wed, 21 Sep. 2022 14:00:01  Node  Tagging 'karura'.
admin       8r6pw7  karura    AUDIT  Wed, 21 Sep. 2022 13:58:11  Node  Untagging 'karura'.
```

*** The limit filter

You can use the `limit` filter to restrict the number of records listed, as we have been doing in many of the examples above.  We can expand the last example to `limit=7`, for instance:

```nohighlight
$ maas $PROFILE events query level=AUDIT hostname=karura hostname=polong limit=7 \
| jq -r '(["USERNAME","NODE","HOSTNAME","LEVEL","DATE","TYPE","EVENT"] | 
(., map(length*"-"))),
(.events[] | [.username,.node,.hostname,.level,.created,.type,.description]) 
| @tsv' | column -t -s$'\t'
```

This gives us a slightly longer list:

```nohighlight
USERNAME    NODE    HOSTNAME  LEVEL  DATE                        TYPE  EVENT
--------    ----    --------  -----  ----                        ----  -----
stormrider  7h3cw7  polong    AUDIT  Tue, 27 Sep. 2022 21:53:38  Node  Set the zone to 'twilight' on 'polong'.
stormrider  8r6pw7  karura    AUDIT  Tue, 27 Sep. 2022 21:53:34  Node  Set the zone to 'twilight' on 'karura'.
admin       8r6pw7  karura    AUDIT  Wed, 21 Sep. 2022 14:00:47  Node  Untagging 'karura'.
admin       8r6pw7  karura    AUDIT  Wed, 21 Sep. 2022 14:00:01  Node  Tagging 'karura'.
admin       8r6pw7  karura    AUDIT  Wed, 21 Sep. 2022 13:58:11  Node  Untagging 'karura'.
admin       8r6pw7  karura    AUDIT  Wed, 21 Sep. 2022 13:57:48  Node  Tagging 'karura'.
admin       7h3cw7  polong    AUDIT  Tue, 13 Sep. 2022 14:14:24  Node  Powered on 'polong'.
```

*** The before and after filters

Let's suppose that we want to repeat the query in the last example, but we want to start from the beginning of the event log (whenever that might have been).  We could modify the above command to something like this:

```nohighlight
$ maas $PROFILE events query level=AUDIT hostname=karura hostname=polong after=0 limit=7 \
| jq -r '(["USERNAME","NODE","HOSTNAME","LEVEL","DATE","TYPE","EVENT"] | 
(., map(length*"-"))),
(.events[] | [.username,.node,.hostname,.level,.created,.type,.description]) 
| @tsv' | column -t -s$'\t'
```

This would give us a different view:

```nohighlight
USERNAME  NODE    HOSTNAME  LEVEL  DATE                        TYPE  EVENT
--------  ----    --------  -----  ----                        ----  -----
ed        8r6pw7  karura    AUDIT  Tue, 12 Jul. 2022 15:08:27  Node  Powered on 'karura'.
ed        8r6pw7  karura    AUDIT  Tue, 12 Jul. 2022 15:08:23  Node  Powered on 'karura'.
ed        8r6pw7  karura    AUDIT  Tue, 12 Jul. 2022 15:08:08  Node  Powered off 'karura'.
admin     8r6pw7  karura    AUDIT  Thu, 23 Jun. 2022 23:26:53  Node  Set the zone to 'asd' on 'karura'.
peter     8r6pw7  karura    AUDIT  Fri, 22 Apr. 2022 08:06:59  Node  Powered off 'karura'.
peter     8r6pw7  karura    AUDIT  Fri, 22 Apr. 2022 07:09:55  Node  Powered on 'karura'.
ed        7h3cw7  polong    AUDIT  Thu, 27 Jan. 2022 14:34:34  Node  Powered on 'polong'.
```

We could also retrieve very recent records using "before":

```nohighlight
$ maas $PROFILE events query level=AUDIT before=500000 limit=7 \
| jq -r '(["USERNAME","NODE","HOSTNAME","LEVEL","DATE","TYPE","EVENT"] | 
(., map(length*"-"))),
(.events[] | [.username,.node,.hostname,.level,.created,.type,.description]) 
| @tsv' | column -t -s$'\t'
```

This would return:

```nohighlight
USERNAME  NODE    HOSTNAME     LEVEL  DATE                        TYPE  EVENT
--------  ----    --------     -----  ----                        ----  -----
peter     8r6pw7  karura       AUDIT  Fri, 22 Apr. 2022 08:06:59  Node  Powered off 'karura'.
peter     8r6pw7  karura       AUDIT  Fri, 22 Apr. 2022 07:09:55  Node  Powered on 'karura'.
admin     ebd7dc  new-name     AUDIT  Thu, 14 Apr. 2022 02:24:02  Node  Aborted 'commissioning' on 'new-name'.
admin     ebd7dc  new-name     AUDIT  Thu, 14 Apr. 2022 02:23:44  Node  Started commissioning on 'new-name'.
ed        mm3tc8  fair-marten  AUDIT  Fri, 08 Apr. 2022 11:02:15  Node  Untagging 'fair-marten'.
ed        mm3tc8  fair-marten  AUDIT  Fri, 08 Apr. 2022 11:02:14  Node  Tagging 'fair-marten'.
admin     mm3tc8  fair-marten  AUDIT  Fri, 11 Feb. 2022 11:00:00  Node  Set the zone to 'twilight' on 'fair-marten'.
```

** Using different event levels

As mentioned earlier, the `AUDIT` events are [discussed elsewhere](/t/understanding-maas-audit-events/6372).  It may be useful, though to take a closer look at the other event levels here.

*** INFO and DEBUG events

We walked the MAAS machine `fun-zebra` through the following states:

- Commissioning
- Allocation
- Deployment
- Releasing
- Testing (with a premature manual abort)
- Rescue mode

The resulting `level=INFO` and `level=DEBUG` event sets are enlightening.

<details><summary>The raw log output, for reference only.</summary>

```nohighlight
maas.log:2022-09-29T15:00:16.461402-05:00 neuromancer maas.interface: [info] eth0 (physical) on fun-zebra: IP address automatically unlinked: None:type=AUTO
maas.log:2022-09-29T15:00:16.553396-05:00 neuromancer maas.node: [info] fun-zebra: Status transition from NEW to COMMISSIONING
maas.log:2022-09-29T15:00:16.754265-05:00 neuromancer maas.power: [info] Changing power state (on) of node: fun-zebra (bk7mg8)
maas.log:2022-09-29T15:00:16.759676-05:00 neuromancer maas.node: [info] fun-zebra: Commissioning started
maas.log:2022-09-29T15:00:18.039441-05:00 neuromancer maas.power: [info] Changed power state (on) of node: fun-zebra (bk7mg8)
maas.log:2022-09-29T15:03:26.946507-05:00 neuromancer maas.node: [info] fun-zebra: Storage layout was set to flat.
maas.log:2022-09-29T15:04:07.795515-05:00 neuromancer maas.node: [info] fun-zebra: Status transition from COMMISSIONING to TESTING
maas.log:2022-09-29T15:04:17.288763-05:00 neuromancer maas.node: [info] fun-zebra: Status transition from TESTING to READY
maas.log:2022-09-29T16:14:21.778320-05:00 neuromancer maas.node: [info] fun-zebra: Status transition from READY to ALLOCATED
maas.log:2022-09-29T16:14:21.793566-05:00 neuromancer maas.node: [info] fun-zebra: allocated to user case
maas.log:2022-09-29T16:14:27.662829-05:00 neuromancer maas.node: [info] fun-zebra: Status transition from ALLOCATED to DEPLOYING
maas.log:2022-09-29T16:14:31.019526-05:00 neuromancer maas.power: [info] Changing power state (on) of node: fun-zebra (bk7mg8)
maas.log:2022-09-29T16:14:32.334589-05:00 neuromancer maas.power: [info] Changed power state (on) of node: fun-zebra (bk7mg8)
maas.log:2022-09-29T16:22:41.935983-05:00 neuromancer maas.node: [info] fun-zebra: Status transition from DEPLOYING to DEPLOYED
maas.log:2022-09-29T16:23:37.084128-05:00 neuromancer maas.node: [info] fun-zebra: Releasing node
maas.log:2022-09-29T16:23:37.085876-05:00 neuromancer maas.node: [info] fun-zebra: Status transition from DEPLOYED to RELEASING
maas.log:2022-09-29T16:23:37.196437-05:00 neuromancer maas.power: [info] Changing power state (off) of node: fun-zebra (bk7mg8)
maas.log:2022-09-29T16:23:38.546649-05:00 neuromancer maas.node: [info] fun-zebra: Status transition from RELEASING to READY
maas.log:2022-09-29T16:23:38.591042-05:00 neuromancer maas.power: [info] Changed power state (off) of node: fun-zebra (bk7mg8)
maas.log:2022-09-29T16:23:51.876495-05:00 neuromancer maas.node: [info] fun-zebra: Status transition from READY to TESTING
maas.log:2022-09-29T16:23:51.997139-05:00 neuromancer maas.power: [info] Changing power state (on) of node: fun-zebra (bk7mg8)
maas.log:2022-09-29T16:23:52.001167-05:00 neuromancer maas.node: [info] fun-zebra: Testing starting
maas.log:2022-09-29T16:23:53.291863-05:00 neuromancer maas.power: [info] Changed power state (on) of node: fun-zebra (bk7mg8)
maas.log:2022-09-29T16:26:14.607386-05:00 neuromancer maas.power: [info] Changing power state (off) of node: fun-zebra (bk7mg8)
maas.log:2022-09-29T16:26:14.622643-05:00 neuromancer maas.node: [info] fun-zebra: Status transition from TESTING to READY
maas.log:2022-09-29T16:26:14.678433-05:00 neuromancer maas.node: [info] fun-zebra: Testing aborted, stopping node
maas.log:2022-09-29T16:26:16.051940-05:00 neuromancer maas.power: [info] Changed power state (off) of node: fun-zebra (bk7mg8)
maas.log:2022-09-29T16:26:23.081533-05:00 neuromancer maas.node: [info] fun-zebra: Status transition from READY to ENTERING_RESCUE_MODE
maas.log:2022-09-29T16:26:23.160687-05:00 neuromancer maas.power: [info] Changing power state (cycle) of node: fun-zebra (bk7mg8)
maas.log:2022-09-29T16:26:23.163274-05:00 neuromancer maas.node: [info] fun-zebra: Rescue mode starting
maas.log:2022-09-29T16:26:24.528007-05:00 neuromancer maas.power: [info] Changed power state (on) of node: fun-zebra (bk7mg8)
maas.log:2022-09-29T16:28:58.268558-05:00 neuromancer maas.node: [info] fun-zebra: Status transition from ENTERING_RESCUE_MODE to RESCUE_MODE
maas.log:2022-09-29T16:29:52.204837-05:00 neuromancer maas.node: [info] fun-zebra: Status transition from RESCUE_MODE to EXITING_RESCUE_MODE
maas.log:2022-09-29T16:29:52.323798-05:00 neuromancer maas.power: [info] Changing power state (off) of node: fun-zebra (bk7mg8)
maas.log:2022-09-29T16:29:53.708975-05:00 neuromancer maas.node: [info] fun-zebra: Status transition from EXITING_RESCUE_MODE to READY
maas.log:2022-09-29T16:29:53.745776-05:00 neuromancer maas.power: [info] Changed power state (off) of node: fun-zebra (bk7mg8)
maas.log:2022-09-29T16:32:20.147958-05:00 neuromancer maas.node: [info] fun-zebra: moved from default zone to new-zone zone.
regiond.log:2022-09-29 20:00:16 maasserver.models.signals.interfaces: [info] eth0 (physical) on fun-zebra: deleted IP addresses due to VLAN update (5001 -> 5002).
regiond.log:2022-09-29 20:00:32 maasserver.region_controller: [info] Reloaded DNS configuration; ip 10.103.114.192 connected to fun-zebra on eth0
regiond.log:2022-09-29 20:01:09 maasserver.rpc.leases: [info] Lease update: commit for 10.103.114.192 on 0:16:3e:a2:73:5c at 2022-09-29 20:01:09 (lease time: 600s) (hostname: fun-zebra)
regiond.log:2022-09-29 20:01:14 maasserver.region_controller: [info] Reloaded DNS configuration; ip 10.103.114.192 connected to fun-zebra on eth0
regiond.log:     * node fun-zebra renamed interface eth0 to enp5s0
regiond.log:     * ip 10.103.114.192 connected to fun-zebra on eth0
regiond.log:     * ip 10.103.114.192 disconnected from fun-zebra on eth0
regiond.log:2022-09-29 21:15:31 maasserver.rpc.leases: [info] Lease update: commit for 10.103.114.2 on 0:16:3e:a2:73:5c at 2022-09-29 21:15:31 (lease time: 600s) (hostname: fun-zebra)
regiond.log:2022-09-29 21:21:48 maasserver.models.node: [info] fun-zebra: Turning off netboot for node
regiond.log:2022-09-29 21:22:41 metadataserver: [info] No user data registered for node named fun-zebra
regiond.log:2022-09-29 21:23:37 maasserver.models.node: [info] fun-zebra: Turning on netboot for node
regiond.log:2022-09-29 21:23:37 maasserver.models.node: [info] fun-zebra: Turning ephemeral deploy off for node
regiond.log:2022-09-29 21:24:06 maasserver.region_controller: [info] Reloaded DNS configuration; ip 10.103.114.192 connected to fun-zebra on enp5s0
regiond.log:2022-09-29 21:24:43 maasserver.rpc.leases: [info] Lease update: commit for 10.103.114.192 on 0:16:3e:a2:73:5c at 2022-09-29 21:24:43 (lease time: 600s) (hostname: fun-zebra)
regiond.log:2022-09-29 21:24:46 maasserver.region_controller: [info] Reloaded DNS configuration; ip 10.103.114.192 connected to fun-zebra on enp5s0
regiond.log:2022-09-29 21:27:18 maasserver.rpc.leases: [info] Lease update: commit for 10.103.114.192 on 0:16:3e:a2:73:5c at 2022-09-29 21:27:18 (lease time: 600s) (hostname: fun-zebra)
```
</details>

First, let's try this command:

```nohighlight
 maas $PROFILE events query level=INFO hostname=fun-zebra limit=1000 | jq -r '(["USERNAME","NODE","HOSTNAME","LEVEL","DATE","TYPE","EVENT"] | (., map(length*"-"))),(.events[] | [.username,.node,.hostname,.level,.created,.type,.description]) | @tsv' | column -t -s$'\t'
 ```
 
 This will yield a surprisingly compact report:
 
 ```nohighlight
 USERNAME  NODE    HOSTNAME   LEVEL  DATE                        TYPE                   EVENT
--------  ----    --------   -----  ----                        ----                   -----
unknown   bk7mg8  fun-zebra  INFO   Thu, 29 Sep. 2022 21:29:53  Exited rescue mode     
unknown   bk7mg8  fun-zebra  INFO   Thu, 29 Sep. 2022 21:29:52  Powering off           
unknown   bk7mg8  fun-zebra  INFO   Thu, 29 Sep. 2022 21:28:58  Rescue mode            
unknown   bk7mg8  fun-zebra  INFO   Thu, 29 Sep. 2022 21:27:18  Loading ephemeral      
unknown   bk7mg8  fun-zebra  INFO   Thu, 29 Sep. 2022 21:26:40  Performing PXE boot    
unknown   bk7mg8  fun-zebra  INFO   Thu, 29 Sep. 2022 21:26:23  Power cycling          
unknown   bk7mg8  fun-zebra  INFO   Thu, 29 Sep. 2022 21:26:23  Entering rescue mode   
unknown   bk7mg8  fun-zebra  INFO   Thu, 29 Sep. 2022 21:26:14  Powering off           
unknown   bk7mg8  fun-zebra  INFO   Thu, 29 Sep. 2022 21:26:14  Aborted testing        
unknown   bk7mg8  fun-zebra  INFO   Thu, 29 Sep. 2022 21:24:08  Performing PXE boot    
unknown   bk7mg8  fun-zebra  INFO   Thu, 29 Sep. 2022 21:23:51  Powering on            
unknown   bk7mg8  fun-zebra  INFO   Thu, 29 Sep. 2022 21:23:51  Testing                
unknown   bk7mg8  fun-zebra  INFO   Thu, 29 Sep. 2022 21:23:38  Released               
unknown   bk7mg8  fun-zebra  INFO   Thu, 29 Sep. 2022 21:23:37  Powering off           
unknown   bk7mg8  fun-zebra  INFO   Thu, 29 Sep. 2022 21:23:37  Releasing              
unknown   bk7mg8  fun-zebra  INFO   Thu, 29 Sep. 2022 21:22:41  Deployed               
unknown   bk7mg8  fun-zebra  INFO   Thu, 29 Sep. 2022 21:21:49  Rebooting              
unknown   bk7mg8  fun-zebra  INFO   Thu, 29 Sep. 2022 21:18:42  Configuring OS         
unknown   bk7mg8  fun-zebra  INFO   Thu, 29 Sep. 2022 21:17:42  Installing OS          
unknown   bk7mg8  fun-zebra  INFO   Thu, 29 Sep. 2022 21:17:30  Configuring storage    
unknown   bk7mg8  fun-zebra  INFO   Thu, 29 Sep. 2022 21:15:31  Loading ephemeral      
unknown   bk7mg8  fun-zebra  INFO   Thu, 29 Sep. 2022 21:14:48  Performing PXE boot    
unknown   bk7mg8  fun-zebra  INFO   Thu, 29 Sep. 2022 21:14:31  Powering on            
unknown   bk7mg8  fun-zebra  INFO   Thu, 29 Sep. 2022 21:14:27  Deploying              
unknown   bk7mg8  fun-zebra  INFO   Thu, 29 Sep. 2022 20:04:17  Ready                  
unknown   bk7mg8  fun-zebra  INFO   Thu, 29 Sep. 2022 20:04:07  Running test           smartctl-validate on sda
unknown   bk7mg8  fun-zebra  INFO   Thu, 29 Sep. 2022 20:01:27  Gathering information  
unknown   bk7mg8  fun-zebra  INFO   Thu, 29 Sep. 2022 20:01:10  Loading ephemeral      
unknown   bk7mg8  fun-zebra  INFO   Thu, 29 Sep. 2022 20:00:35  Performing PXE boot    
unknown   bk7mg8  fun-zebra  INFO   Thu, 29 Sep. 2022 20:00:16  Powering on            
unknown   bk7mg8  fun-zebra  INFO   Thu, 29 Sep. 2022 20:00:16  Commissioning          
 ```
 
Note that most of the `INFO` events are either machine life-cycle events or key operations within those state changes, such as `Loading ephemeral` after a PXE boot.  `DEBUG` events, on the other hand, include `INFO` events for reference, but provide a much more extensive report of individual actions within each state change.  For instance, here is just the snippet of `DEBUG` information for the host's exit from rescue mode:

```nohighlight
USERNAME  NODE    HOSTNAME   LEVEL  DATE                        TYPE                              EVENT
--------  ----    --------   -----  ----                        ----                              -----
unknown   bk7mg8  fun-zebra  DEBUG  Thu, 29 Sep. 2022 21:29:53  Node powered off                  
unknown   bk7mg8  fun-zebra  DEBUG  Thu, 29 Sep. 2022 21:29:53  Node changed status               From 'Exiting rescue mode' to 'Ready'
unknown   bk7mg8  fun-zebra  INFO   Thu, 29 Sep. 2022 21:29:53  Exited rescue mode                
unknown   bk7mg8  fun-zebra  DEBUG  Thu, 29 Sep. 2022 21:28:58  Node status event                 'cloudinit' running config-power-state-change with frequency once-per-instance
unknown   bk7mg8  fun-zebra  DEBUG  Thu, 29 Sep. 2022 21:28:58  Node status event                 'cloudinit' running config-final-message with frequency always
unknown   bk7mg8  fun-zebra  DEBUG  Thu, 29 Sep. 2022 21:28:58  Node status event                 'cloudinit' running config-phone-home with frequency once-per-instance
unknown   bk7mg8  fun-zebra  DEBUG  Thu, 29 Sep. 2022 21:28:58  Node status event                 'cloudinit' running config-install-hotplug with frequency once-per-instance
unknown   bk7mg8  fun-zebra  DEBUG  Thu, 29 Sep. 2022 21:28:58  Node status event                 'cloudinit' running config-keys-to-console with frequency once-per-instance
unknown   bk7mg8  fun-zebra  DEBUG  Thu, 29 Sep. 2022 21:28:58  Node status event                 'cloudinit' running config-ssh-authkey-fingerprints with frequency once-per-instance
unknown   bk7mg8  fun-zebra  DEBUG  Thu, 29 Sep. 2022 21:28:57  Node status event                 'cloudinit' running config-scripts-user with frequency once-per-instance
unknown   bk7mg8  fun-zebra  DEBUG  Thu, 29 Sep. 2022 21:28:57  Node status event                 'cloudinit' running config-scripts-per-instance with frequency once-per-instance
unknown   bk7mg8  fun-zebra  DEBUG  Thu, 29 Sep. 2022 21:28:57  Node status event                 'cloudinit' running config-scripts-per-boot with frequency always
unknown   bk7mg8  fun-zebra  DEBUG  Thu, 29 Sep. 2022 21:28:57  Node status event                 'cloudinit' running config-scripts-per-once with frequency once
unknown   bk7mg8  fun-zebra  DEBUG  Thu, 29 Sep. 2022 21:28:57  Node status event                 'cloudinit' running config-scripts-vendor with frequency once-per-instance
unknown   bk7mg8  fun-zebra  DEBUG  Thu, 29 Sep. 2022 21:28:57  Node status event                 'cloudinit' running config-rightscale_userdata with frequency once-per-instance
unknown   bk7mg8  fun-zebra  DEBUG  Thu, 29 Sep. 2022 21:28:57  Node status event                 'cloudinit' running config-refresh_rmc_and_interface with frequency always
unknown   bk7mg8  fun-zebra  DEBUG  Thu, 29 Sep. 2022 21:28:57  Node status event                 'cloudinit' running config-reset_rmc with frequency once-per-instance
unknown   bk7mg8  fun-zebra  DEBUG  Thu, 29 Sep. 2022 21:28:57  Node status event                 'cloudinit' running config-salt-minion with frequency once-per-instance
unknown   bk7mg8  fun-zebra  DEBUG  Thu, 29 Sep. 2022 21:28:57  Node status event                 'cloudinit' running config-mcollective with frequency once-per-instance
unknown   bk7mg8  fun-zebra  DEBUG  Thu, 29 Sep. 2022 21:28:57  Node status event                 'cloudinit' running config-chef with frequency always
unknown   bk7mg8  fun-zebra  DEBUG  Thu, 29 Sep. 2022 21:28:57  Node status event                 'cloudinit' running config-puppet with frequency once-per-instance
unknown   bk7mg8  fun-zebra  DEBUG  Thu, 29 Sep. 2022 21:28:57  Node status event                 'cloudinit' running config-write-files-deferred with frequency once-per-instance
unknown   bk7mg8  fun-zebra  DEBUG  Thu, 29 Sep. 2022 21:28:57  Node status event                 'cloudinit' running config-ubuntu-drivers with frequency once-per-instance
unknown   bk7mg8  fun-zebra  DEBUG  Thu, 29 Sep. 2022 21:28:57  Node status event                 'cloudinit' running config-lxd with frequency once-per-instance
unknown   bk7mg8  fun-zebra  DEBUG  Thu, 29 Sep. 2022 21:28:57  Node status event                 'cloudinit' running config-landscape with frequency once-per-instance
unknown   bk7mg8  fun-zebra  DEBUG  Thu, 29 Sep. 2022 21:28:57  Node status event                 'cloudinit' running config-fan with frequency once-per-instance
unknown   bk7mg8  fun-zebra  INFO   Thu, 29 Sep. 2022 21:29:52  Powering off                      
unknown   bk7mg8  fun-zebra  DEBUG  Thu, 29 Sep. 2022 21:29:52  Node changed status               From 'Rescue mode' to 'Exiting rescue mode'
case      bk7mg8  fun-zebra  DEBUG  Thu, 29 Sep. 2022 21:29:52  User stopping rescue mode         (case)
```

Notice the detailed `cloudinit` actions necessary to change the machine's state.  The other state changes have similarly detailed outputs in `DEBUG`.

*** ERROR and WARNING events

Here are a few representative `ERROR` event descriptions taken from a live MAAS machine:

```nohighlight
Node has not been heard from for the last 30 minutes
Node operation 'Commissioning' timed out after 30 minutes.
Unable to import boot images: HTTPConnectionPool(host='localhost', port=5240): Read timed out.
Node operation 'Testing' timed out after 30 minutes.
Power on for the node failed: Failed talking to node's BMC: Failed to login to virsh console.
Unable to import boot images: Invalid sha256 Checksum at http://localhost:5240/MAAS/images-stream/ubuntu/amd64/ga-18.04-lowlatency/bionic/20200206/boot-initrd. Found 834c0eacb1a19526f715f9947bd47904b18ad8c733b0762e690edf6143e10561. Expected addfa86d7c054bd0dc085333ad2850e93223d511d04b59ee516d42d801522324. read 38 bytes expected 61715624 bytes. (size 38 expected 61715624)
``` 

Notice that these `ERROR` events flag failures that are probably going to prevent MAAS from operating properly.  Changing the level to `WARNING` picks up all `ERROR` events, but also includes warnings such as this one:

```nohighlight
Finished importing boot images, the region does not have any boot images available.
```

`WARNINGS` tend to be failures, as well, but failures which are more easily fixed (such as having not successfully downloaded any images).

*** CRITICAL errors

`CRITICAL` errors represent major failures, often code failures or trace-backs.  Any `CRITICAL` errors should be immediately examined and resolved, if possible, and [reported as a bug](/t/how-to-report-a-bug/4446) if not resolvable.

* General reference

This section provides general reference information about MAAS.

- [Release notes](/t/what-is-new-with-maas/5292): These are general release notes for the latest version of MAAS, with links to release notes for older versions.

- [Installation requirements](/t/maas-installation-requirements/6233): MAAS has some minimal installation requirements that you'll want to follow.

- [MAAS settings](/t/maas-settings-reference/6347): Many things about MAAS can be customised.

- [MAAS source code](https://launchpad.net/maas): If you're interested in how MAAS works, the source code is available.

- [Documentation style guide](/t/maas-documentation-style-guide/4186): Our documentation is crowd-sourced, but we don't want it to look like it was built by random passers-by.

- [Glossary](/t/maas-glossary/5416): MAAS has a fair amount of special terminology that you'll want to know.

- [Our code of conduct](https://ubuntu.com/community/code-of-conduct): Canonical has a code of conduct, which we adhere to on the MAAS project.

* Get fancy CLI output
The JSON output from the MAAS CLI can be very lengthy for even one machine. You can imagine how large a listing 10 or 12 or 600 machines might present. Traditional JSON output is both consistent and comprehensive, but it's sometimes hard for humans to process.

Enter `jq`, a command-line tool dedicated to filtering and formatting JSON output, so that you can more easily summarise data. For instance, consider a small MAAS install with 12 virtual machines. Six of these machines are LXD VMs, and six are libvirt VMs. Suppose we enter the MAAS command to list all those machines:

```nohighlight
maas admin machines read
```

The listing would be many pages long, and likely very time-consuming to pick through, so we'll just skip it here. On the other hand, I can apply the jq command, a couple of other Ubuntu CLI commands, and just a little bit of finesse to get something more useful:

```nohighlight
maas admin machines read | jq -r '(["HOSTNAME","SYSID",
"POWER","STATUS","OWNER", "TAGS", "POOL","VLAN","FABRIC",
"SUBNET"] | (., map(length*"-"))),(.[] | [.hostname, .system_id, 
.power_state, .status_name, .owner // "-",.tag_names[0] // "-", 
.pool.name,.boot_interface.vlan.name,.boot_interface.vlan.fabric,
.boot_interface.links[0].subnet.name]) | @tsv' | column -t
```

In fact, with this command, we can produce an useful and compact machine listing that serves about 99% of our routine MAAS information needs:

```nohighlight
HOSTNAME      SYSID   POWER  STATUS     OWNER  TAGS                 POOL     VLAN      FABRIC    SUBNET
 --------      -----   -----  ------     -----  ----                 ----     ----      ------    ------
 lxd-vm-1      r8d6yp  off    Deployed   admin  pod-console-logging  default  untagged  fabric-1  10.124.141.0/24
 lxd-vm-2      tfftrx  off    Allocated  admin  pod-console-logging  default  untagged  fabric-1  10.124.141.0/24
 lxd-vm-3      grwpwc  off    Ready      -      pod-console-logging  default  untagged  fabric-1  10.124.141.0/24
 lxd-vm-4      6s8dt4  off    Deployed   admin  pod-console-logging  default  untagged  fabric-1  10.124.141.0/24
 lxd-vm-5      pyebgm  off    Allocated  admin  pod-console-logging  default  untagged  fabric-1  10.124.141.0/24
 lxd-vm-6      ebnww6  off    New        -      pod-console-logging  default  untagged  fabric-1  
 libvirt-vm-1  m7ffsg  off    Ready      -      pod-console-logging  default  untagged  fabric-1  10.124.141.0/24
 libvirt-vm-2  kpawad  off    Ready      -      pod-console-logging  default  untagged  fabric-1  10.124.141.0/24
 libvirt-vm-3  r44hr6  error  Ready      -      pod-console-logging  default  untagged  fabric-1  10.124.141.0/24
 libvirt-vm-4  s3sdkw  off    Ready      -      pod-console-logging  default  untagged  fabric-1  10.124.141.0/24
 libvirt-vm-5  48dg8m  off    Ready      -      pod-console-logging  default  untagged  fabric-1  10.124.141.0/24
 libvirt-vm-6  bacx77  on     Deployed   admin  pod-console-logging  default  untagged  fabric-1  10.124.141.0/24
```

Here we have a clean text table listing the machine hostnames, along with the system IDs, power states, machines statuses, tags, pools, and networking information. These parameters represent only a small fraction of the available JSON output, of course. Let's break this command down, piece by piece, and see how it works.

** Basic jq usage

First, we'll just pull the hostnames from these machines, with no qualifiers or formatting rules, like this:


```nohighlight
maas admin machines read | jq '(.[] | [.hostname])'
```

This command returns output that looks something like this:

```nohighlight
[
  "lxd-vm-1"
]
[
  "lxd-vm-2"
]
[
  "lxd-vm-3"
]
[
  "lxd-vm-4"
]
[
  "lxd-vm-5"
]
[
  "lxd-vm-6"
]
[
  "libvirt-vm-1"
]
[
  "libvirt-vm-2"
]
[
  "libvirt-vm-3"
]
[
  "libvirt-vm-4"
]
[
  "libvirt-vm-5"
]
[
  "libvirt-vm-6"
]
```

Note a couple of things about this command:

```nohighlight
maas admin machines read | jq '(.[] | [.hostname])'
```

First, the `jq` instructions are enclosed in single quotes. As such, they can span lines if necessary, without any line continuations (\), like this:

```nohighlight
maas admin machines read | jq '(.[]
| [.hostname])'
```

Second, notice the structure of the jq instructions. The .[] tells jq that it's decoding an array of data sets — in this case, an array of machine data sets — and that it should iterate through each of the outer data sets (each machine) individually. The pipe symbol (|) completes the “for each” construct, so this command basically says, “for each set of machine data you get, pull out (and return) the value associated with the JSON key hostname". The return value reflects this structure:

```nohighlight

[
   "libvirt-vm-5"
]
[
   "libvirt-vm-6"
]
```

The outer square brackets represent the boundaries of each machine's data set, and the value in quotes corresponds to the value of the key hostname in successive machine data sets. It can get a little complicated sometimes, but that's basically the way to parse JSON with jq.

For practice let's try pulling the value of the key that holds machine status, again with no qualifiers or special formatting:

```nohighlight
maas admin machines read | jq '(.[] | [.hostname, .status_name])'
```

This command essentially tells jq to do the same thing as last time, but also collect the value of the key “status_name” for each machine. The results looks something like this:

```nohighlight

[
  "lxd-vm-1",
  "Deployed"
]
[
  "lxd-vm-2",
  "Allocated"
]
[
  "lxd-vm-3",
  "Ready"
]
[
  "lxd-vm-4",
  "Deployed"
]
[
  "lxd-vm-5",
  "Allocated"
]
[
  "lxd-vm-6",
  "New"
]
[
  "libvirt-vm-1",
  "Ready"
]
[
  "libvirt-vm-2",
  "Ready"
]
[
  "libvirt-vm-3",
  "Ready"
]
[
  "libvirt-vm-4",
  "Ready"
]
[
  "libvirt-vm-5",
  "Ready"
]
[
  "libvirt-vm-6",
  "Deployed"
]
```

So much for printing the values of JSON keys. There are still some nuances (arrays, nested keys, …), but this is the lion's share of the syntax. Let's divert for a minute and look at how to format the output in a more human-readable way.

** Improved formatting

Most of the Ubuntu text-processing commands use tabs as field delimiters, which is a trait inherited from grandfather UNIX. Currently, the output is clean, but relatively hard to format into lines. Luckily jq has a filter for this: the “tab-separated values” filter, known as @tsv. This filter transforms the output records into individual lines with values separated by tabs.

Adding @tsv to the mix:

```nohighlight
maas admin machines read | jq '(.[] | [.hostname, .status_name]) | @tsv'
```

we get something like this:

```nohighlight
"lxd-vm-1\tDeployed"
"lxd-vm-2\tAllocated"
"lxd-vm-3\tReady"
"lxd-vm-4\tDeployed"
"lxd-vm-5\tAllocated"
"lxd-vm-6\tNew"
"libvirt-vm-1\tReady"
"libvirt-vm-2\tReady"
"libvirt-vm-3\tReady"
"libvirt-vm-4\tReady"
"libvirt-vm-5\tReady"
"libvirt-vm-6\tDeployed"
```

That's a step in the right direction, but it's still pretty far from human-readable output. If only there were some way to get rid of the quotes and just do the tab, instead of representing it as a regex character. In fact, the jq “raw” output option (-r) takes care of this:

```nohighlight
maas admin machines read | jq -r '(.[] | [.hostname, .status_name]) | @tsv'
```

Feeding the raw output into our three-filter set gives us a more readable result:

```nohighlight
lxd-vm-1	Deployed
lxd-vm-2	Allocated
lxd-vm-3	Ready
lxd-vm-4	Deployed
lxd-vm-5	Allocated
lxd-vm-6	New
libvirt-vm-1	Ready
libvirt-vm-2	Ready
libvirt-vm-3	Ready
libvirt-vm-4	Ready
libvirt-vm-5	Ready
libvirt-vm-6	Deployed
```

This is tabulated, but the number of spaces between the columns is a little big, and, if there's an unusually long value in one of the fields, it may throw the tabulation off for that line. Something could have been added to jq for that, but there is no need, since Ubuntu already has the column utility. Piping the output of the command so far to column -t (-t for “tabs”) will normalise the tab spacing to the data and ensure that each column is exactly long enough for the longest value in that column:

```nohighlight
maas admin machines read | jq -r '(.[] | [.hostname, .status_name]) | @tsv' | column -t
```

This command result is very similar to the previous output, though you'll notice that the field spacing is neatly optimised to the data itself:

```nohighlight
lxd-vm-1      Deployed
lxd-vm-2      Allocated
lxd-vm-3      Ready
lxd-vm-4      Deployed
lxd-vm-5      Allocated
lxd-vm-6      New
libvirt-vm-1  Ready
libvirt-vm-2  Ready
libvirt-vm-3  Ready
libvirt-vm-4  Ready
libvirt-vm-5  Ready
libvirt-vm-6  Deployed
```

** Making real tables

So far, so good, but this still isn't a presentable data table. First of all, there are no headings. These can be added by passing a literal row to jq, like this:

```nohighlight
maas admin machines read | jq -r '(["HOSTNAME","STATUS"]), (.[] | [.hostname, .status_name]) | @tsv' | column -t
```

You'll note that there are two expressions in parenthesis (representing individual lines or rows). The first just contains the two column headings, while the second contains the “for each” construct that pulls the hostname and status out of the JSON. In essence, the first expression evaluates to just one row, since there's nothing to tell it to iterate. The second expression evaluates to one row per machine, since that's the level of data we're reading. Here's what we get from this command:

```nohighlight
HOSTNAME      STATUS
lxd-vm-1      Deployed
lxd-vm-2      Allocated
lxd-vm-3      Ready
lxd-vm-4      Deployed
lxd-vm-5      Allocated
lxd-vm-6      New
libvirt-vm-1  Ready
libvirt-vm-2  Ready
libvirt-vm-3  Ready
libvirt-vm-4  Ready
libvirt-vm-5  Ready
libvirt-vm-6  Deployed
```

Nice, but it needs a horizontal rule, like a line of dashes, to separate the headings from the data. We can do this by essentially turning the one header row into two, using some jq macros to generate dashes lines of appropriate length:

```nohighlight
maas admin machines read | jq -r '(["HOSTNAME","STATUS"] | 
(.,map(length*"-"))), (.[] | [.hostname, .status_name]) | @tsv' | column -t
```

The expression | (.,) tells jq to convert the foregoing header row into two rows: the first contains the two headers, as in the previous row, and the second contains the result of a couple of macros (map and length). We won't detail those here, but the use of this construct produces the following output:

```nohighlight
HOSTNAME      STATUS
--------      ------
lxd-vm-1      Deployed
lxd-vm-2      Allocated
lxd-vm-3      Ready
lxd-vm-4      Deployed
lxd-vm-5      Allocated
lxd-vm-6      New
libvirt-vm-1  Ready
libvirt-vm-2  Ready
libvirt-vm-3  Ready
libvirt-vm-4  Ready
libvirt-vm-5  Ready
libvirt-vm-6  Deployed
```

** Extending the list

Let's add a couple more fields, owner (which is sometimes blank), and system_id (which is never blank), to the output:

```nohighlight
maas admin machines read | jq -r '(["HOSTNAME","STATUS", "OWNER", "SYSTEM-ID"] 
| (.,map(length*"-"))), (.[] | [.hostname, .status_name,.owner,.system_id]) 
| @tsv' | column -t
```

This gives us the following result:

```nohighlight
HOSTNAME      STATUS     OWNER   SYSTEM-ID
--------      ------     -----   ---------
lxd-vm-1      Deployed   admin   r8d6yp
lxd-vm-2      Allocated  admin   tfftrx
lxd-vm-3      Ready      grwpwc  
lxd-vm-4      Deployed   admin   6s8dt4
lxd-vm-5      Allocated  admin   pyebgm
lxd-vm-6      New        ebnww6  
libvirt-vm-1  Ready      m7ffsg  
libvirt-vm-2  Ready      kpawad  
libvirt-vm-3  Ready      r44hr6  
libvirt-vm-4  Ready      s3sdkw  
libvirt-vm-5  Ready      48dg8m  
libvirt-vm-6  Deployed   admin   bacx77
```

You'll notice right away there's a problem with the columns. Remember that only machines in the “Allocated” or “Deployed” state are owned by anyone, since that's what allocate/acquire means. The lines for the deployed and allocated machines lay out correctly, but the lines for the unowned machines are incorrectly formatted. We can fix this by using the jq “alternate value” construct (a // "b"), which can be loosely read, “if not a, then b.” We add it to the owner key like this:

```nohighlight
maas admin machines read | jq -r '(["HOSTNAME","STATUS", "OWNER", "SYSTEM-ID"] 
| (.,map(length*"-"))), (.[] | [.hostname, .status_name,.owner // "-",.system_id]) 
| @tsv' | column -t
```

Then the results line up nicely, based on the longest value in each key column:

```nohighlight
HOSTNAME      STATUS     OWNER  SYSTEM-ID
--------      ------     -----  ---------
lxd-vm-1      Deployed   admin  r8d6yp
lxd-vm-2      Allocated  admin  tfftrx
lxd-vm-3      Ready      -      grwpwc
lxd-vm-4      Deployed   admin  6s8dt4
lxd-vm-5      Allocated  admin  pyebgm
lxd-vm-6      New        -      ebnww6
libvirt-vm-1  Ready      -      m7ffsg
libvirt-vm-2  Ready      -      kpawad
libvirt-vm-3  Ready      -      r44hr6
libvirt-vm-4  Ready      -      s3sdkw
libvirt-vm-5  Ready      -      48dg8m
libvirt-vm-6  Deployed   admin  bacx77
```


** Nested arrays

Machines have a nested array (of indeterminate length) for machine tags. In JSON terms, instead of having a single key-value pair at the top level, like this:

```nohighlight
"hostname": "libvirt-vm-6",
```

tags are represented by nested arrays, like this:

```nohighlight
"tag_names": [
    "pod-console-logging",
    "virtual"
],
```

Incorporating a random number of tags per machine into a neat table is beyond the scope of this particular post, but we can show the first tag in the table rows:

```nohighlight
maas admin machines read | jq -r '(["HOSTNAME","STATUS", "OWNER", "SYSTEM-ID",
"FIRST TAG"] | (.,map(length*"-"))), (.[] | [.hostname, .status_name,
.owner // "-",.system_id,.tag_names[0] // "-"]) | @tsv' | column -t
```

Where we would use .json-key-name for a non-nested value, we need only use .json-key-name[0] to refer to the first element of the nested array. Doing this produces the following result:

```nohighlight
HOSTNAME      STATUS     OWNER  SYSTEM-ID  FIRST                TAG
--------      ------     -----  ---------  ---------            
lxd-vm-1      Deployed   admin  r8d6yp     pod-console-logging  
lxd-vm-2      Allocated  admin  tfftrx     pod-console-logging  
lxd-vm-3      Ready      -      grwpwc     pod-console-logging  
lxd-vm-4      Deployed   admin  6s8dt4     pod-console-logging  
lxd-vm-5      Allocated  admin  pyebgm     pod-console-logging  
lxd-vm-6      New        -      ebnww6     pod-console-logging  
libvirt-vm-1  Ready      -      m7ffsg     pod-console-logging  
libvirt-vm-2  Ready      -      kpawad     pod-console-logging  
libvirt-vm-3  Ready      -      r44hr6     pod-console-logging  
libvirt-vm-4  Ready      -      s3sdkw     pod-console-logging  
libvirt-vm-5  Ready      -      48dg8m     pod-console-logging  
libvirt-vm-6  Deployed   admin  bacx77     pod-console-logging
```

That's almost right, but notice that the heading separates on spaces between words. Let's try a better way, with an underscore:

```nohighlight
maas admin machines read | jq -r '(["HOSTNAME","STATUS", "OWNER", "SYSTEM-ID",
"FIRST_TAG"] | (.,map(length*"-"))), (.[] | [.hostname, .status_name,
.owner // "-",.system_id,.tag_names[0] // "-"]) | @tsv' | column -t
```

This version of the command produces the expected output:

```nohighlight
HOSTNAME      STATUS     OWNER  SYSTEM-ID  FIRST_TAG
--------      ------     -----  ---------  ---------
lxd-vm-1      Deployed   admin  r8d6yp     pod-console-logging
lxd-vm-2      Allocated  admin  tfftrx     pod-console-logging
lxd-vm-3      Ready      -      grwpwc     pod-console-logging
lxd-vm-4      Deployed   admin  6s8dt4     pod-console-logging
lxd-vm-5      Allocated  admin  pyebgm     pod-console-logging
lxd-vm-6      New        -      ebnww6     pod-console-logging
libvirt-vm-1  Ready      -      m7ffsg     pod-console-logging
libvirt-vm-2  Ready      -      kpawad     pod-console-logging
libvirt-vm-3  Ready      -      r44hr6     pod-console-logging
libvirt-vm-4  Ready      -      s3sdkw     pod-console-logging
libvirt-vm-5  Ready      -      48dg8m     pod-console-logging
libvirt-vm-6  Deployed   admin  bacx77     pod-console-logging
```

** Nested keys

These aren't all the routine key-value pairs we want in the table, though. It would also be nice to print the pool to which each machine is assigned. Just asking for .pool as a single key-value pair:

```nohighlight
maas admin machines read | jq -r '(["HOSTNAME","STATUS", "OWNER", "SYSTEM-ID",
"FIRST_TAG","POOL"] | (.,map(length*"-"))), (.[] | [.hostname, .status_name,
.owner // "-",.system_id,.tag_names[0] // "-", .pool]) | @tsv' | column -t
```

produces an error:

```nohighlight
jq: error (at &lt;stdin&gt;:5639): object ({"name":"de...") is not valid in a csv row
```

Looking at the JSON output, we see that .pool is a nested key, not a key-value pair:

```nohighlight
"pool": {
    "name": "default",
    "description": "Default pool",
    "id": 0,
    "resource_uri": "/MAAS/api/2.0/resourcepool/0/"
},
```

What we really want is the pool name, so we need to add one level of indirection to that particular key to reach the actual key-value pair, like this:

```nohighlight
maas admin machines read | jq -r '(["HOSTNAME","STATUS", "OWNER", "SYSTEM-ID",
"FIRST_TAG","POOL"] | (.,map(length*"-"))), (.[] | [.hostname, .status_name,
.owner // "-",.system_id,.tag_names[0] // "-", .pool.name]) | @tsv' | column -t
```

which gives us what we want:

```nohighlight
HOSTNAME      STATUS     OWNER  SYSTEM-ID  FIRST_TAG            POOL
--------      ------     -----  ---------  ---------            ----
lxd-vm-1      Deployed   admin  r8d6yp     pod-console-logging  default
lxd-vm-2      Allocated  admin  tfftrx     pod-console-logging  default
lxd-vm-3      Ready      -      grwpwc     pod-console-logging  default
lxd-vm-4      Deployed   admin  6s8dt4     pod-console-logging  default
lxd-vm-5      Allocated  admin  pyebgm     pod-console-logging  default
lxd-vm-6      New        -      ebnww6     pod-console-logging  default
libvirt-vm-1  Ready      -      m7ffsg     pod-console-logging  default
libvirt-vm-2  Ready      -      kpawad     pod-console-logging  default
libvirt-vm-3  Ready      -      r44hr6     pod-console-logging  default
libvirt-vm-4  Ready      -      s3sdkw     pod-console-logging  default
libvirt-vm-5  Ready      -      48dg8m     pod-console-logging  default
libvirt-vm-6  Deployed   admin  bacx77     pod-console-logging  default
```

It's also useful to list the VLAN and fabric names in the output table. Looking at the JSON again, these values present like this:

```nohighlight
"boot_interface": {
	    "vlan": {
		"vid": 0,
		"mtu": 1500,
		"dhcp_on": true,
		"external_dhcp": null,
		"relay_vlan": null,
		"secondary_rack": null,
		"name": "untagged",
		"id": 5001,
		"fabric_id": 1,
		"space": "undefined",
		"fabric": "fabric-1",
		"primary_rack": "wnmkpn",
		"resource_uri": "/MAAS/api/2.0/vlans/5001/"
	     },
```

This means they are doubly-nested. No problem; just use double indirection (two levels of . separators) to retrieve them:

```nohighlight
maas admin machines read | jq -r '(["HOSTNAME","SYSID","POWER","STATUS","OWNER", 
"TAGS", "POOL", "VLAN","FABRIC"] | (., map(length*"-"))), (.[] | [.hostname, 
.system_id, .power_state, .status_name, .owner // "-", .tag_names[0] // "-", 
.pool.name, .boot_interface.vlan.name, .boot_interface.vlan.fabric]) 
| @tsv' | column -t
```

The modified command yields the desired results:

```nohighlight
HOSTNAME      SYSID   POWER  STATUS     OWNER  TAGS                 POOL     VLAN      FABRIC
--------      -----   -----  ------     -----  ----                 ----     ----      ------
lxd-vm-1      r8d6yp  off    Deployed   admin  pod-console-logging  default  untagged  fabric-1
lxd-vm-2      tfftrx  off    Allocated  admin  pod-console-logging  default  untagged  fabric-1
lxd-vm-3      grwpwc  off    Ready      -      pod-console-logging  default  untagged  fabric-1
lxd-vm-4      6s8dt4  off    Deployed   admin  pod-console-logging  default  untagged  fabric-1
lxd-vm-5      pyebgm  off    Allocated  admin  pod-console-logging  default  untagged  fabric-1
lxd-vm-6      ebnww6  off    New        -      pod-console-logging  default  untagged  fabric-1
libvirt-vm-1  m7ffsg  off    Ready      -      pod-console-logging  default  untagged  fabric-1
libvirt-vm-2  kpawad  off    Ready      -      pod-console-logging  default  untagged  fabric-1
libvirt-vm-3  r44hr6  error  Ready      -      pod-console-logging  default  untagged  fabric-1
libvirt-vm-4  s3sdkw  off    Ready      -      pod-console-logging  default  untagged  fabric-1
libvirt-vm-5  48dg8m  off    Ready      -      pod-console-logging  default  untagged  fabric-1
libvirt-vm-6  bacx77  on     Deployed   admin  pod-console-logging  default  untagged  fabric-1
```

There's just one more (deeply nested) value we want to retrieve, and that's the fully-qualified subnet address in CIDR form. That's a little trickier, because it's buried in JSON like this:

```nohighlight
"boot_interface": {
     "vlan": {
	 "vid": 0,
	 "mtu": 1500,
	 "dhcp_on": true,
	 ...
	 "resource_uri": "/MAAS/api/2.0/vlans/5001/"
     },
     "parents": [],
     "product": null,
     ...
     "link_connected": true,
     "type": "physical",
     "links": [
	 {
	     "id": 79,
	     "mode": "auto",
	     "ip_address": "10.124.141.4",
	     "subnet": {
		 "name": "10.124.141.0/24",
```

So the value we want is in the nested key boot_interface, in a nested array links[], which contains the doubly-nested key subnet.name. We can finish our basic CLI machine list — the one we started with — by adding this complex formulation to the command:

```nohighlight
maas admin machines read | jq -r '(["HOSTNAME","SYSID","POWER","STATUS",
"OWNER", "TAGS", "POOL", "VLAN","FABRIC","SUBNET"] | (., map(length*"-"))),
(.[] | [.hostname, .system_id, .power_state, .status_name, .owner // "-", 
.tag_names[0] // "-", .pool.name,
.boot_interface.vlan.name, .boot_interface.vlan.fabric,
.boot_interface.links[0].subnet.name]) | @tsv' | column -t
```

Sure enough, this command gives us the same table we had at the beginning of this post:

```nohighlight
HOSTNAME      SYSID   POWER  STATUS     OWNER  TAGS                 POOL     VLAN      FABRIC    SUBNET
--------      -----   -----  ------     -----  ----                 ----     ----      ------    ------
lxd-vm-1      r8d6yp  off    Deployed   admin  pod-console-logging  default  untagged  fabric-1  10.124.141.0/24
lxd-vm-2      tfftrx  off    Allocated  admin  pod-console-logging  default  untagged  fabric-1  10.124.141.0/24
lxd-vm-3      grwpwc  off    Ready      -      pod-console-logging  default  untagged  fabric-1  10.124.141.0/24
lxd-vm-4      6s8dt4  off    Deployed   admin  pod-console-logging  default  untagged  fabric-1  10.124.141.0/24
lxd-vm-5      pyebgm  off    Allocated  admin  pod-console-logging  default  untagged  fabric-1  10.124.141.0/24
lxd-vm-6      ebnww6  off    New        -      pod-console-logging  default  untagged  fabric-1  
libvirt-vm-1  m7ffsg  off    Ready      -      pod-console-logging  default  untagged  fabric-1  10.124.141.0/24
libvirt-vm-2  kpawad  off    Ready      -      pod-console-logging  default  untagged  fabric-1  10.124.141.0/24
libvirt-vm-3  r44hr6  error  Ready      -      pod-console-logging  default  untagged  fabric-1  10.124.141.0/24
libvirt-vm-4  s3sdkw  off    Ready      -      pod-console-logging  default  untagged  fabric-1  10.124.141.0/24
libvirt-vm-5  48dg8m  off    Ready      -      pod-console-logging  default  untagged  fabric-1  10.124.141.0/24
libvirt-vm-6  bacx77  on     Deployed   admin  pod-console-logging  default  untagged  fabric-1  10.124.141.0/24
```

** Chaining Ubuntu CLI commands

Although the machine list above looks fairly neat, it's actually not sorted by hostname, exactly. To accomplish this, we'd need to add a couple of Ubuntu CLI commands to the mix. Sorting on hostname means we want to sort on field 1 of the current command's output. We can try just feeding that to sort like this:

```nohighlight
maas admin machines read | jq -r '(["HOSTNAME","SYSID","POWER","STATUS", "OWNER", 
"TAGS", "POOL", "VLAN","FABRIC","SUBNET"] | (., map(length*"-"))), (.[] | 
[.hostname, .system_id, .power_state, .status_name, .owner // "-", 
.tag_names[0] // "-", .pool.name, .boot_interface.vlan.name, 
.boot_interface.vlan.fabric, .boot_interface.links[0].subnet.name]) 
| @tsv' | column -t | sort -k 1
```

This command does indeed sort by hostname:


```nohighlight
--------      -----   -----  ------     -----  ----                 ----     ----      ------    ------
HOSTNAME      SYSID   POWER  STATUS     OWNER  TAGS                 POOL     VLAN      FABRIC    SUBNET
libvirt-vm-1  m7ffsg  off    Ready      -      pod-console-logging  default  untagged  fabric-1  10.124.141.0/24
libvirt-vm-2  kpawad  off    Ready      -      pod-console-logging  default  untagged  fabric-1  10.124.141.0/24
libvirt-vm-3  r44hr6  error  Ready      -      pod-console-logging  default  untagged  fabric-1  10.124.141.0/24
libvirt-vm-4  s3sdkw  off    Ready      -      pod-console-logging  default  untagged  fabric-1  10.124.141.0/24
libvirt-vm-5  48dg8m  off    Ready      -      pod-console-logging  default  untagged  fabric-1  10.124.141.0/24
libvirt-vm-6  bacx77  on     Deployed   admin  pod-console-logging  default  untagged  fabric-1  10.124.141.0/24
lxd-vm-1      r8d6yp  off    Deployed   admin  pod-console-logging  default  untagged  fabric-1  10.124.141.0/24
lxd-vm-2      tfftrx  off    Allocated  admin  pod-console-logging  default  untagged  fabric-1  10.124.141.0/24
lxd-vm-3      grwpwc  off    Ready      -      pod-console-logging  default  untagged  fabric-1  10.124.141.0/24
lxd-vm-4      6s8dt4  off    Deployed   admin  pod-console-logging  default  untagged  fabric-1  10.124.141.0/24
lxd-vm-5      pyebgm  off    Allocated  admin  pod-console-logging  default  untagged  fabric-1  10.124.141.0/24
lxd-vm-6      ebnww6  off    New        -      pod-console-logging  default  untagged  fabric-1
```

but it has the unintended side-effect of sorting the header lines into the output. There are probably at least a dozen Ubuntu CLI solutions for this, so we'll just pick one of the most elegant here, using awk:

```nohighlight
maas admin machines read | jq -r '(["HOSTNAME","SYSID","POWER","STATUS","OWNER", 
"TAGS", "POOL", "VLAN","FABRIC","SUBNET"] | (., map(length*"-"))),(.[] | 
[.hostname, .system_id, .power_state, .status_name, .owner // "-", 
.tag_names[0] // "-", .pool.name, .boot_interface.vlan.name, 
.boot_interface.vlan.fabric,.boot_interface.links[0].subnet.name]) 
| @tsv' | column -t | awk 'NR&lt;3{print $0;next}{print $0| "sort -k 1"}'
```

This command gives us the desired output:

```nohighlight
HOSTNAME      SYSID   POWER  STATUS     OWNER  TAGS                 POOL     VLAN      FABRIC    SUBNET
--------      -----   -----  ------     -----  ----                 ----     ----      ------    ------
libvirt-vm-1  m7ffsg  off    Ready      -      pod-console-logging  default  untagged  fabric-1  10.124.141.0/24
libvirt-vm-2  kpawad  off    Ready      -      pod-console-logging  default  untagged  fabric-1  10.124.141.0/24
libvirt-vm-3  r44hr6  error  Ready      -      pod-console-logging  default  untagged  fabric-1  10.124.141.0/24
libvirt-vm-4  s3sdkw  off    Ready      -      pod-console-logging  default  untagged  fabric-1  10.124.141.0/24
libvirt-vm-5  48dg8m  off    Ready      -      pod-console-logging  default  untagged  fabric-1  10.124.141.0/24
libvirt-vm-6  bacx77  on     Deployed   admin  pod-console-logging  default  untagged  fabric-1  10.124.141.0/24
lxd-vm-1      r8d6yp  off    Deployed   admin  pod-console-logging  default  untagged  fabric-1  10.124.141.0/24
lxd-vm-2      tfftrx  off    Allocated  admin  pod-console-logging  default  untagged  fabric-1  10.124.141.0/24
lxd-vm-3      grwpwc  off    Ready      -      pod-console-logging  default  untagged  fabric-1  10.124.141.0/24
lxd-vm-4      6s8dt4  off    Deployed   admin  pod-console-logging  default  untagged  fabric-1  10.124.141.0/24
lxd-vm-5      pyebgm  off    Allocated  admin  pod-console-logging  default  untagged  fabric-1  10.124.141.0/24
lxd-vm-6      ebnww6  off    New        -      pod-console-logging  default  untagged  fabric-1
```

Note that by changing the numerical “-k” argument to “sort,” you can change which field controls the sort:

```nohighlight
maas admin machines read | jq -r '(["HOSTNAME","SYSID","POWER","STATUS","OWNER", 
"TAGS", "POOL", "VLAN","FABRIC","SUBNET"] | (., map(length*"-"))),(.[] | 
[.hostname, .system_id, .power_state, .status_name, .owner // "-", 
.tag_names[0] // "-", .pool.name, .boot_interface.vlan.name, 
.boot_interface.vlan.fabric,.boot_interface.links[0].subnet.name]) 
| @tsv' | column -t | awk 'NR&lt;3{print $0;next}{print $0| "sort -k 4"}'
```

This command sorts by machine state, which is the fourth field:

```nohighlight
HOSTNAME      SYSID   POWER  STATUS     OWNER  TAGS                 POOL     VLAN      FABRIC    SUBNET
--------      -----   -----  ------     -----  ----                 ----     ----      ------    ------
lxd-vm-2      tfftrx  off    Allocated  admin  pod-console-logging  default  untagged  fabric-1  10.124.141.0/24
lxd-vm-5      pyebgm  off    Allocated  admin  pod-console-logging  default  untagged  fabric-1  10.124.141.0/24
libvirt-vm-6  bacx77  on     Deployed   admin  pod-console-logging  default  untagged  fabric-1  10.124.141.0/24
lxd-vm-1      r8d6yp  off    Deployed   admin  pod-console-logging  default  untagged  fabric-1  10.124.141.0/24
lxd-vm-4      6s8dt4  off    Deployed   admin  pod-console-logging  default  untagged  fabric-1  10.124.141.0/24
lxd-vm-6      ebnww6  off    New        -      pod-console-logging  default  untagged  fabric-1  
libvirt-vm-1  m7ffsg  off    Ready      -      pod-console-logging  default  untagged  fabric-1  10.124.141.0/24
libvirt-vm-2  kpawad  off    Ready      -      pod-console-logging  default  untagged  fabric-1  10.124.141.0/24
libvirt-vm-4  s3sdkw  off    Ready      -      pod-console-logging  default  untagged  fabric-1  10.124.141.0/24
libvirt-vm-5  48dg8m  off    Ready      -      pod-console-logging  default  untagged  fabric-1  10.124.141.0/24
lxd-vm-3      grwpwc  off    Ready      -      pod-console-logging  default  untagged  fabric-1  10.124.141.0/24
libvirt-vm-3  r44hr6  error  Ready      -      pod-console-logging  default  untagged  fabric-1  10.124.141.0/24
```

** Summary

At this point, it should be clear that jq is a relatively simple, powerful tool for formatting output from the MAAS CLI. You should also remember that, like any Ubuntu CLI command, jq simply outputs text — so anything you can do with text output, you can do with the output from jq.

* Hardware test scripts reference
** Available test scripts reference

The following hardware testing scripts are available during machine commissioning:

<table style="width:100%;">
<thead>
<tr class="header">
<th align="center">Name</th>
<th align="center">Category Tags</th>
<th align="center">Description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center"><strong>smartctl-short</strong></td>
<td align="center">storage</td>
<td align="center">Run the short SMART self-test and validate SMART health on all drives in parallel</td>
</tr>
<tr class="even">
<td align="center"><strong>smartctl-long</strong></td>
<td align="center">storage</td>
<td align="center">Run the long SMART self-test and validate SMART health on all drives in parallel</td>
</tr>
<tr class="odd">
<td align="center"><strong>smartctl-conveyance</strong></td>
<td align="center">storage</td>
<td align="center">Run the conveyance SMART self-test and validate SMART health on all drives in parallel</td>
</tr>
<tr class="even">
<td align="center"><strong>memtester</strong></td>
<td align="center">memory</td>
<td align="center">Run memtester against all available userspace memory.</td>
</tr>
<tr class="odd">
<td align="center"><strong>internet-connectivity</strong></td>
<td align="center">network, internet, node</td>
<td align="center">Check if the system has access to the internet.</td>
</tr>
<tr class="even">
<td align="center"><strong>stress-ng-cpu-long</strong></td>
<td align="center">cpu</td>
<td align="center">Run stress-ng memory tests for 12 hours.</td>
</tr>
<tr class="odd">
<td align="center"><strong>stress-ng-cpu-short</strong></td>
<td align="center">cpu</td>
<td align="center">Run stress-ng memory tests for 5 minutes.</td>
</tr>
<tr class="even">
<td align="center"><strong>stress-ng-memory-long</strong></td>
<td align="center">memory</td>
<td align="center">Run stress-ng memory tests for 12 hours.</td>
</tr>
<tr class="odd">
<td align="center"><strong>stress-ng-memory-short</strong></td>
<td align="center">memory</td>
<td align="center">Run stress-ng memory tests for 5 minutes.</td>
</tr>
<tr class="even">
<td align="center"><strong>ntp</strong></td>
<td align="center">network, ntp, node</td>
<td align="center">Run ntp clock set to verify NTP connectivity.</td>
</tr>
<tr class="odd">
<td align="center"><strong>badblocks</strong></td>
<td align="center">storage</td>
<td align="center">Run badblocks on disk in read-only mode.</td>
</tr>
<tr class="even">
<td align="center"><strong>badblocks-destructive</strong></td>
<td align="center">destructive, storage</td>
<td align="center">Run badblocks on a disk in read/write destructive mode.</td>
</tr>
<tr class="odd">
<td align="center"><strong>7z</strong></td>
<td align="center">cpu</td>
<td align="center">Run <em>7zip</em> CPU benchmarking.</td>
</tr>
<tr class="even">
<td align="center"><strong>fio</strong></td>
<td align="center">storage, destructive</td>
<td align="center">Run Fio benchmarking against selected storage devices.</td>
</tr>
</tbody>
</table>

After either commissioning, testing, or installation has started, MAAS reports in real-time which script is running.

You can access the verbatim output from any test by selecting a machine, selecting the 'Hardware tests' page and clicking on the 'Log view' link in the 'Results' column for the specific test.

** Hardware test script example: CPU stress test

Here's a simple example of a functional Bash test script, replicating part of the stress-ng script bundled with MAAS:

```nohighlight
#!/bin/bash -e
# --- Start MAAS 1.0 script metadata ---
# name: stress-ng-cpu-test
# title: CPU validation
# description: Run stress-ng memory tests for 5 minutes.
# script_type: test
# hardware_type: cpu
# packages: {apt: stress-ng}
# tags: cpu
# timeout: 00:05:00
# --- End MAAS 1.0 script metadata ---

sudo -n stress-ng --matrix 0 --ignite-cpu --log-brief --metrics-brief --times \
    --tz --verify --timeout 2m
```

This Bash script contains comment-delineated metadata, which configures the script environment and installs any dependencies.  There is also a single line that runs **stress-ng** (a CPU stress-test utility) with various arguments.
* High availability
High availability is built into MAAS: region and rack controllers balance the load and execute failover as part of normal operations. This article will help you understand how to take advantage of these built-in features.

** How to put MAAS in HA mode

You only need to [install multiple rack controllers](/t/how-to-adjust-your-controllers/5172#heading--install-a-rack-controller) to achieve real high availability.  Once that's done, you automatically gain highly-available BMC control and highly-available DHCP.  MAAS is constantly trying to answer three questions:

- How many racks is each region managing?
- How many connections does a given rack controller have?
- How many regions -- and region "worker processes" are running right now?

With just one rack, most of the logic can't function.  On the other hand, when you have multiple racks (and especially multiple regions), MAAS will continuously balance the load.

Every time a rack controller connects to a region controller to do something, MAAS checks whether racks and regions are balanced.  If the ratio for one rack-region connection is above a moderate threshold, compared to other connections, MAAS will re-balance.  This activity includes balancing not only discrete region controllers, but also re-distributing connections so that no single worker process has an uneven share of the load.

Rebalancing is also done at various other opportune times.  For example, if a network change happens (like toggling DHCP or changing a VLAN), MAAS will also re-balance the load.  And MAAS can maintain primary and secondary rack designations, so that faster, more nuanced load-balancing can occur.

*** How to enable highly-available BMC

You can also enable HA for BMC control (node power cycling) just by adding a second rack controller. MAAS will automatically identify which rack controller is responsible for a BMC, continuously balancing the connections.

*** How to enable highly-available DHCP services

You can enable highly-availalbe DHCP services by using MAAS-managed DHCP, and adding rack controllers.  This DHCP HA affects the way MAAS manages nodes, including enlistment, commissioning and deployment. It enables primary and secondary DHCP instances to serve the same VLAN. This VLAN replicates all lease information is between rack controllers, so there's a bit of performance boost for large networks.

MAAS DHCP automatically creates failover peers, using mostly standard parameters:

```nohighlight
failover peer "failover-partner" {
     primary;
     address dhcp-primary.example.com;
     peer address dhcp-secondary.example.com;
     max-response-delay 60;
     max-unacked-updates 10;
     mclt 3600;
     split 255;
     load balance max seconds 3;
}
failover peer "failover-partner" {
     secondary;
     address dhcp-secondary.example.com;
     peer address dhcp-primary.example.com;
     max-response-delay 60;
     max-unacked-updates 10;
     load balance max seconds 3;
}
```
Note that the only difference from a standard 50/50 split (`split 128`) is that the primary DHCP server answers any requests that it can (`split 255`), within the maximum response delay of 60 seconds and an unacknowledged update count of 10.  In this sense, highly-available MAAS DHCP fails over only when absolutely necessary.

If you are enabling DHCP for the first time after adding a second rack controller, please read [Enabling DHCP](/t/how-to-enable-dhcp/5132#heading--enabling-dhcp).  On the other hand, if you have already enabled DHCP on your initial rack controller, you'll need to reconfigure DHCP to get optimum results.

[tabs]
[tab version="v3.4 Snap,v3.4 Packages,v3.3 Snap,v3.3 Packages,v3.2 Snap,v3.2 Packages,v3.1 Snap,v3.1 Packages,v3.0 Snap,v3.0 Packages,v2.9 Snap,v2.9 Packages" view="UI"]
To reconfigure DHCP after adding a new rack controller:

1. Select *Subnets*.

2. Select the appropriate VLAN.

3. Choose *Reconfigure DHCP*.

4. Confirm that you can see the second rack controller under *Secondary controller*.

5. Select *Reconfigure DHCP*.
[/tab]
[tab version="v3.4 Snap,v3.4 Packages,v3.3 Snap,v3.3 Packages,v3.2 Snap,v3.2 Packages,v3.1 Snap,v3.1 Packages,v3.0 Snap,v3.0 Packages,v2.9 Snap,v2.9 Packages" view="CLI"]
To reconfigure DHCP after adding a new rack controller, use the following sequence of commands:

```
vid=$(maas maas subnets read | jq -r '.[] | select(.cidr == "10.0.0.0/24") | .vlan.vid')
fabric_name=$(maas maas subnets read | jq -r '.[] | select(.cidr == "10.0.0.0/24") | .vlan.fabric')
query=".[] | select(.name == \"$fabric_name\") | .id"
fabric_id=$(maas maas fabrics read | jq "$query")
maas maas ipranges create type=reserved start_ip=10.0.0.3 end_ip=10.0.0.49
maas maas ipranges create type=dynamic start_ip=10.0.0.50 end_ip=10.0.0.99
maas maas vlan update ${fabric_id} ${vid} primary_rack=$(hostname) dhcp_on=true
```

Be sure to substitute the sample values for those of your own environment.
[/tab]
[/tabs]

*** How to configure multiple region endpoints

[tabs]
[tab version="v3.4 Snap,v3.4 Packages,v3.3 Snap,v3.2 Snap,v3.1 Snap,v3.0 Snap,v2.9 Snap"]
MAAS will automatically discover and track all reachable region controllers in a single cluster of rack controllers  It will also attempt to automatically connect to them if the one in use becomes inaccessible.  Administrators can alternatively specify multiple region-controller endpoints for a single rack controller by adding entries to `/var/snap/maas/current/rackd.conf`.  For example:
[/tab]
[tab version="v3.3 Packages,v3.2 Packages,v3.1 Packages,v3.0 Packages,v2.9 Packages"]
MAAS will automatically discover and track all reachable region controllers in a single cluster of rack controllers  It will also attempt to automatically connect to them if the one in use becomes inaccessible.  Administrators can alternatively specify multiple region-controller endpoints for a single rack controller by adding entries to `/etc/maas/rackd.conf`.  For example:
[/tab]
[/tabs]
    .
    .
    .
    maas_url:
      - http://<ip 1>:<port>/MAAS/
      - http://<ip 2>:<port>/MAAS/
    .
    .
    .

The setup of highly-available DHCP is now complete.  Note that, for HA purposes, DHCP provisioning will take into account multiple DNS services when there is more than one region controller on a single region.

** How to make region controllers highly available

Implementing highly-available region control is possible when you learn:

- [How to enable highly-available PostgreSQL](#heading--postgresql-ha)
- [How to enable highly-available API services](#heading--secondary-api-servers)
- [How to set up load balancing with HAProxy (optional)](#heading--load-balancing-with-haproxy-optional)

Load balancing is optional, but is highly recommended.

*** How to enable highly-available PostgreSQL

MAAS stores all state information in the PostgreSQL database. It is therefore recommended to run it in HA mode. Configuring HA for PostgreSQL is external to MAAS. You will, therefore, need to study the [PostgreSQL documentation](https://www.postgresql.org/docs/9.5/static/high-availability.html)`↗` and implement the variant of HA that makes you feel most comfortable.

Each region controller uses up to 40 connections to PostgreSQL in high load situations. Running two region controllers requires no modifications to the `max_connections` in `postgresql.conf`. More than two region controllers require that `max_connections` be adjusted to add 40 more connections per added region controller.

*** How to enable highly-available API services

[tabs]
[tab version="v3.4 Snap,v3.4 Packages,v3.3 Snap"]
Setting up high-availability using snaps is relatively easy:

1.  Set up PostgreSQL for high-availability as [explained above](#heading--postgresql-ha). PostgreSQL should run outside of the snap.
2.  [Install](/t/how-to-install-maas/5128#heading--install-from-snap) the MAAS snap on each machine you intend to use as a rack or region controller. You'll need the MAAS shared secret, located here, `/var/snap/maas/common/maas/secret`, on the first region controller you set up.
3.  [Initialise the snap](/t/how-to-install-maas/5128#heading--initialisation) as a `rack` or `region` controller.

Note that if you intend to use a machine as a region controller, you'll need to tell MAAS how to access your PostgreSQL database host with the following arguments:

- `--database-host DATABASE_HOST`
- `--database-name DATABASE_NAME`
- `--database-user DATABASE_USER`
- `--database-pass DATABASE_PASS`
[/tab]
[tab version="v3.3 Packages"]
Please see [Region controllers](/t/how-to-manage-controllers/5172) and [Multiple region endpoints](#heading--multiple-region-endpoints) for more information about how to install and configure rack controllers for multiple region controllers.
[/tab]
[tab version="v3.2 Snap"]
Setting up high-availability using snaps is relatively easy:

1.  Set up PostgreSQL for high-availability as [explained above](#heading--postgresql-ha). PostgreSQL should run outside of the snap.
2.  [Install](/t/how-to-install-maas/5128#heading--install-from-snap) the MAAS snap on each machine you intend to use as a rack or region controller. You'll need the MAAS shared secret, located here, `/var/snap/maas/common/maas/secret`, on the first region controller you set up.
3.  [Initialise the snap](/t/how-to-install-maas/5128#heading--initialisation) as a `rack` or `region` controller.

Note that if you intend to use a machine as a region controller, you'll need to tell MAAS how to access your PostgreSQL database host with the following arguments:

- `--database-host DATABASE_HOST`
- `--database-name DATABASE_NAME`
- `--database-user DATABASE_USER`
- `--database-pass DATABASE_PASS`
[/tab]
[tab version="v3.2 Packages"]
Please see [Region controllers](/t/how-to-manage-controllers/5172) and [Multiple region endpoints](#heading--multiple-region-endpoints) for more information about how to install and configure rack controllers for multiple region controllers.
[/tab]
[tab version="v3.1 Snap"]
Setting up high-availability using snaps is relatively easy:

1.  Set up PostgreSQL for high-availability as [explained above](#heading--postgresql-ha). PostgreSQL should run outside of the snap.
2.  [Install](/t/how-to-install-maas/5128#heading--install-from-snap) the MAAS snap on each machine you intend to use as a rack or region controller. You'll need the MAAS shared secret, located here, `/var/snap/maas/common/maas/secret`, on the first region controller you set up.
3.  [Initialise the snap](/t/how-to-install-maas/5128#heading--initialisation) as a `rack` or `region` controller.

Note that if you intend to use a machine as a region controller, you'll need to tell MAAS how to access your PostgreSQL database host with the following arguments:

- `--database-host DATABASE_HOST`
- `--database-name DATABASE_NAME`
- `--database-user DATABASE_USER`
- `--database-pass DATABASE_PASS`
[/tab]
[tab version="v3.1 Packages"]
Please see [Region controllers](/t/how-to-manage-controllers/5172) and [Multiple region endpoints](#heading--multiple-region-endpoints) for more information about how to install and configure rack controllers for multiple region controllers.
[/tab]
[tab version="v3.0 Snap"]
Setting up high-availability using snaps is relatively easy:

1.  Set up PostgreSQL for high-availability as [explained above](#heading--postgresql-ha). PostgreSQL should run outside of the snap.
2.  [Install](/t/how-to-install-maas/5128#heading--install-from-snap) the MAAS snap on each machine you intend to use as a rack or region controller. You'll need the MAAS shared secret, located here, `/var/snap/maas/common/maas/secret`, on the first region controller you set up.
3.  [Initialise the snap](/t/how-to-install-maas/5128#heading--initialisation) as a `rack` or `region` controller.

Note that if you intend to use a machine as a region controller, you'll need to tell MAAS how to access your PostgreSQL database host with the following arguments:

- `--database-host DATABASE_HOST`
- `--database-name DATABASE_NAME`
- `--database-user DATABASE_USER`
- `--database-pass DATABASE_PASS`
[/tab]
[tab version="v3.0 Packages"]
Please see [Region controllers](/t/how-to-manage-controllers/5172) and [Multiple region endpoints](#heading--multiple-region-endpoints) for more information about how to install and configure rack controllers for multiple region controllers.
[/tab]
[tab version="v2.9 Snap"]
Setting up high-availability using snaps is relatively easy:

1.  Set up PostgreSQL for high-availability as [explained above](#heading--postgresql-ha). PostgreSQL should run outside of the snap.
2.  [Install](/t/how-to-install-maas/5128#heading--install-from-snap) the MAAS snap on each machine you intend to use as a rack or region controller. You'll need the MAAS shared secret, located here, `/var/snap/maas/common/maas/secret`, on the first region controller you set up.
3.  [Initialise the snap](/t/how-to-install-maas/5128#heading--initialisation) as a `rack` or `region` controller.

Note that if you intend to use a machine as a region controller, you'll need to tell MAAS how to access your PostgreSQL database host with the following arguments:

- `--database-host DATABASE_HOST`
- `--database-name DATABASE_NAME`
- `--database-user DATABASE_USER`
- `--database-pass DATABASE_PASS`
[/tab]
[tab version="v2.9 Packages"]
Please see [Region controllers](/t/how-to-manage-controllers/5172) and [Multiple region endpoints](#heading--multiple-region-endpoints) for more information about how to install and configure rack controllers for multiple region controllers.
[/tab]
[/tabs]

*** How to enable load balancing for API services

You can add load balancing with [HAProxy](http://www.haproxy.org/)`↗` load-balancing software to support multiple API servers. In this setup, HAProxy provides access to the MAAS web UI and API.

[note]
If you happen to have Apache running on the same server where you intend to install HAProxy, you will need to stop and disable `apache2`, because HAProxy binds to port 80.
[/note]

**** How to install HAProxy

``` bash
sudo apt install haproxy
```

**** How to configure HAProxy

Configure each API server's load balancer by copying the following into `/etc/haproxy/haproxy.cfg` (see the [upstream configuration manual (external link)](http://cbonte.github.io/haproxy-dconv/1.6/configuration.html)`↗` as a reference). Replace $PRIMARY_API_SERVER_IP and $SECONDARY_API_SERVER_IP with their respective IP addresses:

``` yaml
frontend maas
    bind    *:80
    retries 3
    option  redispatch
    option  http-server-close
    default_backend maas

backend maas
    timeout server 90s
    balance source
    hash-type consistent
    server localhost localhost:5240 check
    server maas-api-1 $PRIMARY_API_SERVER_IP:5240 check
    server maas-api-2 $SECONDARY_API_SERVER_IP:5240 check
```

where `maas-api-1` and `maas-api-2` are arbitrary server labels.

Now restart the load balancer to have these changes take effect:

``` bash
sudo systemctl restart haproxy
```

The configuration of region controller HA is now complete.

**The API server(s) must be now be referenced (e.g. web UI, MAAS CLI) using port 80 (as opposed to port 5240).**

** Move a rack controller from one MAAS instance to another

[tabs]
[tab version="v3.4 Snap,v3.4 Packages,v3.3 Snap,v3.3 Packages,v3.2 Snap,v3.2 Packages,v3.1 Snap,v3.1 Packages,v3.0 Snap,v3.0 Packages,v2.9 Snap,v2.9 Packages" view="UI"]
In effect, there is no such action as moving a rack controller, although you can delete a rack controller from one MAAS and reinstantiate the same controller (binary-wise) on another MAAS instance.  First, delete the rack controller.  In the "Controllers" tab in the UI, select the rack controller you with to delete, choose "Take action" and select "Delete."  You will be asked to confirm with a red button, entitled "Delete 1 controller."
[/tab]
[tab version="v3.4 Snap,v3.4 Packages,v3.3 Snap,v3.3 Packages,v3.2 Snap,v3.2 Packages,v3.1 Snap,v3.1 Packages,v3.0 Snap,v3.0 Packages,v2.9 Snap,v2.9 Packages" view="CLI"]
In effect, there is no such action as moving a rack controller, although you can delete a rack controller from one MAAS and reinstantiate the same controller (binary-wise) on another MAAS instance.  First, delete the rack controller, with the command:

```
maas $PROFILE rack-controller delete $SYSTEM_ID
```

where `$PROFILE` is your admin profile name, and `$SYSTEM_ID` can be found by examining the output of the command:

```
maas $PROFILE rack-controllers read
```

There is no confirmation step, so make sure you have the right rack controller before proceeding.
[/tab]
[/tabs]

Next, you must register a new rack controller, which is always done from the command line.

[tabs]
[tab version="v3.4 Snap,v3.4 Packages,v3.3 Snap,v3.3 Packages,v3.2 Snap,v3.2 Packages,v3.1 Snap,v3.1 Packages,v3.0 Snap,v3.0 Packages,v2.9 Snap,v2.9 Packages" view="UI"]
For this exercise, we're assuming you are using the already installed rack controller code that was previously running on the "from" MAAS instance.  All that's necessary is that you register a new rack controller with the "to" MAAS instance, like this:

```
sudo maas-rack register --url $MAAS_URL_OF_NEW_MAAS --secret $SECRET_FOR_NEW_MAAS
```

where the secret is found in `/var/lib/maas/secret`.

Note that in the UI, if you go to the "Controllers" tab and press the button entitled, "Add rack controller," at the top of the Controllers screen, MAAS will give you a complete command string, including the correct URL and secret values.  Simply cut and paste that string to move the rack controller, paying attention to whether you are using snap or package build modes.
[/tab]
[tab version="v3.4 Snap,v3.4 Packages,v3.3 Snap,v3.3 Packages,v3.2 Snap,v3.2 Packages,v3.1 Snap,v3.1 Packages,v3.0 Snap,v3.0 Packages,v2.9 Snap,v2.9 Packages" view="CLI"]
For this exercise, we're assuming you are using the already installed rack controller code that was previously running on the "from" MAAS instance.  All that's necessary is that you register a new rack controller with the "to" MAAS instance, like this:

```
sudo maas init rack --maas-url $MAAS_URL_OF_NEW_MAAS --secret $SECRET_FOR_NEW_MAAS
```

where the secret is found in `/var/snap/maas/common/maas/secret`.
[/tab]
[/tabs]

*** How to avoid the potential pitfalls of moving a rack controller

There are dangers associate with moving a rack controller -- dangers that may generate errors, get you into a non-working state, or cause you significant data loss.  These dangers are precipitated by one caveat and two potential mistakes:

- **Using the same system as a rack controller and a VM host:** While not forbidden or inherently dangerous, using the same machine as both a rack controller and a VM host may cause resource contention and poor performance.  If the resources on the system are not more than adequate to cover both tasks, you may see slowdowns (or even apparent "freeze" events) on the system.

- **Moving a rack controller from one version of MAAS to another:** MAAS rack controller software is an integral part of each version of MAAS.  If you delete a rack controller from, say, a 2.6 version of MAAS, and attempt to register that 2.6 version of the rack controller code to, say, a 2.9 version of MAAS, you may experience errors and potential data loss.  Using the above example, if you are running both a VM host and a rack controller for MAAS 2.6 on one system, and you suddenly decide to delete that rack controller from 2.6 and attempt to register the same code to a 2.9 MAAS, the VM host may fail or disappear.  This will possibly delete all the VMs you have created or connected to that VM host -- which may result in data loss.  This action is not supported.

- **Connecting one instance of a rack controller to two instances of MAAS, regardless of version:** Trying to connect a single rack controller to two different instances of MAAS can result in all sorts of unpredictable (and potentially catastrophic) behaviour.  It is not a supported configuration.

Take these warnings to heart.  It may seem like a faster approach to "bridge" your existing rack controllers from one MAAS to another -- or from one version of MAAS to another -- while they're running.  Ultimately, though, it will probably result in more work than just following the recommended approach.

* How MAAS works
When you [add a new machine](/t/how-to-make-machines-available/5160#heading--how-to-add-a-machine-manually) to MAAS, or elect to add a machine that MAAS has [enlisted](/t/how-to-deploy-physical-machines/6193#heading--about-enlistment), MAAS [commissions](/t/how-to-deploy-physical-machines/6193#heading--about-commissioning-machines) it for service and adds it to the pool.  At that point, the machine is ready for use. MAAS keeps things simple, marking machines as "New," "Commissioning," "Ready," and so on.

<details><summary>Tell me, quickly, about enlistment and commissioning.</summary>

There are two ways to add a machine to MAAS.  Assuming it's on the network and capable of PXE-booting, you can add it explicitly -- or MAAS can simply discover it when you turn it on.

Enlistment just means that MAAS discovers a machine when you turn it on, and presents it to the MAAS administrator, so that they can choose whether or not to commission it.  Machines that have only been enlisted will show up in the machine list as "New."

Commissioning means that MAAS has successfully booted the machine, scanned and recorded its resources, and prepared it for eventual deployment.  Machines that you explicitly add are automatically commissioned.  MAAS marks a successfully-commissioned machine as "Ready" in the machine list.

</details>

[tabs]
[tab version="v3.4 Snap,v3.4 Packages,v3.3 Snap,v3.3 Packages,v3.2 Snap,v3.2 Packages,v3.1 Snap,v3.1 Packages,v3.0 Snap,v3.0 Packages,v2.9 Snap,v2.9 Packages" view="UI"]
<a href="https://discourse.maas.io/uploads/default/original/1X/605019de31078dd70df72ff199d812de13a30d00.jpeg" target = "_blank"><img src="https://discourse.maas.io/uploads/default/original/1X/605019de31078dd70df72ff199d812de13a30d00.jpeg"></a>
[/tab]
[tab version="v3.4 Snap,v3.4 Packages,v3.3 Snap,v3.3 Packages,v3.2 Snap,v3.2 Packages,v3.1 Snap,v3.1 Packages,v3.0 Snap,v3.0 Packages,v2.9 Snap,v2.9 Packages" view="CLI"]
<a href="https://discourse.maas.io/uploads/default/original/1X/6aec9b567022216d80596411e689a14e1f594674.jpeg" target = "_blank"><img src="https://discourse.maas.io/uploads/default/original/1X/6aec9b567022216d80596411e689a14e1f594674.jpeg"></a>
[/tab]
[/tabs]

MAAS controls machines through IPMI (or another BMC). It can also manage machines through a converged chassis controller, such as Cisco UCS.  You can choose how you want to control power on your machines based on what is available.  MAAS overwrites the machine's disk space with your chosen, pre-cached OS images.

[note]
*The above comment about disk space bears repeating: MAAS will overwrite the disk space of all machines it enlists. All pool machines are under the control of MAAS; you should provision them using other methods.*
[/note]

MAAS users allocate machines for use when needed. The web UI also allows you to allocate machines manually, such as when you are reserving specific hardware for certain users. You can remotely access and customise the installed operating system via SSH.

[tabs]
[tab version="v3.4 Snap,v3.4 Packages,v3.3 Snap,v3.3 Packages,v3.2 Snap,v3.2 Packages,v3.1 Snap,v3.1 Packages,v3.0 Snap,v3.0 Packages,v2.9 Snap,v2.9 Packages" view="UI"]
<a href="https://discourse.maas.io/uploads/default/original/1X/8101d641c55d912cd66646bd99bbee9bb8f196ab.jpeg" target = "_blank"><img src="https://discourse.maas.io/uploads/default/original/1X/8101d641c55d912cd66646bd99bbee9bb8f196ab.jpeg"></a>
[/tab]
[tab version="v3.4 Snap,v3.4 Packages,v3.3 Snap,v3.3 Packages,v3.2 Snap,v3.2 Packages,v3.1 Snap,v3.1 Packages,v3.0 Snap,v3.0 Packages,v2.9 Snap,v2.9 Packages" view="CLI"]
<a href="https://discourse.maas.io/uploads/default/original/1X/ac3b251a916bb18a7e7e463d7fa3c57ef32628da.jpeg" target = "_blank"><img src="https://discourse.maas.io/uploads/default/original/1X/ac3b251a916bb18a7e7e463d7fa3c57ef32628da.jpeg"></a>

When allocating machines from the API/CLI, you can specify requirements ("constraints"). Common constraints are memory, CPU cores, connected networks, and assigned physical zone.
[/tab]
[/tabs]

An allocated MAAS machine is more flexible than a virtual instance in a cloud. You have complete control, including hardware drivers and root access. If you want to upgrade the BIOS, for example, you can allocate a machine to yourself and complete the upgrade.  Once you have completed the upgrade, you can send the machine back to the pool.

Note that [Juju](https://juju.is/docs/olm/maas)`↗` is designed to work with MAAS. MAAS becomes a back-end Juju resource pool with all functionality fully available. For instance, if Juju removes a machine, then MAAS will release that machine to the pool.  With Juju, MAAS can become an integral part of your data centre strategy and operations.

* How to annotate machines
Annotations are descriptive, searchable phrases that apply only to machines.  There are two types of annotations: notes (always present in any machine state), and dynamic annotations (only present in allocated or deployed states).  Annotations help you identify, characterise, and inform others about your machines.

[note]
Dynamic annotations are not available in MAAS version 2.9.
[/note]

** How to use notes

Notes persist throughout the life-cycle of a machine -- or until you change them.

*** How to work with notes in the MAAS UI

To work with notes for a given machine, using the MAAS UI:201

1. Select *Machines > Machine name > Configuration > Edit*.

2. Existing notes are displayed in the *Note* block.

2. Add notes to *Note*.

3. Alternatively, edit notes by changing the text in the *Note* block.

4. Delete any notes no longer needed by removing them from the *Note* block.

5. Be sure to *Save changes* to register your updates.

*** How to work with notes in the MAAS CLI

It's also possible to manage notes in the MAAS CLI.

**** How to identify your machines

The MAAS CLI refers to machines by their system ID, which can be obtained in this way:

```bash
maas $PROFILE machines read \
| jq -r '(["hostname","system_id"]
|(.,map(length*"-"))),(.[]|[.hostname,.system_id])
|@tsv' | column -t
```

For example:

```bash
maas admin machines read \
| jq -r '(["hostname","system_id"]
|(.,map(length*"-"))),(.[]|[.hostname,.system_id])
|@tsv' | column -t
```

Typical output might look something like this:

```nohighlight
hostname       system_id
--------       ---------
divine-stork   8b3ypp
casual-prawn   4end6r
```

**** How to manage machine notes in the MAAS CLI

Add a note to a given machine like this:

```bash
maas $PROFILE machine update $SYSTEM_ID description="$NOTE"
```

For example:

```bash
maas admin machine update ke3wc7 description="kilo-echo-3-whisky-charlie-7"
```

The same command can be used to change the note, like this:

```bash
maas $PROFILE machine update $SYSTEM_ID description="$A_DIFFERENT_NOTE"
```

The existing note will be overwritten by the new one you enter.  You can also remove a note by entering and empty description, like this:

```bash
maas admin machine update ke3wc7 description=""
```

**** How to list notes for all machines

List notes for all machines like this:

```bash
maas $PROFILE machines read \
| jq -r '(["hostname","system_id","description"]
|(.,map(length*"-"))),(.[]|[.hostname,.system_id,.description])
|@tsv' | column -t
```

For example:

```bash
maas admin machines read \
| jq -r '(["hostname","system_id","description"]
|(.,map(length*"-"))),(.[]|[.hostname,.system_id,.description])
|@tsv' | column -t
```

Output might look something like this:

```nohighlight
hostname       system_id  description
--------       ---------  -----------
driven-teal    tgaat6     tango-golf
humble-bunny   srqnnb     sierra-romeo
tough-kit      ke3wc7     kilo-echo
```

You can also check the note for one machine like this -- note the use of the *singular* "machine" versus "machines":

```bash
 maas $PROFILE machine read $SYSTEM_ID \
| jq -r '(["hostname","system_id","description"]
|(.,map(length*"-"))),([.hostname,.system_id,.description])
|@tsv' | column -t
```

For example:

```bash
 maas admin machine read tgaat6 \
| jq -r '(["hostname","system_id","description"]
|(.,map(length*"-"))),([.hostname,.system_id,.description])
|@tsv' | column -t
```

A command like this might produce output as follows:

```nohighlight
hostname     system_id  description
--------     ---------  -----------
driven-teal  tgaat6     tango-golf
```

** How to work with dynamic (workload) annotations

Dynamic annotations persist only as long as a machine is allocated or deployed.  They help alert others to the status of your running workloads.  Dynamic annotations are managed in the MAAS CLI, but can be viewed and filtered using the MAAS UI.

*** How to identify machines that can receive dynamic annotations

You can only set dynamic annotations for machines that are in the "Allocated" or "Deployed" state.  To identify which of your machines are in these states, you can execute the following command:

```bash
maas $PROFILE machines read \
| jq -r '(["hostname","system_id","status"]
|(.,map(length*"-"))),(.[]|[.hostname,.system_id,.status_name])
|@tsv' | column -t
```

For example:

```bash
maas admin machines read \
| jq -r '(["hostname","system_id","status"]
|(.,map(length*"-"))),(.[]|[.hostname,.system_id,.status_name])
|@tsv' | column -t
```

Output might look something like this:

```nohighlight
hostname       system_id  status
--------       ---------  ------
divine-stork   8b3ypp     Deployed
casual-prawn   4end6r     Ready
driven-teal    tgaat6     Allocated
immune-beetle  43xand     Allocated
good-osprey    napfxk     Allocated
smart-hen      c4rwq7     Allocated
boss-satyr     xn8taa     Ready
golden-martin  8fxery     Allocated
crack-guinea   qk4b3g     Allocated
finer-leech    cy3dtr     Deployed
free-mouse     gxtbq4     Allocated
humble-bunny   srqnnb     Allocated
wanted-muskox  ekw7fh     Deployed
one-boa        by477d     Allocated
great-urchin   srnx4g     Allocated
ace-frog       g6arwg     Ready
alive-marlin   gbwnfb     Deployed
picked-parrot  am77wn     Allocated
tough-kit      ke3wc7     Deployed
legal-whale    8nq3mt     Allocated
game-sponge    76pdc6     Allocated
fun-ghoul      qxfm7k     Allocated
aware-earwig   8m8hs7     Deployed
chief-crane    7fapx7     Ready
select-tapir   4ascbr     Allocated
on-slug        snfs8d     Allocated
polite-llama   dbqd4m     Allocated
frank-coyote   wcmk48     Allocated
usable-condor  ed8hmy     Deployed
still-imp      h6ra6d     Allocated
```

*** How to set dynamic annotations

Dynamic annotations, otherwise known as "workload annotations" or "owner data," can be used to keep track of the runtime status of machines that are allocated or deployed.  These annotations are set using `key=value` pairs.  You can set any `key=value` pair that you wish for any machine, although it's probably more useful if you standardise your key names.

To set a dynamic annotation for a machine, you can enter a command like this:

```bash
maas $PROFILE machine set-owner-data $SYSTEM_ID $KEY=$VALUE
```

For example:

```bash
maas admin machine set-owner-data tgaat6 owner=gsmith@zorko.com
```

This command will return a JSON string representative of the machine's new configuration, including the dynamic annotations you've added. 

*** How to clear or change dynamic annotations

You can change dynamic annotations for a machine simply by executing a new `set-owner-data` command:

```bash
maas $PROFILE machine set-owner-data $SYSTEM_ID $KEY=$NEW_VALUE
```

You can clear a dynamic annotation by entering the empty string (`""`) as the $VALUE:

```bash
maas $PROFILE machine set-owner-data $SYSTEM_ID $KEY=""
```

*** How to list dynamic annotations for all machines

You can list the current dynamic annotations for all machines with a command like this:

```bash
maas $PROFILE machines read \
| jq -r '(["hostname","system_id","owner_data"]
|(.,map(length*"-"))),(.[]|[.hostname,.system_id,.owner_data[]])
|@tsv' | column -t
```

For example:

```bash
maas admin machines read \
| jq -r '(["hostname","system_id","owner_data"]
|(.,map(length*"-"))),(.[]|[.hostname,.system_id,.owner_data[]])
|@tsv' | column -t
```

This command output might look something like this:

```nohighlight
hostname       system_id  owner_data
--------       ---------  ----------
divine-stork   8b3ypp
casual-prawn   4end6r
driven-teal    tgaat6     farquar     foobar
immune-beetle  43xand
good-osprey    napfxk
smart-hen      c4rwq7
```

*** How to list dynamic annotations for one machine

You can list the dynamic annotations for one machine by entering a command of the form:

```bash
maas $PROFILE machine read $SYSTEM_ID \
| jq -r '(["hostname","system_id","owner_data"]
|(.,map(length*"-"))),(.[]|[.hostname,.system_id,.owner_data[]])
|@tsv' | column -t
```

For example:

```bash
maas admin machine read tgaat6 \
| jq -r '(["hostname","system_id","owner_data"]
|(.,map(length*"-"))),(.[]|[.hostname,.system_id,.owner_data[]])
|@tsv' | column -t
```

This will produce output similar to the following:

```nohighlight
hostname     system_id  owner_data
--------     ---------  ----------
driven-teal  tgaat6     farquar     foobar
```

*** How to view dynamic annotations via the MAAS UI

To view the dynamic (workload) annotations for one machine -- via the MAAS UI -- do the following:

1. Select *Machines*.

2. Select the machine of interest by clicking on its hyperlinked name.

3. In the machine summary that comes up, look for the *Workload Annotations* card.

*** How to filter on dynamic annotations via the MAAS UI

To filter machines by dynamic (workload) annotations, use the following procedure:

1. Select *Machines*.

2. Select the *Filters* dropdown.

3. Select *Workload*.

4. Select one or more values from the *Workload* list to filter the machine list.

* How to audit MAAS

This article gives concise procedures for working with [audit events](/t/maas-audit-events/6372).  Here you will learn:

- [How to report audit events](#heading--How-to-report-audit-events)
- [How to filter audit events by hostname](#heading--How-to-filter-audit-events-by-hostname)
- [How to filter audit events by MAC address](#heading--How-to-filter-audit-events-by-MAC-address)
- [How to filter audit events by system ID](#heading--How-to-filter-audit-events-by-system-ID)
- [How to filter audit events by zone](#heading--How-to-filter-audit-events-by-zone)
- [How to filter audit events by owner](#heading--How-to-filter-audit-events-by-owner)
- [How to limit the number of audit events displayed](#heading--How-to-limit-the-number-of-audit-events-displayed)
- [How to move the audit event window](#heading--How-to-move-the-audit-event-window)
- [How to audit a machine's life-cycle with audit events](#heading--How-to-audit-a-machines-life-cycle-with-audit-events)

Note that for this article, we will assume you have installed [the `jq` tool](https://stedolan.github.io/jq/)`↗`, which makes the JSON output of `events query` more human-readable.

** How to report audit events

To get a list of MAAS audit events, enter this MAAS CLI command:

```nohighlight
$ maas $PROFILE events query level=AUDIT \
| jq -r '(["USERNAME","HOSTNAME","DATE","EVENT"] | 
(., map(length*"-"))),
(.events[] | [.username,.hostname,.created,.description]) 
| @tsv' | column -t -s$'\t'
```

This produces a table of 100 of the most recent audit events.  The truncated table might look something like this:

```nohighlight
USERNAME       HOSTNAME         DATE                        EVENT
--------       --------         ----                        -----
paulomalovich  ruling-bobcat    Tue, 04 Oct. 2022 13:19:22  Started testing on 'ruling-bobcat'.
paulomalovich  new-name         Tue, 04 Oct. 2022 11:45:44  Started testing on 'new-name'.
paulomalovich  fine-hornet      Tue, 04 Oct. 2022 11:44:48  Powered off 'fine-hornet'.
paulomalovich  fine-hornet      Tue, 04 Oct. 2022 11:44:39  Powered off 'fine-hornet'.
admin          polong           Tue, 04 Oct. 2022 11:39:44  Started testing on 'polong'.
paulomalovich  fine-hornet      Tue, 04 Oct. 2022 11:35:32  Powered off 'fine-hornet'.
admin          fair-marten      Mon, 03 Oct. 2022 14:13:54  Started testing on 'fair-marten'.
admin          new-name         Mon, 03 Oct. 2022 14:13:54  Started testing on 'new-name'.
paulomalovich  new-name         Fri, 30 Sep. 2022 09:44:25  Started testing on 'new-name'.
stormrider     ruling-bobcat    Wed, 28 Sep. 2022 21:36:36  Marked 'ruling-bobcat' broken.
stormrider     ruling-bobcat    Wed, 28 Sep. 2022 20:46:46  Started commissioning on 'ruling-bobcat'.
stormrider     ruling-bobcat    Wed, 28 Sep. 2022 20:46:38  Set the zone to 'default' on 'ruling-bobcat'.
paulomalovich  top-monkey       Wed, 28 Sep. 2022 14:43:47  Tagging 'top-monkey'.
paulomalovich  fake-controller  Wed, 28 Sep. 2022 14:24:36  Powered on 'fake-controller'.
paulomalovich  fake-controller  Wed, 28 Sep. 2022 14:24:06  Powered on 'fake-controller'.
paulomalovich  fake-controller  Wed, 28 Sep. 2022 14:24:03  Powered off 'fake-controller'.
paulomalovich  new-name         Wed, 28 Sep. 2022 14:19:35  Untagging 'new-name'.
paulomalovich  new-name         Wed, 28 Sep. 2022 14:19:30  Tagging 'new-name'.
stormrider     ruling-bobcat    Tue, 27 Sep. 2022 22:53:35  Aborted 'commissioning' on 'ruling-bobcat'.
stormrider     ruling-bobcat    Tue, 27 Sep. 2022 22:53:30  Aborted 'commissioning' on 'ruling-bobcat'.
stormrider     ruling-bobcat    Tue, 27 Sep. 2022 22:53:11  Started commissioning on 'ruling-bobcat'.
...
```

** How to filter audit events by hostname

To get a list of MAAS audit events for a specific host, enter this MAAS CLI command:

```nohighlight
$ maas $PROFILE events query level=AUDIT hostname=ruling-bobcat \
| jq -r '(["USERNAME","HOSTNAME","DATE","EVENT"] | 
(., map(length*"-"))),
(.events[] | [.username,.hostname,.created,.description]) 
| @tsv' | column -t -s$'\t'
```

This produces a table of 100 of the most recent audit events for the specified hostname.  The truncated table might look something like this:

```nohighlight
paulomalovich  ruling-bobcat  Tue, 04 Oct. 2022 13:19:22  Started testing on 'ruling-bobcat'.
stormrider     ruling-bobcat  Wed, 28 Sep. 2022 21:36:36  Marked 'ruling-bobcat' broken.
stormrider     ruling-bobcat  Wed, 28 Sep. 2022 20:46:46  Started commissioning on 'ruling-bobcat'.
stormrider     ruling-bobcat  Wed, 28 Sep. 2022 20:46:38  Set the zone to 'default' on 'ruling-bobcat'.
stormrider     ruling-bobcat  Tue, 27 Sep. 2022 22:53:35  Aborted 'commissioning' on 'ruling-bobcat'.
stormrider     ruling-bobcat  Tue, 27 Sep. 2022 22:53:30  Aborted 'commissioning' on 'ruling-bobcat'.
stormrider     ruling-bobcat  Tue, 27 Sep. 2022 22:53:11  Started commissioning on 'ruling-bobcat'.
admin          ruling-bobcat  Thu, 22 Sep. 2022 15:19:00  Started commissioning on 'ruling-bobcat'.
admin          ruling-bobcat  Wed, 21 Sep. 2022 10:57:53  Started commissioning on 'ruling-bobcat'.
admin          ruling-bobcat  Tue, 20 Sep. 2022 15:16:41  Started commissioning on 'ruling-bobcat'.
admin          ruling-bobcat  Tue, 20 Sep. 2022 15:02:40  Started commissioning on 'ruling-bobcat'.
admin          ruling-bobcat  Tue, 20 Sep. 2022 11:17:14  Started commissioning on 'ruling-bobcat'.
sugarsmacks    ruling-bobcat  Tue, 23 Aug. 2022 09:24:20  Started releasing 'ruling-bobcat'.
admin          ruling-bobcat  Thu, 23 Jun. 2022 23:26:57  Set the zone to 'forbidden' on 'ruling-bobcat'.
sugarsmacks    ruling-bobcat  Wed, 22 Jun. 2022 10:32:24  Started releasing 'ruling-bobcat'.
...
```

** How to filter audit events by MAC address

To get a list of MAAS audit events for a specific MAC address, enter this MAAS CLI command:

```nohighlight
$ maas $PROFILE events query level=AUDIT mac_address=00:07:fe:72:a6:00 \
| jq -r '(["USERNAME","HOSTNAME","DATE","EVENT"] | 
(., map(length*"-"))),
(.events[] | [.username,.hostname,.created,.description]) 
| @tsv' | column -t -s$'\t'
```

This produces a table of 100 of the most recent audit events for the hostname with the specified MAC address.  The truncated table might look something like this:

```nohighlight
USERNAME     HOSTNAME     DATE                        EVENT
--------     --------     ----                        -----
admin        fair-marten  Mon, 03 Oct. 2022 14:13:54  Started testing on 'fair-marten'.
sugarsmacks  fair-marten  Wed, 21 Sep. 2022 14:21:25  Tagging 'fair-marten'.
sugarsmacks  fair-marten  Wed, 21 Sep. 2022 14:10:38  Untagging 'fair-marten'.
sugarsmacks  fair-marten  Wed, 22 Jun. 2022 10:20:13  Tagging 'fair-marten'.
edward       fair-marten  Fri, 08 Apr. 2022 11:02:15  Untagging 'fair-marten'.
edward       fair-marten  Fri, 08 Apr. 2022 11:02:14  Tagging 'fair-marten'.
admin        fair-marten  Fri, 11 Feb. 2022 11:00:00  Set the zone to 'twilight' on 'fair-marten'.
admin        fair-marten  Fri, 11 Feb. 2022 10:59:56  Set the zone to 'danger' on 'fair-marten'.
admin        fair-marten  Fri, 11 Feb. 2022 10:59:50  Acquired 'fair-marten'.
carlo        fair-marten  Tue, 11 Jan. 2022 01:56:32  Started testing on 'fair-marten'.
carlo        fair-marten  Tue, 11 Jan. 2022 01:56:26  Marked 'fair-marten' broken.
carlo        fair-marten  Tue, 11 Jan. 2022 01:56:23  Aborted 'testing' on 'fair-marten'.
...
```
Note that the JSON output from `events query` does not print the MAC address, so it is not possible to include that parameter in the output.

** How to filter audit events by system ID

To get a list of MAAS audit events for a specific system ID, enter this MAAS CLI command:

```nohighlight
$ maas $PROFILE events query level=AUDIT id=8r6pw7 \
| jq -r '(["USERNAME","HOSTNAME","SYSTEM_ID", "DATE","EVENT"] | 
(., map(length*"-"))),
(.events[] | [.username,.hostname,.node,.created,.description]) 
| @tsv' | column -t -s$'\t'
```

This produces a table of 100 of the most recent audit events for the hostname with the specified system ID.  The truncated table might look something like this:

```nohighlight
USERNAME       HOSTNAME  SYSTEM_ID  DATE                        EVENT
--------       --------  ---------  ----                        -----
stormrider     karura    8r6pw7     Tue, 27 Sep. 2022 21:53:34  Set the zone to 'twilight' on 'karura'.
admin          karura    8r6pw7     Wed, 21 Sep. 2022 14:00:47  Untagging 'karura'.
admin          karura    8r6pw7     Wed, 21 Sep. 2022 14:00:01  Tagging 'karura'.
admin          karura    8r6pw7     Wed, 21 Sep. 2022 13:58:11  Untagging 'karura'.
admin          karura    8r6pw7     Wed, 21 Sep. 2022 13:57:48  Tagging 'karura'.
hmrxopxi       karura    8r6pw7     Fri, 12 Aug. 2022 00:16:51  Tagging 'karura'.
john           karura    8r6pw7     Tue, 12 Jul. 2022 15:08:27  Powered on 'karura'.
john           karura    8r6pw7     Tue, 12 Jul. 2022 15:08:23  Powered on 'karura'.
john           karura    8r6pw7     Tue, 12 Jul. 2022 15:08:08  Powered off 'karura'.
admin          karura    8r6pw7     Thu, 23 Jun. 2022 23:26:53  Set the zone to 'asd' on 'karura'.
paulomalovich  karura    8r6pw7     Fri, 22 Apr. 2022 08:06:59  Powered off 'karura'.
paulomalovich  karura    8r6pw7     Fri, 22 Apr. 2022 07:09:55  Powered on 'karura'.
```

** How to filter audit events by zone

To get a list of MAAS audit events that are in a specific zone, enter this MAAS CLI command:

```nohighlight
$ maas $PROFILE events query level=AUDIT zone=twilight \
| jq -r '(["USERNAME","HOSTNAME","DATE","EVENT"] | 
(., map(length*"-"))),
(.events[] | [.username,.hostname,.created,.description]) 
| @tsv' | column -t -s$'\t'
```

This produces a table of 100 of the most recent audit events for the hosts that are in the specified zone.  The truncated table might look something like this:

```nohighlight
USERNAME       HOSTNAME     DATE                        EVENT
--------       --------     ----                        -----
admin          polong       Tue, 04 Oct. 2022 11:39:44  Started testing on 'polong'.
admin          fair-marten  Mon, 03 Oct. 2022 14:13:54  Started testing on 'fair-marten'.
stormrider     polong       Tue, 27 Sep. 2022 21:53:38  Set the zone to 'twilight' on 'polong'.
stormrider     karura       Tue, 27 Sep. 2022 21:53:34  Set the zone to 'twilight' on 'karura'.
sugarsmacks    fair-marten  Wed, 21 Sep. 2022 14:21:25  Tagging 'fair-marten'.
sugarsmacks    fair-marten  Wed, 21 Sep. 2022 14:10:38  Untagging 'fair-marten'.
admin          karura       Wed, 21 Sep. 2022 14:00:47  Untagging 'karura'.
admin          karura       Wed, 21 Sep. 2022 14:00:01  Tagging 'karura'.
admin          karura       Wed, 21 Sep. 2022 13:58:11  Untagging 'karura'.
admin          karura       Wed, 21 Sep. 2022 13:57:48  Tagging 'karura'.
admin          polong       Tue, 13 Sep. 2022 14:14:24  Powered on 'polong'.
```
Note that the JSON output from `events query` does not print the zone, so it is not possible to include that parameter in the output.

** How to filter audit events by owner

To get a list of MAAS audit events for machines owned by a specific username, enter this MAAS CLI command:

```nohighlight
$ maas stormrider events query level=AUDIT owner=stormrider | jq -r '(["USERNAME","HOSTNAME","DATE","EVENT"] | 
(., map(length*"-"))),
(.events[] | [.username,.hostname,.created,.description]) 
| @tsv' | column -t -s$'\t'
```

This produces a table of 100 of the most recent audit events for the hosts that are owned by the specified username ("owner").  The truncated table might look something like this:

```nohighlight
USERNAME    HOSTNAME       DATE                        EVENT
--------    --------       ----                        -----
stormrider  ruling-bobcat  Wed, 28 Sep. 2022 21:36:36  Marked 'ruling-bobcat' broken.
stormrider  ruling-bobcat  Wed, 28 Sep. 2022 20:46:46  Started commissioning on 'ruling-bobcat'.
stormrider  ruling-bobcat  Wed, 28 Sep. 2022 20:46:38  Set the zone to 'default' on 'ruling-bobcat'.
stormrider  ruling-bobcat  Tue, 27 Sep. 2022 22:53:35  Aborted 'commissioning' on 'ruling-bobcat'.
stormrider  ruling-bobcat  Tue, 27 Sep. 2022 22:53:30  Aborted 'commissioning' on 'ruling-bobcat'.
stormrider  ruling-bobcat  Tue, 27 Sep. 2022 22:53:11  Started commissioning on 'ruling-bobcat'.
stormrider  polong         Tue, 27 Sep. 2022 21:53:38  Set the zone to 'twilight' on 'polong'.
stormrider  karura         Tue, 27 Sep. 2022 21:53:34  Set the zone to 'twilight' on 'karura'.
stormrider  new-name       Tue, 27 Sep. 2022 21:53:14  Set the zone to 'forbidden' on 'new-name'.
```

** How to limit the number of audit events displayed

To limit or expand the number of MAAS audit events shown for any given query, use the `limit=` parameter:

```nohighlight
$ maas stormrider events query level=AUDIT owner=stormrider limit=3 | jq -r '(["USERNAME","HOSTNAME","DATE","EVENT"] | 
(., map(length*"-"))),
(.events[] | [.username,.hostname,.created,.description]) 
| @tsv' | column -t -s$'\t'
```

This particular command repeats the query used in the previous section, but limits the number of records returned to three:

```nohighlight
USERNAME    HOSTNAME       DATE                        EVENT
--------    --------       ----                        -----
stormrider  ruling-bobcat  Wed, 28 Sep. 2022 21:36:36  Marked 'ruling-bobcat' broken.
stormrider  ruling-bobcat  Wed, 28 Sep. 2022 20:46:46  Started commissioning on 'ruling-bobcat'.
stormrider  ruling-bobcat  Wed, 28 Sep. 2022 20:46:38  Set the zone to 'default' on 'ruling-bobcat'.
```

This parameter can also be used to expand the number of reported records, up to a maximum of 1000.  Note that the query returns only as many records as are available.  For example, we can attempt to return 1000 records for `owner=stormrider`:

```nohighlight
$ maas stormrider events query level=AUDIT owner=stormrider limit=1000 | jq -r '(["USERNAME","HOSTNAME","DATE","EVENT"] | 
(., map(length*"-"))),
(.events[] | [.username,.hostname,.created,.description]) 
| @tsv' | column -t -s$'\t'
```

Since only a few audit events exist, though, the returned list is much smaller:

```nohighlight
USERNAME    HOSTNAME       DATE                        EVENT
--------    --------       ----                        -----
stormrider  ruling-bobcat  Wed, 28 Sep. 2022 21:36:36  Marked 'ruling-bobcat' broken.
stormrider  ruling-bobcat  Wed, 28 Sep. 2022 20:46:46  Started commissioning on 'ruling-bobcat'.
stormrider  ruling-bobcat  Wed, 28 Sep. 2022 20:46:38  Set the zone to 'default' on 'ruling-bobcat'.
stormrider  ruling-bobcat  Tue, 27 Sep. 2022 22:53:35  Aborted 'commissioning' on 'ruling-bobcat'.
stormrider  ruling-bobcat  Tue, 27 Sep. 2022 22:53:30  Aborted 'commissioning' on 'ruling-bobcat'.
stormrider  ruling-bobcat  Tue, 27 Sep. 2022 22:53:11  Started commissioning on 'ruling-bobcat'.
stormrider  polong         Tue, 27 Sep. 2022 21:53:38  Set the zone to 'twilight' on 'polong'.
stormrider  karura         Tue, 27 Sep. 2022 21:53:34  Set the zone to 'twilight' on 'karura'.
stormrider  new-name       Tue, 27 Sep. 2022 21:53:14  Set the zone to 'forbidden' on 'new-name'.
```

** How to move the audit event window

To return records that occur before a certain record ID number, enter a command like this one:

```nohighlight
$ maas stormrider events query level=AUDIT after=0 | jq -r '(["USERNAME","HOSTNAME","DATE","EVENT"] | 
(., map(length*"-"))),
(.events[] | [.username,.hostname,.created,.description]) 
| @tsv' | column -t -s$'\t'
```

This command will yield a list that starts with the following events:

```nohighlight
USERNAME     HOSTNAME       DATE                        EVENT
--------     --------       ----                        -----
admin        ruling-bobcat  Wed, 17 Nov. 2021 00:04:51  Started deploying 'ruling-bobcat'.
admin        active-amoeba  Mon, 15 Nov. 2021 05:39:48  Set the resource pool to 'default' on 'active-amoeba'.
admin        fair-marten    Mon, 15 Nov. 2021 05:39:48  Set the resource pool to 'default' on 'fair-marten'.
admin        active-amoeba  Mon, 15 Nov. 2021 05:37:26  Set the resource pool to 'new' on 'active-amoeba'.
admin        fair-marten    Mon, 15 Nov. 2021 05:37:26  Set the resource pool to 'new' on 'fair-marten'.
admin        active-amoeba  Mon, 08 Nov. 2021 04:07:44  Started testing on 'active-amoeba'.
admin        active-amoeba  Mon, 08 Nov. 2021 04:05:40  Marked 'active-amoeba' broken.
admin        active-amoeba  Mon, 08 Nov. 2021 04:05:05  Started testing on 'active-amoeba'.
admin        active-amoeba  Mon, 08 Nov. 2021 04:04:57  Marked 'active-amoeba' broken.
admin        bolla          Wed, 16 Jun. 2021 04:35:50  Started importing images on 'bolla'.
```

You can move the start of this window by moving the `after=` setting, like this:

```nohighlight
$ maas stormrider events query level=AUDIT after=2000 | jq -r '(["USERNAME","HOSTNAME","DATE","EVENT"] | 
(., map(length*"-"))),
(.events[] | [.username,.hostname,.created,.description]) 
| @tsv' | column -t -s$'\t'
```

This command will yield a list that starts with later events:

```nohighlight
USERNAME     HOSTNAME       DATE                        EVENT
--------     --------       ----                        -----
carlo        fair-marten    Wed, 01 Dec. 2021 06:13:45  Tagging 'fair-marten'.
carlo        new-name       Wed, 01 Dec. 2021 06:13:45  Tagging 'new-name'.
carlo        ruling-bobcat  Wed, 01 Dec. 2021 06:09:55  Tagging 'ruling-bobcat'.
carlo        fair-marten    Wed, 01 Dec. 2021 06:09:55  Tagging 'fair-marten'.
carlo        new-name       Wed, 01 Dec. 2021 06:09:55  Tagging 'new-name'.
carlo        ruling-bobcat  Wed, 01 Dec. 2021 05:33:41  Started commissioning on 'ruling-bobcat'.
carlo        fair-marten    Wed, 01 Dec. 2021 05:33:40  Started commissioning on 'fair-marten'.
carlo        new-name       Wed, 01 Dec. 2021 05:33:40  Started commissioning on 'new-name'.
carlo        ruling-bobcat  Wed, 01 Dec. 2021 05:21:09  Aborted 'commissioning' on 'ruling-bobcat'.
carlo        ruling-bobcat  Wed, 01 Dec. 2021 05:21:07  Aborted 'commissioning' on 'ruling-bobcat'.
carlo        ruling-bobcat  Wed, 01 Dec. 2021 05:21:04  Aborted 'commissioning' on 'ruling-bobcat'.
carlo        ruling-bobcat  Wed, 01 Dec. 2021 05:21:02  Aborted 'commissioning' on 'ruling-bobcat'.
carlo        new-name       Wed, 01 Dec. 2021 05:20:50  Started commissioning on 'new-name'.
carlo        fair-marten    Wed, 01 Dec. 2021 05:20:50  Started commissioning on 'fair-marten'.
carlo        ruling-bobcat  Wed, 01 Dec. 2021 05:20:40  Started commissioning on 'ruling-bobcat'.
carlo        new-name       Wed, 01 Dec. 2021 05:20:39  Started commissioning on 'new-name'.
carlo        fair-marten    Wed, 01 Dec. 2021 05:20:39  Started commissioning on 'fair-marten'.
```

Note that the starting point of the list (newest date first) has moved forward in time somewhat.  You can also use the `before=` parameter to move the window, as desired.

** How to audit a machine's life-cycle with audit events

To audit a machine's life-cycle, using audit events, do the following:

1. Collect a fair amount of audit data on that particular machine using, for example, the hostname to filter events:

```nohighlight
maas stormrider events query hostname=ruling-bobcat level=AUDIT after=0 limit=1000 | jq -r '(.events[] | [.id,.level,.type,.created,.description]) 
| @tsv' | column -t -s$'\t' > /tmp/audit-data
maas stormrider events query hostname=ruling-bobcat level=AUDIT after=1000 limit=1000 | jq -r '(.events[] | [.id,.level,.type,.created,.description]) 
| @tsv' | column -t -s$'\t' >> /tmp/audit-data
maas stormrider events query hostname=ruling-bobcat level=AUDIT after=2000 limit=1000 | jq -r '(.events[] | [.id,.level,.type,.created,.description]) 
| @tsv' | column -t -s$'\t' >> /tmp/audit-data
maas stormrider events query hostname=ruling-bobcat level=AUDIT after=3000 limit=1000 | jq -r '(.events[] | [.id,.level,.type,.created,.description]) 
| @tsv' | column -t -s$'\t' >> /tmp/audit-data
maas stormrider events query hostname=ruling-bobcat level=AUDIT after=4000 limit=1000 | jq -r '(.events[] | [.id,.level,.type,.created,.description]) 
| @tsv' | column -t -s$'\t' >> /tmp/audit-data
maas stormrider events query hostname=ruling-bobcat level=AUDIT after=5000 limit=1000 | jq -r '(.events[] | [.id,.level,.type,.created,.description]) 
| @tsv' | column -t -s$'\t' >> /tmp/audit-data
...to the number of events you wish to collect
```

2. Collect non-audit data by excluding the `level=` parameter from the call:

```nohighlight
maas stormrider events query hostname=ruling-bobcat after=0 limit=1000 | jq -r '(.events[] | [.id,.level,.type,.created,.description]) 
| @tsv' | column -t -s$'\t' >> /tmp/audit-data
maas stormrider events query hostname=ruling-bobcat after=1000 limit=1000 | jq -r '(.events[] | [.id,.level,.type,.created,.description]) 
| @tsv' | column -t -s$'\t' >> /tmp/audit-data
maas stormrider events query hostname=ruling-bobcat after=2000 limit=1000 | jq -r '(.events[] | [.id,.level,.type,.created,.description]) 
| @tsv' | column -t -s$'\t' >> /tmp/audit-data
maas stormrider events query hostname=ruling-bobcat after=3000 limit=1000 | jq -r '(.events[] | [.id,.level,.type,.created,.description]) 
| @tsv' | column -t -s$'\t' >> /tmp/audit-data
maas stormrider events query hostname=ruling-bobcat after=4000 limit=1000 | jq -r '(.events[] | [.id,.level,.type,.created,.description]) 
| @tsv' | column -t -s$'\t' >> /tmp/audit-data
maas stormrider events query hostname=ruling-bobcat after=5000 limit=1000 | jq -r '(.events[] | [.id,.level,.type,.created,.description]) 
| @tsv' | column -t -s$'\t' >> /tmp/audit-data
...again, to the number of events you wish to collect
```

3. Collect state information by using the `level=DEBUG` parameter:

```nohighlight
maas stormrider events query hostname=ruling-bobcat level=DEBUG after=0 limit=1000 | jq -r '(.events[] | [.id,.level,.type,.created,.description]) 
| @tsv' | column -t -s$'\t' >> /tmp/audit-data
maas stormrider events query hostname=ruling-bobcat level=DEBUG after=1000 limit=1000 | jq -r '(.events[] | [.id,.level,.type,.created,.description]) 
| @tsv' | column -t -s$'\t' >> /tmp/audit-data
maas stormrider events query hostname=ruling-bobcat level=DEBUG after=2000 limit=1000 | jq -r '(.events[] | [.id,.level,.type,.created,.description]) 
| @tsv' | column -t -s$'\t' >> /tmp/audit-data
maas stormrider events query hostname=ruling-bobcat level=DEBUG after=3000 limit=1000 | jq -r '(.events[] | [.id,.level,.type,.created,.description]) 
| @tsv' | column -t -s$'\t' >> /tmp/audit-data
maas stormrider events query hostname=ruling-bobcat level=DEBUG after=4000 limit=1000 | jq -r '(.events[] | [.id,.level,.type,.created,.description]) 
| @tsv' | column -t -s$'\t' >> /tmp/audit-data
maas stormrider events query hostname=ruling-bobcat level=DEBUG after=5000 limit=1000 | jq -r '(.events[] | [.id,.level,.type,.created,.description]) 
| @tsv' | column -t -s$'\t' >> /tmp/audit-data
...and again, to the number of events you wish to collect
```

4. Sort your collected data and remove any duplicates (since ranges may overlap at times):

```nohighlight
sort -u /tmp/audit-data > /tmp/life-cycle
```

Let's assume at this point, you have a `life-cycle` file that begins something like this:

```nohighlight
418606  ERROR    Marking node broken               Wed, 17 Nov. 2021 00:02:52  A Physical Interface requires a MAC address.
418607  DEBUG    Node changed status               Wed, 17 Nov. 2021 00:02:52  From 'New' to 'Broken'
418608  DEBUG    Marking node fixed                Wed, 17 Nov. 2021 00:04:24  
418609  DEBUG    Node changed status               Wed, 17 Nov. 2021 00:04:24  From 'Broken' to 'Ready'
418613  DEBUG    User acquiring node               Wed, 17 Nov. 2021 00:04:51  (admin)
418614  DEBUG    Node changed status               Wed, 17 Nov. 2021 00:04:51  From 'Ready' to 'Allocated' (to admin)
418615  DEBUG    User starting deployment          Wed, 17 Nov. 2021 00:04:51  (admin)
418616  DEBUG    Node changed status               Wed, 17 Nov. 2021 00:04:51  From 'Allocated' to 'Deploying'
418617  INFO     Deploying                         Wed, 17 Nov. 2021 00:04:51  
418618  AUDIT    Node                              Wed, 17 Nov. 2021 00:04:51  Started deploying 'ruling-bobcat'.
418619  INFO     Powering on                       Wed, 17 Nov. 2021 00:04:55  
418625  ERROR    Marking node failed               Wed, 17 Nov. 2021 00:05:32  Power on for the node failed: Failed talking to node's BMC: Failed to power pbpncx. BMC never transitioned from off to on.
418626  DEBUG    Node changed status               Wed, 17 Nov. 2021 00:05:32  From 'Deploying' to 'Failed deployment'
418627  ERROR    Failed to power on node           Wed, 17 Nov. 2021 00:05:32  Power on for the node failed: Failed talking to node's BMC: Failed to power pbpncx. BMC never transitioned from off to on.
```

5. Walk down the collected life-cycle data to find out what happened to the machine.  In this case, we can see that:

- The machine was marked "Broken" because one of its physical interfaces lacked a MAC address.
- Someone marked the machine fixed (did they actually fix the problem?), which moved it back to a "Ready" state.
- Someone acquired the machine (successfully) and attempted to deploy it.
- MAAS could not talk to the machine's BMC, so the deployment failed.

In this case, a first thought might be to see whether the machine interface was actually fixed, that is, whether the machine was ever adjusted so that it could communicate with MAAS, or whether an interface issues persists.

Note that you must use more than just AUDIT events when debugging life-cycle issues with a machine.  Also note that issue the command `maas $PROFILE events query` without a specified `level=` parameter does not report audit events, so it's best to collect three sets of information: AUDIT, INFO, and DEBUG, and then process the information according to the event ID.
* How to keep MAAS backed up
MAAS currently does not provide specific tools to back up and restore a working MAAS configuration. MAAS servers are part of your data centre, just like other Linux-based servers, so your current backup and disaster recovery solution should be sufficient to back up your MAAS environment.  Even so, you should know which files and actions are critical -- to ensure that you get a clean backup, and further ensure that you can restore it cleanly.

[tabs]
[tab version="v3.4 Snap,v3.3 Snap,v3.2 Snap,v3.1 Snap,v3.0 Snap,v2.9 Snap"]
To back up your MAAS snap instance and restore it elsewhere, follow these steps:

1. Backup your PostgreSQL database to a file called `dump.sql` in your home directory:

``` bash
sudo -u postgres pg_dumpall -c > ~/dump.sql
```

2. Ensure this has completed and that there are no other established sessions with the following command:

``` bash
sudo -u postgres psql -c  "SELECT * FROM pg_stat_activity"
```
Running sessions, such as pg_dumpall, will appear in the `application_name` column of the output alongside `psql` running the above `pg_stat_activity` query.

3. Stop the PostgreSQL service:

```bash
sudo systemctl stop postgresql.service
```
4. Take a snapshot of the snap service (you don't have to stop it to do this):

```bash
sudo snap save maas
```

Note the snapshot id (number) that this command returns.

5. Verify that the snapshot is valid:

```bash
sudo snap check-snapshot <snapshot-id>
```

6. Export the snapshot to some external media:

```bash
sudo snap export-snapshot <snapshot-id> <external-media-path/snapshot-filename>
```

7. If reinstalling MAAS on the same system, use the following command to completely remove the old MAAS instance from your system:

```bash
sudo snap remove --purge maas
```

8. Restore the PostgreSQL dump with the following command:

```bash
sudo -u postgres psql -f dump.sql postgres
```
8. Restart the PostgreSQL service:

```bash
sudo systemctl start postgresql.service
```

10. Install the MAAS snap (same version) on your target machine, using the standard installation instructions for the version you're (re)installing.

11. Import the snapshot onto your target machine:

```bash
sudo snap import-snapshot <external-media-path/snapshot-filename>
```

The import function should give you the same snapshot ID.  If an existing snapshot has the same number (shouldn't happen here), that snapshot is overwritten.

12. Restore the snapshot:

```bash
sudo snap restore <snapshot-id>
```

13. Reset the database triggers with the following command:

```bash
sudo maas-region dbupgrade
```
[note]
You only need to run this command on one of the Region Controllers in a multi-region MAAS cluster.
[/note]

At this point, you should be up and running with a restored MAAS backup.
[/tab]
[tab version="v3.4 Packages,v3.3 Packages,v3.2 Packages,v3.1 Packages,v3.0 Packages,v2.9 Packages"]
To back up your MAAS snap instance and restore it elsewhere, follow these steps:

1. Backup your PostgreSQL database to a file called `dump.sql` in your home directory:

``` bash
sudo -u postgres pg_dumpall -c > ~/dump.sql
```

2. Ensure this has completed and that there are no other established sessions with the following command:

``` bash
sudo -u postgres psql -c  "SELECT * FROM pg_stat_activity"
```
Running sessions, such as pg_dumpall, will appear in the `application_name` column of the output alongside `psql` running the above `pg_stat_activity` query.

3. Stop MAAS-related services:

```bash
sudo systemctl stop postgresql.service
sudo systemctl stop maas-dhcpd.service
sudo systemctl stop maas-rackd.service
sudo systemctl stop maas-regiond.service
```

4. Archive the DB backup and the needed MAAS configuration files to an external drive with the following command:

``` bash
sudo tar cvpzf <some-external-path>/backup.tgz --exclude=/var/lib/maas/boot-resources /etc/maas /var/lib/maas ~/dump.sql
```

If you're not replacing the existing MAAS, make sure to restart the services you stopped prior to completing the backup.

** How to restore the system when needed

To restore the MAAS backup to a new machine:

1. Start with a freshly-updated installation of Ubuntu on identical hardware.

2. Reinstall MAAS via the standard installation procedure.

3. Stop the following services (note that PostgreSQL needs to keep running):

```bash
sudo systemctl stop maas-dhcpd.service
sudo systemctl stop maas-rackd.service
sudo systemctl stop maas-regiond.service
```

4. Copy the backup file to the new machine and untar its contents:

```bash
sudo tar xvzpf backup.tgz
```

5. Restore the database with the following command:

``` bash
sudo -u postgres psql -f dump.sql postgres
```

6. Copy across the old configuration files to their new locations, taking care to move the originals aside just in case:

```bash
sudo sh -c "mv /etc/maas /etc/_maas; mv /var/lib/maas /var/lib/_maas"
sudo sh -c "cp -prf etc/maas /etc/; cp -prf var/lib/maas /var/lib/"
```
Take care to preserve the correct permissions when restoring files and directories.

7. If your restore process regenerated the `/var/lib/maas/secret` file, make sure update this secret on any additional rack controllers.

8. Reset the database triggers:

``` bash
sudo maas-region dbupgrade
```

[note]
You only need to run this command on one of the Region Controllers in a multi-region MAAS cluster.
[/note]

9. Restart the stopped services:

```bash
sudo systemctl start maas-dhcpd.service
sudo systemctl start maas-rackd.service
sudo systemctl start maas-regiond.service
```

At this point, you should be up and running with a restored MAAS backup.
[/tab]
[/tabs]

* How to change MAAS settings

MAAS has a significant number of configuration settings.  This article will list these settings and their possible values, with brief instructions on how to adjust each setting.

[tabs]
[tab version="v3.4 Snap,v3.4 Packages" view="UI"]
*Settings* is available near the bottom of the left navigation panel.

** General

The following options are found in *Settings > Configuration > General*.

Managing MAAS site identity is useful when you are running more than one MAAS instance - say,  *Test* and *Production* environments.  This section also provides data collection and version notification options. 

- *MAAS name*: The "* MAAS name" is a text box that sets the text which appears at the bottom of every MAAS screen, in front of the version descriptor.

- *MAAS name emoji*: You may also paste a suitable emoji in front of the MAAS name to help identify it.

- *MAAS theme main colour*: You may also help identify your MAAS instance by changing the colour of the top bar; several colour choices are available.

- *Google Analytics*: MAAS uses Google Analytics, Usabilla and Sentry Error Tracking to improve user experience.  You can opt in or out of this service by setting or clearing this checkbox.

- *Release notification*: If you select this checkbox, MAAS will notify all users when new releases are available.

** Security

Choosing *Settings* > *Configuration* > *Security* provides instructions for enabling TLS with a certificate and a private key.  This is a CLI-operation; use the listed command at the command line, after logging into your MAAS instance.

** Commissioning

The parameters under *Settings* > *Configuration* > *Commissioning* allow you to change the way machines are commissioned:

- *Default commissioning release*: You can choose the default Ubuntu release that will be used for commissioning from a dropdown menu.

- *Default minimum kernel version*: The default minimum kernel version used on all new and commissioned nodes.  You can also choose this default from a dropdown menu.

- *IPMI username*: You can set the default IPMI username, which will control IPMI access to machines.

- *K_g BMC key*: Specify this key to encrypt all communication between IPMI clients and the BMC. Leave this blank for no encryption. 

- *IPMI privilege level*: You can choose the privilege level for IPMI access from a set of radio buttons (admin, operator, user).

** Deployment

*Settings* > *Configuration* > *Deployment* lets you configure machine deployment:

- *Default deployment OS*: You can choose the default operating system used for deployment from a dropdown list.

- *Default deployment OS release*: You can also choose the default OS release used for deployment, also from a dropdown.

- *Default hardware sync interval*: You can set the default hardware sync interval, in minutes.

** Kernel parameters

Under *Configuration* > *General* > *Kernel parameters*, you can set global boot that are always passed to the machine kernel.

** Users

*Settings* > *Users* MAAS gives you the ability to manage your users in a tabular format:

- *Add user button*: This button can be used to add a new user.

- *Sortable columns*: some of the column headings are clickable, allowing you to sort those columns.  These are "three click" sorts: ascending, descending, and none.

- *Actions column*: Each table row also has an "Actions" column, which allows you to delete or edit the information in that row.  Note that the delete and/or edit buttons may be greyed out (unavailable) based on your role.

Note that if the table becomes longer than one screen will accommodate, paging buttons will appear at the bottom of the screen.  A search bar is also provided to help you locate a particular user in a longer list.

** Images

*Settings* > *Images* allows you to specify parameters that control different types of MAAS images.

*** Ubuntu images

Under *Settings* > *Images* > *Ubuntu*, you can enable the installation of proprietary drives by selecting the appropriate checkbox.

*** Windows images

*Settings* > *Images* > *Windows* allows you to specify the Windows KMS activation host.  This is the FQDN or IP address of the host that provides the KMS Windows activation service, which is needed for Windows deployments that use KMS activation.

*** VMWare images

If you are using VMWare images, *Settings* > *Images* > *VMware* offers several parameters that you can adjust:

- *VMware vCenter server FQDN or IP address*: the VMware vCenter server FQDN or IP address which is passed to a deployed VMware ESXi host.

- *VMware vCenter username*: the VMware vCenter server username which is passed to a deployed VMware ESXi host.

- *VMware vCenter password*: the VMware vCenter server password which is passed to a deployed VMware ESXi host.

- *VMware vCenter datacenter*: the VMware vCenter datacenter which is passed to a deployed VMware ESXi host.

** License keys

*Settings* > *License keys* gives you the ability to manage your product licenses in a tabular format:

- *Add license key button*: This button can be used to add a new license key.

- *Sortable columns*: Note that some of the column headings are clickable, allowing you to sort those columns.  These are "three click" sorts: ascending, descending, and none.

- *Actions column*: These action buttons allow you to delete or edit the information in that row.  Note that the delete and/or edit buttons may be greyed out (unavailable) based on your role.

Note that if the table becomes longer than one screen will accommodate, paging buttons will appear at the bottom of the screen. A search bar is also provided to help you locate a particular license key in a longer list.

** Storage

Under *Settings* > *Storage*, you can set some parameters related to machine disks:

- *Default storage layout*: The default storage layout that is applied to a machine when it is commissioned.

- *Erase before releasing*: Checking this box forces users to always erase disks when releasing machines.

- *Use secure erase*: Check this box to use secure erase by default when erasing disks. This will only be used on devices that support secure erase. Other devices will fall back to full wipe or quick erase depending on the selected options.

- *Use quick erase*: Check this box to use quick erase by default when erasing disks.  This box is selected separately to provide a fallback for devices that do not support secure erase, should you have selected secure erase as the default method.  Note that this is not a secure erase; it wipes only the beginning and end of each disk.

** Network

*Settings* > *Network* allows you to set several network defaults for MAAS machines.

*** HTTP proxy

By choosing *Settings* > *Network* > *Proxy*, you can define the HTTP proxy used by MAAS to download images, and used by provisioned machines for APT and YUM packages. Your choices are (1) no proxy, (2) MAAS built-in proxy, (3) external proxy, or (4) peer proxy.  If you choose external or peer proxy, you will be presented with a text box to specify the external proxy URL that the MAAS built-in proxy will use as an upstream cache peer.  Note that machines will be configured to use MAAS' built-in proxy to download APT packages when external or peer proxies are specified.

*** Upstream DNS

*Settings* > *Network* > *DNS* lets you set DNS parameters for your MAAS.  Upstream DNS used to resolve domains not managed by this MAAS (space-separated IP addresses).  This only applies when MAAS is running its own DNS server, since this value is used to define forwarding in the DNS server config.  You can set the following parameters:

- *Enable DNSSEC validation*: If you wish to enable DNSSEC validation of upstream zones, you can choose the method from this dropdown list.  This is only used when MAAS is running its own DNS server. This value is used as the value of 'dnssec_validation' in the DNS server config.

- *List of external networks*: You can also provide a list of external networks to be used for MAAS DNS resolution. MAAS keeps a list of networks that are allowed to use MAAS for DNS resolution. This option allows you to add extra, previously-unknown networks to the trusted ACL where this list of networks is kept. It also supports specifying IPs or ACL names.

*** NTP service

Access the NTP service is controlled using *Settings* > *Network* > *NTP*. You can enter the address of NTP servers, specified as IP addresses or hostnames delimited by commas and/or spaces, to be used as time references for MAAS itself, the machines MAAS deploys, and devices that make use of MAAS DHCP services.  

You can also instruct MAAS to *Use external NTP only*, so that all daemons and machines refer directly to the external NTP server (and not to each other). If this is not set, only region controller hosts will be configured to use those external NTP servers; rack controller hosts will in turn refer to the regions' NTP servers, and deployed machines will refer to the racks' NTP servers.

*** Syslog configuration

You can use *Settings* > *Network* > *Syslog* to specify a remote syslog server to which machine logs should be forwarded.  MAAS will use this remote syslog server for all log messages when enlisting, commissioning, testing, and deploying machines. Conversely, clearing this value will restore the default behaviour of forwarding syslog entries to MAAS.

*** Network discovery

*Settings* > *Network* > *Network discovery*, when enabled, will cause MAAS to use passive techniques (such as listening to ARP requests and mDNS advertisements) to observe networks attached to rack controllers. Active subnet mapping will also be available to be enabled on the configured subnets.  You can set the *Active subnet mapping interval* by choosing a desired interval from a dropdown.  When network discovery is enabled, each rack will scan subnets enabled for active mapping, which helps to ensure that discovery information is accurate and complete.

** Scripts

Under the section *Settings* > *Scripts*, MAAS provides a great deal of flexibility when dealing with commissioning and testing scripts.

*** Commissioning scripts

*Settings* > *Scripts* > *Commissioning scripts* gives you the ability to manage machine commissioning scripts in a tabular format:

- *Upload script button*: This button can be used to upload a new commissioning script.

- *Sortable columns*: Note that some of the column headings are clickable, allowing you to sort those columns.  These are "three click" sorts: ascending, descending, and none.

- *Expandable script contents*: Also note that individual script names are clickable, allowing you to expand that row to see the contents of the script.

- *Actions column*: Each table row has an "Actions" column, which allows you to delete the script in that row, depending upon your role.

Note that if the table becomes longer than one screen will accommodate, paging buttons will appear at the bottom of the screen. A search bar is also provided to help you locate a particular commissioning script in a longer list.

*** Testing scripts

Similar to *Commissioning scripts*, the choices *Settings* > *Scripts* > *Testing scripts* give you the ability to manage your machines testing scripts in a tabular format:

- *Upload script button*: This button can be used to upload a new test script.

- *Sortable columns*: Note that some of the column headings are clickable, allowing you to sort those columns.  These are "three click" sorts: ascending, descending, and none.

- *Expandable script contents*: Also note that individual script names are clickable, allowing you to expand that row to see the contents of the script.

- *Actions column*: Each table row has an "Actions" column, which allows you to delete the script in that row, depending upon your role.

Note that if the table becomes longer than one screen will accommodate, paging buttons will appear at the bottom of the screen. A search bar is also provided to help you locate a particular test script in a longer list.

** DHCP snippets

*Settings* > *DHCP snippets* lets you manage your DHCP snippets in a table:

- *Add snippet button*: This button can be used to add a new DHCP snippet.

- *Sortable columns*: Note that some of the column headings are clickable, allowing you to sort those columns.  These are "three click" sorts: ascending, descending, and none.

- *Expandable snippets*: Also note that individual snippets are clickable, allowing you to expand that row to see the contents of that snippet.

- *Actions column*: Each table row has an "Actions" column, which allows you to edit delete the snippet in that row, depending upon your role.

Note that if the table becomes longer than one screen will accommodate, paging buttons will appear at the bottom of the screen. A search bar is also provided to help you locate a particular snippet in a longer list.

** Package repos

You can manage your MAAS repositories with the *Settings* > *Package repos* option.  Referenced repos are listed in a table:

- *Add PPA button*: This button can be used to add a new PPA to the search path.

- *Add repository button*: This button can be used to add a new repository to the search path.

- *Sortable columns*: Note that some of the column headings are clickable, allowing you to sort those columns.  These are "three click" sorts: ascending, descending, and none.

- *Actions column*: Each table row also has an "Actions" column, which allows you to edit or delete the repository information in that row, depending upon your role.

Note that if the table becomes longer than one screen will accommodate, paging buttons will appear at the bottom of the screen. A search bar is also provided to help you locate a particular test script in a longer list.
[/tab]
[tab version="v3.3 Snap,v3.3 Packages,v3.2 Snap,v3.2 Packages,v3.1 Snap,v3.1 Packages" view="UI"]
A *Settings* tab is available at the top of the MAAS interface.

** General

The following options are found in *Settings* > *Configuration* > *General*.

Managing MAAS site identity is useful when you are running more than one MAAS instance - say,  *Test* and *Production* environments.  This section also provides data collection and version notification options. 

- *MAAS name*: The "* MAAS name" is a text box that sets the text which appears at the bottom of every MAAS screen, in front of the version descriptor.

- *Google Analytics*: MAAS uses Google Analytics, Usabilla and Sentry Error Tracking to improve user experience.  You can opt in or out of this service by setting or clearing this checkbox.

- *Release notification*: If you select this checkbox, MAAS will notify all users when new releases are available.

** Security

Choosing *Settings* > *Configuration* > *Security* provides instructions for enabling TLS with a certificate and a private key.  This is a CLI-operation; use the listed command at the command line, after logging into your MAAS instance.

** Commissioning

The parameters under *Settings* > *Configuration* > *Commissioning* allow you to change the way machines are commissioned:

- *Default commissioning release*: You can choose the default Ubuntu release that will be used for commissioning from a dropdown menu.

- *Default minimum kernel version*: The default minimum kernel version used on all new and commissioned nodes.  You can also choose this default from a dropdown menu.

- *IPMI username*: You can set the default IPMI username, which will control IPMI access to machines.

- *K_g BMC key*: Specify this key to encrypt all communication between IPMI clients and the BMC. Leave this blank for no encryption. 

- *IPMI privilege level*: You can choose the privilege level for IPMI access from a set of radio buttons (admin, operator, user).

** Deployment

*Settings* > *Configuration* > *Deployment* lets you configure machine deployment:

- *Default deployment OS*: You can choose the default operating system used for deployment from a dropdown list.

- *Default deployment OS release*: You can also choose the default OS release used for deployment, also from a dropdown.

- *Default hardware sync interval*: You can set the default hardware sync interval, in minutes.

** Kernel parameters

Under *Configuration* > *General* > *Kernel parameters*, you can set global boot that are always passed to the machine kernel.

** Users

*Settings* > *Users* MAAS gives you the ability to manage your users in a tabular format:

- *Add user button*: This button can be used to add a new user.

- *Sortable columns*: some of the column headings are clickable, allowing you to sort those columns.  These are "three click" sorts: ascending, descending, and none.

- *Actions column*: Each table row also has an "Actions" column, which allows you to delete or edit the information in that row.  Note that the delete and/or edit buttons may be greyed out (unavailable) based on your role.

Note that if the table becomes longer than one screen will accommodate, paging buttons will appear at the bottom of the screen.  A search bar is also provided to help you locate a particular user in a longer list.

** Images

*Settings* > *Images* allows you to specify parameters that control different types of MAAS images.

*** Ubuntu images

Under *Settings* > *Images* > *Ubuntu*, you can enable the installation of proprietary drives by selecting the appropriate checkbox.

*** Windows images

*Settings* > *Images* > *Windows* allows you to specify the Windows KMS activation host.  This is the FQDN or IP address of the host that provides the KMS Windows activation service, which is needed for Windows deployments that use KMS activation.

*** VMWare images

If you are using VMWare images, *Settings* > *Images* > *VMware* offers several parameters that you can adjust:

- *VMware vCenter server FQDN or IP address*: the VMware vCenter server FQDN or IP address which is passed to a deployed VMware ESXi host.

- *VMware vCenter username*: the VMware vCenter server username which is passed to a deployed VMware ESXi host.

- *VMware vCenter password*: the VMware vCenter server password which is passed to a deployed VMware ESXi host.

- *VMware vCenter datacenter*: the VMware vCenter datacenter which is passed to a deployed VMware ESXi host.

** License keys

*Settings* > *License keys* gives you the ability to manage your product licenses in a tabular format:

- *Add license key button*: This button can be used to add a new license key.

- *Sortable columns*: Note that some of the column headings are clickable, allowing you to sort those columns.  These are "three click" sorts: ascending, descending, and none.

- *Actions column*: These action buttons allow you to delete or edit the information in that row.  Note that the delete and/or edit buttons may be greyed out (unavailable) based on your role.

Note that if the table becomes longer than one screen will accommodate, paging buttons will appear at the bottom of the screen. A search bar is also provided to help you locate a particular license key in a longer list.

** Storage

Under *Settings* > *Storage*, you can set some parameters related to machine disks:

- *Default storage layout*: The default storage layout that is applied to a machine when it is commissioned.

- *Erase before releasing*: Checking this box forces users to always erase disks when releasing machines.

- *Use secure erase*: Check this box to use secure erase by default when erasing disks. This will only be used on devices that support secure erase. Other devices will fall back to full wipe or quick erase depending on the selected options.

- *Use quick erase*: Check this box to use quick erase by default when erasing disks.  This box is selected separately to provide a fallback for devices that do not support secure erase, should you have selected secure erase as the default method.  Note that this is not a secure erase; it wipes only the beginning and end of each disk.

** Network

*Settings* > *Network* allows you to set several network defaults for MAAS machines.

*** HTTP proxy

By choosing *Settings* > *Network* > *Proxy*, you can define the HTTP proxy used by MAAS to download images, and used by provisioned machines for APT and YUM packages. Your choices are (1) no proxy, (2) MAAS built-in proxy, (3) external proxy, or (4) peer proxy.  If you choose external or peer proxy, you will be presented with a text box to specify the external proxy URL that the MAAS built-in proxy will use as an upstream cache peer.  Note that machines will be configured to use MAAS' built-in proxy to download APT packages when external or peer proxies are specified.

*** Upstream DNS

*Settings* > *Network* > *DNS* lets you set DNS parameters for your MAAS.  Upstream DNS used to resolve domains not managed by this MAAS (space-separated IP addresses).  This only applies when MAAS is running its own DNS server, since this value is used to define forwarding in the DNS server config.  You can set the following parameters:

- *Enable DNSSEC validation*: If you wish to enable DNSSEC validation of upstream zones, you can choose the method from this dropdown list.  This is only used when MAAS is running its own DNS server. This value is used as the value of 'dnssec_validation' in the DNS server config.

- *List of external networks*: You can also provide a list of external networks to be used for MAAS DNS resolution. MAAS keeps a list of networks that are allowed to use MAAS for DNS resolution. This option allows you to add extra, previously-unknown networks to the trusted ACL where this list of networks is kept. It also supports specifying IPs or ACL names.

*** NTP service

Access the NTP service is controlled using *Settings* > *Network* > *NTP*. You can enter the address of NTP servers, specified as IP addresses or hostnames delimited by commas and/or spaces, to be used as time references for MAAS itself, the machines MAAS deploys, and devices that make use of MAAS DHCP services.  

You can also instruct MAAS to *Use external NTP only*, so that all daemons and machines refer directly to the external NTP server (and not to each other). If this is not set, only region controller hosts will be configured to use those external NTP servers; rack controller hosts will in turn refer to the regions' NTP servers, and deployed machines will refer to the racks' NTP servers.

*** Syslog configuration

You can use *Settings* > *Network* > *Syslog* to specify a remote syslog server to which machine logs should be forwarded.  MAAS will use this remote syslog server for all log messages when enlisting, commissioning, testing, and deploying machines. Conversely, clearing this value will restore the default behaviour of forwarding syslog entries to MAAS.

*** Network discovery

*Settings* > *Network* > *Network discovery*, when enabled, will cause MAAS to use passive techniques (such as listening to ARP requests and mDNS advertisements) to observe networks attached to rack controllers. Active subnet mapping will also be available to be enabled on the configured subnets.  You can set the *Active subnet mapping interval* by choosing a desired interval from a dropdown.  When network discovery is enabled, each rack will scan subnets enabled for active mapping, which helps to ensure that discovery information is accurate and complete.

** Scripts

Under the section *Settings* > *Scripts*, MAAS provides a great deal of flexibility when dealing with commissioning and testing scripts.

*** Commissioning scripts

*Settings* > *Scripts* > *Commissioning scripts* gives you the ability to manage machine commissioning scripts in a tabular format:

- *Upload script button*: This button can be used to upload a new commissioning script.

- *Sortable columns*: Note that some of the column headings are clickable, allowing you to sort those columns.  These are "three click" sorts: ascending, descending, and none.

- *Expandable script contents*: Also note that individual script names are clickable, allowing you to expand that row to see the contents of the script.

- *Actions column*: Each table row has an "Actions" column, which allows you to delete the script in that row, depending upon your role.

Note that if the table becomes longer than one screen will accommodate, paging buttons will appear at the bottom of the screen. A search bar is also provided to help you locate a particular commissioning script in a longer list.

*** Testing scripts

Similar to *Commissioning scripts*, the choices *Settings* > *Scripts* > *Testing scripts* give you the ability to manage your machines testing scripts in a tabular format:

- *Upload script button*: This button can be used to upload a new test script.

- *Sortable columns*: Note that some of the column headings are clickable, allowing you to sort those columns.  These are "three click" sorts: ascending, descending, and none.

- *Expandable script contents*: Also note that individual script names are clickable, allowing you to expand that row to see the contents of the script.

- *Actions column*: Each table row has an "Actions" column, which allows you to delete the script in that row, depending upon your role.

Note that if the table becomes longer than one screen will accommodate, paging buttons will appear at the bottom of the screen. A search bar is also provided to help you locate a particular test script in a longer list.

** DHCP snippets

*Settings* > *DHCP snippets* lets you manage your DHCP snippets in a table:

- *Add snippet button*: This button can be used to add a new DHCP snippet.

- *Sortable columns*: Note that some of the column headings are clickable, allowing you to sort those columns.  These are "three click" sorts: ascending, descending, and none.

- *Expandable snippets*: Also note that individual snippets are clickable, allowing you to expand that row to see the contents of that snippet.

- *Actions column*: Each table row has an "Actions" column, which allows you to edit delete the snippet in that row, depending upon your role.

Note that if the table becomes longer than one screen will accommodate, paging buttons will appear at the bottom of the screen. A search bar is also provided to help you locate a particular snippet in a longer list.

** Package repos

You can manage your MAAS repositories with the *Settings* > *Package repos* option.  Referenced repos are listed in a table:

- *Add PPA button*: This button can be used to add a new PPA to the search path.

- *Add repository button*: This button can be used to add a new repository to the search path.

- *Sortable columns*: Note that some of the column headings are clickable, allowing you to sort those columns.  These are "three click" sorts: ascending, descending, and none.

- *Actions column*: Each table row also has an "Actions" column, which allows you to edit or delete the repository information in that row, depending upon your role.

Note that if the table becomes longer than one screen will accommodate, paging buttons will appear at the bottom of the screen. A search bar is also provided to help you locate a particular test script in a longer list.
[/tab]
[tab version="v3.4 Snap,v3.4 Packages,v3.3 Snap,v3.3 Packages,v3.2 Snap,v3.2 Packages,v3.1 Snap,v3.1 Packages" view="CLI"]

** Changing MAAS settings via the MAAS CLI

Assuming you have successfully logged into the MAAS CLI, you can access configuration values using the `maas $PROFILE maas set-config` command.  This command is used to set MAAS configuration values.

This command accepts keyword arguments.  You must pass each argument as a key-value pair, with an equals sign between the key and the value, like this:

```nohighlight
maas $PROFILE maas set-config key1=value1 key2=value key3=value3 ...
```

These keyword arguments must come after any positional arguments required by a specific command.  The following configuration keywords are currently available:

- *active_discovery_interval*: Active subnet mapping interval. When enabled, each rack will scan subnets enabled for active mapping. This helps ensure discovery information is accurate and complete.
- *boot_images_auto_import*: Automatically import/refresh the boot images every 60 minutes.
- *boot_images_no_proxy*: Set no_proxy with the image repository address when MAAS is behind (or set with) a proxy. By default, when MAAS is behind (and set with) a proxy, it is used to download images from the image repository. In some situations (e.g. when using a local image repository) it doesn't make sense for MAAS to use the proxy to download images because it can access them directly. Setting this option allows MAAS to access the (local) image repository directly by setting the no_proxy variable for the MAAS env with the address of the image repository.
- *commissioning_distro_series*: Default Ubuntu release used for commissioning.
- *completed_intro*: Marks if the initial intro has been completed.
- *curtin_verbose*: Run the fast-path installer with higher verbosity. This provides more detail in the installation logs.
- *default_distro_series*: Default OS release used for deployment.
- *default_dns_ttl*: Default Time-To-Live for the DNS.         If no TTL value is specified at a more specific point this is how long DNS responses are valid, in seconds.
- *default_min_hwe_kernel*: Default Minimum Kernel Version.         The default minimum kernel version used on all new and commissioned nodes.
- *default_osystem*: Default operating system used for deployment.
- *default_storage_layout*: Default storage layout.         Storage layout that is applied to a node when it is commissioned.        Available choices are*: 'bcache' (Bcache layout), 'blank' (No storage (blank) layout), 'custom' (Custom layout (from commissioning storage config)), 'flat' (Flat layout), 'lvm' (LVM layout), 'vmfs6' (VMFS6 layout), 'vmfs7' (VMFS7 layout).
- *disk_erase_with_quick_erase*: Use quick erase by default when erasing disks..         This is not a secure erase; it wipes only the beginning and end of each disk.
- *disk_erase_with_secure_erase*: Use secure erase by default when erasing disks.         Will only be used on devices that support secure erase.  Other devices will fall back to full wipe or quick erase depending on the selected options.
- *dns_trusted_acl*: List of external networks (not previously known), that will be allowed to use MAAS for DNS resolution..         MAAS keeps a list of networks that are allowed to use MAAS for DNS resolution. This option allows to add extra networks (not previously known) to the trusted ACL where this list of networks is kept. It also supports specifying IPs or ACL names.
- *dnssec_validation*: Enable DNSSEC validation of upstream zones.         Only used when MAAS is running its own DNS server. This value is used as the value of 'dnssec_validation' in the DNS server config.
- *enable_analytics*: Enable Google Analytics in MAAS UI to shape improvements in user experience.
- *enable_disk_erasing_on_release*: Erase node disks prior to releasing.         Forces users to always erase disks when releasing.
- *enable_http_proxy*: Enable the use of an APT or YUM and HTTP/HTTPS proxy.         Provision nodes to use the built-in HTTP proxy (or user specified proxy) for APT or YUM. MAAS also uses the proxy for downloading boot images.
- *enable_third_party_drivers*: Enable the installation of proprietary drivers (i.e. HPVSA).
- *enlist_commissioning*: Whether to run commissioning during enlistment..         Enables running all built-in commissioning scripts during enlistment.
- *force_v1_network_yaml*: Always use the legacy v1 YAML (rather than Netplan format, also known as v2 YAML) when composing the network configuration for a machine..
- *hardware_sync_interval*: Hardware Sync Interval.         The interval to send hardware info to MAAS from hardware sync enabled machines, in systemd time span syntax.
- *http_proxy*: Proxy for APT or YUM and HTTP/HTTPS.         This will be passed onto provisioned nodes to use as a proxy for APT or YUM traffic. MAAS also uses the proxy for downloading boot images. If no URL is provided, the built-in MAAS proxy will be used.
- *kernel_opts*: Boot parameters to pass to the kernel by default.
- *maas_auto_ipmi_cipher_suite_id*: MAAS IPMI Default Cipher Suite ID.         The default IPMI cipher suite ID to use when connecting to the BMC via ipmitools        Available choices are*: '' (freeipmi-tools default), '12' (12 - HMAC-MD5::MD5-128::AES-CBC-128), '17' (17 - HMAC-SHA256::HMAC_SHA256_128::AES-CBC-128), '3' (3 - HMAC-SHA1::HMAC-SHA1-96::AES-CBC-128), '8' (8 - HMAC-MD5::HMAC-MD5-128::AES-CBC-128).
- *maas_auto_ipmi_k_g_bmc_key*: The IPMI K_g key to set during BMC configuration..         This IPMI K_g BMC key is used to encrypt all IPMI traffic to a BMC. Once set, all clients will REQUIRE this key upon being commissioned. Any current machines that were previously commissioned will not require this key until they are recommissioned.
- *maas_auto_ipmi_user*: MAAS IPMI user..         The name of the IPMI user that MAAS automatically creates during enlistment/commissioning.
- *maas_auto_ipmi_user_privilege_level*: MAAS IPMI privilege level.         The default IPMI privilege level to use when creating the MAAS user and talking IPMI BMCs        Available choices are*: 'ADMIN' (Administrator), 'OPERATOR' (Operator), 'USER' (User).
- *maas_auto_ipmi_workaround_flags*: IPMI Workaround Flags.         The default workaround flag (-W options) to use for ipmipower commands        Available choices are*: '' (None), 'authcap' (Authcap), 'endianseq' (Endianseq), 'forcepermsg' (Forcepermsg), 'idzero' (Idzero), 'integritycheckvalue' (Integritycheckvalue), 'intel20' (Intel20), 'ipmiping' (Ipmiping), 'nochecksumcheck' (Nochecksumcheck), 'opensesspriv' (Opensesspriv), 'sun20' (Sun20), 'supermicro20' (Supermicro20), 'unexpectedauth' (Unexpectedauth).
- *maas_internal_domain*: Domain name used by MAAS for internal mapping of MAAS provided services..         This domain should not collide with an upstream domain provided by the set upstream DNS.
- *maas_name*: MAAS name.
- *maas_proxy_port*: Port to bind the MAAS built-in proxy (default*: 8000).         Defines the port used to bind the built-in proxy. The default port is 8000.
- *maas_syslog_port*: Port to bind the MAAS built-in syslog (default*: 5247).         Defines the port used to bind the built-in syslog. The default port is 5247.
- *max_node_commissioning_results*: The maximum number of commissioning results runs which are stored.
- *max_node_installation_results*: The maximum number of installation result runs which are stored.
- *max_node_testing_results*: The maximum number of testing results runs which are stored.
- *network_discovery*: .         When enabled, MAAS will use passive techniques (such as listening to ARP requests and mDNS advertisements) to observe networks attached to rack controllers. Active subnet mapping will also be available to be enabled on the configured subnets.
- *node_timeout*: Time, in minutes, until the node times out during commissioning, testing, deploying, or entering rescue mode..         Commissioning, testing, deploying, and entering rescue mode all set a timeout when beginning. If MAAS does not hear from the node within the specified number of minutes the node is powered off and set into a failed status.
- *ntp_external_only*: Use external NTP servers only.         Configure all region controller hosts, rack controller hosts, and subsequently deployed machines to refer directly to the configured external NTP servers. Otherwise only region controller hosts will be configured to use those external NTP servers, rack controller hosts will in turn refer to the regions' NTP servers, and deployed machines will refer to the racks' NTP servers.
- *ntp_servers*: Addresses of NTP servers.         NTP servers, specified as IP addresses or hostnames delimited by commas and/or spaces, to be used as time references for MAAS itself, the machines MAAS deploys, and devices that make use of MAAS DHCP services.
- *prefer_v4_proxy*: Sets IPv4 DNS resolution before IPv6.         If prefer_v4_proxy is set, the proxy will be set to prefer IPv4 DNS resolution before it attempts to perform IPv6 DNS resolution.
- *prometheus_enabled*: Enable sending stats to a prometheus gateway..         Allows MAAS to send statistics to Prometheus. This requires the 'prometheus_push_gateway' to be set.
- *prometheus_push_gateway*: Address or hostname of the Prometheus push gateway..         Defines the address or hostname of the Prometheus push gateway where MAAS will send data to.
- *prometheus_push_interval*: Interval of how often to send data to Prometheus (default*: to 60 minutes)..         The internal of how often MAAS will send stats to Prometheus in minutes.
- *promtail_enabled*: Enable streaming logs to Promtail..         Whether to stream logs to Promtail
- *promtail_port*: TCP port of the Promtail Push API..         Defines the TCP port of the Promtail push API where MAAS will stream logs to.
- *release_notifications*: Enable or disable notifications for new MAAS releases..
- *remote_syslog*: Remote syslog server to forward machine logs.         A remote syslog server that MAAS will set on enlisting, commissioning, testing, and deploying machines to send all log messages. Clearing this value will restore the default behaviour of forwarding syslog to MAAS.
- *subnet_ip_exhaustion_threshold_count*: If the number of free IP addresses on a subnet becomes less than or equal to this threshold, an IP exhaustion warning will appear for that subnet.
- *tls_cert_expiration_notification_enabled*: Notify when the certificate is due to expire.         Enable/Disable notification about certificate expiration.
- *tls_cert_expiration_notification_interval*: Certificate expiration reminder (days).         Configure notification when certificate is due to expire in (days).
- *upstream_dns*: Upstream DNS used to resolve domains not managed by this MAAS (space-separated IP addresses).         Only used when MAAS is running its own DNS server. This value is used as the value of 'forwarders' in the DNS server config.
- *use_peer_proxy*: Use the built-in proxy with an external proxy as a peer.         If enable_http_proxy is set, the built-in proxy will be configured to use http_proxy as a peer proxy. The deployed machines will be configured to use the built-in proxy.
- *use_rack_proxy*: Use DNS and HTTP metadata proxy on the rack controllers when a machine is booted..         All DNS and HTTP metadata traffic will flow through the rack controller that a machine is booting from. This isolated region controllers from machines.
- *vcenter_datacenter*: VMware vCenter datacenter.         VMware vCenter datacenter which is passed to a deployed VMware ESXi host.
- *vcenter_password*: VMware vCenter password.         VMware vCenter server password which is passed to a deployed VMware ESXi host.
- *vcenter_server*: VMware vCenter server FQDN or IP address.         VMware vCenter server FQDN or IP address which is passed to a deployed VMware ESXi host.
- *vcenter_username*: VMware vCenter username.         VMware vCenter server username which is passed to a deployed VMware ESXi host.
- *windows_kms_host*: Windows KMS activation host.         FQDN or IP address of the host that provides the KMS Windows activation service. (Only needed for Windows deployments using KMS activation.)
[/tab]
[/tabs]

* How to configure controllers
A rack controller can connect to multiple VLANs, each from a different network interface. A rack controller can only connect to one MAAS instance at any given time, and must connect to an instance that matches its MAAS version (major and minor).  This configuration provides a scaling factor that can help as a network architecture grows in size.

By contrast, a region controller manages communication with the user, via the Web UI/API, as well as managing the rack controller(s) in your system.  The MAAS postgres database is also managed by the region controller.  Typical region-level responsibilities include requesting that a rack controller boot a machine, and providing the ephemeral Ubuntu image needed to commission or enlist a machine.  

This article will help you learn:

- [How to install a rack controller](#heading--install-a-rack-controller)
- [How to list rack controllers](#heading--list-rack-controllers)
- [How to configure MAAS for multiple API servers](/t/how-to-enable-high-availability/5120#heading--multiple-region-endpoints)
- [How to unregister a rack controller](#heading--unregister-a-rack-controller)
- [About the potential dangers of moving a rack controller](#heading--dangers-moving-rack-controller)
- [How to move a rack controller from one MAAS instance to another](#heading--move-rack-controller)
- [How to set up PostgreSQL for the region](#heading--postgresql-setup)
- [How to add a new region host](#heading--adding-a-new-region-host)
- [How to improve region controller performance](#heading--increasing-regiond-daemon-workers)

** How to install a rack controller

[tabs]
[tab version="v3.4 Snap,v3.3 Snap"]
To install and register a rack controller with the MAAS:

``` bash
sudo snap install maas
sudo maas init rack --maas-url $MAAS_URL --secret $SECRET
```

The $SECRET is stored in file `/var/snap/maas/common/maas/secret` on the API server.
[/tab]
[tab version="v3.4 Packages,v3.3 Packages"]
To install and register a rack controller with the MAAS:

``` bash
sudo apt install maas-rack-controller
sudo maas-rack register --url $MAAS_URL --secret $SECRET
```

[note]
The register command is not required when you are adding a rack controller to a system that already houses an API server.
[/note]

The $SECRET is stored in file `/var/lib/maas/secret` on the API server.
[/tab]
[tab version="v3.2 Snap"]
To install and register a rack controller with the MAAS:

``` bash
sudo snap install maas
sudo maas init rack --maas-url $MAAS_URL --secret $SECRET
```

The $SECRET is stored in file `/var/snap/maas/common/maas/secret` on the API server.
[/tab]
[tab version="v3.2 Packages"]
To install and register a rack controller with the MAAS:

``` bash
sudo apt install maas-rack-controller
sudo maas-rack register --url $MAAS_URL --secret $SECRET
```

[note]
The register command is not required when you are adding a rack controller to a system that already houses an API server.
[/note]

The $SECRET is stored in file `/var/lib/maas/secret` on the API server.
[/tab]
[tab version="v3.1 Snap"]
To install and register a rack controller with the MAAS:

``` bash
sudo snap install maas
sudo maas init rack --maas-url $MAAS_URL --secret $SECRET
```

The $SECRET is stored in file `/var/snap/maas/common/maas/secret` on the API server.
[/tab]
[tab version="v3.1 Packages"]
To install and register a rack controller with the MAAS:

``` bash
sudo apt install maas-rack-controller
sudo maas-rack register --url $MAAS_URL --secret $SECRET
```

[note]
The register command is not required when you are adding a rack controller to a system that already houses an API server.
[/note]

The $SECRET is stored in file `/var/lib/maas/secret` on the API server.
[/tab]
[tab version="v3.0 Snap"]
To install and register a rack controller with the MAAS:

``` bash
sudo snap install maas
sudo maas init rack --maas-url $MAAS_URL --secret $SECRET
```

The $SECRET is stored in file `/var/snap/maas/common/maas/secret` on the API server.
[/tab]
[tab version="v3.0 Packages"]
To install and register a rack controller with the MAAS:

``` bash
sudo apt install maas-rack-controller
sudo maas-rack register --url $MAAS_URL --secret $SECRET
```

[note]
The register command is not required when you are adding a rack controller to a system that already houses an API server.
[/note]

The $SECRET is stored in file `/var/lib/maas/secret` on the API server.
[/tab]
[tab version="v2.9 Snap"]
To install and register a rack controller with the MAAS:

``` bash
sudo snap install maas
sudo maas init rack --maas-url $MAAS_URL --secret $SECRET
```

The $SECRET is stored in file `/var/snap/maas/common/maas/secret` on the API server.
[/tab]
[tab version="v2.9 Packages"]
To install and register a rack controller with the MAAS:

``` bash
sudo apt install maas-rack-controller
sudo maas-rack register --url $MAAS_URL --secret $SECRET
```

[note]
The register command is not required when you are adding a rack controller to a system that already houses an API server.
[/note]

The $SECRET is stored in file `/var/lib/maas/secret` on the API server.
[/tab]
[/tabs]

Note that on the UI, you can find complete instructions for adding a rack controller under the "Controllers" tab.  Simply click on the button labelled, "Add rack controller" and choose the instructions relevant to your build model (snap or packages).  The commands there will already include the correct MAAS URL and secret, so you can cut and paste them at the command line.

** How to list rack controllers

[tabs]
[tab version="v3.4 Snap,v3.4 Packages,v3.3 Snap,v3.3 Packages,v3.2 Snap,v3.2 Packages,v3.1 Snap,v3.1 Packages,v3.0 Snap,v3.0 Packages,v2.9 Snap,v2.9 Packages" view="UI"]
You can also list and confirm all registered rack controllers via the CLI; select the link at the top of the page to find out how.  Note that you will need multiple rack controllers to achieve specific [high availability](/t/how-to-enable-high-availability/5120) configurations.
[/tab]
[tab version="v3.4 Snap,v3.4 Packages,v3.3 Snap,v3.3 Packages,v3.2 Snap,v3.2 Packages,v3.1 Snap,v3.1 Packages,v3.0 Snap,v3.0 Packages,v2.9 Snap,v2.9 Packages" view="CLI"]
You can also list and confirm all registered rack controllers:

``` bash
maas $PROFILE rack-controllers read | grep hostname | cut -d '"' -f 4
```

Note that you will need multiple rack controllers to achieve specific [high availability](/t/how-to-enable-high-availability/5120) configurations.
[/tab]
[/tabs]

<div class="p-notification">
<p class="p-notification__response">If you are using VM nodes, you must ensure that the new rack controller can communicate with the VM host.</p>
</div>

** How to unregister a rack controller

Most likely, you would only “unregister” an extra, unnecessary rack controller.  In this case, you need to <em>delete</em> it from the region API server; there is no ‘unregister’ command.

To do so, navigate to the ‘Controllers’ page of the web UI. Enter the controller’s page by clicking on the machine you want to delete and select ‘Delete’ from the drop-down (and then ‘Delete controller’). MAAS will do the right thing if the controller is used for DHCP HA; that is, the DHCP HA needs to be disabled.

Although similar, this is not the same as deletion. Here, you are deleting a machine that is a part of MAAS itself.

<div class="p-notification">
<p class="p-notification__response">Unless you remove the software on this machine, rebooting it will cause the machine to re-instate itself as a rack controller. This behaviour may change with future versions of MAAS.</p>
</div>

** About the potential dangers of moving a rack controller

There are dangers associate with moving a rack controller -- dangers that may generate errors, get you into a non-working state, or cause you significant data loss.  These dangers are precipitated by one caveat and two potential mistakes:

- **Using the same system as a rack controller and a VM host:** While not forbidden or inherently dangerous, using the same machine as both a rack controller and a VM host may cause resource contention and poor performance.  If the resources on the system are not more than adequate to cover both tasks, you may see slowdowns (or even apparent "freeze" events) on the system.

- **Moving a rack controller from one version of MAAS to another:** MAAS rack controller software is an integral part of each version of MAAS.  If you delete a rack controller from, say, a 2.6 version of MAAS, and attempt to register that 2.6 version of the rack controller code to, say, a 2.9 version of MAAS, you may experience errors and potential data loss.  Using the above example, if you are running both a VM host and a rack controller for MAAS 2.6 on one system, and you suddenly decide to delete that rack controller from 2.6 and attempt to register the same code to a 2.9 MAAS, the VM host may fail or disappear.  This will possibly delete all the VMs you have created or connected to that VM host -- which may result in data loss.  This action is not supported.

- **Connecting one instance of a rack controller to two instances of MAAS, regardless of version:** Trying to connect a single rack controller to two different instances of MAAS can result in all sorts of unpredictable (and potentially catastrophic) behaviour.  It is not a supported configuration.

Take these warnings to heart.  It may seem like a faster approach to "bridge" your existing rack controllers from one MAAS to another -- or from one version of MAAS to another -- while they're running.  Ultimately, though, it will probably result in more work than just following the recommended approach.

** How to move a rack controller from one MAAS instance to another

[tabs]
[tab version="v3.4 Snap,v3.4 Packages" view="UI"]
To move a rack controller, you must delete the rack controller from one MAAS instance and reinstantiate it on another one.  To delete a rack controller:

1. Select *Controllers*.

2. Checkbox the controller you want to remove.

3. Select *Take action*.

4. Choose *Delete*.

5. Confirm your choice by selecting *Delete controller*.

[/tab]
[tab version="v3.3 Snap,v3.3 Packages,v3.2 Snap,v3.2 Packages,v3.1 Snap,v3.1 Packages,v3.0 Snap,v3.0 Packages,v2.9 Snap,v2.9 Packages" view="UI"]
In effect, there is no such action as moving a rack controller, although you can delete a rack controller from one MAAS and reinstantiate the same controller (binary-wise) on another MAAS instance.  First, delete the rack controller.  In the "Controllers" tab in the UI, select the rack controller you with to delete, choose "Take action" and select "Delete."  You will be asked to confirm with a red button, entitled "Delete 1 controller."
[/tab]
[tab version="v3.4 Snap,v3.4 Packages,v3.3 Snap,v3.3 Packages,v3.2 Snap,v3.2 Packages,v3.1 Snap,v3.1 Packages,v3.0 Snap,v3.0 Packages,v2.9 Snap,v2.9 Packages" view="CLI"]
In effect, there is no such action as moving a rack controller, although you can delete a rack controller from one MAAS and reinstantiate the same controller (binary-wise) on another MAAS instance.  First, delete the rack controller, with the command:

```
maas $PROFILE rack-controller delete $SYSTEM_ID
```

where `$PROFILE` is your admin profile name, and `$SYSTEM_ID` can be found by examining the output of the command:

```
maas $PROFILE rack-controllers read
```

There is no confirmation step, so make sure you have the right rack controller before proceeding.
[/tab]
[/tabs]

Next, you must register a new rack controller, which is always done from the command line.

[tabs]
[tab version="v3.4 Snap,v3.3 Snap"]
For this exercise, we're assuming you are using the already installed rack controller code that was previously running on the "from" MAAS instance.  All that's necessary is that you register a new rack controller with the "to" MAAS instance, like this:

```
sudo maas init rack --maas-url $MAAS_URL_OF_NEW_MAAS --secret $SECRET_FOR_NEW_MAAS
```

where the secret is found in `/var/snap/maas/common/maas/secret`.
[/tab]
[tab version="v3.3 Packages,v3.3 Packages"]
For this exercise, we're assuming you are using the already installed rack controller code that was previously running on the "from" MAAS instance.  All that's necessary is that you register a new rack controller with the "to" MAAS instance, like this:

```
sudo maas-rack register --url $MAAS_URL_OF_NEW_MAAS --secret $SECRET_FOR_NEW_MAAS
```

where the secret is found in `/var/lib/maas/secret`.
[/tab]
[tab version="v3.2 Snap"]
For this exercise, we're assuming you are using the already installed rack controller code that was previously running on the "from" MAAS instance.  All that's necessary is that you register a new rack controller with the "to" MAAS instance, like this:

```
sudo maas init rack --maas-url $MAAS_URL_OF_NEW_MAAS --secret $SECRET_FOR_NEW_MAAS
```

where the secret is found in `/var/snap/maas/common/maas/secret`.
[/tab]
[tab version="v3.2 Packages"]
For this exercise, we're assuming you are using the already installed rack controller code that was previously running on the "from" MAAS instance.  All that's necessary is that you register a new rack controller with the "to" MAAS instance, like this:

```
sudo maas-rack register --url $MAAS_URL_OF_NEW_MAAS --secret $SECRET_FOR_NEW_MAAS
```

where the secret is found in `/var/lib/maas/secret`.
[/tab]
[tab version="v3.1 Snap"]
For this exercise, we're assuming you are using the already installed rack controller code that was previously running on the "from" MAAS instance.  All that's necessary is that you register a new rack controller with the "to" MAAS instance, like this:

```
sudo maas init rack --maas-url $MAAS_URL_OF_NEW_MAAS --secret $SECRET_FOR_NEW_MAAS
```

where the secret is found in `/var/snap/maas/common/maas/secret`.
[/tab]
[tab version="v3.1 Packages"]
For this exercise, we're assuming you are using the already installed rack controller code that was previously running on the "from" MAAS instance.  All that's necessary is that you register a new rack controller with the "to" MAAS instance, like this:

```
sudo maas-rack register --url $MAAS_URL_OF_NEW_MAAS --secret $SECRET_FOR_NEW_MAAS
```

where the secret is found in `/var/lib/maas/secret`.
[/tab]
[tab version="v3.0 Snap"]
For this exercise, we're assuming you are using the already installed rack controller code that was previously running on the "from" MAAS instance.  All that's necessary is that you register a new rack controller with the "to" MAAS instance, like this:

```
sudo maas init rack --maas-url $MAAS_URL_OF_NEW_MAAS --secret $SECRET_FOR_NEW_MAAS
```

where the secret is found in `/var/snap/maas/common/maas/secret`.
[/tab]
[tab version="v3.0 Packages"]
For this exercise, we're assuming you are using the already installed rack controller code that was previously running on the "from" MAAS instance.  All that's necessary is that you register a new rack controller with the "to" MAAS instance, like this:

```
sudo maas-rack register --url $MAAS_URL_OF_NEW_MAAS --secret $SECRET_FOR_NEW_MAAS
```

where the secret is found in `/var/lib/maas/secret`.
[/tab]
[tab version="v2.9 Snap"]
For this exercise, we're assuming you are using the already installed rack controller code that was previously running on the "from" MAAS instance.  All that's necessary is that you register a new rack controller with the "to" MAAS instance, like this:

```
sudo maas init rack --maas-url $MAAS_URL_OF_NEW_MAAS --secret $SECRET_FOR_NEW_MAAS
```

where the secret is found in `/var/snap/maas/common/maas/secret`.
[/tab]
[tab version="v2.9 Packages"]
For this exercise, we're assuming you are using the already installed rack controller code that was previously running on the "from" MAAS instance.  All that's necessary is that you register a new rack controller with the "to" MAAS instance, like this:

```
sudo maas-rack register --url $MAAS_URL_OF_NEW_MAAS --secret $SECRET_FOR_NEW_MAAS
```

where the secret is found in `/var/lib/maas/secret`.
[/tab]
[/tabs]

Note that in the UI, if you go to the "Controllers" tab and press the button entitled, "Add rack controller," at the top of the Controllers screen, MAAS will give you a complete command string, including the correct URL and secret values.  Simply cut and paste that string to move the rack controller, paying attention to whether you are using snap or package build modes.

*** About moving a rack controller from one MAAS instance to another

In the course of normal operations, you may wish to move a device acting as a rack controller from one MAAS instance to another.  From the point of view of MAAS, there is no such action as moving a rack controller, although you can delete a rack controller from one MAAS and reinstantiate the same controller (binary-wise) on another MAAS instance. From your perspective, of course, you are moving one box performing rack controller functions, either physically or network-wise, from one MAAS to another. 

*** How to move the controller

[tabs]
[tab version="v3.4 Snap,v3.4 Packages,v3.3 Snap,v3.3 Packages,v3.2 Snap,v3.2 Packages,v3.1 Snap,v3.1 Packages,v3.0 Snap,v3.0 Packages,v2.9 Snap,v2.9 Packages" view="UI"]
To move a rack controller using the MAAS UI, first, delete the rack controller.  In the "Controllers" tab in the UI, select the rack controller you with to delete, choose "Take action" and select "Delete."  You will be asked to confirm with a red button, entitled "Delete 1 controller."
[/tab]
[tab version="v3.4 Snap,v3.4 Packages,v3.3 Snap,v3.3 Packages,v3.2 Snap,v3.2 Packages,v3.1 Snap,v3.1 Packages,v3.0 Snap,v3.0 Packages,v2.9 Snap,v2.9 Packages" view="CLI"]
In effect, there is no such action as moving a rack controller, although you can delete a rack controller from one MAAS and reinstantiate the same controller (binary-wise) on another MAAS instance.  First, delete the rack controller, with the command:

```
maas $PROFILE rack-controller delete $SYSTEM_ID
```

where `$PROFILE` is your admin profile name, and `$SYSTEM_ID` can be found by examining the output of the command:

```
maas $PROFILE rack-controllers read
```

There is no confirmation step, so make sure you have the right rack controller before proceeding.
[/tab]
[/tabs]

Next, you must register a new rack controller, which is always done from the command line.

[tabs]
[tab version="v3.4 Snap,v3.4 Packages,v3.3 Snap"]
For this exercise, we're assuming you are using the already installed rack controller code that was previously running on the "from" MAAS instance.  All that's necessary is that you register a new rack controller with the "to" MAAS instance, like this:

```
sudo maas init rack --maas-url $MAAS_URL_OF_NEW_MAAS --secret $SECRET_FOR_NEW_MAAS
```

where the secret is found in `/var/snap/maas/common/maas/secret`.
[/tab]
[tab version="v3.3 Packages"]
For this exercise, we're assuming you are using the already installed rack controller code that was previously running on the "from" MAAS instance.  All that's necessary is that you register a new rack controller with the "to" MAAS instance, like this:

```
sudo maas-rack register --url $MAAS_URL_OF_NEW_MAAS --secret $SECRET_FOR_NEW_MAAS
```

where the secret is found in `/var/lib/maas/secret`.
[/tab]
[tab version="v3.2 Snap"]
For this exercise, we're assuming you are using the already installed rack controller code that was previously running on the "from" MAAS instance.  All that's necessary is that you register a new rack controller with the "to" MAAS instance, like this:

```
sudo maas init rack --maas-url $MAAS_URL_OF_NEW_MAAS --secret $SECRET_FOR_NEW_MAAS
```

where the secret is found in `/var/snap/maas/common/maas/secret`.
[/tab]
[tab version="v3.2 Packages"]
For this exercise, we're assuming you are using the already installed rack controller code that was previously running on the "from" MAAS instance.  All that's necessary is that you register a new rack controller with the "to" MAAS instance, like this:

```
sudo maas-rack register --url $MAAS_URL_OF_NEW_MAAS --secret $SECRET_FOR_NEW_MAAS
```

where the secret is found in `/var/lib/maas/secret`.
[/tab]
[tab version="v3.1 Snap"]
For this exercise, we're assuming you are using the already installed rack controller code that was previously running on the "from" MAAS instance.  All that's necessary is that you register a new rack controller with the "to" MAAS instance, like this:

```
sudo maas init rack --maas-url $MAAS_URL_OF_NEW_MAAS --secret $SECRET_FOR_NEW_MAAS
```

where the secret is found in `/var/snap/maas/common/maas/secret`.
[/tab]
[tab version="v3.1 Packages"]
For this exercise, we're assuming you are using the already installed rack controller code that was previously running on the "from" MAAS instance.  All that's necessary is that you register a new rack controller with the "to" MAAS instance, like this:

```
sudo maas-rack register --url $MAAS_URL_OF_NEW_MAAS --secret $SECRET_FOR_NEW_MAAS
```

where the secret is found in `/var/lib/maas/secret`.
[/tab]
[tab version="v3.0 Snap"]
For this exercise, we're assuming you are using the already installed rack controller code that was previously running on the "from" MAAS instance.  All that's necessary is that you register a new rack controller with the "to" MAAS instance, like this:

```
sudo maas init rack --maas-url $MAAS_URL_OF_NEW_MAAS --secret $SECRET_FOR_NEW_MAAS
```

where the secret is found in `/var/snap/maas/common/maas/secret`.
[/tab]
[tab version="v3.0 Packages"]
For this exercise, we're assuming you are using the already installed rack controller code that was previously running on the "from" MAAS instance.  All that's necessary is that you register a new rack controller with the "to" MAAS instance, like this:

```
sudo maas-rack register --url $MAAS_URL_OF_NEW_MAAS --secret $SECRET_FOR_NEW_MAAS
```

where the secret is found in `/var/lib/maas/secret`.
[/tab]
[tab version="v2.9 Snap"]
For this exercise, we're assuming you are using the already installed rack controller code that was previously running on the "from" MAAS instance.  All that's necessary is that you register a new rack controller with the "to" MAAS instance, like this:

```
sudo maas init rack --maas-url $MAAS_URL_OF_NEW_MAAS --secret $SECRET_FOR_NEW_MAAS
```

where the secret is found in `/var/snap/maas/common/maas/secret`.
[/tab]
[tab version="v2.9 Packages"]
For this exercise, we're assuming you are using the already installed rack controller code that was previously running on the "from" MAAS instance.  All that's necessary is that you register a new rack controller with the "to" MAAS instance, like this:

```
sudo maas-rack register --url $MAAS_URL_OF_NEW_MAAS --secret $SECRET_FOR_NEW_MAAS
```

where the secret is found in `/var/lib/maas/secret`.
[/tab]
[/tabs]

Note that in the UI, if you go to the "Controllers" tab and press the button entitled, "Add rack controller," at the top of the Controllers screen, MAAS will give you a complete command string, including the correct URL and secret values.  Simply cut and paste that string to move the rack controller, paying attention to whether you are using snap or package build modes.

** How to set up PostgreSQL for the region

Any number of API servers (region controllers) can be present as long as each connects to the same PostgreSQL database and allows the required number of connections.

On the primary database host, edit file <code>/etc/postgresql/9.5/main/pg_hba.conf</code> to allow the eventual secondary API server to contact the primary PostgreSQL database. Include the below line, replacing
<code>$SECONDARY_API_SERVER_IP</code> with the IP address of the host that will contain the secondary API server:

    host maasdb maas $SECONDARY_API_SERVER_IP/32 md5

[note]
The primary database and API servers often reside on the same host.
[/note]

Apply this change by restarting the database:

    sudo systemctl restart postgresql

** How to add a new region host

On a secondary host, add the new region controller by installing <code>maas-region-api</code>:

    sudo apt install maas-region-api

You will need the <code>/etc/maas/regiond.conf</code> file from the primary API server. Below, we assume it can be copied (scp) from the ‘ubuntu’ account home directory using password authentication (adjust otherwise). The <code>local_config_set</code> command will edit that file by pointing to the host that contains the primary PostgreSQL database. Do not worry: MAAS will rationalise the DNS (<code>bind9</code>) configuration options so that they match those used within MAAS:

    sudo systemctl stop maas-regiond
    sudo scp ubuntu@$PRIMARY_API_SERVER:regiond.conf /etc/maas/regiond.conf
    sudo chown root:maas /etc/maas/regiond.conf
    sudo chmod 640 /etc/maas/regiond.conf
    sudo maas-region local_config_set --database-host $PRIMARY_PG_SERVER
    sudo systemctl restart bind9
    sudo systemctl start maas-regiond

Check three log files for any errors:

[tabs]
[tab version="v3.4 Snap,v3.4 Packages,v3.3 Snap"]
1. <code>/var/snap/maas/common/log/regiond.log</code>
2. <code>/var/snap/maas/common/log/maas.log</code>
3. <code>/var/snap/maas/common/log/rsyslog/</code>
[/tab]
[tab version="v3.3 Packages"]
1. <code>/var/log/maas/regiond.log</code>
2. <code>/var/log/maas/maas.log</code>
3. <code>/var/log/syslog</code>
[/tab]
[tab version="v3.2 Snap"]
1. <code>/var/snap/maas/common/log/regiond.log</code>
2. <code>/var/snap/maas/common/log/maas.log</code>
3. <code>/var/snap/maas/common/log/rsyslog/</code>
[/tab]
[tab version="v3.2 Packages"]
1. <code>/var/log/maas/regiond.log</code>
2. <code>/var/log/maas/maas.log</code>
3. <code>/var/log/syslog</code>
[/tab]
[tab version="v3.1 Snap"]
1. <code>/var/snap/maas/common/log/regiond.log</code>
2. <code>/var/snap/maas/common/log/maas.log</code>
3. <code>/var/snap/maas/common/log/rsyslog/</code>
[/tab]
[tab version="v3.1 Packages"]
1. <code>/var/log/maas/regiond.log</code>
2. <code>/var/log/maas/maas.log</code>
3. <code>/var/log/syslog</code>
[/tab]
[tab version="v3.0 Snap"]
1. <code>/var/snap/maas/common/log/regiond.log</code>
2. <code>/var/snap/maas/common/log/maas.log</code>
3. <code>/var/snap/maas/common/log/rsyslog/</code>
[/tab]
[tab version="v3.0 Packages"]
1. <code>/var/log/maas/regiond.log</code>
2. <code>/var/log/maas/maas.log</code>
3. <code>/var/log/syslog</code>
[/tab]
[tab version="v2.9 Snap"]
1. <code>/var/snap/maas/common/log/regiond.log</code>
2. <code>/var/snap/maas/common/log/maas.log</code>
3. <code>/var/snap/maas/common/log/rsyslog/</code>
[/tab]
[tab version="v2.9 Packages"]
1. <code>/var/log/maas/regiond.log</code>
2. <code>/var/log/maas/maas.log</code>
3. <code>/var/log/syslog</code>
[/tab]
[/tabs]

** How to improve region controller performance

[note]
This functionality is available starting from MAAS 2.4.
[/note]

The MAAS Region Controller is a daemon collection of 4 workers that are in charge of handling all the internals of MAAS. The regiond workers handle the UI, API and the internal communication between Region and Rack controllers.

In larger environments, which multiple rack controllers, you can easily improve performance within a region.  You can increase the number of workers, which allows faster (parallel) handling of internal communication between region and rack controllers.

Increasing the number of workers will also increase the number of required database connections by 11 per extra worker. This may required PostgreSQL to have an increased number of allowed connections; please see [the high availability article](/t/how-to-enable-high-availability/5120#heading--region-controller-ha) for more information on increasing the connections.

To increase the number of workers, simply edit <code>regiond.conf (/etc/maas/regiond.conf)</code> and set <code>num_workers</code>. For example:

    [...]
    num_workers: 8

Keep in mind that adding too many workers may <em>reduce</em> performance. We recommended one worker per CPU, up to eight workers in total. Increasing beyond that is possible but use at your own risk.

* How to connect MAAS networks
You can easily manage the basic networking elements of MAAS, including subnets, fabrics, VLANs, spaces, IP ranges, machine interfaces, and proxies.

This article will help you learn:

- [How to manage MAAS network elements](#heading--how-to-manage-MAAS-network-elements)
- [How to manage machine interfaces](#heading--how-to-manage-machine-interfaces)
- [How to manage proxies](#heading--how-to-manage-proxies)
- [How to set up Network Time Protocol (NTP)](#heading--how-to-set-up-ntp)

Feel free to review some of the [theory](/t/networking/6680) first.

** How to manage MAAS network elements

This section will show you:

[tabs]
[tab version="v3.4 Snap,v3.4 Packages,v3.3 Snap,v3.3 Packages,v3.2 Snap,v3.2 Packages,v3.1 Snap,v3.1 Packages,v3.0 Snap,v3.0 Packages,v2.9 Snap,v2.9 Packages" view="UI"]
- [How to enable network discovery](#heading--how-to-enable-network-discovery)
- [How to toggle subnet management](#heading--how-to-toggle-subnet-management)
- [How to access the main networking view](#heading--how-to-access-ui-main-networking-view)
- [How to display the subnet window](#heading--ui-how-to-display-the-subnet-window)
- [How to view the subnet summary](#heading--ui-how-to-view-the-subnet-summary)
- [How to view utilisation](#heading--ui-how-to-view-utilisation)
- [How to manage static routes between subnets](#heading--how-to-manage-static-routes)
- [How to view reserved ranges](#heading--how-to-view-reserved-ranges)
- [How to view used IP addresses](#heading--ui-how-to-view-used-ip-addresses)
- [How to set up a bridge with MAAS](#heading--how-to-set-up-a-bridge-with-maas)
- [How to set up a bridge with netplan](#heading--how-to-set-up-a-bridge-with-netplan)
[/tab]
[tab version="v3.4 Snap,v3.4 Packages,v3.3 Snap,v3.3 Packages,v3.2 Snap,v3.2 Packages,v3.1 Snap,v3.1 Packages,v3.0 Snap,v3.0 Packages,v2.9 Snap,v2.9 Packages" view="CLI"]
- [How to enable network discovery](#heading--how-to-enable-network-discovery)
- [How to toggle subnet management](#heading--how-to-toggle-subnet-management)
- [How to determine fabric ID](#heading--how-to-determine-fabric-id)
- [How to set a default gateway](#heading--how-to-set-a-default-gateway)
- [How to set a DNS server](#heading--how-to-set-a-dns-server)
- [How to list available subnets](#heading--cli-list-available-subnets)
- [How to view subnet details](#heading--cli-how-to-view-subnet-details)
- [How to manage static routes between subnets](#heading--how-to-manage-static-routes)
- [How to set up a bridge with MAAS](#heading--how-to-set-up-a-bridge-with-maas)
- [How to set up a bridge with netplan](#heading--how-to-set-up-a-bridge-with-netplan)
[/tab]
[/tabs]

*** How to enable network discovery

[tabs]
[tab version="v3.4 Snap,v3.4 Packages" view="UI"]
To enable network discovery:

1. Select *Networking > Network discovery* in the left navigation panel.

2. Select the *Configuration* tab at the top of the *Network discovery* panel.

3. In the *Network discovery* dropdown, select "Enabled".

4. Select *Save* to register your changes.
[/tab]
[tab version="v3.4 Snap,v3.4 Packages,v3.3 Snap,v3.3 Packages,v3.2 Snap,v3.2 Packages,v3.1 Snap,v3.1 Packages,v3.0 Snap,v3.0 Packages,v2.9 Snap,v2.9 Packages" view="UI"]
To enable network discovery:

1. Click on "Canonical MAAS" at the top left of the screen.

2. Click on "Configuration".

3. In the dropdown labeled "Network discovery", choose "Enabled" or "Disabled".

[/tab]
[tab version="v3.4 Snap,v3.4 Packages,v3.3 Snap,v3.3 Packages,v3.2 Snap,v3.2 Packages,v3.1 Snap,v3.1 Packages,v3.0 Snap,v3.0 Packages,v2.9 Snap,v2.9 Packages" view="CLI"]
To enable network discovery, enter the following at the command line:

```nohighlight
maas $PROFILE maas set-config name=network_discovery value="enabled"
```

If successful, you should receive output similar to:

```nohighlight
Success.
Machine-readable output follows:
OK
```

Network discovery can be disabled or re-enabled at any time with this CLI command.
[/tab]
[/tabs]

*** How to toggle subnet management

[tabs]
[tab version="v3.4 Snap,v3.4 Packages" view="UI"]
To toggle subnet management:

1. Select *Subnets* from the left navigation panel.

2. Select the subnet you wish to change by clicking on its address.

3. Select *Edit* in the upper right of the *Subnet summary* panel.

4. Select *Managed allocation* to toggle between enabled and disabled.

5. Select *Save* to register your changes.

You can (re)enable subnet management at any time by checking *Managed allocation*.

*** How to access the main networking view

To access the main networking view, select *Networking > Subnets*.

This main view can also be filtered through the use of the 'Filters' drop-down.
[/tab]
[tab version="v3.3 Snap,v3.3 Packages,v3.2 Snap,v3.2 Packages,v3.1 Snap,v3.1 Packages,v3.0 Snap,v3.0 Packages,v2.9 Snap,v2.9 Packages" view="UI"]
To disable (or re-enable) subnet management, use the following procedure:

1. Select *Subnets*.

2. Select the subnet in question

3. Select *Edit*.

4. Select *Managed allocation* to toggle between enabled and disabled.

5. Select *Save summary*.

*** How to access the main networking view

To access the main networking view, select *Subnets*.

This main view can also be filtered via the *Group by* drop-down.
[/tab]
[tab version="v3.4 Snap,v3.4 Packages,v3.3 Snap,v3.3 Packages,v3.2 Snap,v3.2 Packages,v3.1 Snap,v3.1 Packages,v3.0 Snap,v3.0 Packages,v2.9 Snap,v2.9 Packages" view="CLI"]
To enable or disable subnet management:

``` nohighlight
maas $PROFILE subnet update $SUBNET_CIDR managed=false|true
```

For example, to disable subnet management:

``` nohighlight
maas $PROFILE subnet update 192.168.1.0/24 managed=false
```

You can use the subnets ID in place of the CIDR address.

*** How to determine fabric ID

To determine a fabric ID based on a subnet address:

```nohighlight
FABRIC_ID=$(maas $PROFILE subnet read $SUBNET_CIDR \
    | grep fabric | cut -d ' ' -f 10 | cut -d '"' -f 2)
```

This may come in handy when you need a fabric ID for other CLI calls.

*** How to set a default gateway

To set the default gateway for a subnet:

```nohighlight
maas $PROFILE subnet update $SUBNET_CIDR gateway_ip=$MY_GATEWAY
```

*** How to set a DNS server

To set the DNS server for a subnet:

```nohighlight
maas $PROFILE subnet update $SUBNET_CIDR dns_servers=$MY_NAME SERVER
```

*** How to list available subnets

To view the list of available subnets, enter the following command:

```nohighlight
maas admin subnets read | \
jq -r '(["FABRIC", "VLAN", "DHCP", "SUBNET"]
| (., map(length*"-"))),
(.[] | [.vlan.fabric, .vlan.name, .vlan.dhcp_on, .cidr])
| @tsv' \
| column -t
```
[/tab]
[/tabs]

[tabs]
[tab version="v3.4 Snap,v3.4 Packages,v3.3 Snap,v3.3 Packages,v3.2 Snap,v3.2 Packages,v3.1 Snap,v3.1 Packages,v3.0 Snap,v3.0 Packages,v2.9 Snap,v2.9 Packages" view="UI"]
*** How to display the subnet window

Selecting a subnet will display its [detail screen](/t/networking/6680#heading--the-subnet-summary).

*** How to view subnet utilisation

This section of the subnet page presents [metrics](/t/about-networking/6680/#heading--subnet-utilisation).
[/tab]
[tab version="v3.4 Snap,v3.4 Packages,v3.3 Snap,v3.3 Packages,v3.2 Snap,v3.2 Packages,v3.1 Snap,v3.1 Packages,v3.0 Snap,v3.0 Packages,v2.9 Snap,v2.9 Packages" view="CLI"]
*** How to view subnet details

View the details of an individual subnet with the command:

```nohighlight
maas $PROFILE subnet read $SUBNET_ID \
| jq -r '(["NAME","CIDR","GATEWAY","DNS","DISCOVERY","FABRIC","VLAN"]
| (., map(length*"-"))), ([.name,.cidr,.gateway_ip // "-", .allow_dns,.active_discovery,.vlan.name,.vlan.fabric]) | @tsv' | column -t
```

Look up the subnet ID like this:

```nohighlight
maas $PROFILE subnets read \
| jq -r '(["NAME", "SUBNET_ID"]
| (., map(length*"-"))), (.[] | [.name, .id]) | @tsv' \
| column -t | grep $SUBNET_NAME
```

For example, using the "admin" profile with a subnet name containing "192.168.123," find the subnet ID with this command:

```nohighlight
maas admin subnets read \
| jq -r '(["NAME", "SUBNET_ID"]
| (., map(length*"-"))), (.[] | [.name, .id]) | @tsv' \
| column -t | grep 192.168.123
```
[/tab]
[/tabs]

 
*** How to manage static routes between subnets

[tabs]
[tab version="v3.4 Snap,v3.4 Packages" view="UI"]
To create a static route:

1. Select *Networking > Subnets*.

2. Select the Subnet you want to change by clicking on its IP address.

3. In the *Subnet summary* pane, scroll down to *Add static route* and select it.

4. Enter a *Gateway IP* address.

5. Select a *Destination* subnet from the dropdown.

6. Enter a routing *Metric* value, if desired.

7. Select *Save* to register your changes.
[/tab]
[tab version="v3.3 Snap,v3.3 Packages,v3.2 Snap,v3.2 Packages,v3.1 Snap,v3.1 Packages,v3.0 Snap,v3.0 Packages,v2.9 Snap,v2.9 Packages" view="UI"]
To create a static route:

1. Click the 'Add static route' button to reveal the edit pane. 

2. Enter a Gateway IP address.

3. Select a destination subnet from the 'Destination' drop-down list.

4. Edit the routing metric value if needed. 

5. Click 'Add' to activate the route. 

Routes can be edited and removed using the icons to the right of each entry.

[/tab]
[tab version="v3.4 Snap,v3.4 Packages,v3.3 Snap,v3.3 Packages,v3.2 Snap,v3.2 Packages,v3.1 Snap,v3.1 Packages,v3.0 Snap,v3.0 Packages,v2.9 Snap,v2.9 Packages" view="CLI"]
To create a static route between two subnets, use the following command:

```nohighlight
maas admin static-routes create source=$SOURCE_SUBNET destination=$DEST_SUBNET \
gateway_ip=$GATEWAY_IP
```
[/tab]
[/tabs]

[tabs]
[tab version="v3.4 Snap,v3.4 Packages,v3.3 Snap,v3.3 Packages,v3.2 Snap,v3.2 Packages,v3.1 Snap,v3.1 Packages,v3.0 Snap,v3.0 Packages,v2.9 Snap,v2.9 Packages" view="UI"]
*** How to view reserved ranges

The reserved ranges section of the subnet screen contains information on [defined IP ranges](/t/how-to-enable-dhcp/5132#heading--how-to-manage-ip-ranges).

*** How to view used IP addresses

The "Used IP addresses" section displays hosts (including controllers) associated with the used addresses along with related bits of host information.
[/tab]
[/tabs]

*** How to set up a bridge with MAAS

[tabs]
[tab version="v3.4 Snap,v3.4 Packages,v3.3 Snap,v3.3 Packages,v3.2 Snap,v3.2 Packages,v3.1 Snap,v3.1 Packages,v3.0 Snap,v3.0 Packages,v2.9 Snap,v2.9 Packages" view="UI"]
To configure a bridge with the MAAS UI: 

1. Select *Machines*.

2. Select the machine you want to bridge. 

3. Select *Network*. 

4. Checkbox the network where you want to create the bridge.

5. Select *Create bridge*.

6. Fill in the bridge details in the form which appears.

7. Optionally include the bridge in the spanning tree protocol by selecting the slider under *Advanced options*.

8. Optionally set the *Forward delay* in milliseconds.

9. Select *Save interface* to register your changes.

You can then deploy machines using this bridge.

[note]
You can create an "Open switch" bridge if desired, and MAAS will create the netplan model for you.
[/note]
[/tab] 
[tab version="v3.4 Snap,v3.4 Packages,v3.3 Snap,v3.3 Packages,v3.2 Snap,v3.2 Packages,v3.1 Snap,v3.1 Packages,v3.0 Snap,v3.0 Packages,v2.9 Snap,v2.9 Packages" view="CLI"]
You can use the MAAS CLI/API to configure a bridge via the following procedure:

1. Select the interface on which you wish to configure the bridge. This example uses the boot interface, since the boot interface must be connected to a MAAS controlled network -- but any interface is allowed:

        INTERFACE_ID=$(maas $PROFILE machine read $SYSTEM_ID | jq .boot_interface.id)

2. Create the bridge:

         BRIDGE_ID=$(maas $PROFILE interfaces create-bridge $SYSTEM_ID name=br0 parent=$INTERFACE_ID | jq .id)

3. Select the subnet where you want the bridge (this should be a MAAS controlled subnet):

        SUBNET_ID=$(maas $PROFILE subnets read | jq -r '.[] | select(.cidr == "10.0.0.0/24" and .managed == true).id')

4. Connect the bridge to the subnet:

          maas $PROFILE interface link-subnet $SYSTEM_ID $BRIDGE_ID subnet=$SUBNET_ID mode="STATIC" ip_address="10.0.0.101"

[/tab]
[/tabs]

*** How to set up a bridge with netplan

To use netplan to configure a bridge:

1. Open your netplan configuration file.  This should be in `/etc/netplan`.  It could be called `50-cloud-init.yaml`, `netplan.yaml`, or something else.  

2. Modify the file to add a bridge, using the following example as a guide:

```nohighlight
network:
    bridges:
        br0:
            addresses:
            - 10.0.0.101/24
            gateway4: 10.0.0.1
            interfaces:
            - enp1s0
            mac address: 52:54:00:39:9d:f9
            mtu: 1500
            name servers:
                addresses:
                - 10.0.0.2
                search:
                - maas
            parameters:
                forward-delay: 15
                stp: false
    Ethernet's:
        enp1s0:
            match:
                mac address: 52:54:00:39:9d:f9
            mtu: 1500
            set-name: enp1s0
        enp2s0:
            match:
                mac address: 52:54:00:df:87:ac
            mtu: 1500
            set-name: enp2s0
        enp3s0:
            match:
                mac address: 52:54:00:a7:ac:46
            mtu: 1500
            set-name: enp3s0
    version: 2
```

3. Apply the new configuration with `netplan apply`.

** How to manage machine interfaces

This section will explain the following procedures related to machine interfaces:

- [How to edit machine interfaces](#heading--how-to-edit-interfaces")
- [How to create a bond interface](#heading--bond-interfaces)
- [How to create a bridge interface](#heading--bridge-interfaces)
- [How to delete an interface](#heading--delete-an-interface)
- [How to assign a network interface to a fabric](#heading--assign-a-network-interface-to-a-fabric)
- [How to discover interface identifiers](#heading--interface-identifiers)
- [How to create a VLAN interface](#heading--create-a-vlan-interface)
- [How to delete a VLAN interface](#heading--delete-a-vlan-interface)

*** How to edit machine interfaces

[tabs]
[tab version="v3.4 Snap,v3.4 Packages" view="UI"]
To edit a machine interface:

1. Select *Machines*.

2. Select the machine in question by clicking on its name.

3. Select *Network*.

4. Checkbox the interface in question.

5. Click the *Actions* dropdown at the right end of row for that interface.

6. Select *Edit physical*.

7. Change any of the desired parameters in the form which appears.

8. Select an [*IP mode*](/t/networking/6680#heading--IP-modes).

9. Select *Save interface* to register your changes.
[/tab]
[tab version="v3.3 Snap,v3.3 Packages,v3.2 Snap,v3.2 Packages,v3.1 Snap,v3.1 Packages,v3.0 Snap,v3.0 Packages,v2.9 Snap,v2.9 Packages" view="UI"]
To edit a machine interface:

1. From a machine's "Interfaces" page, click the menu icon for the interface to be edited and select *Edit Physical* from the resulting menu.

2. Select an *IP mode* from the drop-down menu.  More information about IP modes is found in [About networks](/t/about-networks/6880**heading--IP-modes).

3. Press *Save* to apply the changes.
[/tab]
[tab version="v3.4 Snap,v3.4 Packages,v3.3 Snap,v3.3 Packages,v3.2 Snap,v3.2 Packages,v3.1 Snap,v3.1 Packages,v3.0 Snap,v3.0 Packages,v2.9 Snap,v2.9 Packages" view="CLI"]
To edit the IP assignment mode of a network interface, perform the following steps:

1. Find the interface ID and subnet link ID with the command:

``` bash
maas $PROFILE node read $SYSTEM_ID
```

2. Unlink the old interface:

``` bash
maas $PROFILE interface unlink-subnet $SYSTEM_ID $INTERFACE_ID id=$SUBNET_LINK_ID
```

3. Link the new interface:

```bash
maas $PROFILE interface link-subnet $SYSTEM_ID $INTERFACE_ID mode=$IP_MODE subnet=$SUBNET_CIDR [$OPTIONS]
```
[/tab]
[/tabs]

See [the glossary](/t/maas-glossary/5416#heading--ip-ranges) for the definitions of reserved range types.

*** How to create a bond interface

[tabs]
[tab version="v3.4 Snap,v3.4 Packages,v3.3 Snap,v3.3 Packages,v3.2 Snap,v3.2 Packages,v3.1 Snap,v3.1 Packages,v3.0 Snap,v3.0 Packages,v2.9 Snap,v2.9 Packages" view="UI"]

1. Select more than one interface. 

2. Select *Create bond*; the bond configuration pane will appear.

3. Rename the bond, if desired. 

4. Select a bond mode:

-   **balance-rr**: Transmit packets in sequential order from the first available follower through to the last. This mode provides load balancing and fault tolerance.

-   **active-backup**: Only one follower in the bond is active. A different follower becomes active if, and only if, the active follower fails. The bond's MAC address is externally visible on only one port (network adaptor) to avoid confusing the switch.

-   **balance-xor**: Transmit based on the selected transmit hash policy. The default policy is simple, which means that an XOR operation selects packages.  This XOR compares the source MAC address and the resultant XOR between the destination MAC address, the packet type identifier, and the modulo follower count.

-   **broadcast**: Transmit everything on all follower interfaces. This mode provides fault tolerance.

-   **802.3ad**: Creates aggregation groups that share the same speed and duplex settings. This mode utilises all followers in the active aggregation, following the IEEE 802.3ad specification.

-   **balance-tlb**: Adaptive transmit load balancing, channel bonding that does not require any special switch support.

-   **balance-alb**: Adaptive load balancing, includes balance-tlb plus receive load balancing (rlb) for IPV4 traffic. This mode does not require any special switch support.  ARP negotiation achieves load balancing in this case.

5. Assign a *MAC address* to the aggregate device.

6. Attach one or more *Tags*, if desired.

7. Select the *Primary* device.

8. Select *Save* to register your changes.

[note]
The MAC address defaults to the MAC address of the primary interface.
[/note]
[/tab]
[tab version="v3.4 Snap,v3.4 Packages,v3.3 Snap,v3.3 Packages,v3.2 Snap,v3.2 Packages,v3.1 Snap,v3.1 Packages,v3.0 Snap,v3.0 Packages,v2.9 Snap,v2.9 Packages" view="CLI"]
To create a bond, execute the following command:

```
maas $PROFILE interfaces create-bond $SYSTEM_ID name=$BOND_NAME \
parents=$IFACE1_ID mac_address=$MAC_ADDR \ 
parents=$IFACE2_ID bond_mode=$BOND_MODE \
bond_updelay=$BOND_UP bond_downdelay=$BOND_DOWN mtu=$MTU
```
Note that: 

- The `parents` parameters define which interfaces form the aggregate interface.

- The `bond_updelay` and `bond_downdelay` parameters specify the number of milliseconds to wait before either enabling or disabling a follower after a failure has been detected.

- There are a wide range of bond parameters you can choose when creating a bond:

| Parameter | Type and description |
|:----------|:---------------------|
| `mac_address`| Optional string.  MAC address of the interface. |
| `tags`| Optional string.  Tags for the interface. |
| `vlan`| Optional string.  VLAN the interface is connected to. If not provided then the interface is considered disconnected. |
| `parents`| Required integer.  Parent interface ids that make this bond. |
| `bond_miimon`| Optional integer.  The link monitoring frequency in milliseconds. (Default: 100). |
| `bond_downdelay`| Optional integer.  Specifies the time, in milliseconds, to wait before disabling a follower after a link failure has been detected. |
| `bond_updelay`| Optional integer.  Specifies the time, in milliseconds, to wait before enabling a follower after a link recovery has been detected. |
| `bond_lacp_rate`| Optional string.  Option specifying the rate at which to ask the link partner to transmit LACPDU packets in 802.3ad mode. Available options are ``fast`` or ``slow``. (Default: ``slow``). |
| `bond_xmit_hash_policy`| Optional string.  The transmit hash policy to use for follower selection in balance-xor, 802.3ad, and tlb modes. Possible values are: ``layer2``, ``layer2+3``, ``layer3+4``, ``encap2+3``, ``encap3+4``. (Default: ``layer2``) |
| `bond_num_grat_arp`| Optional integer.  The number of peer notifications (IPv4 ARP or IPv6 Neighbour Advertisements) to be issued after a failover. (Default: 1) |
| `mtu`| Optional integer.  Maximum transmission unit. |
| `accept_ra`| Optional Boolean.  Accept router advertisements. (IPv6 only) |
| `autoconf`| Optional Boolean.  Perform stateless autoconfiguration. (IPv6 only) |
| `bond_mode`| Optional string.  The operating mode of the bond.  (Default: active-backup). |

- Supported bonding modes include:

| Mode | Behaviour |
|:-----|:---------|
|  `balance-rr`:| Transmit packets in sequential order from the first available follower through the last. This mode provides load balancing and fault tolerance. |
|  `active-backup`| Only one follower in the bond is active. A different follower becomes active if, and only if, the active follower fails. The bond's MAC address is externally visible on only one port (network adaptor) to avoid confusing the switch. |
|  `balance-xor`| Transmit based on the selected transmit hash policy. The default policy is a simple [(source MAC address XOR'd with destination MAC address XOR packet type ID) modulo follower count]. |
|  `broadcast`| Transmits everything on all follower interfaces. This mode provides fault tolerance. |
|  `802.3ad`| IEEE 802.3ad dynamic link aggregation. Creates aggregation groups that share the same speed and duplex settings. Uses all followers in the active aggregator according to the 802.3ad specification. |
|  `balance-tlb`| Adaptive transmit load balancing: channel bonding that does not require any special switch support. |
|  `balance-alb`| Adaptive load balancing: includes balance-tlb plus receive load balancing (rlb) for IPV4 traffic, and does not require any special switch support. The receive load balancing is achieved by ARP negotiation. |
[/tab]
[/tabs]

*** How to create a bridge interface

[tabs]
[tab version="v3.4 Snap,v3.4 Packages,v3.3 Snap,v3.3 Packages,v3.2 Snap,v3.2 Packages,v3.1 Snap,v3.1 Packages,v3.0 Snap,v3.0 Packages,v2.9 Snap,v2.9 Packages" view="UI"]

To create a bridge interface:

1. Select *Machines*.

2. Select a machine.

3. Select *Network*.

4. Select an interface by choosing its checkbox.

5. Select *Create bridge*.

6. Optionally enter a unique *Bridge name*.

7. Choose a *Bridge type*.

8. Enter a valid *MAC address*.

9. Choose a *Fabric*.

10. Choose a *VLAN*.

11. Optionally choose a *Subnet*.

12. Optionally enter *Tags*.

13. Optionally turn on *STP*.

14. Register your new bridge by selecting *Save interface*.

[/tab]
[tab version="v3.4 Snap,v3.4 Packages,v3.3 Snap,v3.3 Packages,v3.2 Snap,v3.2 Packages,v3.1 Snap,v3.1 Packages,v3.0 Snap,v3.0 Packages,v2.9 Snap,v2.9 Packages" view="CLI"]
Please use the UI interface to create a bridge interface.  Select the "UI" dropdown above to see how.
[/tab]
[/tabs]

*** How to delete an interface

[tabs]
[tab version="v3.4 Snap,v3.4 Packages,v3.3 Snap,v3.3 Packages,v3.2 Snap,v3.2 Packages,v3.1 Snap,v3.1 Packages,v3.0 Snap,v3.0 Packages,v2.9 Snap,v2.9 Packages" view="UI"]
An interface can only be deleted via the MAAS CLI.  Choose the "CLI" dropdown above to see how.
[/tab]
[tab version="v3.4 Snap,v3.4 Packages,v3.3 Snap,v3.3 Packages,v3.2 Snap,v3.2 Packages,v3.1 Snap,v3.1 Packages,v3.0 Snap,v3.0 Packages,v2.9 Snap,v2.9 Packages" view="CLI"]
To create a bridge interface:

```
maas $PROFILE interfaces create-bridge $SYSTEM_ID name=$BRIDGE_NAME \
parent=$IFACE_ID
```

Use `parent` to define the primary interface used for the bridge:

```
maas admin interfaces create-bridge 4efwb4 name=bridged0 parent=4
```

The following parameters may be applied when creating a bridge:

- `name`: Optional string.  Name of the interface.

- `mac_address`: Optional string.  MAC address of the interface.

- `tags`: Optional string.  Tags for the interface.

- `vlan`: Optional string.  VLAN the interface is connected to.

- `parent`: Optional integer.  Parent interface id for this bridge interface.

- `bridge_type`: Optional string.  The type of bridge to create. Possible values are: ``standard``, ``ovs``.

- `bridge_stp`: Optional Boolean.  Turn spanning tree protocol on or off. (Default: False).

- `bridge_fd`: Optional integer.  Set bridge forward delay to time seconds. (Default: 15).

- `mtu`: Optional integer.  Maximum transmission unit.

- `accept_ra`: Optional Boolean.  Accept router advertisements. (IPv6 only)

- `autoconf`: Optional Boolean.  Perform stateless autoconfiguration. (IPv6 only)
[/tab]
[/tabs]

The "delete" command can be used to delete a bridge interface, a bond interface or a physical interface:

```
maas $PROFILE interface delete $SYSTEM_ID $IFACE_ID
```

For example:

```
maas admin interface delete 4efwb4 15
```

The following is output after the successful deletion of an interface:

```
Success.
Machine-readable output follows:
```

Note that while the label is presented, there is no machine-readable output expected after the successful execution of the delete command.

*** How to assign a network interface to a fabric

[tabs]
[tab version="v3.4 Snap,v3.4 Packages,v3.3 Snap,v3.3 Packages,v3.2 Snap,v3.2 Packages,v3.1 Snap,v3.1 Packages,v3.0 Snap,v3.0 Packages,v2.9 Snap,v2.9 Packages" view="UI"]
A network interface may be assigned to a fabric with the MAAS CLI only.  Choose the "CLI" dropdown above to see how.
[/tab]
[tab version="v3.4 Snap,v3.4 Packages,v3.3 Snap,v3.3 Packages,v3.2 Snap,v3.2 Packages,v3.1 Snap,v3.1 Packages,v3.0 Snap,v3.0 Packages,v2.9 Snap,v2.9 Packages" view="CLI"]
This task is made easier with the aid of the `jq` utility. It filters the `maas` command (JSON formatted) output and prints it in the desired way, which allows you to view and compare data quickly. Go ahead and install it:

``` bash
sudo apt install jq
```

In summary, MAAS assigns an interface to a fabric by assigning it to a VLAN. First, we need to gather various bits of data.

List some information on all machines:

``` bash
maas $PROFILE machines read | jq ".[] | \
    {hostname:.hostname, system_id: .system_id, status:.status}" --compact-output
```

Example output:

``` no-highlight
{"hostname":"machine1","system_id":"dfgnnd","status":4}
{"hostname":"machine2","system_id":"bkaf6e","status":6}
{"hostname":"machine4","system_id":"63wqky","status":6}
{"hostname":"machine3","system_id":"qwkmar","status":4}
```

[note]
You can only edit an interface when the corresponding machine has a status of 'Ready'. This state is numerically denoted by the integer '4'.
[/note]

List some information for all interfaces on the machine in question (identified by its system id 'dfgnnd'):

``` bash
maas $PROFILE interfaces read dfgnnd | jq ".[] | \
    {id:.id, name:.name, mac:.mac_address, vid:.vlan.vid, fabric:.vlan.fabric}" --compact-output
```

Example output:

``` no-highlight
{"id":8,"name":"eth0","mac":"52:54:00:01:01:01","vid":0,"fabric":"fabric-1"}
{"id":9,"name":"eth1","mac":"52:54:00:01:01:02","vid":null,"fabric":null}
```

List some information for all fabrics:

``` bash
maas $PROFILE fabrics read | jq ".[] | \
    {name:.name, vlans:.vlans[] | {id:.id, vid:.vid}}" --compact-output
```

Example output:

``` no-highlight
{"name":"fabric-0","vlans":{"id":5001,"vid":0}}
{"name":"fabric-1","vlans":{"id":5002,"vid":0}}
{"name":"fabric-2","vlans":{"id":5003,"vid":0}}
```

This example will show how to move interface '8' (on machine 'dfgnnd') from 'fabric-1' to 'fabric-0'. Based on the gathered information, this will consist of changing the interface's VLAN from '5002' to '5001':

``` bash
maas $PROFILE interface update dfgnnd 8 vlan=5001 >/dev/null
```

Verify the operation by relisting information for the machine's interface:

``` bash
maas $PROFILE interfaces read dfgnnd | jq ".[] | \
    {id:.id, name:.name, mac:.mac_address, vid:.vlan.vid, fabric:.vlan.fabric}" --compact-output
```

The output shows that the interface is now on fabric-0:

``` no-highlight
{"id":8,"name":"eth0","mac":"52:54:00:01:01:01","vid":0,"fabric":"fabric-0"}
{"id":9,"name":"eth1","mac":"52:54:00:01:01:02","vid":null,"fabric":null}
```
[/tab]
[/tabs]

*** How to discover interface identifiers

[tabs]
[tab version="v3.4 Snap,v3.4 Packages,v3.3 Snap,v3.3 Packages,v3.2 Snap,v3.2 Packages,v3.1 Snap,v3.1 Packages,v3.0 Snap,v3.0 Packages,v2.9 Snap,v2.9 Packages" view="UI"]
Interface identifiers can only be discovered via the MAAS CLI.  Choose the "CLI" dropdown above to see how.
[/tab]
[tab version="v3.4 Snap,v3.4 Packages,v3.3 Snap,v3.3 Packages,v3.2 Snap,v3.2 Packages,v3.1 Snap,v3.1 Packages,v3.0 Snap,v3.0 Packages,v2.9 Snap,v2.9 Packages" view="CLI"]
The MAAS CLI uses a numeric interface identifier for many interface operations. Use the following command to retrieve the identifier(s):

``` bash
maas $PROFILE interfaces read $SYSTEM_ID
```

Look for either id or the number at the end of an interface's resource URI, such as **15** in the following example output:

``` json
"id": 15,
"mac_address": "52:54:00:55:06:40",
...
"name": "ens9",
...
"resource_uri": "/MAAS/api/2.0/nodes/4efwb4/interfaces/15/"
```
[/tab]
[/tabs]

*** How to create a VLAN interface

[tabs]
[tab version="v3.4 Snap,v3.4 Packages,v3.3 Snap,v3.3 Packages,v3.2 Snap,v3.2 Packages,v3.1 Snap,v3.1 Packages,v3.0 Snap,v3.0 Packages,v2.9 Snap,v2.9 Packages" view="UI"]
VLAN interfaces can only be created via the MAAS CLI. Select the "CLI" dropdown above to see how.
[/tab]
[tab version="v3.4 Snap,v3.4 Packages,v3.3 Snap,v3.3 Packages,v3.2 Snap,v3.2 Packages,v3.1 Snap,v3.1 Packages,v3.0 Snap,v3.0 Packages,v2.9 Snap,v2.9 Packages" view="CLI"]
To create a VLAN interface, use the following syntax:

``` bash
maas $PROFILE vlans create $FABRIC_ID name=$NAME vid=$VLAN_ID
```

For example, the following command creates a VLAN called 'Storage network:

``` bash
maas admin vlans create 0 name="Storage network" vid=100
```

The above command generates the following output:

``` no-output
Success.
Machine-readable output follows:
{
    "vid": 100,
    "mtu": 1500,
    "dhcp_on": false,
    "external_dhcp": null,
    "relay_vlan": null,
    "name": "Storage network",
    "space": "undefined",
    "fabric": "fabric-0",
    "id": 5004,
    "primary_rack": null,
    "fabric_id": 0,
    "secondary_rack": null,
    "resource_uri": "/MAAS/api/2.0/vlans/5004/"
}
```

Be aware that the $VLAN_ID parameter does not indicate a VLAN ID that corresponds to the VLAN tag. You must first create the VLAN and then associate it with the interface:

``` bash
maas $PROFILE interfaces create-vlan $SYSTEM_ID vlan=$OUTPUT_VLAN_ID \
parent=$IFACE_ID
```

[note]
**OUTPUT_VLAN_ID** corresponds to the id value output when MAAS created the VLAN.
[/note]

The following example contains values that correspond to the output above:

``` bash
maas admin interfaces create-vlan 4efwb4 vlan=5004 parent=4
```

The above command generates the following output:

``` json
Success.
Machine-readable output follows:
{
    "tags": [],
    "type": "vlan",
    "enabled": true,
    "system_id": "4efwb4",
    "id": 21,
    "children": [],
    "mac_address": "52:54:00:eb:f2:29",
    "params": {},
    "vlan": {
        "vid": 100,
        "mtu": 1500,
        "dhcp_on": false,
        "external_dhcp": null,
        "relay_vlan": null,
        "id": 5004,
        "secondary_rack": null,
        "fabric_id": 0,
        "space": "undefined",
        "fabric": "fabric-0",
        "name": "Storage network",
        "primary_rack": null,
        "resource_uri": "/MAAS/api/2.0/vlans/5004/"
    },
    "parents": [
        "ens3"
    ],
    "effective_mtu": 1500,
    "links": [
        {
            "id": 55,
            "mode": "link_up"
        }
    ],
    "discovered": null,
    "name": "ens3.100",
    "resource_uri": "/MAAS/api/2.0/nodes/4efwb4/interfaces/21/"
}
```
[/tab]
[/tabs]

*** How to delete a VLAN interface

[tabs]
[tab version="v3.4 Snap,v3.4 Packages,v3.3 Snap,v3.3 Packages,v3.2 Snap,v3.2 Packages,v3.1 Snap,v3.1 Packages,v3.0 Snap,v3.0 Packages,v2.9 Snap,v2.9 Packages" view="UI"]
VLAN interfaces can only be deleted via the MAAS CLI. Select the "CLI" dropdown above to see how.
[/tab]
[tab version="v3.4 Snap,v3.4 Packages,v3.3 Snap,v3.3 Packages,v3.2 Snap,v3.2 Packages,v3.1 Snap,v3.1 Packages,v3.0 Snap,v3.0 Packages,v2.9 Snap,v2.9 Packages" view="CLI"]
The following command outlines the syntax required to delete a VLAN interface from the command line:

``` bash
maas $PROFILE vlan delete $FABRIC__ID $VLAN_ID
```

Using the values from previous examples, you executed this step as follows:

``` bash
maas admin vlan delete 0 100
```
[/tab]
[/tabs]

** How to manage proxies

MAAS provides a way for its managed machines to use a proxy server when they need to access HTTP/HTTPS-based resources, such as the Ubuntu package archive.

There are three possible options:

1.   internal proxy (default)
2.   external proxy
3.   no proxy

Configuring a proxy with MAAS consists of enabling/disabling one of the above three options and enabling/disabling proxying on a specific subnet. This article will help you learn:

- [About the MAAS internal proxy](#heading--internal-proxy-maas-proxy)
- [How to create an external proxy](#heading--configure-proxy)

*** How to create an external proxy

[tabs]
[tab version="v3.4 Snap,v3.4 Packages,v3.3 Snap,v3.3 Packages,v3.2 Snap,v3.2 Packages,v3.1 Snap,v3.1 Packages,v3.0 Snap,v3.0 Packages,v2.9 Snap,v2.9 Packages" view="UI"]

1. In the web UI, visit the 'Settings' page and select the 'Network services' tab. 

2. Modify the 'Proxy' section at the top, as desired:

- To enable the internal proxy, ensure that the checkbox adjacent to 'MAAS Built-in' is selected. This internal proxy is the default configuration.

- To enable an external proxy, activate the 'External' checkbox and use the new field that is displayed to define the proxy's URL (and port if necessary).

- An upstream cache peer can be defined by enabling the 'Peer' checkbox and entering the external proxy URL into the field. With this enabled, machines will be configured to use the MAAS built-in proxy to download cached APT packages.

- To prevent MAAS machines from using a proxy, enable the 'Don't use a proxy' checkbox.
[/tab]
[tab version="v3.4 Snap,v3.4 Packages,v3.3 Snap,v3.3 Packages,v3.2 Snap,v3.2 Packages,v3.1 Snap,v3.1 Packages,v3.0 Snap,v3.0 Packages,v2.9 Snap,v2.9 Packages" view="CLI"]
Enabling and disabling proxying, in general, is done via a Boolean option ('true' or 'false'). The following command will disable proxying completely:

``` bash
maas $PROFILE maas set-config name=enable_http_proxy value=false
```

To set an external proxy, ensure proxying is enabled (see above) and then define it:

``` bash
maas $PROFILE maas set-config name=http_proxy value=$EXTERNAL_PROXY
```

For example,

``` bash
maas $PROFILE maas set-config name=enable_http_proxy value=true
maas $PROFILE maas set-config name=http_proxy value=http://squid.example.com:3128/
```

Enabling and disabling proxying per subnet is done via a Boolean option ('true' or 'false'). Here is how you can disable proxying on a per-subnet basis:

``` bash
maas $PROFILE subnet update $SUBNET_CIDR allow_proxy=false
```

For example,

``` bash
maas $PROFILE subnet update 192.168.0.0/22 allow_proxy=false
```

[/tab]
[/tabs]

**NOTE** that the proxy service will still be running.

** How to set up Network Time Protocol (NTP)

MAAS provides managed NTP services (with [Chrony](https://chrony.tuxfamily.org/)`↗`) for all region and rack controllers. This arrangement allows MAAS to both keep its controllers synchronised, and keep deployed machines synchronised as well. You can configure NTP on the 'Network services' tab of the 'Settings' page. 

The region controller configures the NTP service to keep its time synchronised from one or more external sources. By default, the MAAS region controller uses `ntp.ubuntu.com`. Rack controllers also configure the NTP service, synchronising their time with the region controllers.  Rack controllers also configure DHCP with the correct NTP information, so that the DHCP servers can manage the NTP clients for the rack. Any machine on the network that obtains a DHCP lease from MAAS will benefit from NTP support.

** Setting an external NTP server

External sites, such as an existing NTP infrastructure, can be used directly as a time source for both rack controllers and machines.

[tabs]
[tab version="v3.4 Snap,v3.4 Packages,v3.3 Snap,v3.3 Packages,v3.2 Snap,v3.2 Packages,v3.1 Snap,v3.1 Packages,v3.0 Snap,v3.0 Packages,v2.9 Snap,v2.9 Packages" view="UI"]
You can specify an external site by choosing the NTP server(s) and selecting the 'External Only' option. The region controller always uses an external site.

1. On the 'Settings' page, select the 'Network services' tab and scroll down to the 'NTP' section.

2. Enter the address of the desired NTP server.
[/tab]
[tab version="v3.4 Snap,v3.4 Packages,v3.3 Snap,v3.3 Packages,v3.2 Snap,v3.2 Packages,v3.1 Snap,v3.1 Packages,v3.0 Snap,v3.0 Packages,v2.9 Snap,v2.9 Packages" view="CLI"]
You can specify an external NTP server with two successive commands:

```
maas $PROFILE maas set-config name=ntp_servers value=$NTP_IP_ADDRESS
```

followed by:

```
maas admin maas set-config name=ntp_external_only value=true
```

[/tab]
[/tabs]

* How to contribute code
** Soliciting contributions to this document

Please add steps, ideas, sentence fragments, or even complete paragraphs about this topic:

```nohighlight
How should one go about adding code to MAAS?
```

There are the basics of PRs, etc., but I'd like opinions from some experts before creating an outline for this page.  Please add your thoughts.

* How to contribute documentation

[note]
Before contributing documentation, you should review the [MAAS documentation style guide](/t/maas-documentation-style-guide/4186).
[/note]

There are essentially two ways to contribute to the MAAS documentation: by editing an existing document, or comment on it if you're unsure exactly what to edit.

** Editing an existing document

You can edit an existing document this way:

1. Go to the [MAAS documentation](https://maas.io/docs).

2. Go to the document you want to edit.

3. Select *Help improve this document in the forum.* at the bottom of the browser window.

4. Choose the "pencil in a box next to a number" icon, just below and to the right of the title bar. A popup will appear and load.

5. Choose *Edit Wiki*, lower right center of the popup window.  The popup window will vanish, and another popup will appear.  This popup has side-by-side editing and preview screens, which track each other somewhat.

6. In the left (editing) pane, find the text you want to change and edit it.

7. Choose *Save Edit* at the lower left of the popup window.

Your changes are now part of the document.  Note that it may take 24 hours for your updates to be posted to the main documentation site.

** Commenting on an existing document

If you're not sure exactly what to chagne, you can comment on an existing document this way:

1. Go to the [MAAS documentation](https://maas.io/docs).

2. Go to the document you want to comment on.

3. Select *Help improve this document in the forum.* at the bottom of the browser window.

4. Go to the bottom of the the Discourse document that comes up.

5. Choose *Reply* toward the bottom center of the document text.

6. Enter your comment where it says *Type here....*

7. Choose *Reply* to save your comment, or *Cancel* to change your mind.

Someone should see your comment shortly and respond.


* How to customise images
[tabs]
[tab version="v3.4 Snap,v3.4 Packages,v3.3 Snap,v3.3 Packages,v3.2 Snap,v3.2 Packages,v3.1 Snap,v3.1 Packages"]
MAAS supports deploying custom OS images.  Canonical provides both [lp:maas-image-builder](https://launchpad.net/maas-image-builder)`↗` and [gh:canonical/packer-maas](https://github.com/canonical/packer-maas)`↗` to support creating custom images. These custom images can include static Ubuntu images, created with whatever tool you choose, as well as other OS images.  Note that MAAS Image Builder requires the purchase of Ubuntu Advantage support.

[note]
While it may be possible to deploy a certain image with MAAS, the particular use case may not be supported by that image’s vendor due to licensing or technical reasons. Canonical recommends that, whenever possible, you should customise machines using cloud-init user_data or Curtin preseed data, instead of creating a custom image.
[/note]

[/tab]
[tab version="v3.0 Snap,v3.0 Packages,v2.9 Snap,v2.9 Packages"]
MAAS supports deploying custom DD or TGZ images. Canonical provides both [lp:maas-image-builder](https://launchpad.net/maas-image-builder)`↗` and [gh:canonical/packer-maas](https://github.com/canonical/packer-maas)`↗` to support creating custom images; however, these tools do not currently support Ubuntu. Instead Canonical suggests [customising Ubuntu](/t/5108)`↗` using cloud-init user_data or Curtin preseed data.

This section will help you learn:

- [Why customised Ubuntu deployments aren't supported](#heading--why-customised-ubuntu-deployments-arent-supported)
- [Warnings on creating a custom Ubuntu image](#heading--warnings-on-creating-a-custom-ubuntu-image)
- [How to create a custom Ubuntu image for MAAS](#heading--how-to-create-a-custom-ubuntu-image-for-maas)
- [How to build MAAS images](#heading--how-to-build-maas-images)

*** Why customised Ubuntu deployments aren't supported

When the [MAAS stream](https://images.maas.io/ephemeral-v3/stable/)`↗` is generated by [lp:maas-images](https://launchpad.net/maas-images)`↗` it starts by downloading the base SquashFS rootfs from cloud-images.ubuntu.com that is used for all clouds. The SquashFS does not contain a kernel so [lp:maas-images](https://launchpad.net/maas-images)`↗` mounts the SquashFS with an overlay and chroots in. It then  installs a kernel and extra initramfs scripts from the cloud-initramfs-rooturl and cloud-initramfs-copymods packages to allow network booting. Once everything is installed the kernel and newly generated initramfs are pulled out of the overlay and everything is unmounted. [lp:maas-images](https://launchpad.net/maas-images)`↗` provides the unmodified SquashFS, installed kernel, and generated initramfs as separate files on images.maas.io.

MAAS uses the kernel, initramfs, and SquashFS to perform network booting which allows commissioning, testing, and deployments. When deploying Ubuntu MAAS uses the same version of Ubuntu to network boot into and perform the deployment. This ensures there are no compatibility issues. Other operating systems use the Ubuntu version selected for the ephemeral environment for deployment.

Currently MAAS only allows custom images to be loaded as a single TGZ or DD.GZ, there is no way to upload a kernel, `initrd`, and `rootfs` as separate files. Even if MAAS was modified to allow the kernel, `initrd`, and `rootfs` as separate files MAAS requires `cloud-initramfs-copymods` and `cloud-initramfs-rooturl` to be included in the initrd. It is difficult for MAAS to detect if these scripts are missing and even harder for users to debug if they are missing.

*** Warnings on creating a custom Ubuntu image

Note that:

- Custom images are always deployed with the ephemeral operating system. This can cause hard to debug errors. For example CentOS 6 can only be deployed by Ubuntu Xenial due to advances in the ext4 filesystem.
- MAAS will still install and configure the GA kernel. If your custom image contains a kernel it most likely won't be used. MAAS will not allow you to select which kernel(GA, HWE, lowlatency, etc) when deploying a custom Ubuntu image.
- All GNU/Linux custom images should be created as a TGZ to enable storage customisation. When a DD.GZ is used MAAS/Curtin simply writes the file to the filesystem.

*** How to create a custom Ubuntu image for MAAS

Note: LXD may prevent device files from being created when extracting the rootfs, it is suggested to do this on metal or on a VM:

- Download the rootfs from images.maas.io
    `wget http://cloud-images.ubuntu.com/focal/current/focal-server-cloudimg-amd64-root.tar.xz`
- Create a work directory
    `mkdir /tmp/work`
- Extract the rootfs
    `cd /tmp/work`
    `sudo tar xf ../focal-server-cloudimg-amd64-root.tar.xz`

    or

   `unsquashfs ../focal-server-cloudimg-amd64-root.squash`
- Setup chroot
   `sudo mount -o bind /proc /tmp/work/proc`
   `sudo mount -o bind /dev /tmp/work/dev`
   `sudo mount -o bind /sys /tmp/work/sys`
   `sudo mv /tmp/work/etc/resolv.conf /tmp/work/etc/resolv.conf.bak`
   `sudo cp /etc/resolv.conf /tmp/work/etc/`
- Chroot in
    `sudo chroot /tmp/work /bin/bash`
- Customise image
    `apt update`
    `apt dist-upgrade`
   `apt install emacs-nox jq tree`
    `apt autoremove`
- Exit chroot and unmount binds
  `exit`
   `sudo umount /tmp/work/proc`
   `sudo umount /tmp/work/dev`
   `sudo umount /tmp/work/sys`
   `sudo mv /tmp/work/etc/resolv.conf.bak /tmp/work/etc/resolv.conf`
- Create TGZ
   `sudo tar -czf ~/focal-custom.tgz -C /tmp/work .`
- Upload it to MAAS
    Note: Ubuntu release names and versions are reserved
    `maas $PROFILE boot-resources create name='custom/focal-custom' title='Ubuntu 20.04 Custom Image' architecture='amd64/generic' filetype='tgz' content@=~/focal-custom.tgz`
- Configure and deploy as normal

** How to build MAAS images

[/tab]
[/tabs]

There are two methods for building custom images to be deployed to MAAS machines: MAAS Image Builder, and [packer](https://www.packer.io/)`↗`.  This section will help you learn:

- [How to verify packer prequisites](#heading--how-to-verify-packer-prequisites)
- [How to verify packer deployment requirements](#heading--how-to-verify-packer-deployment-requirements)
- [How to customise images](#heading--how-to-customise-images)
- [How to build images via a proxy](#heading--how-to-build-images-via-a-proxy)
- [How to use packer to build MAAS images](#heading--how-to-use-packer-to-build-maas-images)
- [How to pack an Ubuntu image for MAAS deployment](#heading--how-to-pack-an-ubuntu-image-for-maas-deployment)
- [How to pack a RHEL7 image for MAAS deployment](#heading--how-to-pack-a-rhel7-image-for-maas-deployment)
- [How to pack a RHEL8 image for MAAS deployment](#heading--how-to-pack-a-rhel8-image-for-maas-deployment)
- [How to pack a CentOS 7 image for MAAS deployment](#heading--how-to-pack-a-centos7-image-for-maas-deployment)
- [How to pack an ESXi image for MAAS deployment](#heading--how-to-pack-an-exsi-image-for-maas-deployment)
- [How to use MAAS Image Builder to build MAAS images](#heading--how-to-use-maas-image-builder-to-build-maas-images)

*** How to verify packer prequisites

The following are required to to create a packer MAAS image:

- A machine running Ubuntu 18.04+ or higher with the ability to run KVM virtual machines.
- Various dependencies required by the chosen template (see the template directory for details)
- [Packer](https://www.packer.io/intro/getting-started/install.html)`↗`

As an example for this article, we will be building a custom Ubuntu image from the default template, which has the following prerequisites:

- qemu-utils
- qemu-system
- ovmf
- cloud-image-utils

Note that these requirements may vary by template and target image.

*** How to verify packer deployment requirements

The following are required to deploy a packer MAAS image:

- [MAAS](https://maas.io)`↗` 3.0+
- [Curtin](https://launchpad.net/curtin)`↗` 21.0+

*** How to customise images

It is possible to customize the image either during the Ubuntu installation, or afterwards (before packing the final image). The former is done by providing [autoinstall config](https://ubuntu.com/server/docs/install/autoinstall)`↗`, editing the _user-data-flat_ and _user-data-lvm_ files. The latter is performed by the _install-custom-packages_ script.

*** How to build images via a proxy

The Packer template downloads the Ubuntu net installer from the Internet. To tell Packer to use a proxy, set the HTTP_PROXY environment variable to your proxy server. Alternatively, you may redefine iso_url to a local file, set iso_checksum_type to none to disable the checksums, and remove iso_checksum_url.

*** How to use packer to build MAAS images

[Packer](https://www.packer.io/)`↗` can be used to build images to be deployed by MAAS.  In order to use packer, you must have a [packer template](https://github.com/canonical/packer-maas)`↗` for the OS version you intend to build.

Currently, templates are available for:

- Ubuntu custom images
- CentOS6
- CentOS7
- CentOS8
- RHEL7
- RHEL8
- VMWare EXSi

This section will help you learn:

- [How to install packer](#heading--how-to-install-packer)
- [How to obtain templates](#heading--how-to-obtain-templates)
- [How to create and use packer images](#heading--how-to-create-a-packer-image)

Note that additional templates will be made available from time to time.

**** How to install packer

Packer is easily installed from its Debian package:

```nohighlight
sudo apt install packer
```

For this example Ubuntu template, the following dependencies should also be installed -- but note that these dependencies may vary by template and/or target image:

```nohighlight
sudo apt install qemu-utils
sudo apt install qemu-system
sudo apt install ovmf
sudo apt install cloud-image-utils
```

All of these should install without additional prompts.

**** How to obtain templates

You can obtain the packer templates by cloning the [packer-maas github repository](https://github.com/canonical/packer-maas.git)`↗`, like this:

```nohighlight
git clone https://github.com/canonical/packer-maas.git
```

Make sure to pay attention to where the repository is cloned.  The Packer template in this cloned repository creates a Ubuntu AMD64 image for use with MAAS.

**** How to create and use packer images

This subsection will help you learn:

- [How to build a packer image](#heading--how-to-build-a-packer-image)
- [How to upload packer images to MAAS](#heading--how-to-upload-packer-images-to-maas)
- [About the default image username](#heading--about-the-default-image-username)

***** How to build a packer raw image

To build a packer image, you must change to the template repository directory, then to the subdirectory for the image you want to build.  From that subdirectory, you can easily build a raw image with LVM, using the Makefile:

```nohighlight
$ make custom-ubuntu-lvm.dd.gz
```

This makefile will run for a couple of minutes before attempting to boot the image.  While waiting for the image to boot, packer will attempt to SSH into the machine repeatedly until it succeeds.  You will see terminal messages similar to this one for upwards of three to five minutes:

```nohighlight
2022/05/09 15:50:46 packer-builder-qemu plugin: [DEBUG] handshaking with SSH
2022/05/09 15:50:50 packer-builder-qemu plugin: [DEBUG] SSH handshake err: ssh: handshake failed: ssh: unable to authenticate, attempted methods [none password], no supported methods remain
2022/05/09 15:50:50 packer-builder-qemu plugin: [DEBUG] Detected authentication error. Increasing handshake attempts.
```

Eventually, you should see a successful SSH connection:

```nohighlight
2022/05/09 15:50:57 packer-builder-qemu plugin: [INFO] Attempting SSH connection to 127.0.0.1:2351...
2022/05/09 15:50:57 packer-builder-qemu plugin: [DEBUG] reconnecting to TCP connection for SSH
2022/05/09 15:50:57 packer-builder-qemu plugin: [DEBUG] handshaking with SSH
2022/05/09 15:51:17 packer-builder-qemu plugin: [DEBUG] handshake complete!
```

If the process seems to run for a long time, you can predict whether it's going to work by doing a series of `netstat -a` on the `IP:port` given in the connection attempt.  Attempts may fail repeatedly, but if you repeat the `netstat -a` command frequently, you will see some tentative connections, like this one:

```nohighlight
stormrider@neuromancer:~$ netstat -a | grep 2281
tcp        0      0 0.0.0.0:2281            0.0.0.0:*               LISTEN     
tcp        0      0 localhost:46142         localhost:2281          TIME_WAIT  
tcp        0      0 localhost:46120         localhost:2281          TIME_WAIT  
tcp        0      0 localhost:46138         localhost:2281          TIME_WAIT  
tcp        0      0 localhost:46134         localhost:2281          TIME_WAIT  
tcp        0      0 localhost:46130         localhost:2281          TIME_WAIT  
tcp        0      0 localhost:46124         localhost:2281          TIME_WAIT  
stormrider@neuromancer:~$ netstat -a | grep 2281
tcp        0      0 0.0.0.0:2281            0.0.0.0:*               LISTEN     
tcp        0      0 localhost:46142         localhost:2281          TIME_WAIT  
tcp        0      0 localhost:46146         localhost:2281          ESTABLISHED
tcp        0      0 localhost:2281          localhost:46146         ESTABLISHED
tcp        0      0 localhost:46138         localhost:2281          TIME_WAIT  
tcp        0      0 localhost:46134         localhost:2281          TIME_WAIT  
tcp        0      0 localhost:46130         localhost:2281          TIME_WAIT  
tcp        0      0 localhost:46124         localhost:2281          TIME_WAIT
```

This `ESTABLISHED` connection may not hold the first few times, but eventually, the SSH connection will be made, and the provisioning process will finish.  If you want to walk away and come back, be advised that the Makefile clears the terminal buffer at the end, but echoes one final instruction:

```nohighlight
rm OVMF_VARS.fd
```

You can check the validity of the operation with `ls`, like this:

```nohighlight
stormrider@neuromancer:~/mnt/Dropbox/src/git/packer-maas/ubuntu$ ls
custom-ubuntu-lvm.dd.gz  packages      seeds-lvm.iso     user-data-lvm
http                     packer_cache  ubuntu-flat.json
Makefile                 README.md     ubuntu-lvm.json
meta-data                scripts       user-data-flat
```

You can also manually run packer. Your current working directory must be in the subdirectory where the packer template is located.  In the case of this example, that's `packer-maas/ubuntu`. Once in `packer-maas/ubuntu`, you can generate an image with the following command sequence:

```nohighlight
$ sudo PACKER_LOG=1 packer build ubuntu-lvm.json
```

[note]
ubuntu-lvm.json is configured to run Packer in headless mode. Only Packer output will be seen. If you wish to see the installation output connect to the VNC port given in the Packer output, or change the value of headless to false in the JSON file.
[/note]

This process is non-interactive.

***** How to upload packer images to MAAS

You can upload a packer image with the following command:

```nohighlight
$ maas admin boot-resources create \
    name='custom/ubuntu-raw' \
    title='Ubuntu Custom RAW' \
    architecture='amd64/generic' \
    filetype='ddgz' \
    content@=custom-ubuntu-lvm.dd.gz
```

Before relying on it in production, you should test your custom image by deploying it to a test (virtual) machine.  It's the machine named `open-gannet` in this listing:

```nohighlight
maas admin machines read | jq -r '(["HOSTNAME","SYSID","POWER","STATUS",
"OWNER", "OS", "DISTRO"] | (., map(length*"-"))),
(.[] | [.hostname, .system_id, .power_state, .status_name, .owner // "-",
.osystem, .distro_series]) | @tsv' | column -t

HOSTNAME     SYSID   POWER  STATUS    OWNER  OS      DISTRO
--------     -----   -----  ------    -----  --      ------
valued-moth  e86c7h  on     Deployed  admin  ubuntu  focal
open-gannet  nk7x8y  on     Deployed  admin  custom  ubuntu-raw
```

***** About the default image username

The default username for packer-created images is `ubuntu`, the same as the default username for other MAAS images.

** How to pack an Ubuntu image for MAAS deployment

Here's how to build a deployable Ubuntu image with packer:

**# Install packer

Packer is easily installed from its Debian package:

```nohighlight
sudo apt install packer
```

This should install with no additional prompts.

**# Install Ubuntu template dependencies

```nohighlight
sudo apt install qemu-utils
sudo apt install qemu-system
sudo apt install ovmf
sudo apt install cloud-image-utils
```

All of these should install with no additional prompts.

**# Get the available packer templates

You can obtain the packer templates by cloning the [packer-maas github repository](https://github.com/canonical/packer-maas.git)`↗`, like this:

```nohighlight
git clone https://github.com/canonical/packer-maas.git
```

Make sure to pay attention to where the repository is cloned.  The Packer template in this cloned repository creates a Ubuntu AMD64 image for use with MAAS.

**# Build an Ubuntu raw image with packer

To build a packer image, you must change to the template repository directory, then to the subdirectory for the image you want to build.  From that subdirectory, you can easily build a raw image with LVM, using the Makefile:

```nohighlight
$ make custom-ubuntu-lvm.dd.gz
```

This makefile will run for a couple of minutes before attempting to boot the image.  While waiting for the image to boot, packer will attempt to SSH into the machine repeatedly until it succeeds.  You will see terminal messages similar to this one for upwards of three to five minutes:

```nohighlight
2022/05/09 15:50:46 packer-builder-qemu plugin: [DEBUG] handshaking with SSH
2022/05/09 15:50:50 packer-builder-qemu plugin: [DEBUG] SSH handshake err: ssh: handshake failed: ssh: unable to authenticate, attempted methods [none password], no supported methods remain
2022/05/09 15:50:50 packer-builder-qemu plugin: [DEBUG] Detected authentication error. Increasing handshake attempts.
```

Eventually, you should see a successful SSH connection:

```nohighlight
2022/05/09 15:50:57 packer-builder-qemu plugin: [INFO] Attempting SSH connection to 127.0.0.1:2351...
2022/05/09 15:50:57 packer-builder-qemu plugin: [DEBUG] reconnecting to TCP connection for SSH
2022/05/09 15:50:57 packer-builder-qemu plugin: [DEBUG] handshaking with SSH
2022/05/09 15:51:17 packer-builder-qemu plugin: [DEBUG] handshake complete!
```

If the process seems to run for a long time, you can predict whether it's going to work by doing a series of `netstat -a` on the `IP:port` given in the connection attempt.  Attempts may fail repeatedly, but if you repeat the `netstat -a` command frequently, you will see some tentative connections, like this one:

```nohighlight
stormrider@neuromancer:~$ netstat -a | grep 2281
tcp        0      0 0.0.0.0:2281            0.0.0.0:*               LISTEN     
tcp        0      0 localhost:46142         localhost:2281          TIME_WAIT  
tcp        0      0 localhost:46120         localhost:2281          TIME_WAIT  
tcp        0      0 localhost:46138         localhost:2281          TIME_WAIT  
tcp        0      0 localhost:46134         localhost:2281          TIME_WAIT  
tcp        0      0 localhost:46130         localhost:2281          TIME_WAIT  
tcp        0      0 localhost:46124         localhost:2281          TIME_WAIT  
stormrider@neuromancer:~$ netstat -a | grep 2281
tcp        0      0 0.0.0.0:2281            0.0.0.0:*               LISTEN     
tcp        0      0 localhost:46142         localhost:2281          TIME_WAIT  
tcp        0      0 localhost:46146         localhost:2281          ESTABLISHED
tcp        0      0 localhost:2281          localhost:46146         ESTABLISHED
tcp        0      0 localhost:46138         localhost:2281          TIME_WAIT  
tcp        0      0 localhost:46134         localhost:2281          TIME_WAIT  
tcp        0      0 localhost:46130         localhost:2281          TIME_WAIT  
tcp        0      0 localhost:46124         localhost:2281          TIME_WAIT
```

This `ESTABLISHED` connection may not hold the first few times, but eventually, the SSH connection will be made, and the provisioning process will finish.  If you want to walk away and come back, be advised that the Makefile clears the terminal buffer at the end, but echoes one final instruction:

```nohighlight
rm OVMF_VARS.fd
```

**# Validate the build

You can check the validity of the operation with `ls`, like this:

```nohighlight
stormrider@neuromancer:~/mnt/Dropbox/src/git/packer-maas/ubuntu$ ls
custom-ubuntu-lvm.dd.gz  packages      seeds-lvm.iso     user-data-lvm
http                     packer_cache  ubuntu-flat.json
Makefile                 README.md     ubuntu-lvm.json
meta-data                scripts       user-data-flat
```

**# Alternative: Run packer manually

You can also manually run packer. Your current working directory must be in the subdirectory where the packer template is located.  In the case of this example, that's `packer-maas/ubuntu`. Once in `packer-maas/ubuntu`, you can generate an image with the following command sequence:

```nohighlight
$ sudo PACKER_LOG=1 packer build ubuntu-lvm.json
```

[note]
ubuntu-lvm.json is configured to run Packer in headless mode. Only Packer output will be seen. If you wish to see the installation output connect to the VNC port given in the Packer output, or change the value of headless to false in the JSON file.
[/note]

This process is non-interactive.

**# Upload the Ubuntu image to MAAS

You can upload an Ubuntu raw packer image with the following command:

```nohighlight
$ maas admin boot-resources create \
    name='custom/ubuntu-raw' \
    title='Ubuntu Custom RAW' \
    architecture='amd64/generic' \
    filetype='ddgz' \
    content@=custom-ubuntu-lvm.dd.gz
```

**# Verify your custom image

Before relying on it in production, you should test your custom image by deploying it to a test (virtual) machine.  It's the machine named `open-gannet` in this listing:

```nohighlight
maas admin machines read | jq -r '(["HOSTNAME","SYSID","POWER","STATUS",
"OWNER", "OS", "DISTRO"] | (., map(length*"-"))),
(.[] | [.hostname, .system_id, .power_state, .status_name, .owner // "-",
.osystem, .distro_series]) | @tsv' | column -t

HOSTNAME     SYSID   POWER  STATUS    OWNER  OS      DISTRO
--------     -----   -----  ------    -----  --      ------
valued-moth  e86c7h  on     Deployed  admin  ubuntu  focal
open-gannet  nk7x8y  on     Deployed  admin  custom  ubuntu-raw
```

**# Log into your deployed image and verify that it's right

You should log into your newly-deployed image and verify that it has all the customisations you added to the build process.  The default username for packer-created images is `ubuntu`, the same as the default username for other MAAS images.

** How to pack a RHEL7 image for MAAS deployment

You can create a RHEL7 image for MAAS deployment via the following procedure.

**# Verify the requirements

To create a RHEL7 image to upload to MAAS, you must have a machine running Ubuntu 18.04+ or higher with the ability to run KVM virtual machines.  You must also aquire the following items, as described in later instructions:

- qemu-utils
- packer
- The RHEL 7 DVD ISO

To deploy the image to MAAS, you must also have:

- MAAS 2.3+
- Curtin 18.1-59+

**# Install packer

Packer is easily installed from its Debian package:

```nohighlight
sudo apt install packer
```

This should install with no additional prompts.

**# Install RHEL7 template dependencies

```nohighlight
sudo apt install qemu-utils
```

**# Get the available packer templates

You can obtain the packer templates by cloning the [packer-maas github repository](https://github.com/canonical/packer-maas.git)`↗`, like this:

```nohighlight
git clone https://github.com/canonical/packer-maas.git
```

Make sure to pay attention to where the repository is cloned.  The Packer template in this cloned repository creates a RHEL7 AMD64 image for use with MAAS.

This package should install with no additional prompts.

**# Locate the RHEL7 template

The packer template in the directory `rhel7` subdirectory creates a RHEL 7 AMD64 image for use with MAAS.

**# Obtain the RHEL7 DVD ISO

Download the [RHEL7 DVD ISO](https://developers.redhat.com/products/rhel/download)`↗` to your `rhel7` subdirectory.

[note]
You may have to scroll down the page at the above link to find the RHEL7 version. **Be sure you are getting the ISO image** (a much larger file) and not the bootstrap file.
[/note]

**# Customizing the image, if desired

The deployment image may be customized by modifying http/rhel7.ks. See the CentOS kickstart documentation for more information.

**# Set up a proxy for building the image, if desired

The Packer template pulls all packages from the DVD except for Canonical's cloud-init repository. To use a proxy during the installation add the --proxy=$HTTP_PROXY flag to every line starting with url or repo in http/rhel7.ks. Alternatively you may set the --mirrorlist values to a local mirror.

**# Build the RHEL7 image

You can easily build the image using the Makefile:

```nohighlight
$ make ISO=/PATH/TO/rhel-server-7.9-x86_64-dvd.iso
```

Almost immediately, the terminal will prompt you for your user password (to access `sudo` rights).  Then, for a few minutes, you will encounter a stream `qemu` text similar to this:

```nohighlight
2022/06/07 13:28:00 packer-builder-qemu plugin: Qemu path: /usr/bin/qemu-system-x86_64, Qemu Image page: /usr/bin/qemu-img
==> qemu: Retrieving ISO
2022/06/07 13:28:00 packer-builder-qemu plugin: Leaving retrieve loop for ISO
2022/06/07 13:28:00 packer-builder-qemu plugin: No floppy files specified. Floppy disk will not be made.
2022/06/07 13:28:00 packer-builder-qemu plugin: No CD files specified. CD disk will not be made.
2022/06/07 13:28:00 packer-builder-qemu plugin: [INFO] Creating disk with Path: output-qemu/packer-qemu and Size: 4G
2022/06/07 13:28:00 packer-builder-qemu plugin: Executing qemu-img: []string{"create", "-f", "qcow2", "output-qemu/packer-qemu", "4G"}
2022/06/07 13:28:00 packer-builder-qemu plugin: stdout: Formatting 'output-qemu/packer-qemu', fmt=qcow2 cluster_size=65536 extended_l2=off compression_type=zlib size=4294967296 lazy_refcounts=off refcount_bits=16
2022/06/07 13:28:00 packer-builder-qemu plugin: stderr:
2022/06/07 13:28:00 packer-builder-qemu plugin: Found available port: 8144 on IP: 0.0.0.0
==> qemu: Starting HTTP server on port 8144
    qemu: No communicator is set; skipping port forwarding setup.
==> qemu: Looking for available port between 5900 and 6000 on 127.0.0.1
2022/06/07 13:28:00 packer-builder-qemu plugin: Looking for available port between 5900 and 6000 on 127.0.0.1
2022/06/07 13:28:00 packer-builder-qemu plugin: Found available port: 5970 on IP: 127.0.0.1
2022/06/07 13:28:00 packer-builder-qemu plugin: Found available VNC port: 5970 on IP: 127.0.0.1
2022/06/07 13:28:00 packer-builder-qemu plugin: Qemu --version output: QEMU emulator version 6.2.0 (Debian 1:6.2+dfsg-2ubuntu6)
2022/06/07 13:28:00 packer-builder-qemu plugin: Copyright (c) 2003-2021 Fabrice Bellard and the QEMU Project developers
2022/06/07 13:28:00 packer-builder-qemu plugin: Qemu version: 6.2.0
==> qemu: Starting VM, booting from CD-ROM
    qemu: view the screen of the VM, connect via VNC without a password to
    qemu: vnc://127.0.0.1:70
2022/06/07 13:28:00 packer-builder-qemu plugin: Qemu Builder has no floppy files, not attaching a floppy.
    qemu: The VM will be run headless, without a GUI. If you want to
    qemu: view the screen of the VM, connect via VNC without a password to
    qemu: vnc://127.0.0.1:70
2022/06/07 13:28:00 packer-builder-qemu plugin: Executing /usr/bin/qemu-system-x86_64: []string{"-netdev", "user,id=user.0", "-vnc", "127.0.0.1:70", "-serial", "stdio", "-device", "virtio-net,netdev=user.0", "-drive", "file=output-qemu/packer-qemu,if=virtio,cache=writeback,discard=ignore,format=qcow2", "-drive", "file=/home/stormrider/mnt/Dropbox/src/git/packer-maas/...
==> qemu: Overriding default Qemu arguments with qemuargs template option...
2022/06/07 13:28:00 packer-builder-qemu plugin: Started Qemu. Pid: 7970
2022/06/07 13:28:00 packer-builder-qemu plugin: Qemu stderr: qemu-system-x86_64: warning: host doesn't support requested feature: CPUID.80000001H:ECX.svm [bit 2]
==> qemu: Waiting 3s for boot...
==> qemu: Connecting to VM via VNC (127.0.0.1:5970)
2022/06/07 13:28:05 packer-builder-qemu plugin: Connected to VNC desktop: QEMU (packer-qemu)
==> qemu: Typing the boot command over VNC...
2022/06/07 13:28:05 packer-builder-qemu plugin: Special code '<up>' found, replacing with: 0xFF52
2022/06/07 13:28:06 packer-builder-qemu plugin: Special code '<tab>' found, replacing with: 0xFF09
2022/06/07 13:28:06 packer-builder-qemu plugin: Sending char ' ', code 0x20, shift false
2022/06/07 13:28:06 packer-builder-qemu plugin: Sending char 'i', code 0x69, shift false
2022/06/07 13:28:06 packer-builder-qemu plugin: Sending char 'n', code 0x6E, shift false
2022/06/07 13:28:06 packer-builder-qemu plugin: Sending char 's', code 0x73, shift false
2022/06/07 13:28:07 packer-builder-qemu plugin: Sending char 't', code 0x74, shift false
2022/06/07 13:28:07 packer-builder-qemu plugin: Sending char '.', code 0x2E, shift false
```

Eventually, the screen will clear and you will see an `anaconda` window.  Anaconda is the RedHat installer tool, which is preparing your custom image:

```nohighlight
7) [x] Network configuration             8) [ ] User creation
       (Wired (ens3) connected)                 (No user will be created)
2022/06/07 13:29:19 packer-builder-qemu plugin: Qemu stdout: 
================================================================================
================================================================================
Progress07 13:29:19 packer-builder-qemu plugin: Qemu stdout: 
2022/06/07 13:29:19 packer-builder-qemu plugin: Qemu stdout: 
.022/06/07 13:29:20 packer-builder-qemu plugin: Qemu stdout: 
2022/06/07 13:29:20 packer-builder-qemu plugin: Qemu stdout: Setting up the installation environment
2022/06/07 13:29:20 packer-builder-qemu plugin: Qemu stdout: Setting up com_redhat_kdump addon
2022/06/07 13:29:20 packer-builder-qemu plugin: Qemu stdout: Setting up org_fedora_oscap addon
2022/06/07 13:29:20 packer-builder-qemu plugin: Qemu stdout: ..
2022/06/07 13:29:23 packer-builder-qemu plugin: Qemu stdout: Configuring storage
...2/06/07 13:29:20 packer-builder-qemu plugin: Qemu stdout: 
Running pre-installation scriptser-qemu plugin: Qemu stdout:
.022/06/07 13:29:23 packer-builder-qemu plugin: Qemu stdout: 
Running pre-installation taskslder-qemu plugin: Qemu stdout:
...2/06/07 13:29:24 packer-builder-qemu plugin: Qemu stdout: 
2022/06/07 13:29:24 packer-builder-qemu plugin: Qemu stdout: Installing.
2022/06/07 13:29:25 packer-builder-qemu plugin: Qemu stdout: Starting package installation process
Downloading packagespacker-builder-qemu plugin: Qemu stdout: 
2022/06/07 13:29:29 packer-builder-qemu plugin: Qemu stdout: 

[anaconda]1:main* 2:shell  3:log  4:storage-log >Switch tab: Alt+Tab | Help: F1 :shell  3:log  4:sto><'echo -n "Switch tab: Alt+Tab | Help: F
```

Anaconda will run for three to five minutes.  When it finishes, it will clear the screen and return you to the shell prompt.

**# Alternative: Run packer manually

Alternatively, you can manually run packer. Your current working directory must be in `packer-maas/rhel7`, where this file is located. Once in `packer-maas/rhel7`, you can generate an image with:

```nohighlight
$ sudo PACKER_LOG=1 packer build -var 'rhel7_iso_path=/PATH/TO/rhel-server-7.9-x86_64-dvd.iso' rhel7.json
```

[note]
`rhel7.json` is configured to run Packer in headless mode. Only Packer output will be seen. If you wish to see the installation output, connect to the VNC port given in the Packer output or change the value of headless to false in `rhel7.json`.
[/note]

**# Upload the RHEL7 image to MAAS

You can upload the RHEL7 raw packer image with the following command:

```nohighlight
$ maas $PROFILE boot-resources create
name='rhel/7-custom' title='RHEL 7 Custom' architecture='amd64/generic' filetype='tgz' content@=rhel7.tar.gz
```

**# Verify your custom image

Before relying on it in production, you should test your custom image by deploying it to a test (virtual) machine.  It's the machine named `open-gannet` in this listing:

```nohighlight
maas admin machines read | jq -r '(["HOSTNAME","SYSID","POWER","STATUS",
"OWNER", "OS", "DISTRO"] | (., map(length*"-"))),
(.[] | [.hostname, .system_id, .power_state, .status_name, .owner // "-",
.osystem, .distro_series]) | @tsv' | column -t

HOSTNAME     SYSID   POWER  STATUS    OWNER  OS      DISTRO
--------     -----   -----  ------    -----  --      ------
valued-moth  e86c7h  on     Deployed  admin  ubuntu  focal
open-gannet  nk7x8y  on     Deployed  admin  custom  rhel7-raw
```

**# Log into your deployed image and verify that it's right

You should log into your newly-deployed image and verify that it has all the customisations you added to the build process.  The default username for packer-created RHEL images is `cloud-user`.

** How to pack a RHEL8 image for MAAS deployment

You can create a RHEL8 image for MAAS deployment via the following procedure.

**# Verify the requirements

To create a RHEL8 image to upload to MAAS, you must have a machine running Ubuntu 18.04+ or higher with the ability to run KVM virtual machines.  You must also aquire the following items, as described in later instructions:

- qemu-utils
- packer
- The RHEL 8 DVD ISO

To deploy the image to MAAS, you must also have:

- MAAS 2.3+
- Curtin 18.1-59+

**# Install packer

Packer is easily installed from its Debian package:

```nohighlight
sudo apt install packer
```

This should install with no additional prompts.

**# Install RHEL8 template dependencies

```nohighlight
sudo apt install qemu-utils
```

**# Get the available packer templates

You can obtain the packer templates by cloning the [packer-maas github repository](https://github.com/canonical/packer-maas.git)`↗`, like this:

```nohighlight
git clone https://github.com/canonical/packer-maas.git
```

Make sure to pay attention to where the repository is cloned.  The Packer template in this cloned repository creates a RHEL8 AMD64 image for use with MAAS.

This package should install with no additional prompts.

**# Locate the RHEL8 template

The packer template in the directory `rhel8` subdirectory creates a RHEL 8 AMD64 image for use with MAAS.

**# Obtain the RHEL8 DVD ISO

Download the [RHEL8 DVD ISO](https://developers.redhat.com/products/rhel/download)`↗` to your `rhel8` subdirectory.

[note]
You may have to scroll down the page at the above link to find the RHEL8 version. **Be sure you are getting the ISO image** (a much larger file) and not the bootstrap file.
[/note]

**# Customizing the image, if desired

The deployment image may be customized by modifying http/rhel8.ks. See the CentOS kickstart documentation for more information.

**# Set up a proxy for building the image, if desired

The Packer template pulls all packages from the DVD except for Canonical's cloud-init repository. To use a proxy during the installation add the --proxy=$HTTP_PROXY flag to every line starting with url or repo in http/rhel8.ks. Alternatively you may set the --mirrorlist values to a local mirror.

**# Build the RHEL8 image

You can easily build the image using the Makefile:

```nohighlight
$ make ISO=/PATH/TO/rhel-server-8.6-x86_64-dvd.iso
```

Almost immediately, the terminal will prompt you for your user password (to access `sudo` rights).  Then, for a few minutes, you will encounter a stream `qemu` text similar to this:

```nohighlight
2022/06/07 13:28:00 packer-builder-qemu plugin: Qemu path: /usr/bin/qemu-system-x86_64, Qemu Image page: /usr/bin/qemu-img
==> qemu: Retrieving ISO
==> qemu: Trying ./rhel-8.6-x86_64-dvd.iso
2022/06/07 13:28:00 packer-builder-qemu plugin: Acquiring lock for: ./rhel-8.6-x86_64-dvd.iso (/home/stormrider/mnt/Dropbox/src/git/packer-maas/rhel8/packer_cache/4142f50427ed611570687aee099b796c00359ae6.iso.lock)
==> qemu: Trying ./rhel-8.6-x86_64-dvd.iso
2022/06/07 13:28:00 packer-builder-qemu plugin: Leaving retrieve loop for ISO
==> qemu: ./rhel-8.6-x86_64-dvd.iso => /home/stormrider/mnt/Dropbox/src/git/packer-maas/rhel8/rhel-8.6-x86_64-dvd.iso
2022/06/07 13:28:00 packer-builder-qemu plugin: No floppy files specified. Floppy disk will not be made.
2022/06/07 13:28:00 packer-builder-qemu plugin: No CD files specified. CD disk will not be made.
2022/06/07 13:28:00 packer-builder-qemu plugin: [INFO] Creating disk with Path: output-qemu/packer-qemu and Size: 4G
2022/06/07 13:28:00 packer-builder-qemu plugin: Executing qemu-img: []string{"create", "-f", "qcow2", "output-qemu/packer-qemu", "4G"}
2022/06/07 13:28:00 packer-builder-qemu plugin: stdout: Formatting 'output-qemu/packer-qemu', fmt=qcow2 cluster_size=65536 extended_l2=off compression_type=zlib size=4294967296 lazy_refcounts=off refcount_bits=16
2022/06/07 13:28:00 packer-builder-qemu plugin: stderr:
2022/06/07 13:28:00 packer-builder-qemu plugin: Found available port: 8144 on IP: 0.0.0.0
==> qemu: Starting HTTP server on port 8144
    qemu: No communicator is set; skipping port forwarding setup.
==> qemu: Looking for available port between 5900 and 6000 on 127.0.0.1
2022/06/07 13:28:00 packer-builder-qemu plugin: Looking for available port between 5900 and 6000 on 127.0.0.1
2022/06/07 13:28:00 packer-builder-qemu plugin: Found available port: 5970 on IP: 127.0.0.1
2022/06/07 13:28:00 packer-builder-qemu plugin: Found available VNC port: 5970 on IP: 127.0.0.1
2022/06/07 13:28:00 packer-builder-qemu plugin: Qemu --version output: QEMU emulator version 6.2.0 (Debian 1:6.2+dfsg-2ubuntu6)
2022/06/07 13:28:00 packer-builder-qemu plugin: Copyright (c) 2003-2021 Fabrice Bellard and the QEMU Project developers
2022/06/07 13:28:00 packer-builder-qemu plugin: Qemu version: 6.2.0
==> qemu: Starting VM, booting from CD-ROM
    qemu: view the screen of the VM, connect via VNC without a password to
    qemu: vnc://127.0.0.1:70
2022/06/07 13:28:00 packer-builder-qemu plugin: Qemu Builder has no floppy files, not attaching a floppy.
    qemu: The VM will be run headless, without a GUI. If you want to
    qemu: view the screen of the VM, connect via VNC without a password to
    qemu: vnc://127.0.0.1:70
2022/06/07 13:28:00 packer-builder-qemu plugin: Executing /usr/bin/qemu-system-x86_64: []string{"-netdev", "user,id=user.0", "-vnc", "127.0.0.1:70", "-serial", "stdio", "-device", "virtio-net,netdev=user.0", "-drive", "file=output-qemu/packer-qemu,if=virtio,cache=writeback,discard=ignore,format=qcow2", "-drive", "file=/home/stormrider/mnt/Dropbox/src/git/packer-maas/rhel8/rhel-8.6-x86_64-dvd.iso,media=cdrom", "-boot", "once=d", "-name", "packer-qemu", "-machine", "type=pc,accel=kvm", "-m", "2048M"}
==> qemu: Overriding default Qemu arguments with qemuargs template option...
2022/06/07 13:28:00 packer-builder-qemu plugin: Started Qemu. Pid: 7970
2022/06/07 13:28:00 packer-builder-qemu plugin: Qemu stderr: qemu-system-x86_64: warning: host doesn't support requested feature: CPUID.80000001H:ECX.svm [bit 2]
==> qemu: Waiting 3s for boot...
==> qemu: Connecting to VM via VNC (127.0.0.1:5970)
2022/06/07 13:28:05 packer-builder-qemu plugin: Connected to VNC desktop: QEMU (packer-qemu)
==> qemu: Typing the boot command over VNC...
2022/06/07 13:28:05 packer-builder-qemu plugin: Special code '<up>' found, replacing with: 0xFF52
2022/06/07 13:28:06 packer-builder-qemu plugin: Special code '<tab>' found, replacing with: 0xFF09
2022/06/07 13:28:06 packer-builder-qemu plugin: Sending char ' ', code 0x20, shift false
2022/06/07 13:28:06 packer-builder-qemu plugin: Sending char 'i', code 0x69, shift false
2022/06/07 13:28:06 packer-builder-qemu plugin: Sending char 'n', code 0x6E, shift false
2022/06/07 13:28:06 packer-builder-qemu plugin: Sending char 's', code 0x73, shift false
2022/06/07 13:28:07 packer-builder-qemu plugin: Sending char 't', code 0x74, shift false
2022/06/07 13:28:07 packer-builder-qemu plugin: Sending char '.', code 0x2E, shift false
```

Eventually, the screen will clear and you will see an `anaconda` window.  Anaconda is the RedHat installer tool, which is preparing your custom image:

```nohighlight
7) [x] Network configuration             8) [ ] User creation
       (Wired (ens3) connected)                 (No user will be created)
2022/06/07 13:29:19 packer-builder-qemu plugin: Qemu stdout: 
================================================================================
================================================================================
Progress07 13:29:19 packer-builder-qemu plugin: Qemu stdout: 
2022/06/07 13:29:19 packer-builder-qemu plugin: Qemu stdout: 
.022/06/07 13:29:20 packer-builder-qemu plugin: Qemu stdout: 
2022/06/07 13:29:20 packer-builder-qemu plugin: Qemu stdout: Setting up the installation environment
2022/06/07 13:29:20 packer-builder-qemu plugin: Qemu stdout: Setting up com_redhat_kdump addon
2022/06/07 13:29:20 packer-builder-qemu plugin: Qemu stdout: Setting up org_fedora_oscap addon
2022/06/07 13:29:20 packer-builder-qemu plugin: Qemu stdout: ..
2022/06/07 13:29:23 packer-builder-qemu plugin: Qemu stdout: Configuring storage
...2/06/07 13:29:20 packer-builder-qemu plugin: Qemu stdout: 
Running pre-installation scriptser-qemu plugin: Qemu stdout:
.022/06/07 13:29:23 packer-builder-qemu plugin: Qemu stdout: 
Running pre-installation taskslder-qemu plugin: Qemu stdout:
...2/06/07 13:29:24 packer-builder-qemu plugin: Qemu stdout: 
2022/06/07 13:29:24 packer-builder-qemu plugin: Qemu stdout: Installing.
2022/06/07 13:29:25 packer-builder-qemu plugin: Qemu stdout: Starting package installation process
Downloading packagespacker-builder-qemu plugin: Qemu stdout: 
2022/06/07 13:29:29 packer-builder-qemu plugin: Qemu stdout: 

[anaconda]1:main* 2:shell  3:log  4:storage-log >Switch tab: Alt+Tab | Help: F1 :shell  3:log  4:sto><'echo -n "Switch tab: Alt+Tab | Help: F
```

Anaconda will run for three to five minutes.  When it finishes, it will clear the screen and return you to the shell prompt.

**# Alternative: Run packer manually

Alternatively, you can manually run packer. Your current working directory must be in `packer-maas/rhel8`, where this file is located. Once in `packer-maas/rhel8`, you can generate an image with:

```nohighlight
$ sudo PACKER_LOG=1 packer build -var 'rhel8_iso_path=/PATH/TO/rhel-server-8.6-x86_64-dvd.iso' rhel8.json
```

[note]
`rhel8.json` is configured to run Packer in headless mode. Only Packer output will be seen. If you wish to see the installation output, connect to the VNC port given in the Packer output or change the value of headless to false in `rhel8.json`.
[/note]

**# Upload the RHEL8 image to MAAS

You can upload the RHEL8 raw packer image with the following command:

```nohighlight
$ maas $PROFILE boot-resources create
name='rhel/8-custom' title='RHEL 8 Custom' architecture='amd64/generic' filetype='tgz' content@=rhel8.tar.gz
```

**# Verify your custom image

Before relying on it in production, you should test your custom image by deploying it to a test (virtual) machine.  It's the machine named `open-gannet` in this listing:

```nohighlight
maas admin machines read | jq -r '(["HOSTNAME","SYSID","POWER","STATUS",
"OWNER", "OS", "DISTRO"] | (., map(length*"-"))),
(.[] | [.hostname, .system_id, .power_state, .status_name, .owner // "-",
.osystem, .distro_series]) | @tsv' | column -t

HOSTNAME     SYSID   POWER  STATUS    OWNER  OS      DISTRO
--------     -----   -----  ------    -----  --      ------
valued-moth  e86c7h  on     Deployed  admin  ubuntu  focal
open-gannet  nk7x8y  on     Deployed  admin  custom  rhel8-raw
```

**# Log into your deployed image and verify that it's right

You should log into your newly-deployed image and verify that it has all the customisations you added to the build process.  The default username for packer-created RHEL images is `cloud-user`.

** How to pack a CentOS 7 image for MAAS deployment

You can create a CentOS 7 image for MAAS deployment via the following procedure.

**# Verify the requirements

To create a CentOS 7 image to upload to MAAS, you must have a machine running Ubuntu 18.04+ or higher with the ability to run KVM virtual machines.  You must also aquire the following items, as described in later instructions:

- qemu-utils
- packer

Note that a suitable CentOS 7 image is downloaded as part of the template.

To deploy the image to MAAS, you must also have:

- MAAS 2.3+
- Curtin 18.1-59+

**# Install packer

Packer is easily installed from its Debian package:

```nohighlight
sudo apt install packer
```

This should install with no additional prompts.

**# Install CentOS 7 template dependencies

```nohighlight
sudo apt install qemu-utils
```

**# Get the available packer templates

You can obtain the packer templates by cloning the [packer-maas github repository](https://github.com/canonical/packer-maas.git)`↗`, like this:

```nohighlight
git clone https://github.com/canonical/packer-maas.git
```

Make sure to pay attention to where the repository is cloned.  The Packer template in this cloned repository creates a CentOS 7 AMD64 image for use with MAAS.

This package should install with no additional prompts.

**# Locate the CentOS 7 template

The packer template in the directory `centos7` subdirectory creates a CentOS 7 AMD64 image for use with MAAS.

**# Customise the image, if desired

The deployment image may be customized by modifying http/centos7.ks. See the CentOS kickstart documentation for more information.

**# Set up a proxy for building the image, if desired

The Packer template downloads the CentOS net installer from the Internet. To tell Packer to use a proxy set the HTTP_PROXY environment variable to your proxy server. Alternatively you may redefine iso_url to a local file, set iso_checksum_type to none to disable checksuming, and remove iso_checksum_url.

To use a proxy during the installation add the --proxy=$HTTP_PROXY flag to every line starting with url or repo in http/centos7.ks. Alternatively you may set the --mirrorlist values to a local mirror.

**# Build the CentOS 7 image

You can easily build the image using the Makefile:

```nohighlight
$ make
```

For a few minutes, you will encounter a stream `qemu` text similar to this:

```nohighlight
2022/06/07 13:28:00 packer-builder-qemu plugin: Qemu path: /usr/bin/qemu-system-x86_64, Qemu Image page: /usr/bin/qemu-img
==> qemu: Retrieving ISO
2022/06/07 13:28:00 packer-builder-qemu plugin: Leaving retrieve loop for ISO
2022/06/07 13:28:00 packer-builder-qemu plugin: No floppy files specified. Floppy disk will not be made.
2022/06/07 13:28:00 packer-builder-qemu plugin: No CD files specified. CD disk will not be made.
2022/06/07 13:28:00 packer-builder-qemu plugin: [INFO] Creating disk with Path: output-qemu/packer-qemu and Size: 4G
2022/06/07 13:28:00 packer-builder-qemu plugin: Executing qemu-img: []string{"create", "-f", "qcow2", "output-qemu/packer-qemu", "4G"}
2022/06/07 13:28:00 packer-builder-qemu plugin: stdout: Formatting 'output-qemu/packer-qemu', fmt=qcow2 cluster_size=65536 extended_l2=off compression_type=zlib size=4294967296 lazy_refcounts=off refcount_bits=16
2022/06/07 13:28:00 packer-builder-qemu plugin: stderr:
2022/06/07 13:28:00 packer-builder-qemu plugin: Found available port: 8144 on IP: 0.0.0.0
==> qemu: Starting HTTP server on port 8144
    qemu: No communicator is set; skipping port forwarding setup.
==> qemu: Looking for available port between 5900 and 6000 on 127.0.0.1
2022/06/07 13:28:00 packer-builder-qemu plugin: Looking for available port between 5900 and 6000 on 127.0.0.1
2022/06/07 13:28:00 packer-builder-qemu plugin: Found available port: 5970 on IP: 127.0.0.1
2022/06/07 13:28:00 packer-builder-qemu plugin: Found available VNC port: 5970 on IP: 127.0.0.1
2022/06/07 13:28:00 packer-builder-qemu plugin: Qemu --version output: QEMU emulator version 6.2.0 (Debian 1:6.2+dfsg-2ubuntu6)
2022/06/07 13:28:00 packer-builder-qemu plugin: Copyright (c) 2003-2021 Fabrice Bellard and the QEMU Project developers
2022/06/07 13:28:00 packer-builder-qemu plugin: Qemu version: 6.2.0
==> qemu: Starting VM, booting from CD-ROM
    qemu: view the screen of the VM, connect via VNC without a password to
    qemu: vnc://127.0.0.1:70
2022/06/07 13:28:00 packer-builder-qemu plugin: Qemu Builder has no floppy files, not attaching a floppy.
    qemu: The VM will be run headless, without a GUI. If you want to
    qemu: view the screen of the VM, connect via VNC without a password to
    qemu: vnc://127.0.0.1:70
2022/06/07 13:28:00 packer-builder-qemu plugin: Executing /usr/bin/qemu-system-x86_64: []string{"-netdev", "user,id=user.0", "-vnc", "127.0.0.1:70", "-serial", "stdio", "-device", "virtio-net,netdev=user.0", "-drive", "file=output-qemu/packer-qemu,if=virtio,cache=writeback,discard=ignore,format=qcow2", "-drive", "file=/home/stormrider/mnt/Dropbox/src/git/packer-maas/...
==> qemu: Overriding default Qemu arguments with qemuargs template option...
2022/06/07 13:28:00 packer-builder-qemu plugin: Started Qemu. Pid: 7970
2022/06/07 13:28:00 packer-builder-qemu plugin: Qemu stderr: qemu-system-x86_64: warning: host doesn't support requested feature: CPUID.80000001H:ECX.svm [bit 2]
==> qemu: Waiting 3s for boot...
==> qemu: Connecting to VM via VNC (127.0.0.1:5970)
2022/06/07 13:28:05 packer-builder-qemu plugin: Connected to VNC desktop: QEMU (packer-qemu)
==> qemu: Typing the boot command over VNC...
2022/06/07 13:28:05 packer-builder-qemu plugin: Special code '<up>' found, replacing with: 0xFF52
2022/06/07 13:28:06 packer-builder-qemu plugin: Special code '<tab>' found, replacing with: 0xFF09
2022/06/07 13:28:06 packer-builder-qemu plugin: Sending char ' ', code 0x20, shift false
2022/06/07 13:28:06 packer-builder-qemu plugin: Sending char 'i', code 0x69, shift false
2022/06/07 13:28:06 packer-builder-qemu plugin: Sending char 'n', code 0x6E, shift false
2022/06/07 13:28:06 packer-builder-qemu plugin: Sending char 's', code 0x73, shift false
2022/06/07 13:28:07 packer-builder-qemu plugin: Sending char 't', code 0x74, shift false
2022/06/07 13:28:07 packer-builder-qemu plugin: Sending char '.', code 0x2E, shift false
```

Eventually, the screen will clear and you will see an `anaconda` window.  Anaconda is the RedHat installer tool, which is preparing your custom image:

```nohighlight
7) [x] Network configuration             8) [ ] User creation
       (Wired (ens3) connected)                 (No user will be created)
2022/06/07 13:29:19 packer-builder-qemu plugin: Qemu stdout: 
================================================================================
================================================================================
Progress07 13:29:19 packer-builder-qemu plugin: Qemu stdout: 
2022/06/07 13:29:19 packer-builder-qemu plugin: Qemu stdout: 
.022/06/07 13:29:20 packer-builder-qemu plugin: Qemu stdout: 
2022/06/07 13:29:20 packer-builder-qemu plugin: Qemu stdout: Setting up the installation environment
2022/06/07 13:29:20 packer-builder-qemu plugin: Qemu stdout: Setting up com_redhat_kdump addon
2022/06/07 13:29:20 packer-builder-qemu plugin: Qemu stdout: Setting up org_fedora_oscap addon
2022/06/07 13:29:20 packer-builder-qemu plugin: Qemu stdout: ..
2022/06/07 13:29:23 packer-builder-qemu plugin: Qemu stdout: Configuring storage
...2/06/07 13:29:20 packer-builder-qemu plugin: Qemu stdout: 
Running pre-installation scriptser-qemu plugin: Qemu stdout:
.022/06/07 13:29:23 packer-builder-qemu plugin: Qemu stdout: 
Running pre-installation taskslder-qemu plugin: Qemu stdout:
...2/06/07 13:29:24 packer-builder-qemu plugin: Qemu stdout: 
2022/06/07 13:29:24 packer-builder-qemu plugin: Qemu stdout: Installing.
2022/06/07 13:29:25 packer-builder-qemu plugin: Qemu stdout: Starting package installation process
Downloading packagespacker-builder-qemu plugin: Qemu stdout: 
2022/06/07 13:29:29 packer-builder-qemu plugin: Qemu stdout: 

[anaconda]1:main* 2:shell  3:log  4:storage-log >Switch tab: Alt+Tab | Help: F1 :shell  3:log  4:sto><'echo -n "Switch tab: Alt+Tab | Help: F
```

Anaconda will run for three to five minutes.  When it finishes, it will clear the screen and return you to the shell prompt.

**# Alternative: Run packer manually

Alternatively you can manually run packer. Your current working directory must be in packer-maas/centos7, where this file is located. Once in packer-maas/centos7 you can generate an image with:

```nohighlight
$ sudo PACKER_LOG=1 packer build centos7.json
```

[note]
centos7.json is configured to run Packer in headless mode. Only Packer output will be seen. If you wish to see the installation output connect to the VNC port given in the Packer output or change the value of headless to false in centos7.json.
[/note]

Installation is non-interactive.

**# Upload the CentOS 7 image to MAAS

You can upload the CentOS 7 raw packer image with the following command:

```nohighlight
$ maas $PROFILE boot-resources create
name='centos/7-custom' title='CentOS 7 Custom' architecture='amd64/generic' filetype='tgz' content@=centos7.tar.gz
```

**# Verify your custom image

Before relying on it in production, you should test your custom image by deploying it to a test (virtual) machine.  It's the machine named `open-gannet` in this listing:

```nohighlight
maas admin machines read | jq -r '(["HOSTNAME","SYSID","POWER","STATUS",
"OWNER", "OS", "DISTRO"] | (., map(length*"-"))),
(.[] | [.hostname, .system_id, .power_state, .status_name, .owner // "-",
.osystem, .distro_series]) | @tsv' | column -t

HOSTNAME     SYSID   POWER  STATUS    OWNER  OS      DISTRO
--------     -----   -----  ------    -----  --      ------
valued-moth  e86c7h  on     Deployed  admin  ubuntu  focal
open-gannet  nk7x8y  on     Deployed  admin  custom  centos7-raw
```

**# Log into your deployed image and verify that it's right

You should log into your newly-deployed image and verify that it has all the customisations you added to the build process.  The default username for packer-created CentOS 7 images is `centos`.

** How to pack an ESXi image for MAAS deployment

You can create an ESXi image for MAAS deployment via the following procedure. MAAS cannot directly deploy the VMware ESXi ISO; a specialized image must be created from the ISO. Canonical has created a Packer template to automatically do this for you.

*** Verify the requirements and accept the limitations

VMware ESXi has a specific set of requirements and limitations which are more stringent than MAAS.

**** Basic requirements

The machine building the deployment image must be a GNU/Linux host with a dual core x86_64 processor supporting hardware virtualization with at least 4GB of RAM and 10GB of disk space available. Additionally the qemu-kvm and qemu-utils packages must be installed on the build system.

**** libvirt testing

While VMware ESXi does not support running in any virtual machine it is possible to deploy to one. The libvirt machine must be a KVM instance with at least CPU 2 cores and 4GB of RAM. To give VMware ESXi access to hardware virtualization go into machine settings, CPUs, and select 'copy host CPU configuration.'

VMware ESXi has no support for libvirt drivers.  Instead an emulated IDE disk and an emulated e1000 NIC must be used.

**** Known storage limitations

Only datastores may be configured using the devices available on the system. The first 9 partitions of the disk are reserved for VMware ESXi operating system usage.

[note]
**WARNING**: VMware does not support cloning boot devices - you may run into issues triggered by non-unique UUID. This may lead to data corruption on VMFS datastores when using cloned boot devices.
[/note]

**** Known networking limitations

Bridges are not supported in VMware ESXi. In addition, certain MAAS bond modes are mapped to VMware ESXi NIC team sharing with load balancing, as follows:

- balance-rr - portid
- active-backup - explicit
- 802.3ad - iphash, LACP rate and XMIT hash policy settings are ignored.

No other bond modes are currently supported.

[note]
**WARNING**: VMware ESXi does not allow VMs to use a PortGroup that has a VMK attached to it. All configured devices will have a VMK attached. To use a vSwitch with VMs you must leave a device or alias unconfigured in MAAS.
[/note]

**** Image fails to build due to qemu-nbd error

If the image fails to build due to a `qemu-nbd` error, try disconnecting the device with: 

```nohighlight
$ sudo qemu-nbd -d /dev/nbd4
```

**** Prerequisites to create and deploy images

- A machine running Ubuntu 18.04+ with the ability to run KVM virtual machines.
- qemu-utils
- Python Pip
- Packer
- The VMware ESXi installation ISO must be [downloaded manually](https://www.vmware.com/go/get-free-esxi)`↗`.
- MAAS 2.5 or above, MAAS 2.6 required for storage configuration

*** Install packer

Packer is easily installed from its Debian package:

```nohighlight
sudo apt install packer
```

This should install with no additional prompts.

*** Install ESXi template dependencies

```nohighlight
sudo apt install qemu-utils
```

*** Install Python `pip`, if not installed

```nohighlight
sudo apt install pip
```

*** Get the available packer templates

You can obtain the packer templates by cloning the [packer-maas github repository](https://github.com/canonical/packer-maas.git)`↗`, like this:

```nohighlight
git clone https://github.com/canonical/packer-maas.git
```

Make sure to pay attention to where the repository is cloned. This package should install with no additional prompts.

*** Locate the ESXi template

The appropriate packer template can be found in the subdirectory `vmware-esxi` in the packer repository.

<a href="#heading--Customise-the-image,-if-desired">*** id="heading--Customise-the-image,-if-desired">Customise the image, if desired

The deployment image may be customized by modifying `packer-maas/vmware-esxi/KS.CFG` see Installation and Upgrade Scripts in the [VMware ESXi installation and Setup manual](https://docs.vmware.com/en/VMware-vSphere/6.7/vsphere-esxi-67-installation-setup-guide.pdf)`↗` for more information.

*** Build the ESXi image

You can easily build the image using the Makefile:

```nohighlight
$ make ISO=/path/to/VMware-VMvisor-Installer-6.7.0.update03-14320388.x86_64.iso
````

**# Alternative: Run packer manually

Alternatively, you can manually run packer. Your current working directory must be in `packer-maas/vmware-esxi`, where this file is located. Once in `packer-maas/vmware-esxi`, you can generate an image with:

```nohighlight
$ sudo PACKER_LOG=1 packer build -var 'vmware_esxi_iso_path=/path/to/VMware-VMvisor-Installer-6.7.0.update03-14320388.x86_64.iso' vmware-esxi.json
```

[note]
`vmware-esxi.json` is configured to runpPacker in headless mode. Only packer output will be seen. If you wish to see the installation outputj, connect to the VNC port given in the packer output, or remove the line containing "headless" in `vmware-esxi.json`.
[/note]

Installation is non-interactive.

*** Upload the ESXi image to MAAS

You can upload the ESXi image to MAAS with the following command:

```nohighlight
$ maas $PROFILE boot-resources create name='esxi/6.7' title='VMware ESXi 6.7' architecture='amd64/generic' filetype='ddgz' content@=vmware-esxi.dd.gz
```

** How to use MAAS Image Builder to build MAAS images

MAAS Image Builder is an older tool, still required to build some images (e.g., Windows images).  Wherever possible, we recommend you use `packer`, as described above.

[note]
In order to use MAAS Image Builder, you must purchase [Ubuntu Advantage for Infrastructure](https://support.canonical.com/ua/s/article/How-to-access-the-MAAS-Image-Builder-tool)`↗`.
[/note]

This article will help you learn:

- [How to install MAAS Image Builder](#heading--install-mib) via a private Canonical PPA (which you can request)
- [How to create custom CentOS images](#heading--custom-centos-images)
- [How to create custom RHEL images](#heading--custom-rhel-images)
- [How to create Windows images](#heading--custom-windows-images)
- [How to create other kinds of custom images](#heading--other-custom-images)

You can customise most images as much or as little as you wish, then use them to commission machines with MAAS. 

*** How to install MAAS Image Builder

To get MAAS Image Builder, you must be subscribed to a private PPA provided by Canonical Support to those customers who have purchased [Ubuntu Advantage for Infrastructure](https://support.canonical.com/ua/s/article/How-to-access-the-MAAS-Image-Builder-tool)`↗`.  Note that the steps below will fail if you have not purchased Ubuntu Advantage and been subscribed to the private PPA by your Canonical support rep.

Once subscribed, you need to obtain your credentials at this external link:

https://launchpad.net/~/+archivesubscriptions

Also, you must add the repository with the <code>add-apt-repository</code> command.  Note: Be sure to substitute your unique URL in the command below:

    $ sudo add-apt-repository \
    “https://LaunchpadID:Password@private-ppa.launchpad.net/maas-image-builder-partners/stable/ubuntu"

Once you have added the private PPA, you can install the Image Builder like this:

    $ sudo apt-get install maas-image-builder

All done? Great!  Now you can build and customise images for MAAS machines, as shown in the sections below.

*** How to create custom CentOS images

MAAS already provides the latest available CentOS 7 and CentOS 8 for automatic download. If you need something else, though, MAAS Image Builder supports the ability to create various CentOS images.

**** Network Requirements for CentOS

Access to the Internet is required, since you will need to start with one of these sites:

- http://mirror.centos.org - OS, updates, and extra repositories
- https://download.fedoraproject.org - EPEL
- https://copr-be.cloud.fedoraproject.org - Canonical maintained cloud-init repository

**** Creating CentOS images behind a proxy

MAAS Image Builder can create CentOS images behind a proxy -- just set the ‘http_proxy’ environment variable to _your_ particular proxy. Once deployed, <code>yum</code> will use this MAAS-configured proxy.

**** Creating the CentOS images

<code>maas-image-builder</code> is designed to automate the process of generating the images for MAAS and <code>curtin</code>.  Here are some specific examples:

    $ sudo maas-image-builder -o centos6-amd64-root-tgz --arch amd64 centos --edition 6 
    $ sudo maas-image-builder -o centos6-i386-root-tgz --arch i386 centos --edition 6
    $ sudo maas-image-builder -o centos7-amd64-root-tgz --arch amd64 centos --edition 7

**** Customising CentOS images

Starting from MAAS Image Builder 1.0.4, customisation of CentOS images is now supported.  You can provide a custom kickstart, in _addition_ to the kickstart that MAAS Image Builder uses to create the images. You can customise your image like this:

    $ sudo maas-image-builder -o centos7-amd64-root-tgz --arch amd64 centos --edition 7 --custom-kickstart ./custom.ks

**** Uploading the CentOS image into MAAS

Custom CentOS images can be uploaded to MAAS as shown in the command below.  _Do note_ that the name **must** start with ‘centos’ and **must** be one line:

    maas admin boot-resources create name=centos/centos6-custom architecture=amd64/generic content@=./build-output/centos-amd64-root-tgz

You can use the MAAS WebUI to check that your custom CentOS image is valid and selectable for deployment.

*** How to create custom RHEL images

Currently, MAAS _only_ supports RHEL as a custom image. In future versions of MAAS, RHEL will be natively supported.

**** RHEL image Requirements

In order to create RHEL images, you will need access to these sites:

- A RHEL DVD ISO - Contains all RHEL archives which are not available without a license
- https://download.fedoraproject.org `↗` - Access to the EPEL repository to install required deps
- https://copr-be.cloud.fedoraproject.org `↗` - Access to the Canonical maintained cloud-init copr repository

**** Creating RHEL images behind a proxy

MAAS image builder supports creating RHEL images behind a proxy. To use a proxy when building a RHEL image, just set the ‘http_proxy’ environment variable to _your_ local proxy. Once deployed, <code>yum</code> will use the MAAS-configured proxy.

**** Creating the RHEL images

To generate a usable RHEL image, <code>maas-image-builder</code> automates image generation; these images can be used by MAAS and <code>curtin</code>.

     $ sudo maas-image-builder -a amd64 -o rhel8-amd64-root-tgz rhel --rhel-iso blah.iso

**** Install the RHEL image into MAAS

The custom RHEL image can be uploaded to MAAS, but note that the name **must** start with ‘rhel’ and **must** be expressed as a single line, like this:

    maas admin boot-resources create name=rhel/8 title="RedHat Enterprise Linux 8" architecture=amd64/generic content@=rhel8-amd64-root-tgz

*** How to create Windows images

Since Windows is a proprietary operating system, MAAS can't download these images. You need to manually generate images to use with MAAS by using Windows ISO images.  On the upside, the end result will be much simpler, since there are CLI and WebUI tools to upload a Windows image -- which _helps_ automate the process.

You can obtain Windows ISO images at the Microsoft Evaluation Center:

https://www.microsoft.com/en-us/evalcenter `↗`

<b>Windows editions</b>

There are several Windows editions/install options supported by `maas-image-builder` (`--windows-edition` options):

- `win2008r2`
- `win2008hvr2`
- `win2012`
- `win2012hv`
- `win2012r2`
- `win2012hvr2`
- `win2016`
- `win2016-core`
- `win2016hv`
- `win2016dc`
- `win2016dc-core`
- `win2019`
- `win2019-core`
- `win2019dc`
- `win2019dc-core`
- `win10ent`
- `win10ent-eval`
- `win2022`
- `win2022-core`

The examples in this section use Windows Hyper-V 2012 R2.

**** MAAS Image Builder for Window

MAAS Image Builder (also known as "MIB") can automate the process of generating images for MAAS and <code>curtin</code>.

Note, though, you may need Windows drivers to deploy the image on your specific hardware (see the `--windows-drivers` option).

In order to obtain Windows updates, provide the <code>--windows-updates</code> option (and sufficient disk space, depending on the Windows edition/updates size, with the <code>--disk-size</code> option). This requires access to a bridged connection with a DHCP server (provide a network interface with the <code>maas-image-builder -i</code> option).

Important: <b>UEFI and BIOS</b> systems require different Windows images, built with or without the `--uefi` option, respectively.
(Windows ISO images in UEFI mode usually require connecting using a VNC client early to press any key to start Windows setup; see below.)

Important: <b>LXD Virtual Machines</b> require an UEFI image (`--uefi`) and VirtIO drivers (`--windows-drivers`).
(In order to use/test the VirtIO drivers during image build, not just during image deploy, use `--virtio` and `--driver-store`.)

    sudo maas-image-builder -o windows-win2012hvr2-amd64-root-dd windows \
    --windows-iso win2012hvr2.iso  --windows-edition win2012hvr2 \
    --windows-language en-US \
    [--windows-drivers ~/Win2012hvr2_x64/DRIVERS/] \
    [--windows-updates] [--disk-size 128G] \
    [--uefi] [--virtio] [--driver-store]

**** Image Builder options for Windows

MAAS Image Builder options for Windows images can be listed with the following command:

	sudo maas-image-builder -o windows --help

Note that this is different from the MAAS Image Builder generic/image-independent options, which can be listed with the following command:

	sudo maas-image-builder --help

Some of the Windows-specific options include:

- `--windows-iso`: path to the Windows ISO image.
- `--windows-edition`: identifier for the Windows edition/option being installed (see above).
- `--windows-license-key`: Windows license key (required with non-evaluation editions)
- `--windows-language`: Windows installation language (default: `en-US`)
- `--windows-updates`: download and install Windows Updates (requires internet access; might require a larger `--disk-size` option)
- `--windows-drivers`: path to directory with Windows drivers to be installed (requires internet access; uses the Windows Driver Kit, by default)
- `--driver-store`: combined with `--windows-drivers`, uses the Windows Driver Store to install drivers early into Windows Setup and image (does not require internet access; does not use the Windows Driver Kit).

Some Windows-specific platform options:

- `--uefi`: use UEFI partition layout and firmware
- `--virtio`: use paravirtualized VirtIO SCSI and VirtIO NET devices (instead of emulated devices) for installation (requires `--windows-drivers`)
- `--disk-size`: specify the (virtual) disk size for Windows setup (must be larger for `--windows-updates`; increases deployment/copy-to-disk time, and is expanded to physical disk size during deployment)

**** Debugging custom Windows images

You can debug the Windows installation process by connecting to <code>localhost:5901</code> using a VNC client (e.g., `vncviewer`).

You can pause the Windows installation process at the last step for inspection/debugging in PowerShell with the `--powershell` option.

**** Installing Windows images into MAAS

The generated images need to be placed into the correct directories so MAAS can deploy them onto a node:

    maas admin boot-resources create name=windows/win2012hvr2 \
    architecture=amd64/generic filetype=ddtgz \ 
    content@=./build-output/windows-win2012hvr2-amd64-root-dd 

Now, using the MAAS WebUI, a node can be selected to use Windows Hyper-V 2012 R2. This selection gets reset when a node is stopped, so make sure to set it _before_ starting nodes. You can also set the default operating system (and release) in the settings menu, which removes the need to set it per-node.

*** How to create other kinds of custom images

To install other custom images, use the following command sequence:

    maas <user> boot-resources create name=<custom-image-codename> title="Ubuntu Custom Image" \
    architecture=amd64/generic content@=/location/of/custom/image/ubuntu-custom-root-tgz

As an example:

    maas admin boot-resources create name=custom1 \
    title=”Ubuntu Custom Image” architecture=amd64/generic \
    content@=/home/ubuntu/ubuntu-custom-root-tgz

* How to customise machines
MAAS provides the capability to customise machines.  This article will help you learn:

[tabs]
[tab version="v3.4 Snap,v3.4 Packages,v3.3 Snap,v3.3 Packages,v3.2 Snap,v3.2 Packages"]
- [How to customise machine storage](#heading--how-to-customise-machine-storage)
- [How to pre-seed with curtin](#heading--how-to-pre-seed-with-curtin)
- [How to pre-seed with cloud-init](#heading--cloud-init)
- [How to choose Ubuntu kernels](#heading--how-to-choose-ubuntu-kernels)
- [How to set global kernel boot options](#heading--how-to-set-global-kernel-boot-options)
- [How to create tags with built-in kernel options](#heading--create-tags-with-built-in-kernel-options)
- [How to use resource pools](#heading--how-to-use-resource-pools)
- [How to enable hardware sync on a machine](#heading--how-to-enable-hardware-sync-on-a-machine)
- [How to configure hardware sync interval](#heading--how-to-configure-hardware-sync-interval)
- [How to view updates from hardware sync](#heading--how-to-view-updates-from-hardware-sync)
[/tab]
[tab version="v3.1 Snap,v3.1 Packages,v3.0 Snap,v3.0 Packages,v2.9 Snap,v2.9 Packages"]
- [How to customise machine storage](#heading--how-to-customise-machine-storage)
- [How to pre-seed with curtin](#heading--how-to-pre-seed-with-curtin)
- [How to pre-seed with cloud-init](#heading--cloud-init)
- [How to choose Ubuntu kernels](#heading--how-to-choose-ubuntu-kernels)
- [How to set global kernel boot options](#heading--how-to-set-global-kernel-boot-options)
- [How to create tags with built-in kernel options](#heading--create-tags-with-built-in-kernel-options)
- [How to use resource pools](#heading--how-to-use-resource-pools)
[/tab]
[/tabs]

** How to customise machine storage

This section will show you:

[tabs]
[tab version="v3.4 Snap,v3.4 Packages,v3.3 Snap,v3.3 Packages,v3.2 Snap,v3.2 Packages,v3.1 Snap,v3.1 Packages,v3.0 Snap,v3.0 Packages,v2.9 Snap,v2.9 Packages" view="UI"]
- [How to set global storage layouts](#heading--how-to-set-global-storage-layouts)
- [How to set per-machine storage layouts](#heading--how-to-set-per-machine-storage-layouts)
- [How to set the default erasure configuration](#heading--how-to-set-default-erasure-configuration)
[/tab]
[tab version="v3.4 Snap,v3.4 Packages,v3.3 Snap,v3.3 Packages,v3.2 Snap,v3.2 Packages,v3.1 Snap,v3.1 Packages,v3.0 Snap,v3.0 Packages,v2.9 Snap,v2.9 Packages" view="CLI"]
- [How to set global storage layouts](#heading--how-to-set-global-storage-layouts)
- [How to set per-machine storage layouts](#heading--how-to-set-per-machine-storage-layouts)
- [How to erase disks](#heading--how-to-erase-disks)
- [How to specify conditional erasure types](#heading--how-to-specify-conditional-erasure-types)
- [How to list block devices](#heading--how-to-list-block-devices)
- [How to read a block device](#heading--how-to-read-a-block-device)
- [How to create a block device](#heading--how-to-create-a-block-device)
- [How to update a block device](#heading--how-to-update-a-block-device)
- [How to delete a block device](#heading--how-to-delete-a-block-device)
- [How to format a block device](#heading--format-block-device)
- [How to unformat a block device](#heading--how-to-unformat-a-block-device)
- [How to mount a block device](#heading--how-to-mount-a-block-device)
- [How to unmount a block device](#heading--how-to-unmount-a-block-device)
- [How to set a block device as a boot disk](#heading--how-to-set-a-block-device-as-a-boot-disk)
- [How to list partitions](#heading--how-to-list-partitions)
- [How to create a partition](#heading--how-to-create-a-partition)
- [How to delete a partition](#heading--how-to-delete-a-partition)
- [How to format a partition](#heading--how-to-format-a-partition)
- [How to unformat a partition](#heading--how-to-unformat-a-partition)
- [How to mount a partition](#heading--how-to-mount-a-partition)
- [How to unmount a partition](#heading--how-to-unmount-a-partition)
- [How to list VMFS datastores](#heading--how-to-list-vmfs-datastores)
- [How to view a VMFS datastore](#heading--how-to-view-a-vmfs-datastore)
- [How to create a VMFS datastore](#heading--how-to-create-a-vmfs-datastore)
- [How to edit a VMFS datastore](#heading--how-to-edit-a-vmfs-datastore)
- [How to delete a VMFS datastore](#heading--how-to-delete-a-vmfs-datastore)
[/tab]
[/tabs]

Note that layouts can be set globally and on a per-machine basis.  For additional information on storage layouts, see the [Storage layouts reference](/t/storage-layouts-reference/5973) article.

*** How to set global storage layouts

[tabs]
[tab version="v3.4 Snap,v3.4 Packages,v3.3 Snap,v3.3 Packages,v3.2 Snap,v3.2 Packages,v3.1 Snap,v3.1 Packages,v3.0 Snap,v3.0 Packages,v2.9 Snap,v2.9 Packages" view="UI"]
All machines will have a default layout applied when commissioned. An administrator can configure the default layout on the 'Settings' page, under the 'Storage' tab.
[/tab]
[tab version="v3.4 Snap,v3.4 Packages,v3.3 Snap,v3.3 Packages,v3.2 Snap,v3.2 Packages,v3.1 Snap,v3.1 Packages,v3.0 Snap,v3.0 Packages,v2.9 Snap,v2.9 Packages" view="CLI"]
All machines will have a default layout applied when commissioned. To set the default storage layout for all machines:

```
maas $PROFILE maas set-config name=default_storage_layout value=$LAYOUT_TYPE
```

For example, to set the default layout to Flat:

```
maas $PROFILE maas set-config name=default_storage_layout value=flat
```

Important: The new default will only apply to newly-commissioned machines.

[/tab]
[/tabs]

[note]
The new default will only apply to newly-commissioned machines.
[/note]

*** How to set per-machine storage layouts

[tabs]
[tab version="v3.4 Snap,v3.4 Packages,v3.3 Snap,v3.3 Packages,v3.2 Snap,v3.2 Packages,v3.1 Snap,v3.1 Packages,v3.0 Snap,v3.0 Packages,v2.9 Snap,v2.9 Packages" view="UI"]
An administrator can change the layout for a single machine as well as customise that layout providing this is done while the machine has a status of 'Ready'. This is only possible via the CLI: to see how, click the "CLI" option for your version and delivery method above.
[/tab]
[tab version="v3.4 Snap,v3.4 Packages,v3.3 Snap,v3.3 Packages,v3.2 Snap,v3.2 Packages,v3.1 Snap,v3.1 Packages,v3.0 Snap,v3.0 Packages,v2.9 Snap,v2.9 Packages" view="CLI"]
An administrator can set a storage layout for a machine with a status of ‘Ready’ like this:

```
maas $PROFILE machine set-storage-layout $SYSTEM_ID storage_layout=$LAYOUT_TYPE [$OPTIONS]
```

For example, to set an LVM layout where the logical volume has a size of 5 GB:

```
maas $PROFILE machine set-storage-layout $SYSTEM_ID storage_layout=lvm lv_size=5368709120

```
You must specify all storage sizes in bytes.

This action will remove the configuration that may exist on any block device.
[/tab]
[/tabs]

[note]
Only an administrator can modify storage at the block device level (providing the machine has a status of 'Ready').
[/note]

[tabs]
[tab version="v3.4 Snap,v3.4 Packages,v3.3 Snap,v3.3 Packages,v3.2 Snap,v3.2 Packages,v3.1 Snap,v3.1 Packages,v3.0 Snap,v3.0 Packages,v2.9 Snap,v2.9 Packages" view="UI"]
*** How to set the default erasure configuration

1. Click on *Settings --> Storage*.

2. Set or clear *Erase machines' disks prior to releasing*.  If you set this option, users will be compelled to use disk erasure: that option will be pre-filled in the machine's view and the user will be unable to remove the option.

[/tab]
[tab version="v3.4 Snap,v3.4 Packages,v3.3 Snap,v3.3 Packages,v3.2 Snap,v3.2 Packages,v3.1 Snap,v3.1 Packages,v3.0 Snap,v3.0 Packages,v2.9 Snap,v2.9 Packages" view="CLI"]
*** How to erase disks

When using the MAAS CLI, you can erase a disk when releasing an individual machine.  Note that this option is not available when releasing multiple machines, so you'll want to make sure you're using:

```
maas $PROFILE machine release...
```

and not:

```
maas $PROFILE machines release...
```

Note the difference in singular and plural "machine/machines" in the commands.  Releasing a machine requires that you have the `system_id` of the machine to be released, which you can obtain with a command like this one:

```
maas admin machines read | jq -r '(["HOSTNAME","SYSID","POWER","STATUS",
"OWNER", "TAGS", "POOL", "VLAN","FABRIC","SUBNET"] | (., map(length*"-"))),
(.[] | [.hostname, .system_id, .power_state, .status_name, .owner // "-", 
.tag_names[0] // "-", .pool.name,
.boot_interface.vlan.name, .boot_interface.vlan.fabric,
.boot_interface.links[0].subnet.name]) | @tsv' | column -t
```

The basic form of the release command, when erasing disks on releasing, is:

```
maas $PROFILE machine release $SYSTEM_ID comment="some comment" erase=true [secure_erase=true ||/&& quick_erase=true]
```

Parameters `secure_erase` and `quick_erase` are both optional, although if you don't specify either of them, the entire disk will be overwritten with null bytes.  Note that this overwrite process is very slow.

Secure erasure uses the drive's secure erase feature, if it has one.  In some cases, this can be much faster than overwriting the entire drive.  Be aware, though, that some drives implement secure erasure as a complete drive overwrite, so this method may still be very slow.  Additionally, if you specify secure erasure and the drive doesn't have this feature, you'll get a complete overwrite anyway -- again, possibly very slow.

Quick erasure wipes 2MB at the start and end of the drive to make recovery both inconvenient and unlikely to happen by accident.  Note, though, that quick erasure is not secure.

*** How to specify conditional erasure types

If you specify both erasure types, like this:

```
maas $PROFILE machine release $SYSTEM_ID comment="some comment" erase=true secure_erase=true quick_erase=true
```

then MAAS will perform a secure erasure if the drive has that feature; if not, it will perform a quick erasure.  Of course, if you're concerned about completely erasing the drive, and you're not sure whether the disk has secure erase features, the best way to handle that is to specify nothing, and allow the full disk to be overwritten by null bytes:

```
maas $PROFILE machine release $SYSTEM_ID comment="some comment" erase=true
```

*** How to list block devices

To view all block devices on a machine use the read operation. This list both physical and virtual block devices, as you can see in the output from the following command:

``` bash
maas admin block-devices read <node-id>
```

Output:

``` nohighlight
Success.
Machine-readable output follows:
[
    {
        "id": 10,
        "path": "/dev/disk/by-dname/vda",
        "serial": "",
        "block_size": 4096,
        "available_size": 0,
        "resource_uri": "/MAAS/api/2.0/nodes/4y3h8a/blockdevices/10/",
        "filesystem": null,
        "id_path": "/dev/vda",
        "size": 5368709120,
        "partition_table_type": "MBR",
        "model": "",
        "type": "physical",
        "uuid": null,
        "used_size": 5365563392,
        "used_for": "MBR partitioned with 1 partition",
        "partitions": [
            {
                "bootable": false,
                "id": 9,
                "resource_uri":"/MAAS/api/2.0/nodes/4y3h8a/blockdevices/10/partition/9",
                "path": "/dev/disk/by-dname/vda-part1",
                "uuid": "aae082cd-8be0-4a64-ab49-e998abd6ea43",
                "used_for": "LVM volume for vgroot",
                "size": 5360320512,
                "type": "partition",
                "filesystem": {
                    "uuid": "a56ebfa6-8ef4-48b5-b6bc-9f9d27065d24",
                    "mount_options": null,
                    "label": null,
                    "fstype": "lvm-pv",
                    "mount_point": null
                }
            }
        ],
        "tags": [
            "rotary"
        ],
        "name": "vda"
    },
    {
        "id": 11,
        "path": "/dev/disk/by-dname/lvroot",
        "serial": null,
        "block_size": 4096,
        "available_size": 0,
        "resource_uri": "/MAAS/api/2.0/nodes/4y3h8a/blockdevices/11/",
        "filesystem": {
            "uuid": "7181a0c0-9e16-4276-8a55-c77364d137ca",
            "mount_options": null,
            "label": "root",
            "fstype": "ext4",
            "mount_point": "/"
        },
        "id_path": null,
        "size": 3221225472,
        "partition_table_type": null,
        "model": null,
        "type": "virtual",
        "uuid": "fc8ba89e-9149-412c-bcea-e596eb7c0d14",
        "used_size": 3221225472,
        "used_for": "ext4 formatted filesystem mounted at /",
        "partitions": [],
        "tags": [],
        "name": "vgroot-lvroot"
    }
]
```

*** How to read a block device

If you want to read just one block device instead of listing all block devices the read operation on the block device endpoint provides that information. To display the details on device '11' from the previous output, for example, we could enter:

``` bash
maas admin block-device read <node-id> 11
```

The above command generates the following output:

``` nohighlight
Success.
Machine-readable output follows:
{
    "available_size": 0,
    "path": "/dev/disk/by-dname/vgroot-lvroot",
    "name": "vgroot-lvroot",
    "used_for": "ext4 formatted filesystem mounted at /",
    "type": "virtual",
    "used_size": 3221225472,
    "filesystem": {
        "uuid": "7181a0c0-9e16-4276-8a55-c77364d137ca",
        "mount_point": "/",
        "mount_options": null,
        "fstype": "ext4",
        "label": "root"
    },
    "id_path": null,
    "id": 11,
    "partition_table_type": null,
    "block_size": 4096,
    "tags": [],
    "resource_uri": "/MAAS/api/2.0/nodes/4y3h8a/blockdevices/11/",
    "uuid": "fc8ba89e-9149-412c-bcea-e596eb7c0d14",
    "serial": null,
    "partitions": [],
    "size": 3221225472,
    "model": null
}
```

It is also possible to use the name of the block device, such as 'sda' or 'vda', instead of its 'id':

``` bash
s admin block-device read <node-id> vda
```

[note]
MAAS allows the name of a block device to be changed. If the block device name has changed then the API call needs to use the new name.
[/note]

    Using the ID is safer as it never changes.

*** How to create a block device

MAAS gathers the required information itself on block devices when re- commissioning a machine. If this doesn't provide the required information, it is also possible - though not recommended - for an administrator to use the API to manually add a physical block device to a machine.

``` bash
maas admin block-devices create <node-id> name=vdb model="QEMU" serial="QM00001" size=21474836480 block_size=4096
```

Depending on your configuration, output should be similar to the following:

``` nohighlight
Success.
Machine-readable output follows:
{
    "available_size": 21474836480,
    "path": "/dev/disk/by-dname/vdb",
    "name": "vdb",
    "used_for": "Unused",
    "type": "physical",
    "used_size": 0,
    "filesystem": null,
    "id_path": "",
    "id": 12,
    "partition_table_type": null,
    "block_size": 4096,
    "tags": [],
    "resource_uri": "/MAAS/api/2.0/nodes/4y3h8a/blockdevices/12/",
    "uuid": null,
    "serial": "QM00001",
    "partitions": [],
    "size": 21474836480,
    "model": "QEMU"
}
```

[note]
The serial number is what MAAS will use when a machine is deployed to find the specific block device. It's important that this be correct. In a rare chance that your block device does not provide a model or serial number you can provide an id_path. The id_path should be a path that is always the same, no matter the kernel version.
[/note]

*** How to update a block device

An administrator can also update the details held on a physical block device, such as its name, from the API:

``` bash
maas admin block-device update <node-id> 12 name=newroot
```

Output from this command will show that the 'name' has changed:

``` nohighlight
Success.
Machine-readable output follows:
{
    "block_size": 4096,
    "size": 21474836480,
    "filesystem": null,
    "model": "QEMU",
    "name": "newroot",
    "partitions": [],
    "tags": [],
    "used_size": 0,
    "path": "/dev/disk/by-dname/newroot",
    "id_path": "",
    "uuid": null,
    "available_size": 21474836480,
    "id": 12,
    "used_for": "Unused",
    "type": "physical",
    "resource_uri": "/MAAS/api/2.0/nodes/4y3h8a/blockdevices/12/",
    "partition_table_type": null,
    "serial": "QM00001"
}
```

*** How to delete a block device

Physical and virtual block devices can be deleted by an administrator, while ordinary users can only delete virtual block devices:

``` bash
maas admin block-device delete <node-id> 12
```

*** How to format a block device

An entire block device can be formatted by defining a filesystem with the 'format' API call:

``` bash
maas admin block-device format <node-id> 11 fstype=ext4
```

Successful output from this command will look similar to this:

``` nohighlight
Success.
Machine-readable output follows:
{
    "block_size": 4096,
    "size": 3221225472,
    "filesystem": {
        "label": "",
        "fstype": "ext4",
        "mount_options": null,
        "uuid": "75e42f49-9a45-466c-8425-87a40e4f4148",
        "mount_point": null
    },
    "model": null,
    "name": "vgroot-lvroot",
    "partitions": [],
    "tags": [],
    "used_size": 3221225472,
    "path": "/dev/disk/by-dname/vgroot-lvroot",
    "id_path": null,
    "uuid": "fc8ba89e-9149-412c-bcea-e596eb7c0d14",
    "available_size": 0,
    "id": 11,
    "used_for": "Unmounted ext4 formatted filesystem",
    "type": "virtual",
    "resource_uri": "/MAAS/api/2.0/nodes/4y3h8a/blockdevices/11/",
    "partition_table_type": null,
    "serial": null
}
```

[note]
You cannot format a block device that contains partitions or is used to make another virtual block device.
[/note]

*** How to unformat a block device

You can remove the filesystem from a block device with the 'unformat' API call:

``` bash
maas admin block-device unformat <node-id> 11
```

The output from this command should show the filesystem is now 'null':

``` nohighlight
Success.
Machine-readable output follows:
{
    "available_size": 3221225472,
    "path": "/dev/disk/by-dname/vgroot-lvroot",
    "name": "vgroot-lvroot",
    "used_for": "Unused",
    "type": "virtual",
    "used_size": 0,
    "filesystem": null,
    "id_path": null,
    "id": 11,
    "partition_table_type": null,
    "block_size": 4096,
    "tags": [],
    "resource_uri": "/MAAS/api/2.0/nodes/4y3h8a/blockdevices/11/",
    "uuid": "fc8ba89e-9149-412c-bcea-e596eb7c0d14",
    "serial": null,
    "partitions": [],
    "size": 3221225472,
    "model": null
}
```

*** How to mount a block device

If a block device has a filesystem, you can use the 'maas' command to mount a block devices at a given mount point:

``` bash
maas admin block-device mount <node-id> 11 mount_point=/srv
```

The mount point is included in the successful output from the command:

``` nohighlight
Success.
Machine-readable output follows:
{
    "available_size": 0,
    "path": "/dev/disk/by-dname/vgroot-lvroot",
    "name": "vgroot-lvroot",
    "used_for": "ext4 formatted filesystem mounted at /srv",
    "type": "virtual",
    "used_size": 3221225472,
    "filesystem": {
        "uuid": "6f5965ad-49f7-42da-95ff-8000b739c39f",
        "mount_point": "/srv",
        "mount_options": "",
        "fstype": "ext4",
        "label": ""
    },
    "id_path": null,
    "id": 11,
    "partition_table_type": null,
    "block_size": 4096,
    "tags": [],
    "resource_uri": "/MAAS/api/2.0/nodes/4y3h8a/blockdevices/11/",
    "uuid": "fc8ba89e-9149-412c-bcea-e596eb7c0d14",
    "serial": null,
    "partitions": [],
    "size": 3221225472,
    "model": null
}
```

*** How to unmount a block device

To remove the mount point from the block device, use the 'unmount' call:

``` bash
maas admin block-device unmount <node-id> 11 mount_point=/srv
```

The previous command will include a nullified 'mount_point' in its output:

``` nohighlight
Success.
Machine-readable output follows:
{
    "available_size": 0,
    "path": "/dev/disk/by-dname/vgroot-lvroot",
    "name": "vgroot-lvroot",
    "used_for": "Unmounted ext4 formatted filesystem",
    "type": "virtual",
    "used_size": 3221225472,
    "filesystem": {
        "uuid": "6f5965ad-49f7-42da-95ff-8000b739c39f",
        "mount_point": null,
        "mount_options": null,
        "fstype": "ext4",
        "label": ""
    },
    "id_path": null,
    "id": 11,
    "partition_table_type": null,
    "block_size": 4096,
    "tags": [],
    "resource_uri": "/MAAS/api/2.0/nodes/4y3h8a/blockdevices/11/",
    "uuid": "fc8ba89e-9149-412c-bcea-e596eb7c0d14",
    "serial": null,
    "partitions": [],
    "size": 3221225472,
    "model": null
}
```

*** How to set a block device as a boot disk

By default, MAAS picks the first added block device to the machine as the boot disk. In most cases this works as expected as the BIOS usually enumerates the boot disk as the first block device. There are cases where this fails and the boot disk needs to be set to another disk. This API allow setting which block device on a machine MAAS should use as the boot disk.:

``` bash
maas admin block-device set-boot-disk <node-id> 10
```

[note]
Only an administrator can set which block device should be used as the boot disk and only a physical block device can be set as as the boot disk. This operation should be done before a machine is allocated or the storage layout will be applied to the previous boot disk.
[/note]

*** How to list partitions

To view all the partitions on a block device, use the 'partitions read' API call:

``` bash
maas admin partitions read <node-id> 10
```

``` nohighlight
Success.
Machine-readable output follows:
[
    {
        "bootable": false,
        "id": 9,
        "resource_uri":
"/MAAS/api/2.0/nodes/4y3h8a/blockdevices/10/partition/9",
        "path": "/dev/disk/by-dname/vda-part1",
        "uuid": "aae082cd-8be0-4a64-ab49-e998abd6ea43",
        "used_for": "LVM volume for vgroot",
        "size": 5360320512,
        "type": "partition",
        "filesystem": {
            "uuid": "a56ebfa6-8ef4-48b5-b6bc-9f9d27065d24",
            "mount_options": null,
            "label": null,
            "fstype": "lvm-pv",
            "mount_point": null
        }
    }
]
```

To view the metadata for a specific partition on a block device, rather than all partitions, use the singular 'partition' API call with an endpoint:

``` bash
maas admin partition read <node-id> 10 9
```

*** How to create a partition

To create a new partition on a block device, use the 'create' API call:

``` bash
maas admin partitions create <node-id> 10 size=5360320512
```

In addition to bytes, as shown above, the 'size' of a partition can also be defined with a 'G' for gigabytes or 'M' for megabytes. The output from the previous command will look like this:

``` nohighlight
Success.
Machine-readable output follows:
{
    "bootable": false,
    "path": "/dev/disk/by-dname/vda-part1",
    "filesystem": null,
    "used_for": "Unused",
    "type": "partition",
    "id": 10,
    "size": 5360320512,
    "resource_uri": "/MAAS/api/2.0/nodes/4y3h8a/blockdevices/10/partition/10",
    "uuid": "3d32adbf-9943-4785-ab38-963758338c6c"
}
```

*** How to delete a partition

Partitions can be deleted from a block device with the 'delete' API call. Make sure you double check the partition details as the partition is deleted immediately, with no further confirmation:

``` bash
maas admin partition delete <node-id> 10 9
```

Successful output from the 'delete' command will look like this:

``` bash
Success.
Machine-readable output follows:
```

*** How to format a partition

Partitions can be formatted in a similar way to block devices:

``` bash
maas admin partition format <node-id> 10 9 fstype=ext4
```

The output from the 'format' command will be similar to the following:

``` nohighlight
Success.
Machine-readable output follows:
{
    "id": 9,
    "used_for": "Unmounted ext4 formatted filesystem",
    "resource_uri": "/MAAS/api/2.0/nodes/4y3h8a/blockdevices/10/partition/9",
    "path": "/dev/disk/by-dname/vda-part1",
    "uuid": "aae082cd-8be0-4a64-ab49-e998abd6ea43",
    "size": 5360320512,
    "bootable": false,
    "type": "partition",
    "filesystem": {
        "uuid": "ea593366-be43-4ea3-b2d5-0adf82085a62",
        "mount_point": null,
        "mount_options": null,
        "fstype": "ext4",
        "label": ""
    }
}
```

[note]
You cannot format partitions that are used to make another virtual block device.
[/note]

*** How to unformat a partition

You can also remove the filesystem from a partition with the 'unformat' API call:

``` bash
maas admin partition unformat <node-id> 10 10 fstype=ext4
```

``` nohighlight
Success.
Machine-readable output follows:
{
    "bootable": false,
    "path": "/dev/disk/by-dname/vda-part1",
    "filesystem": null,
    "used_for": "Unused",
    "type": "partition",
    "id": 10,
    "size": 5360320512,
    "resource_uri": "/MAAS/api/2.0/nodes/4y3h8a/blockdevices/10/partition/10",
    "uuid": "3d32adbf-9943-4785-ab38-963758338c6c"
}
```

*** How to mount a partition

A formatted partition can be mounted at a given mount point with the 'mount' command.

``` bash
maas admin partition mount <node-id> 10 10 mount_point=/srv
```

The mount point and the filesystem is visible in the output from the command:

``` nohighlight
Success.
Machine-readable output follows:
{
    "bootable": false,
    "id": 10,
    "resource_uri": "/MAAS/api/2.0/nodes/4y3h8a/blockdevices/10/partition/10",
    "path": "/dev/disk/by-dname/vda-part1",
    "uuid": "3d32adbf-9943-4785-ab38-963758338c6c",
    "used_for": "ext4 formatted filesystem mounted at /srv",
    "size": 5360320512,
    "type": "partition",
    "filesystem": {
        "uuid": "1949a5fb-f7bd-4ada-8ba5-d06d3f5857a8",
        "mount_options": "",
        "label": "",
        "fstype": "ext4",
        "mount_point": "/srv"
    }
}
```

*** How to unmount a partition

A previous mounted partition can be unmounted with the 'unmount' command:

``` bash
maas admin partition unmount 4y3h8a 10 10
```

After successfully running this command, the mount point will show as 'null' in the output:

``` nohighlight
Success.
Machine-readable output follows:
{
    "bootable": false,
    "id": 10,
    "resource_uri": "/MAAS/api/2.0/nodes/4y3h8a/blockdevices/10/partition/10",
    "path": "/dev/disk/by-dname/vda-part1",
    "uuid": "3d32adbf-9943-4785-ab38-963758338c6c",
    "used_for": "Unmounted ext4 formatted filesystem",
    "size": 5360320512,
    "type": "partition",
    "filesystem": {
        "uuid": "1949a5fb-f7bd-4ada-8ba5-d06d3f5857a8",
        "mount_options": null,
        "label": "",
        "fstype": "ext4",
        "mount_point": null
    }
    "type": "partition",
    "id": 3,
    "size": 2000003072
}
```

*** How to list VMFS datastores

To view all VMFS Datastores on a machine, use the 'vmfs-datastores read' API call:

``` bash
maas $PROFILE vmfs-datastores read $SYSTEM_ID
```

``` nohighlight
[
    {
        "human_size": "45.8 GB",
        "filesystem": {
            "fstype": "vmfs6",
            "mount_point": "/vmfs/volumes/datastore1"
        },
        "uuid": "2779a745-1db3-4fd7-b06e-455b728fffd4",
        "name": "datastore1",
        "system_id": "4qxaga",
        "devices": [
            {
                "uuid": "c55fe657-689d-4570-8492-683dd5fa1c40",
                "size": 35026632704,
                "bootable": false,
                "tags": [],
                "used_for": "VMFS extent for datastore1",
                "filesystem": {
                    "fstype": "vmfs6",
                    "label": null,
                    "uuid": "55ac6422-68b5-440e-ba65-153032605b51",
                    "mount_point": null,
                    "mount_options": null
                },
                "type": "partition",
                "device_id": 5,
                "path": "/dev/disk/by-dname/sda-part3",
                "system_id": "4qxaga",
                "id": 71,
                "resource_uri": "/MAAS/api/2.0/nodes/4qxaga/blockdevices/5/partition/71"
            },
            {
                "uuid": "5182e503-4ad4-446e-9660-fd5052b41cc5",
                "size": 10729029632,
                "bootable": false,
                "tags": [],
                "used_for": "VMFS extent for datastore1",
                "filesystem": {
                    "fstype": "vmfs6",
                    "label": null,
                    "uuid": "a5949b18-d591-4627-be94-346d0fdaf816",
                    "mount_point": null,
                    "mount_options": null
                },
                "type": "partition",
                "device_id": 6,
                "path": "/dev/disk/by-dname/sdb-part1",
                "system_id": "4qxaga",
                "id": 77,
                "resource_uri": "/MAAS/api/2.0/nodes/4qxaga/blockdevices/6/partition/77"
            }
        ],
        "id": 17,
        "size": 45755662336,
        "resource_uri": "/MAAS/api/2.0/nodes/4qxaga/vmfs-datastore/17/"
    }
]
```

*** How to view a VMFS datastore

To view a specific VMFS Datastores on a machine, use the 'vmfs-datastore read' API call:

``` bash
maas $PROFILE vmfs-datastore read $SYSTEM_ID $VMFS_DATASTORE_ID
```

``` nohighlight
{
    "uuid": "fb6fedc2-f711-40de-ab83-77eddc3e19ac",
    "name": "datastore1",
    "system_id": "b66fn6",
    "id": 18,
    "filesystem": {
        "fstype": "vmfs6",
        "mount_point": "/vmfs/volumes/datastore1"
    },
    "human_size": "2.8 GB",
    "devices": [
        {
            "uuid": "b91df576-ba02-4acb-914f-03ba9a2865b7",
            "size": 2814377984,
            "bootable": false,
            "tags": [],
            "system_id": "b66fn6",
            "used_for": "VMFS extent for datastore1",
            "type": "partition",
            "id": 80,
            "filesystem": {
                "fstype": "vmfs6",
                "label": null,
                "uuid": "4a098d71-1e59-4b5f-932d-fc30a1c0dc96",
                "mount_point": null,
                "mount_options": null
            },
            "device_id": 1,
            "path": "/dev/disk/by-dname/vda-part3",
            "resource_uri": "/MAAS/api/2.0/nodes/b66fn6/blockdevices/1/partition/80"
        }
    ],
    "size": 2814377984,
    "resource_uri": "/MAAS/api/2.0/nodes/b66fn6/vmfs-datastore/18/"
}
```

*** How to create a VMFS datastore

A VMware VMFS datastore is created on one or more [block devices](#heading--about-block-devices) or [partitions](#heading--about-partitions).

To create a VMFS Datastores on a machine use the 'vmfs-datastores create' API call:

``` bash
maas $PROFILE vmfs-datastores create $SYSTEM_ID name=$VMFS_NAME block_devices=$BLOCK_ID_1,$BLOCK_ID_2 partitions=$PARTITION_ID_1,$PARTITION_ID_2
```

``` nohighlight
{
    "system_id": "b66fn6",
    "devices": [
        {
            "uuid": "b91df576-ba02-4acb-914f-03ba9a2865b7",
            "size": 2814377984,
            "bootable": false,
            "tags": [],
            "device_id": 1,
            "system_id": "b66fn6",
            "type": "partition",
            "used_for": "VMFS extent for datastore42",
            "filesystem": {
                "fstype": "vmfs6",
                "label": null,
                "uuid": "fc374367-a2fb-4e50-9377-768bfe9705b6",
                "mount_point": null,
                "mount_options": null
            },
            "path": "/dev/disk/by-dname/vda-part3",
            "id": 80,
            "resource_uri": "/MAAS/api/2.0/nodes/b66fn6/blockdevices/1/partition/80"
        }
    ],
    "name": "datastore42",
    "filesystem": {
        "fstype": "vmfs6",
        "mount_point": "/vmfs/volumes/datastore42"
    },
    "id": 19,
    "size": 2814377984,
    "uuid": "2711566c-2df4-4cc4-8c06-7392bb1f9532",
    "human_size": "2.8 GB",
    "resource_uri": "/MAAS/api/2.0/nodes/b66fn6/vmfs-datastore/19/"
}
```

** How to edit a VMFS datastore

To edit an existing VMFS Datastores on a machine use the 'vmfs-datastore update' API call:

``` bash
maas $PROFILE vmfs-datastore update $SYSTEM_ID $VMFS_ID name=$NEW_VMFS_NAME add_block_devices=$NEW_BLOCK_ID_1,$NEW_BLOCK_ID_2 add_partitions=$NEW_PARTITION_ID_1,$NEW_PARTITION_ID_2 remove_partitions=$EXISTING_PARTITION_ID1,$EXISTING_PARTITION_ID2
```

``` nohighlight
{
    "uuid": "2711566c-2df4-4cc4-8c06-7392bb1f9532",
    "name": "datastore42",
    "system_id": "b66fn6",
    "id": 19,
    "filesystem": {
        "fstype": "vmfs6",
        "mount_point": "/vmfs/volumes/datastore42"
    },
    "human_size": "13.5 GB",
    "devices": [
        {
            "uuid": "b91df576-ba02-4acb-914f-03ba9a2865b7",
            "size": 2814377984,
            "bootable": false,
            "tags": [],
            "system_id": "b66fn6",
            "used_for": "VMFS extent for datastore42",
            "type": "partition",
            "id": 80,
            "filesystem": {
                "fstype": "vmfs6",
                "label": null,
                "uuid": "fc374367-a2fb-4e50-9377-768bfe9705b6",
                "mount_point": null,
                "mount_options": null
            },
            "device_id": 1,
            "path": "/dev/disk/by-dname/vda-part3",
            "resource_uri": "/MAAS/api/2.0/nodes/b66fn6/blockdevices/1/partition/80"
        },
        {
            "uuid": "f21fe54e-b5b1-4562-ab6b-e699e99f002f",
            "size": 10729029632,
            "bootable": false,
            "tags": [],
            "system_id": "b66fn6",
            "used_for": "VMFS extent for datastore42",
            "type": "partition",
            "id": 86,
            "filesystem": {
                "fstype": "vmfs6",
                "label": null,
                "uuid": "f3d9b6a3-bab3-4677-becb-bf5a421bfcc2",
                "mount_point": null,
                "mount_options": null
            },
            "device_id": 2,
            "path": "/dev/disk/by-dname/vdb-part1",
            "resource_uri": "/MAAS/api/2.0/nodes/b66fn6/blockdevices/2/partition/86"
        }
    ],
    "size": 13543407616,
    "resource_uri": "/MAAS/api/2.0/nodes/b66fn6/vmfs-datastore/19/"
}
```

*** How to delete a VMFS datastore</h3	></a>

To delete a VMFS Datastores on a machine use the 'vmfs-datastore delete' API call:

``` bash
maas $PROFILE vmfs-datastore delete $SYSTEM_ID $VMFS_ID
```
[/tab]
[/tabs]

** How to pre-seed with curtin

You can customise the Curtin installation by either editing the existing `curtin_userdata` template or by adding a custom file as described above.  For a flowchart, showing where Curtin and pre-seeding fits into the deployment picture, see [How images get deployed](/t/how-to-acquire-images/6192#heading--how-images-deploy).

Curtin provides hooks to execute custom code before and after installation takes place. These hooks are named `early` and `late` respectively, and they can both be overridden to execute the Curtin configuration in the ephemeral environment. Additionally, the `late` hook can be used to execute a configuration for a machine being installed, a state known as in-target.

Curtin commands look like this:

    foo: ["command", "--command-arg", "command-arg-value"]

Each component of the given command makes up an item in an array. Note, however, that the following won't work:

    foo: ["sh", "-c", "/bin/echo", "foobar"]

This syntax won't work because the value of `sh`'s `-c` argument is itself an entire command. The correct way to express this is:

    foo: ["sh", "-c", "/bin/echo foobar"]

The following is an example of an early command that will run before the installation takes place in the ephemeral environment. The command pings an external machine to signal that the installation is about to start:

``` bash
early_commands:
  signal: ["wget", "--no-proxy", "http://example.com/", "--post-data", "system_id=&signal=starting_install", "-O", "/dev/null"]
```

The following is an example of two late commands that run after installation is complete. Both run in-target, on the machine being installed.

The first command adds a PPA to the machine. The second command creates a file containing the machine’s system ID:

``` bash
late_commands:
  add_repo: ["curtin", "in-target", "--", "add-apt-repository", "-y", "ppa:my/ppa"]
  custom: ["curtin", "in-target", "--", "sh", "-c", "/bin/echo -en 'Installed ' > /tmp/maas_system_id"]
```

** How to pre-seed with cloud-init

For a flowchart, showing where cloud-init fits into the deployment picture, see [How images get deployed](/t/how-to-acquire-images/6192#heading--how-images-deploy).

[tabs]
[tab version="v3.4 Snap,v3.4 Packages" view="UI"]
To customise cloud-init via the web UI:

1. Select *Machines*.

2. Select a machine.

3. Select *Actions > Deploy*.

4. Checkbox *Cloud-init user-data...*.

5. Paste, upload, or drag-and-drop the script into the box.

6. Select *Start deployment for machine*.
[/tab]
[tab version="v3.3 Snap,v3.3 Packages,v3.2 Snap,v3.2 Packages,v3.1 Snap,v3.1 Packages,v3.0 Snap,v3.0 Packages,v2.9 Snap,v2.9 Packages" view="UI"]
To customise cloud-init via the web UI:

1. Select a machine. 

2. Choose *Take action >> Deploy*.

3. Select a viable release.

4. Check the box labelled *Cloud-init user-data...*.

5. Paste the desired script directly into the box.

6. Select *Start deployment for machine*.  

For example, to import an SSH key immediately after your machine deployment, you could paste this script:

``` bash
#!/bin/bash
(
echo === $date ===
ssh-import-id foobar_user
) | tee /ssh-key-import.log
```

[note]
No script validation of any kind is provided with this capability.  You will need to test and debug your own cloud-init scripts.
[/note]
[/tab]
[tab version="v3.4 Snap,v3.4 Packages,v3.3 Snap,v3.3 Packages,v3.2 Snap,v3.2 Packages,v3.1 Snap,v3.1 Packages,v3.0 Snap,v3.0 Packages,v2.9 Snap,v2.9 Packages" view="CLI"]
Using cloud-init to customise a machine after deployment is relatively easy. After you're logged in, use the following command to deploy a machine with a custom script you've written:

    maas $PROFILE machine deploy $SYSTEM_ID user_data=<base-64-encoded-script>

The three replaceable parameters shown above decode to:

1.   `$PROFILE`: Your MAAS login. E.g. `admin`
2.   `$SYSTEM_ID`: The machine's system ID (see example below)
3.   `<base-64-encoded-script>`: A base-64 encoded copy of your customisation script. See below for an example.

E.g.:

Suppose you would like to import an SSH key immediately after your machine deployment. First, you want to find the machine's `system_id`, which you can do with this short, command-line `jq` script:

```
maas admin machines read | jq -r '(["HOSTNAME","SYSID"] | (., map(length*"-"))),
(.[] | [.hostname, .system_id]) | @tsv' | column -t
```

You might then use this script, called `import_key.sh`, to retrieve the needed key:

``` bash
#!/bin/bash
(
echo === $date ===
ssh-import-id foobar_user
) | tee /ssh-key-import.log
```

This script echos the date in addition to the output of the `ssh-import-key` command. It also adds that output to a file, `/ssh-key-import.log`.

Base-64 encoding is required because the MAAS command-line interacts with the MAAS API, and base-64 encoding allows MAAS to send the script inside a POST HTTP command.

Use the `base64` command to output a base-64 encoded version of your script:

    base64 -w0 ./import_key.sh

Putting it together:

    maas $PROFILE machine deploy $SYSTEM_ID user_data=$(base64 -w0 ./import_key.sh)

After MAAS deploys the machine, you'll find `/ssh-key-import.log` on the machine you deployed.
[/tab]
[/tabs]


** How to choose Ubuntu kernels

This section will show you:

- [How to set a default minimum kernel for enlistment and commissioning](#heading--set-a-default-minimum-kernel-for-enlistment-and-commissioning)
- [How to set a minimum deployment kernel for a machine](#heading--set-a-minimum-deploy-kernel-for-a-machine)
- [How to set a specific kernel during machine deployment](#heading--set-a-specific-kernel-during-machine-deployment)


*** How to set a default minimum kernel for enlistment and commissioning

[tabs]
[tab version="v3.4 Snap,v3.4 Packages" view="UI"]
To set the default minimum enlistment and commissioning kernel for all machines:

1. Select *Settings*.

2. Select *Configuration*.

3. Select *Commissioning*.

4. Select a kernel from the *Default minimum kernel version* drop-down.

5. Select *Save* to register your changes.
[/tab]
[tab version="v3.3 Snap,v3.3 Packages,v3.2 Snap,v3.2 Packages,v3.1 Snap,v3.1 Packages,v3.0 Snap,v3.0 Packages,v2.9 Snap,v2.9 Packages" view="UI"]
To set the default minimum enlistment and commissioning kernel (based on Ubuntu release: GA kernel) for all machines:

1. Select *Settings >> General*.

2. Select a kernel in the *Default Minimum Kernel Version* field of the *Commissioning* section

*** How to set a minimum deployment kernel for a machine

To set the minimum deploy kernel on a machine basis: 

1. Select *Machines*. 

2. Click on a machine.

3. Select *Configuration*.

4. Select *Edit* in the *Machine configuration* section. 

5. Select a kernel in the *Minimum Kernel* field.

*** How to set a specific kernel during machine deployment

To set a specific kernel during deployment:

1. Select *Machines*.

2. Click on a machine.

3. Choose *Take action >> Deploy*.

4. Choose a kernel from the third kernel field. 

5. Click *Deploy machine* to initiate the deployment.

MAAS verifies that the specified kernel is available for the given Ubuntu release (series) before deploying the machine.
[/tab]
[tab version="v3.4 Snap,v3.4 Packages,v3.3 Snap,v3.3 Packages,v3.2 Snap,v3.2 Packages,v3.1 Snap,v3.1 Packages,v3.0 Snap,v3.0 Packages,v2.9 Snap,v2.9 Packages" view="CLI"]

To set a default minimum kernel for all new and commissioned machines:

``` bash
maas $PROFILE maas set-config name=default_min_hwe_kernel value=$KERNEL
```

For example, to set it to the 16.04 GA kernel:

``` bash
maas $PROFILE maas set-config name=default_min_hwe_kernel value=ga-16.04
```

[note]
The command option `default_min_hwe_kernel` appears to apply to only HWE kernels but this is not the case.
[/note]

*** How to set a minimum deployment kernel for a machine

To set the minimum deploy kernel on a per-machine basis:

``` bash
maas $PROFILE machine update $SYSTEM_ID min_hwe_kernel=$HWE_KERNEL
```

For example, to set it to the HWE 16.04 kernel:

``` bash
maas $PROFILE machine update $SYSTEM_ID min_hwe_kernel=hwe-16.04
```

[note]
The command option `default_min_hwe_kernel` appears to apply to only HWE kernels but this is not the case.
[/note]

*** How to set a specific kernel during machine deployment

To set a specific kernel during the deployment of a machine:

``` bash
maas $PROFILE machine deploy $SYSTEM_ID distro_series=$SERIES hwe_kernel=$KERNEL
```

The operation will fail if the kernel specified by `hwe_kernel` is older than the kernel (possibly) given by `default_min_hwe_kernel`. Similarly, it will not work if the kernel is not available for the given distro series (such as 'hwe-16.10' for 'xenial').

For example, to deploy a Xenial node with the HWE 16.04 edge kernel:

``` bash
maas $PROFILE machine deploy $SYSTEM_ID distro_series=xenial hwe_kernel=hwe-16.04-edge
```

[note]
The command option `hwe_kernel` appears to apply to only HWE kernels but this is not the case.
[/note]
[/tab]
[/tabs]

** How to set global kernel boot options

[tabs]
[tab version="v3.4 Snap,v3.4 Packages" view="UI"]
To set global kernel boot options:

1. Select *Settings*.

2. Select *Configuration > Kernel parameters*.

3. Enter the *Global boot parameters always passed to the kernel* in the text box.

4. Select *Save* to register your changes.
[/tab]
[tab version="v3.3 Snap,v3.3 Packages,v3.2 Snap,v3.2 Packages,v3.1 Snap,v3.1 Packages,v3.0 Snap,v3.0 Packages,v2.9 Snap,v2.9 Packages" view="UI"]
To set kernel boot options globally:

1. Make sure that you are logged in as an administrator.

2. Go to *Settings >> General*.

3. Scroll down to the *Global Kernel Parameters* section:

4. Type in the desired (space separated) options. 

5. Be sure to click *Save*. 

The contents of the field will be used as-is. Do not use extra characters.
[/tab]
[tab version="v3.4 Snap,v3.4 Packages,v3.3 Snap,v3.3 Packages,v3.2 Snap,v3.2 Packages,v3.1 Snap,v3.1 Packages,v3.0 Snap,v3.0 Packages,v2.9 Snap,v2.9 Packages" view="CLI"]
You can set kernel boot options and apply them to all machines with the CLI command:

``` bash
maas $PROFILE maas set-config name=kernel_opts value='$KERNEL_OPTIONS'
```
[/tab]
[/tabs]

** How to create tags with built-in kernel options

You can create tags with embedded kernel boot options.  When you apply such tags to a machine, those kernel boot options will be applied to that machine on the next deployment.

To create a tag with embedded kernel boot options, use the following command:

```nohighlight
maas $PROFILE tags create name='$TAG_NAME' \
    comment='$TAG_COMMENT' kernel_opts='$KERNEL_OPTIONS'
```

For example:

```nohighlight
maas admin tags create name='nomodeset_tag' \
    comment='nomodeset_kernel_option' kernel_opts='nomodeset vga'
```

This command yields the following results:

```nohighlight
Success.
Machine-readable output follows:
{
    "name": "nomodeset_tag",
    "definition": "",
    "comment": "nomodeset_kernel_option",
    "kernel_opts": "nomodeset vga",
    "resource_uri": "/MAAS/api/2.0/tags/nomodeset_tag/"
}
```

You can check your work with a modified form of the listing command:

```nohighlight
maas admin tags read | jq -r \
'(["tag_name","tag_comment","kernel_options"]
|(.,map(length*"-"))),(.[]|[.name,.comment,.kernel_opts]) 
| @tsv' | column -t
```

This should give you results something like this:

```nohighlight
tag_name             tag_comment                  kernel_options                     
--------             -----------                  --------------                     
virtual                                                                              
new_tag              a-new-tag-for-test-purposes                                     
pod-console-logging  console=tty1                 console=ttyS0                      
nomodeset_tag        nomodeset_kernel_option      nomodeset       vga
```

** How to use resource pools

This section will explain:

[tabs]
[tab version="v3.3 Snap,v3.3 Packages" view="UI"]
- [How to add a resource pool](#heading--add-a-resource-pool)
- [How to delete a resource pool](#heading--deleting-a-resource-pool)
- [How to add a node to a resource pool](#heading--add-a-node-to-a-resource-pool)
- [How to remove a node from a resource pool](#heading--removing-a-node-from-a-resource-pool)
- [How to add a VM host to a resource pool](#heading--add-a-vm-host-to-a-resource-pool)
- [How to remove a VM host from a resource pool](#heading--removing-a-vm-host-from-a-resource-pool)


*** How to add a resource pool

To add a resource pool:

1. Select *Organisation > Pools*.

2. Select *Add pool*.

3. Give your new pool a name.

4. Optionally, give your new pool a description.

5. Select *Save pool* to register the changes.

*** How to delete a resource pool

To delete a resource pool:

1. Select *Organisation > Pools*.

2. Select the trash can icon to the far right of the pool row.

3. Select *Delete*.

The pool will be deleted; there is no confirmation dialogue.

[note]
When you delete a resource pool, no machines will be deleted: All machines that belong to that resource pool will return to the default pool.
[/note]

*** How to add a machine to a resource pool

To add a machine to a resource pool:

1. Select *Machines*.

2. Checkbox the machine(s) you want to add to the resource pool. 

3. Select *Categorise > Set pool...*. 

4. Choose *Select pool*.

5. Select the desired resource pool from the *Resource pool* dropdown.

6. Alternatively, you can choose *Create pool* and create a new pool to add to the machine(s).

7. Register your changes by clicking *Set pool...*.

*** How to remove a machine from a resource pool

To remove a machine from a resource pool, follow the procedure for [adding a machine to a resource pool](#heading--add-a-node-to-a-resource-pool), but select "default" as the new resource pool. This action will return the machine to the default resource pool.

*** How to add a VM host to a resource pool

You can add a VM host to a resource pool when you [create a new VM host](/t/how-to-create-vm-hosts/5140#heading--adding-a-vm-host) or you can edit a VM host's configuration:

1. Select *KVM > LXD*.

2. Select the KVM you want to add to a resource pool.

3. Select *KVM host settings*.

4. Select the desired pool in the *Resource pool* dropdown.

5. Register your changes with *Save changes*.

*** How to remove a VM host from a resource pool

To remove a VM host from a resource pool, follow the same procedure you would use to [add a VM host to a resource pool](#heading--add-a-vm-host-to-a-resource-pool), selecting "default" as the new resource pool. This action will return the machine to the default resource pool.
[/tab]
[tab version="v3.3 Snap,v3.3 Packages,v3.2 Snap,v3.2 Packages,v3.1 Snap,v3.1 Packages,v3.0 Snap,v3.0 Packages,v2.9 Snap,v2.9 Packages" view="UI"]
- [How to add a resource pool](#heading--add-a-resource-pool)
- [How to delete a resource pool](#heading--deleting-a-resource-pool)
- [How to add a node to a resource pool](#heading--add-a-node-to-a-resource-pool)
- [How to remove a node from a resource pool](#heading--removing-a-node-from-a-resource-pool)
- [How to add a VM host to a resource pool](#heading--add-a-vm-host-to-a-resource-pool)
- [How to remove a VM host from a resource pool](#heading--removing-a-vm-host-from-a-resource-pool)


*** How to add a resource pool

To add a resource pool:

1. From the machine list, click the top tab *<n> Resource pool*.

2. Select *Add pool*.

3. Give your new pool a name.

4. Optionally, give your new pool a description.

5. Select *Save pool* to register the changes.

*** How to delete a resource pool

To delete a resource pool:

1. From the machine list, click the top tab *<n> Resource pool*.

2. Select the trash can icon to the far right of the pool row.

3. Select *Delete*.

The pool will be deleted; there is no confirmation dialogue.

[note]
When you delete a resource pool, no machines will be deleted: All machines that belong to that resource pool will return to the default pool.
[/note]

*** How to add a machine to a resource pool

To add a machine to a resource pool:

1. Select *Machines*.

2. Select the machine you want to add to the resource pool. 

3. Select *Configuration*. 

4. Select *Edit* in the *Machine configuration* block.

5. Select the desired resource pool from the *Resource pool* dropdown.

6. Register your changes by clicking *Save changes*.


*** How to remove a machine from a resource pool

To remove a machine from a resource pool, follow the procedure for [adding a machine to a resource pool](#heading--add-a-node-to-a-resource-pool), but select "default" as the new resource pool. This action will return the machine to the default resource pool.

*** How to add a VM host to a resource pool

You can add a VM host to a resource pool when you [create a new VM host](/t/how-to-create-vm-hosts/5140#heading--adding-a-vm-host) or you can edit a VM host's configuration:

1. Select *KVM* from the top tab bar.

2. Select the KVM you want to add to a resource pool.

3. Select *KVM host settings*.

4. Select the desired pool in the *Resource pool* dropdown.

5. Register your changes with *Save changes*.

*** How to remove a VM host from a resource pool

To remove a VM host from a resource pool, follow the same procedure you would use to [add a VM host to a resource pool](#heading--add-a-vm-host-to-a-resource-pool), selecting "default" as the new resource pool. This action will return the machine to the default resource pool.
[/tab]
[tab version="v3.4 Snap,v3.4 Packages,v3.3 Snap,v3.3 Packages,v3.2 Snap,v3.2 Packages,v3.1 Snap,v3.1 Packages,v3.0 Snap,v3.0 Packages,v2.9 Snap,v2.9 Packages" view="CLI"]
- [How to create a resource pool](#heading--creating-a-resource-pool)
- [How to list available resource pools](#heading--list-available-resource-pools)
- [How to list a single resource pool](#heading--list-a-single-resource-pool)
- [How to update a resource pool](#heading--update-a-resource-pool)
- [How to delete a resource pool](#heading--delete-a-resource-pool)
- [How to add a machine to a resource pool](#heading--add-a-machine-to-a-resource-pool)

*** How to create a resource pool

Here's an example that demonstrates how to create a new resource pool named `myresource`.

``` bash
maas $PROFILE resource-pools create name=myresource description="A new resource pool."
```

[note]
The `description` field is optional.
[/note]

*** How to list available resource pools

``` bash
maas $PROFILE resource-pools read
```

*** How to list a single resource pool

``` bash
maas $PROFILE resource-pool read $RESOURCE_POOL_ID
```

*** How to update a resource pool

``` bash
maas $PROFILE resource-pool update $RESOURCE_POOL_ID name=newname description="A new description."
```

[note]
The `name` and `description` fields are optional.
[/note]

*** How to delete a resource pool

``` bash
maas $PROFILE resource-pool delete $RESOURCE_POOL_ID
```

*** How to add a machine to a resource pool

``` bash
maas $PROFILE machine update $SYSTEM_ID pool=$POOL_NAME
```

[/tab]
[/tabs]

[tabs]
[tab version="v3.4 Snap,v3.4 Packages" view="UI"]
** How to enable hardware sync on a machine

[note]
MAAS hardware sync may leak the MAAS admin API token.  You may need to rotate all admin tokens and re-deploy all machines that have hardware sync enabled.  To find out whether this is an issue, and how to fix it, see the [troubleshooting instructions](/t/how-to-troubleshoot-maas/5333#heading--Manually-swapping-the-MAAS-admin-API-token) for this problem.
[/note]

To enable Hardware sync on a machine:

1. Select *Machines*.

2. Select a machine(s).

3. Select *Actions > Deploy**.

4. Check *Periodically synch hardware*.

5. Select *Start deployment...*.

Once you've enabled hardware sync, any changes you make to the physical device, or to the VM through the VM host, will show up in the appropriate page for the deployed machine as soon as the sync interval has passed.

** How to view updates from hardware sync

To view updates from hardware sync:

1. Select *Machines*.

2. Select the machine in question.

Any changes to the machine's hardware configuration will be updated on the next sync.  The status bar at the bottom will show times for *Last synced* and *Next sync*. Updated BMC configuration and tags can also be viewed on the machine itself.

** How to configure hardware sync interval

The hardware sync interval is configured globally in [MAAS deployment settings](/t/how-to-change-maas-settings/6347#heading--Deployment).

[/tab]
[tab version="v3.3 Snap,v3.3 Packages,v3.2 Snap,v3.2 Packages" view="UI"]
** How to enable hardware sync on a machine

[note]
MAAS hardware sync may leak the MAAS admin API token.  You may need to rotate all admin tokens and re-deploy all machines that have hardware sync enabled.  To find out whether this is an issue, and how to fix it, see the [troubleshooting instructions](/t/how-to-troubleshoot-maas/5333#heading--Manually-swapping-the-MAAS-admin-API-token) for this problem.
[/note]

To enable Hardware sync on a machine:

1. Select *Machines*.

2. Select a machine.

3. Select *Take action >> Deploy**.

4. Check *Periodically synch hardware*.

5. Select *Start deployment for machine*.

Once you've enabled hardware sync, any changes you make to the physical device, or to the VM through the VM host, will show up in the appropriate page for the deployed machine as soon as the sync interval has passed.

** How to view updates from hardware sync

To view updates from hardware sync:

1. Select *Machines*.

2. Select the machine in question.

Any changes to the machine's hardware configuration will be updated on the next sync.  The status bar at the bottom will show times for *Last synced* and *Next sync*. Updated BMC configuration and tags can also be viewed on the machine itself.

** How to configure hardware sync interval

The hardware sync interval is configured globally in [MAAS deployment settings](/t/how-to-change-maas-settings/6347#heading--Deployment).

[/tab]
[tab version="v3.4 Snap,v3.4 Packages,v3.3 Snap,v3.3 Packages,v3.2 Snap,v3.2 Packages" view="CLI"]
** How to enable hardware sync on a machine

[note]
MAAS hardware sync may leak the MAAS admin API token.  You may need to rotate all admin tokens and re-deploy all machines that have hardware sync enabled.  To find out whether this is an issue, and how to fix it, see the [troubleshooting instructions](/t/how-to-troubleshoot-maas/5333#heading--Manually-swapping-the-MAAS-admin-API-token) for this problem.
[/note]

To enable Hardware sync on a machine, deploy the machine from the command line, adding `enable_hw_sync=true`:

```nohighlight
maas $PROFILE machine deploy $SYSTEM_ID osystem=$OSYSTEM distro_series=$VERSION enable_hw_sync=true
```

Once you've enabled hardware sync, any changes you make to the physical device, or to the VM through the VM host, will show up in the appropriate page for the deployed machine as soon as the sync interval has passed.

** How to view updates from hardware sync

Hardware sync updates the machine’s blockdevice, interface and device sets on a periodic basis. These can be viewed with the CLI command:

```nohighlight
maas $PROFILE machine read $SYSTEM_ID
```

The timestamps of the last time data was synced and the estimated next time the machine will be synced can be seen in the `last_sync` and `next_sync` fields respectively.

[/tab]
[tab version="v3.1 Snap,v3.1 Packages,v3.0 Snap,v3.0 Packages,v2.9 Snap,v2.9 Packages" view="UI,CLI"]
Note that MAAS version 3.2 introduces the capability to sync and update hardware configuration on deployed, running machines.
[/tab]
[/tabs]

* How to deploy a real-time kernel
This article explains how to deploy a machine with Ubuntu and configure it to use a [real-time kernel](https://ubuntu.com/blog/real-time-linux-qa).

The real-time kernel is included in all [Ubuntu Pro](https://ubuntu.com/pro) subscriptions for Ubuntu 22.04 LTS.

[note]
The RT kernel is currently in Beta and scheduled for general availability in the near future.
[/note]

** Understanding how the RT kernel is deployed

While the real-time kernel isn't included in MAAS as a kernel to use, we use cloud-init to first deploy the machine with a generic kernel. The cloud-init kicks in and installs the real-time kernel, and then finally reboots the machine in order to make use of the newly installed kernel.

This means that after the machine has been marked as DEPLOYED in MAAS, there will be a delay until cloud-init has finished running and the machine finishes rebooting.

If you execute the steps in the previous section, MAAS is going to execute the tasks:

1. Deploy this machine with `Ubuntu 22.04 LTS` and a stock kernel
2. Reboot the machine
3. Tell the machine's bootloader to boot from disk
4. The host initialises and asks MAAS for its configuration
5. MAAS builds and sends the host cloud-init configuration, combining the base options with the snippet above
6. Cloud-init enables Ubuntu Pro in this machine
7. Ubuntu Pro agent downloads and installs the RT kernel.
8. Cloud-init reboots the machine again to enable the new kernel
9. The system is ready

** Requirements

1. A valid Ubuntu Pro token (go to https://ubuntu.com/pro/dashboard to find your token)
2. MAAS 3.2 or later with Ubuntu 22.04 LTS images synchronized
3. A host compatible with Ubuntu RT kernel
4. Internet connection

At this moment it's not possible to install the RT kernel without Internet access in the host.

** How to deploy a machine with RT kernel enabled

All steps should be performed in MAAS UI.

To deploy an RT kernel:

1. Enlist/Commission the host as usual

2. Select the host and click `Deploy`

3. Select `Ubuntu` and `Ubuntu 22.04 LTS "Jammy Jellyfish"` as OS and Release respectively

4. Select `Cloud-init user-data`

5. Use the following snippet as template, remember to replace `YOUR_TOKEN` with a valid value

    ```
    #cloud-config
    power_state:
    mode: reboot
    ubuntu_advantage:
    token: YOUR_TOKEN
    enable:
    - esm-infra
    - realtime-kernel
    ```

6. Click `Start deployment for machine`

** How to verify the deployment

The following command can be executed in the host to check if the RT kernel was enabled with success

1. `pro status`
   ```text
    SERVICE          ENTITLED  STATUS    DESCRIPTION
    esm-infra        yes       enabled   Expanded Security Maintenance for Infrastructure
    realtime-kernel  yes       enabled   Ubuntu kernel with PREEMPT_RT patches integrated
   ```

2. `uname -a`
    ```
    Linux vm01 5.15.0-1030-realtime #33-Ubuntu SMP PREEMPT_RT Mon Jan 9 17:28:40 UTC 2023 x86_64 x86_64 x86_64 GNU/Linux
    ```

* How to do a fresh install of MAAS

MAAS is relatively easy to install and configure.  Let's give it a try.

[tabs]
[tab version="v3.3 Snap"]
[note]
PostgreSQL 12 is deprecated with the release of MAAS 3.3, in favour of PostgreSQL 14. Support for PostgreSQL 12 will be discontinued in MAAS 3.4.
[/note]
To install MAAS 3.3 from a snap:

1. Check the [MAAS installation requirements](/t/maas-installation-requirements/6233) to make sure that your hardware will support MAAS.

2. Enter the following command:

```nohighlight
sudo snap install --channel=3.3 maas
```

3. Enter your account password.

At this point, the snap will download and install from the 3.3 channel.
[/tab]
[tab version="v3.3 Packages"]
[note]
PostgreSQL 12 is deprecated with the release of MAAS 3.3, in favour of PostgreSQL 14. Support for PostgreSQL 12 will be discontinued in MAAS 3.4.
[/note]
To install MAAS 3.3 from packages:

1. Check the [MAAS installation requirements](/t/maas-installation-requirements/6233) to make sure that your hardware will support MAAS.

2. Add the MAAS 3.3 PPA to your `apt` repository paths:

```nohighlight
sudo apt-add-repository ppa:maas/3.3-next
```

3. Update your `apt` repository lists:

```nohighlight
sudo apt update
```
	
4. Install MAAS with the following command:

```nohighlight
sudo apt-get -y install maas
```

5. Choose "Y" if asked about whether to continue with the install.

[/tab]
[tab version="v3.2 Snap"]
To install MAAS 3.2 from a snap:

1. Check the [MAAS installation requirements](/t/maas-installation-requirements/6233) to make sure that your hardware will support MAAS.

2. Enter the following command:

```nohighlight
sudo snap install --channel=3.2 maas
```

3. Enter your account password.

At this point, the snap will download and install from the 3.2 channel.

[/tab]
[tab version="v3.2 Packages"]
To install MAAS 3.2 from packages:

1. Check the [MAAS installation requirements](/t/maas-installation-requirements/6233) to make sure that your hardware will support MAAS.

2. Add the MAAS 3.2 PPA to your `apt` repository paths:

```nohighlight
sudo apt-add-repository ppa:maas/3.2
```

3. Update your `apt` repository lists:

```nohighlight
sudo apt update
```
	
4. Install MAAS with the following command:

```nohighlight
sudo apt-get -y install maas
```

5. Choose "Y" if asked about whether to continue with the install.
[/tab]
[tab version="v3.1 Snap"]
To install MAAS 3.1 from a snap:

1. Check the [MAAS installation requirements](/t/maas-installation-requirements/6233) to make sure that your hardware will support MAAS.

2. Install the 3.2 snap:

```nohighlight
sudo snap install --channel=3.1 maas
```

3. Enter your account password. 

The snap will download and install from the 3.1 channel.
[/tab]
[tab version="v3.1 Packages"]
To install MAAS 3.1 from packages:

1. Check the [MAAS installation requirements](/t/maas-installation-requirements/6233) to make sure that your hardware will support MAAS.

2. Add the MAAS 3.1 PPA to your `apt` repository paths:

```nohighlight
sudo apt-add-repository ppa:maas/3.1
```

3. Update your `apt` repository lists:

```nohighlight
sudo apt update
```
	
4. Install MAAS with the following command:

```nohighlight
sudo apt-get -y install maas
```
5. Choose "Y" if asked about whether to continue with the install.
[/tab]
[tab version="v3.0 Snap"]
To install MAAS 3.0 from a snap:

1. Check the [MAAS installation requirements](/t/maas-installation-requirements/6233) to make sure that your hardware will support MAAS.


2. Enter the following command:

```nohighlight
sudo snap install --channel=3.0 maas
```

3. Enter your user password.

The snap will download and install from the 3.0 channel.
[/tab]
[tab version="v3.0 Packages"]
To install MAAS 3.0 from packages:

1. Check the [MAAS installation requirements](/t/maas-installation-requirements/6233) to make sure that your hardware will support MAAS.

2. Add the MAAS 3.0 PPA to your `apt` repository paths:

```nohighlight
sudo apt-add-repository ppa:maas/3.0
```

3. Update your `apt` repository lists:

```nohighlight
sudo apt update
```
	
4. Install MAAS with the following command:

```nohighlight
sudo apt-get -y install maas
```
5. Choose "Y" if asked about whether to continue with the install.
[/tab]
[tab version="v2.9 Snap"]
To install MAAS 2.9 from a snap:

1. Check the [MAAS installation requirements](/t/maas-installation-requirements/6233) to make sure that your hardware will support MAAS.

2. Enter the following command:

```nohighlight
sudo snap install --channel=2.9 maas
```

3. Enter your user password.

The snap will download and install from the 2.9 stable channel.
[/tab]
[tab version="v2.9 Packages"]
To install MAAS 2.9 from packages:

1. Check the [MAAS installation requirements](/t/maas-installation-requirements/6233) to make sure that your hardware will support MAAS.

2. Add the MAAS 2.9 PPA to your `apt` repository paths:

```nohighlight
sudo apt-add-repository ppa:maas/2.9
```

3. Update your `apt` repository lists:

```nohighlight
sudo apt update
```
	
4. Install MAAS with the following command:

```nohighlight
sudo apt-get -y install maas
```
5. Choose "Y" if asked about whether to continue with the install.
[/tab]
[/tabs]

[tabs]
[tab version="v3.3 Snap,v3.2 Snap,v3.1 Snap,v3.0 Snap,v2.9 Snap"]
** How to initialise MAAS for a test or POC

To initialise the MAAS snap in a test/POC configuration, simply use the `--help` flag with `maas init` and follow the instructions.
 
** How to initialise MAAS for production

To install MAAS in a production configuration:

1. Install PostgreSQL on any machine where you want to keep the database with the following commands:

```nohighlight
sudo apt update -y
sudo apt install -y postgresql
```

2. Create desired values for the following variables (replace them in the commands below):

```nohighlight
$MAAS_DBUSER = ___________
$MAAS_DBPASS = ___________
$MAAS_DBNAME = ___________
$HOSTNAME = _________
```

Note that for most situations, you can use `localhost` for `$HOSTNAME`.

3. Create a suitable PostgreSQL user:

```nohighlight
sudo -i -u postgres psql -c "CREATE USER \"$MAAS_DBUSER\" WITH ENCRYPTED PASSWORD '$MAAS_DBPASS'"
```

4. Create the MAAS database:

```nohighlight
sudo -i -u postgres createdb -O "$MAAS_DBUSER" "$MAAS_DBNAME"
```

5. Edit `/etc/postgresql/14/main/pg_hba.conf` and add a line for the newly created database:

```nohighlight
host    $MAAS_DBNAME    $MAAS_DBUSER    0/0     md5
```

6. Initialise MAAS via the following command:

```nohighlight
sudo maas init region+rack --database-uri "postgres://$MAAS_DBUSER:$MAAS_DBPASS@$HOSTNAME/$MAAS_DBNAME"
```
[/tab]
[tab version="v3.3 Packages,v3.2 Packages,v3.1 Packages,v3.0 Packages,v2.9 Packages"]
** How to create a distributed environment

To run MAAS region and rack controllers on separate machines:

1. Check the [MAAS installation requirements](/t/maas-installation-requirements/6233) to make sure that your hardware will support MAAS.

2. Add the MAAS 3.3 PPA to your `apt` repository paths on both region and rack target hosts:

```nohighlight
sudo apt-add-repository ppa:maas/3.3
```

3. Update your `apt` repository lists on both region and rack hosts:

```nohighlight
sudo apt update
```
	
4. Install the MAAS region controller on the target region host:

```nohighlight
sudo apt install maas-region-controller
```

5. Install the MAAS rack controller on the target rack host:

```nohighlight
sudo apt install maas-rack-controller
```
	
6. Register the rack controller with the region controller by running the following command on the rack host:

```nohighlight
sudo maas-rack register
```

These two steps will lead you through two similar <code>apt</code> install sequences.

** How to create a MAAS admin user

To create a MAAS administrative user:

1. Create a MAAS administrator user to access the web UI:

``` nohighlight
sudo maas createadmin --username=$PROFILE --email=$EMAIL_ADDRESS
```
Subtitute `$PROFILE` is the administrative MAAS username you wish to create.  `$EMAIL_ADDRESS` is an email address you may type in at random (currently, MAAS does not use this email address). The `createadmin` option will cause MAAS to ask for an SSH key.

2. To use an SSH key associated with your launchpad accounts, enter `lp:$USERNAME` (substitute your LP username for `$USERNAME`). 

3. Alternatively, to use an SSH key associated with your github account, enter `gh:$USERNAME` (substitute your github username for `$USERNAME`)
[/tab]
[/tabs]

** How to check the status of MAAS services

To check the status of running services, enter:

```nohighlight
sudo maas status
```

Typical output looks like this:

```nohighlight
bind9                            RUNNING   pid 7999, uptime 0:09:17
dhcpd                            STOPPED   Not started
dhcpd6                           STOPPED   Not started
ntp                              RUNNING   pid 8598, uptime 0:05:42
postgresql                       RUNNING   pid 8001, uptime 0:09:17
proxy                            STOPPED   Not started
rackd                            RUNNING   pid 8000, uptime 0:09:17
regiond:regiond-0                RUNNING   pid 8003, uptime 0:09:17
regiond:regiond-1                RUNNING   pid 8008, uptime 0:09:17
regiond:regiond-2                RUNNING   pid 8005, uptime 0:09:17
regiond:regiond-3                RUNNING   pid 8015, uptime 0:09:17
tgt                              RUNNING   pid 8040, uptime 0:09:15
```
Your mileage may vary.

** How to re-initialise MAAS

To switch a machine from from `rack` to `region`:
 
```nohighlight
sudo maas init region
```

** How to list additional MAAS initialisation options

The `init` command can takes optional arguments. To list them, as well as read a brief description of each, you can enter:

```nohighlight
sudo maas init --help
```

** How to configure MAAS

[tabs]
[tab version="v3.3 Snap,v3.3 Packages,v3.2 Snap,v3.2 Packages,v3.1 Snap,v3.1 Packages,v3.0 Snap,v3.0 Packages,v2.9 Snap,v2.9 Packages" view="UI"]
To configure MAAS for first-time use:

1. Access MAAS at this address, where `$API_HOST` is the hostname or IP address of the region API server, which was set during installation:

```
http://${API_HOST}:5240/MAAS
```
2. Log in at the prompts, with the login information you created when initialising MAAS.

3. On the first welcome screen, set the DNS forwarder to a suitable value, e.g., `8.8.8.8`. This could be your own internal DNS server, if you have one.

4. Select an Ubuntu image to import; you may be required to select at least one LTS version.

5. Click *Continue*; a screen labelled, “SSH keys for admin:” appears.

6. In the *Source* drop-down, select “Launchpad,” “Github,” or “Upload.”  

7. If you want to upload your SSH public key from Launchpad, you would enter the following, where `<username>` is your Launchpad username:

```nohighlight
lp:<username>
```

8. If you want to upload your github public SSH key, you would enter the following, where `<username>` is your GitHub username:

```nohighlight
gh:<username>
```

9. If you want to use your existing public key from your home directory, select *Upload*.

10. Copy your entire public key from `.ssh/id_rsa.pub` (or wherever you may have stored the key).

11. Paste the public key into the block labelled “Public key.”  

12. Press the “Import” button to import this key.

13. You should see a message that MAAS has been successfully set up. Click *Go to the Dashboard* to proceed.

14. Select *Subnets* from the top menu.

15. Choose the VLAN on which you want to enable DHCP.

16. Select *Enable DHCP*.

You should now be able to add, commission, and deploy machines.
[/tab]
[tab version="v3.3 Snap,v3.3 Packages,v3.2 Snap,v3.2 Packages,v3.1 Snap,v3.1 Packages,v3.0 Snap,v3.0 Packages,v2.9 Snap,v2.9 Packages" view="CLI"]
Once you've successfully installed MAAS (regardless of method), you can login to the MAAS CLI via the following process:

1. Generate the API-key for the login you're going to use, replacing `$PROFILE` with whatever username you set during the `createadmin` part of the install process.

```
sudo maas apikey --username=$PROFILE > api-key-file
```


2. Login with the following command, substituting `$MAAS_URL` with the URL that was returned to you when you initialised MAAS, for example, `192.168.43.251:5240/MAAS`.  :

```
maas login $PROFILE $MAAS_URL < api-key-file
```

3. Set upstream DNS (8.8.8.8 is always a reliable value):

```
maas $PROFILE maas set-config name=upstream_dns value="8.8.8.8"
```

4. Add a public SSH key to a MAAS user account:

```
maas $PROFILE sshkeys create "key=$SSH_KEY"
```

5. See what images you may have already downloaded:

```
maas $PROFILE boot-resources read | jq -r '.[] | "\(.name)\t\(.architecture)"'
```

6. Selecting it for download (e.g., "trusty" in this example):

```
maas $PROFILE boot-source-selections create 1 os="ubuntu" release="trusty" arches="amd64" subarches="*"  labels="*"
```

7. Import your selected images:

```
maas admin boot-resources import
```

8. Identify a valid fabric ID for DHCP (returns `"fabric_id": $FABRIC_ID,`):

```
maas $PROFILE subnet read $SUBNET_CIDR | grep fabric_id
```
8. Find the name of the primary rack controller:

```
maas $PROFILE rack-controllers read | grep hostname | cut -d '"' -f 4
```

9. reate an IP range for DHCP (in this case, a dynamic range):

```
maas $PROFILE ipranges create type=dynamic start_ip=$START_IP end_ip=$END_IP
```

10. Use this collected information to turn on DHCP:

```
maas $PROFILE vlan update $FABRIC_ID untagged dhcp_on=True primary_rack=$RACK_CONTR_HOSTNAME
```

You should now be able to add, commission, and deploy machines.
[/tab]
[/tabs]
* How to employ VMWare images
MAAS 2.5 and above can deploy VMware ESXi as a custom image. MAAS cannot directly deploy the VMware ESXi ISO; you must create a specialised image from an official VMWare ISO. To automate the image creation process, Canonical [hosts a repository](https://github.com/canonical/packer-maas)`↗` with community-contributed [packer](https://www.packer.io/)`↗` templates.

This article will tell you:

- [About the prerequisites for creating a VMWare image](#heading--prerequisites-to-create-the-images)
- [About the features and limitations of VMWare images in MAAS](#heading--features-and-limitations)
- [How to customise VMWare images](#heading--customising-the-image)
- [How to build a VMWare image](#heading--building-an-image)
- [How to upload a VMWare image](#heading--uploading-an-image)

[note]
VMware [does not support cloning boot devices](https://kb.vmware.com/s/article/84280) - you may run into issues triggered by non-unique UUID.  [One such issue](https://kb.vmware.com/s/article/84349)`↗` may lead to data corruption on VMFS datastores when using cloned boot devices.
[/note]

** About the prerequisites for creating a VMWare image

The following are required in order to create and deploy a VMWare image:

- MAAS 2.5.0+
- A physical machine running Ubuntu 18.04+
- **CPU**: 4 2GHz cores
- **Memory**: 8 GB RAM (16 GB RAM recommended)
- **Disk space**: 11 GB
- [The VMWare ESXi ISO](https://my.vmware.com/en/web/vmware/evalcenter?p=free-esxi6)`↗`
- [Packer - https://www.packer.io/intro/getting-started/install.html](https://www.packer.io/intro/getting-started/install.html)`↗`
- Procedure was tested with precompiled 64-bit Packer 1.3.4 Linux binaries
- <a href="https://github.com/canonical/packer-maas">Packer template</a> `↗` for MAAS custom image

** About the features and limitations of VMWare images in MAAS

*** About cloning VMWare images

VMware [does not support cloning boot devices](https://kb.vmware.com/s/article/84280)`↗` - you may run into issues triggered by non-unique UUID.  [One such issue](https://kb.vmware.com/s/article/84349)`↗` may lead to data corruption on VMFS datastores when using cloned boot devices.

*** About VMWare images and MAAS networking

The following apply to VMWare image creation, with respect to MAAS networking:

- VMware ESXi does not support Linux bridges
- Bonds - MAAS maps the following bond modes to VMware ESXi NIC team sharing with load balancing as follows:
- balance-rr - portid
- active-backup - explicit
- 802.3ad - iphash, LACP rate and XMIT hash policy settings are ignored.
- No other bond modes are currently supported.
- VMware ESXi does not allow VMs to use a PortGroup that has a VMK attached to it. All configured devices will have a VMK attached. To use a vSwitch with VMs, you must leave a device or alias unconfigured in MAAS.

*** About VMWare images and MAAS storage

Custom storage configuration is not supported because VMware ESXi expects specific disk formats. MAAS will extend datastore1 to the full size of the deployment disk. After deployment, VMware tools may be used to access the other disks.

*** About ESXi Hardware Support

VMware has [very specific hardware requirements](https://www.vmware.com/resources/compatibility/search.php)`↗`. In particular, running VMware ESXi is not supported in a virtual machine or MAAS virsh Pod.

** How to customise VMWare images

The image may be customize by modifying packer-maas/vmware-esxi/http/vmware-esxi-ks.cfg see Installation and Upgrade Scripts in the [VMware ESXi installation and Setup manual](https://docs.vmware.com/en/VMware-vSphere/6.7/vsphere-esxi-67-installation-setup-guide.pdf)`↗` for more information.

** How to build a VMWare image

Before an image is built the nbd kernel module must be loaded

    sudo modprobe nbd

Once the nbd kernel module is loaded your current working directory must be in the packer-maas/vmware-esxi directory

    cd /path/to/packer-maas/vmware-esxi

You can now start the image building process using packer with the following command:

    sudo packer build -var
    'vmware_esxi_iso_path=/path/to/VMware-VMvisor-Installer-6.7.0-8169922.x86_64.iso'
    vmware-esxi.json

** How to upload a VMWare image

Once you have created the image, upload it to MAAS, using the CLI, with the following command:

    maas $PROFILE boot-resources create name='esxi/6.7' title='VMware ESXi 6.7'
    architecture='amd64/generic' filetype='ddgz' content@=vmware-esxi.dd.gz

* How to enable DHCP
Management of DHCP and IP ranges is a key element of configuring and managing MAAS:

- [How to manage MAAS DHCP](#heading--how-to-manage-maas-dhcp)
- [How to manage IP ranges](#heading--how-to-manage-ip-ranges)

If needed, you can review an [explanation of MAAS  DHCP](/t/how-to-set-up-networks/6174#heading--Its-always-DHCP) before undertaking to manage or set MAAS DHCP parameters.

This article will help you learn:

[tabs]
[tab version="v3.4 Snap,v3.4 Packages,v3.3 Snap,v3.3 Packages,v3.2 Snap,v3.2 Packages,v3.1 Snap,v3.1 Packages,v3.0 Snap,v3.0 Packages,v2.9 Snap,v2.9 Packages" view="UI"]
- [How to enable MAAS-managed DHCP](#heading--enabling-dhcp)
- [How to resolve IP conflicts](#heading--resolving-ip-conflicts)
- [How to extend a reserved dynamic IP range](#heading--extending-a-reserved-dynamic-ip-range)
- [How to use a DHCP relay](#heading--dhcp-relay)
- [How to customise MAAS with DHCP snippets](#heading--dhcp-snippets)
[/tab]
[tab version="v3.4 Snap,v3.4 Packages,v3.3 Snap,v3.3 Packages,v3.2 Snap,v3.2 Packages,v3.1 Snap,v3.1 Packages,v3.0 Snap,v3.0 Packages,v2.9 Snap,v2.9 Packages" view="CLI"]
- [How to enable MAAS-managed DHCP](#heading--enabling-dhcp)
- [How to resolve IP conflicts](#heading--resolving-ip-conflicts)
- [How to extend a reserved dynamic IP range](#heading--extending-a-reserved-dynamic-ip-range)
- [How to use a DHCP relay](#heading--dhcp-relay)
- [How to customise MAAS with DHCP snippets](#heading--dhcp-snippets)
- [How to list DHCP snippets](#heading--list-snippets)
- [How to update a DHCP snippet](#heading--update-a-snippet)
- [How to enable or disable a DHCP snippet](#heading--enable-or-disable-a-snippet)
- [How to delete a DHCP snippet](#heading--delete-a-snippet)
- [How to create an A or AAAA record in DNS](#heading--create-an-a-or-aaaa-record-in-dns)
- [How to create an alias (CNAME) record in DNS](#heading--create-an-alias-cname-record-in-dns)
- [How to create a Mail Exchange pointer record in DNS](#heading--create-a-mail-exchange-pointer-record-in-dns)
- [How to set a DNS forwarder](#heading--set-a-dns-forwarder)
[/tab]
[/tabs]

*** How to enable MAAS-managed DHCP

[tabs]
[tab version="v3.4 Snap,v3.4 Packages,v3.3 Snap,v3.3 Packages,v3.2 Snap,v3.2 Packages,v3.1 Snap,v3.1 Packages,v3.0 Snap,v3.0 Packages,v2.9 Snap,v2.9 Packages" view="UI"]
To enable MAAS-managed DHCP:

1. Select *Subnets*.

2. Select the desired VLAN.

3. Scroll down to *Reserved ranges*.

4. If there is no reserved dynamic range, select *Reserved dynamic range* from the dropdown on the right.

5. Enter a *Start IP address*.

6. Enter an *End IP address*.

7. Select *Reserve*.

8. Select *Configure DHCP*.  You will see a new screen.

9. The options *MAAS provides DHCP* and *Provide DHCP from a rack controller* will be pre-selected.

5. If you accept these options, you may need to choose a *Rack controller*.

6. If you choose *Relay to another VLAN*, you will need to choose the target VLAN.

7. Under *Reserved dynamic range*, you may have to select a subnet from the dropdown.

8. You will need to select *Configure DHCP* for your changes to be registered with MAAS.

[/tab]
[tab version="v3.4 Snap,v3.4 Packages,v3.3 Snap,v3.3 Packages,v3.2 Snap,v3.2 Packages,v3.1 Snap,v3.1 Packages,v3.0 Snap,v3.0 Packages,v2.9 Snap,v2.9 Packages" view="CLI"]
To enable DHCP on a VLAN in a certain fabric, enter the following command:

``` nohighlight
maas $PROFILE vlan update $FABRIC_ID $VLAN_TAG dhcp_on=True \
    primary_rack=$PRIMARY_RACK_CONTROLLER
```

To enable DHCP HA, you will need both a primary and a secondary controller:

``` nohighlight
maas $PROFILE vlan update $FABRIC_ID $VLAN_TAG dhcp_on=True \
    primary_rack=$PRIMARY_RACK_CONTROLLER \
    secondary_rack=$SECONDARY_RACK_CONTROLLER 
```

[note]
You must enable DHCP for PXE booting on the 'untagged' VLAN.
[/note]

You will also need to set a default gateway:

``` nohighlight
maas $PROFILE subnet update $SUBNET_CIDR gateway_ip=$MY_GATEWAY
```
[/tab]
[/tabs]

*** How to extend a reserved dynamic IP range

To extend a dynamic IP range:

1. Go to *Subnets*.

2. Select the relevant subnet.

3. Reserve a dynamic range. 

DHCP will be enabled automatically.

*** How to use a DHCP relay

To relay from one VLAN (source) to another VLAN (target):

1.  Ensure the target VLAN has DHCP enabled.

2.  Set up the external relay. This relay is set up independently from MAAS. See [DHCP relay](/t/maas-glossary/5246#heading--dhcp-relay) for software suggestions.

[tabs]
[tab version="v3.4 Snap,v3.4 Packages,v3.3 Snap,v3.3 Packages,v3.2 Snap,v3.2 Packages,v3.1 Snap,v3.1 Packages,v3.0 Snap,v3.0 Packages,v2.9 Snap,v2.9 Packages" view="UI"]
3.  Configure MAAS-managed DHCP as normal.

4. Navigate to the source VLAN page.

5. Select the *Relay DHCP* action. 

5. Fill in the fields in the resulting form. Note that the crucial setting is the target VLAN (*Relay VLAN*). 

6. Select *Relay DHCP* to finish.
[/tab]
[tab version="v3.4 Snap,v3.4 Packages,v3.3 Snap,v3.3 Packages,v3.2 Snap,v3.2 Packages,v3.1 Snap,v3.1 Packages,v3.0 Snap,v3.0 Packages,v2.9 Snap,v2.9 Packages" view="CLI"]
3. To relay DHCP traffic for a VLAN (source) through another VLAN (target):

``` nohighlight
maas $PROFILE vlan update $FABRIC_ID $VLAN_VID_SRC relay_vlan=$VLAN_ID_TARGET
```

For example, to relay VLAN with vid 0 (on fabric-2) through VLAN with id 5002 :

``` nohighlight
maas $PROFILE vlan update 2 0 relay_van=5002
```
[/tab]
[/tabs]

*** How to customise MAAS with DHCP snippets

For an explanation of DHCP snippets, see [About networking](/t/about-networks/6680#heading--About-MAAS-DHCP-snippets).

[tabs]
[tab version="v3.4 Snap,v3.4 Packages,v3.3 Snap,v3.3 Packages,v3.2 Snap,v3.2 Packages,v3.1 Snap,v3.1 Packages,v3.0 Snap,v3.0 Packages,v2.9 Snap,v2.9 Packages" view="UI"]

**** How to manage DHCP snippets

To manage snippets:

1. Make sure you are logged in as an administrator.

2. Select *Settings >> DHCP snippets*.

**** How to search DHCP snippets

To search DHCP snippets, enter the text to match in *Search DHCP snippets*.  MAAS will progressively update the list of snippets as you type your search terms.

**** How to add DHCP snippets

To add a snippet:

1. Select *Add snippet*.

2. Enter the *Snippet name*.

3. Optionally, check *Enabled* to enable the snippet now.  Note that MAAS will not apply the snippet unless it is enabled.

4. Optionally, enter a *Description* for the snippet.

5. Optionally, choose a *Type* for the snippet from the dropdown (defaults to *Global*).  This parameter sets the scope of the snippet.  Note that if you choose a type other than global, you may need to choose the specific scope.  For example, if you choose the *Subnet* type, you must identify the specific subnet to which this snippet applies.

6. Enter the *DHCP snippet*.  This is not validated on entry.

7. Select *Save snippet* to register your changes with MAAS

**** How to edit DHCP snippets

To edit a snippet, select the pencil icon to the right of the snippet row and edit the fields as desired.

**** How to delete DHCP snippets

To delete a snippet, select the trash can icon to the right of the snippet.  You will be asked to confirm; be aware that once confirmed, this action cannot be undone.
[/tab]
[tab version="v3.4 Snap,v3.4 Packages,v3.3 Snap,v3.3 Packages,v3.2 Snap,v3.2 Packages,v3.1 Snap,v3.1 Packages,v3.0 Snap,v3.0 Packages,v2.9 Snap,v2.9 Packages" view="CLI"]

**** How to create a global DHCP snippet

To create a **global** snippet:

``` nohighlight
maas $PROFILE dhcpsnippets create name=$DHCP_SNIPPET_NAME \
    value=$DHCP_CONFIG description=$DHCP_SNIPPET_DESCRIPTION \
    global_snippet=true
```
**** How to create a subnet DHCP snippet

To create a **subnet** snippet:

``` nohighlight
maas $PROFILE dhcpsnippets create name=$DHCP_SNIPPET_NAME \
    value=$DHCP_CONFIG description=$DHCP_SNIPPET_DESCRIPTION \
    subnet=$SUBNET_ID
```
**** How to create a node DHCP snippet

To create a **node** snippet:

``` nohighlight
maas $PROFILE dhcpsnippets create name=$DHCP_SNIPPET_NAME \
    value=$DHCP_CONFIG description=$DHCP_SNIPPET_DESCRIPTION \
    node=$NODE_ID
```

**** How to list DHCP snippets

To list all snippets (and their characteristics) in the MAAS:

``` nohighlight
maas $PROFILE dhcpsnippets read
```

To list a specific snippet:

``` nohighlight
maas $PROFILE dhcpsnippet read id=$DHCP_SNIPPET_ID
```

The snippet name can also be used instead of its ID:

``` nohighlight
maas $PROFILE dhcpsnippet read name=$DHCP_SNIPPET_NAME
```

**** How to update a DHCP snippet

To update a DHCP snippet attribute:

``` nohighlight
maas $PROFILE dhcpsnippet update $DHCP_SNIPPET_ID <option=value>
```

You can also use a snippet name instead of its ID.

**** How to enable or disable a DHCP snippet

Enabling and disabling a snippet is considered a snippet update and is done via a Boolean option ('true' or 'false'). You can disable a snippet like this:

``` nohighlight
maas $PROFILE dhcpsnippet update $DHCP_SNIPPET_ID enabled=false
```

When you disable a snippet, MAAS removes the text you added to the dhcpd.conf file when you created the snippet.

**** How to delete a DHCP snippet

To delete a snippet:

``` nohighlight
maas $PROFILE dhcpsnippet delete $DHCP_SNIPPET_ID
```

You can also use a snippet name in place of its ID.

*** How to set DNS parameters

It is possible to set DNS parameters using the MAAS CLI, using the following instructions.

**** How to create an A or AAAA record in DNS

An administrator can create an A record when creating a DNS resource with an IPv4 address:

``` nohighlight
mass $PROFILE dnsresources create fqdn=$HOSTNAME.$DOMAIN ip_addresses=$IPV4ADDRESS
```

An administrator can also create an AAAA record when creating a DNS resource with an IPv6 address:

``` nohighlight
mass $PROFILE dnsresources create fqdn=$HOSTNAME.$DOMAIN ip_addresses=$IPV6ADDRESS
```

**** How to create an alias (CNAME) record in DNS

An administrator can set a DNS Alias (CNAME record) to an already existing DNS entry of a machine:

``` nohighlight
mass $PROFILE dnsresource-records create fqdn=$HOSTNAME.$DOMAIN rrtype=cname rrdata=$ALIAS
```

For example, to set `webserver.maas.io` to alias to `www.maas.io`:

``` nohighlight
maas $PROFILE dnsresource-records create fqdn=webserver.maas.io rrtype=cname rrdata=www
```

**** How to create a Mail Exchange pointer record in DNS

An administrator can set a DNS Mail Exchange pointer record (MX and value) to a domain:

``` nohighlight
maas $PROFILE dnsresource-records create fqdn=$DOMAIN rrtype=mx rrdata='10 $MAIL_SERVER.$DOMAIN'
```

For example, to set the domain.name managed by MAAS to have an MX record and that you own the domain:

``` nohighlight
maas $PROFILE dnsresource-records create fqdn=maas.io rrtype=mx rrdata='10 smtp.maas.io'
```

*** How to set a DNS forwarder

To set a DNS forwarder:

``` nohighlight
maas $PROFILE maas set-config name=upstream_dns value=$MY_UPSTREAM_DNS
```

[/tab]
[/tabs]

** How to manage IP ranges

This section gives specific instructions about creating and managing IP ranges; it will help you learn:

- [How to create an IP range](#heading--create-a-range)
- [How to edit an existing IP range](#heading--edit-a-range)
- [How to delete an existing IP range](#heading--delete-a-range)

For an explanation of MAAS IP ranges, see [About networks](/t/about-networks/6680#heading--MAAS-IP-ranges). Also, see the [glossary](/t/maas-glossary/5416#heading--ip-ranges) for an explanation of the two kinds of reserved IP ranges MAAS uses.

[tabs]
[tab version="v3.4 Snap,v3.4 Packages,v3.3 Snap,v3.3 Packages,v3.2 Snap,v3.2 Packages,v3.1 Snap,v3.1 Packages,v3.0 Snap,v3.0 Packages,v2.9 Snap,v2.9 Packages" view="UI"]

*** How to create an IP range

To create an IP range:

1. Select *Subnets*.

2. In the *SUBNET* column, choose the subnet for which you want to create an IP range.

3. Scroll down to *Reserved ranges*.

4. Select *Reserve range* and choose either *Reserve range* or *Reserve dynamic range*. Note that if you choose a dynamic range, MAAS will automatically provide DHCP for enlistment and commissioning provided that the associated VLAN has DHCP enabled. 

5. A window will appear, allowing you to enter a *Start IP address* and *End IP address*

6. If you didn't select a dynamic range, you may optionally enter a *Purpose* for the range.

6. Select *Reserve* to register your choices with MAAS.

*** How to edit an IP range

To edit an IP range, click on the pencil icon to the right of a range and make changes as desired.  Be sure to *Save* your changes.

*** How to delete an IP range

To delete an IP range, click on the trash can icon to the right of a range.  You will be asked to confirm by selecting *Delete*; there is no undo.
[/tab]
[tab version="v3.4 Snap,v3.4 Packages,v3.3 Snap,v3.3 Packages,v3.2 Snap,v3.2 Packages,v3.1 Snap,v3.1 Packages,v3.0 Snap,v3.0 Packages,v2.9 Snap,v2.9 Packages" view="CLI"]

*** How to create an IP range

To create a range of dynamic IP addresses that will be used by MAAS for node enlistment, commissioning, and possibly deployment:

``` bash
maas $PROFILE ipranges create type=dynamic \
    start_ip=$IP_DYNAMIC_RANGE_LOW end_ip=$IP_DYNAMIC_RANGE_HIGH \
    comment='This is a reserved dynamic range'
```
*** How to create a range of IP addresses not used by MAAS

To create a range of IP addresses that will not be used by MAAS:

``` bash
maas $PROFILE ipranges create type=reserved \
    start_ip=$IP_STATIC_RANGE_LOW end_ip=$IP_STATIC_RANGE_HIGH \
    comment='This is a reserved range'
```
*** How to reserve a single IP address that will not be used by MAAS

To reserve a single IP address that will not be used by MAAS:

``` bash
maas $PROFILE ipaddresses reserve ip_address=$IP_STATIC_SINGLE
```
*** How to remove a single reserved IP address

To remove such a single reserved IP address:

``` bash
maas $PROFILE ipaddresses release ip=$IP_STATIC_SINGLE
```
[/tab]
[/tabs]

[tabs]
[tab version="v3.4 Snap,v3.4 Packages,v3.3 Snap,v3.3 Packages,v3.2 Snap,v3.2 Packages,v3.1 Snap,v3.1 Packages,v3.0 Snap,v3.0 Packages,v2.9 Snap,v2.9 Packages" view="UI"]

*** How to edit an existing IP range

1. Select *Menu* at the far right of the row corresponding to the subnet in question.

2. Select *Edit reserved range* from the menu that appears. 

3. Edit the fields as desired.

4. Select *Save* to register your changes.

*** How to delete an existing IP range

1. Select *Menu* at the far right of the row corresponding to the subnet in question.

2. Select *Remove range* from the menu that appears. 

3. Select *Save* to register your changes.
[/tab]
[tab version="v3.4 Snap,v3.4 Packages,v3.3 Snap,v3.3 Packages,v3.2 Snap,v3.2 Packages,v3.1 Snap,v3.1 Packages,v3.0 Snap,v3.0 Packages,v2.9 Snap,v2.9 Packages" view="CLI"]

*** How to edit an existing IP range

To edit an IP range:

1. Find the ID of the desired IP range with the command:

```
maas admin ipranges read
```

2. Edit the range with the command:

```
maas admin iprange update $ID start_ip="<start ip>" end_ip="<end ip>" comment="freeform comment"
```

This command will update the IP range associated with $ID.
[/tab]
[/tabs]
* How to enable high availability
High availability is built into MAAS: region and rack controllers balance the load and execute failover as part of normal operations. This article will help you understand how to take advantage of these built-in features.

** How to put MAAS in HA mode

You only need to [install multiple rack controllers](/t/how-to-adjust-your-controllers/5172#heading--install-a-rack-controller) to achieve real high availability.  Once that's done, you automatically gain highly-available BMC control and highly-available DHCP.  MAAS is constantly trying to answer three questions:

- How many racks is each region managing?
- How many connections does a given rack controller have?
- How many regions -- and region "worker processes" are running right now?

With just one rack, most of the logic can't function.  On the other hand, when you have multiple racks (and especially multiple regions), MAAS will continuously balance the load.

Every time a rack controller connects to a region controller to do something, MAAS checks whether racks and regions are balanced.  If the ratio for one rack-region connection is above a moderate threshold, compared to other connections, MAAS will re-balance.  This activity includes balancing not only discrete region controllers, but also re-distributing connections so that no single worker process has an uneven share of the load.

Re-balancing is also done at various other opportune times.  For example, if a network change happens (like toggling DHCP or changing a VLAN), MAAS will also re-balance the load.  And MAAS can maintain primary and secondary rack designations, so that faster, more nuanced load-balancing can occur.

*** How to enable highly-available BMC

You can also enable HA for BMC control (node power cycling) just by adding a second rack controller. MAAS will automatically identify which rack controller is responsible for a BMC, continuously balancing the connections.

*** How to enable highly-available DHCP services

You can enable highly-available DHCP services by using MAAS-managed DHCP, and adding rack controllers.  This DHCP HA affects the way MAAS manages nodes, including enlistment, commissioning and deployment. It enables primary and secondary DHCP instances to serve the same VLAN. This VLAN replicates all lease information is between rack controllers, so there's a bit of performance boost for large networks.

MAAS DHCP automatically creates failover peers, using mostly standard parameters:

```nohighlight
failover peer "failover-partner" {
     primary;
     address dhcp-primary.example.com;
     peer address dhcp-secondary.example.com;
     max-response-delay 60;
     max-unacked-updates 10;
     mclt 3600;
     split 255;
     load balance max seconds 3;
}
failover peer "failover-partner" {
     secondary;
     address dhcp-secondary.example.com;
     peer address dhcp-primary.example.com;
     max-response-delay 60;
     max-unacked-updates 10;
     load balance max seconds 3;
}
```
Note that the only difference from a standard 50/50 split (`split 128`) is that the primary DHCP server answers any requests that it can (`split 255`), within the maximum response delay of 60 seconds and an unacknowledged update count of 10.  In this sense, highly-available MAAS DHCP fails over only when absolutely necessary.

If you are enabling DHCP for the first time after adding a second rack controller, please read [Enabling DHCP](/t/how-to-enable-dhcp/5132#heading--enabling-dhcp).  On the other hand, if you have already enabled DHCP on your initial rack controller, you'll need to reconfigure DHCP to get optimum results.

[tabs]
[tab version="v3.4 Snap,v3.4 Packages,v3.3 Snap,v3.3 Packages,v3.2 Snap,v3.2 Packages,v3.1 Snap,v3.1 Packages,v3.0 Snap,v3.0 Packages,v2.9 Snap,v2.9 Packages" view="UI"]
To reconfigure DHCP after adding a new rack controller:

1. Select *Subnets*.

2. Select the appropriate VLAN.

3. Choose *Reconfigure DHCP*.

4. Confirm that you can see the second rack controller under *Secondary controller*.

5. Select *Reconfigure DHCP*.
[/tab]
[tab version="v3.4 Snap,v3.4 Packages,v3.3 Snap,v3.3 Packages,v3.2 Snap,v3.2 Packages,v3.1 Snap,v3.1 Packages,v3.0 Snap,v3.0 Packages,v2.9 Snap,v2.9 Packages" view="CLI"]
To reconfigure DHCP after adding a new rack controller, use the following sequence of commands:

```
vid=$(maas maas subnets read | jq -r '.[] | select(.cidr == "10.0.0.0/24") | .vlan.vid')
fabric_name=$(maas maas subnets read | jq -r '.[] | select(.cidr == "10.0.0.0/24") | .vlan.fabric')
query=".[] | select(.name == \"$fabric_name\") | .id"
fabric_id=$(maas maas fabrics read | jq "$query")
maas maas ipranges create type=reserved start_ip=10.0.0.3 end_ip=10.0.0.49
maas maas ipranges create type=dynamic start_ip=10.0.0.50 end_ip=10.0.0.99
maas maas vlan update ${fabric_id} ${vid} primary_rack=$(hostname) dhcp_on=true
```

Be sure to substitute the sample values for those of your own environment.
[/tab]
[/tabs]

*** How to configure multiple region endpoints

[tabs]
[tab version="v3.4 Snap,v3.4 Packages,v3.3 Snap,v3.2 Snap,v3.1 Snap,v3.0 Snap,v2.9 Snap"]
MAAS will automatically discover and track all reachable region controllers in a single cluster of rack controllers  It will also attempt to automatically connect to them if the one in use becomes inaccessible.  Administrators can alternatively specify multiple region-controller endpoints for a single rack controller by adding entries to `/var/snap/maas/current/rackd.conf`.  For example:
[/tab]
[tab version="v3.3 Packages,v3.2 Packages,v3.1 Packages,v3.0 Packages,v2.9 Packages"]
MAAS will automatically discover and track all reachable region controllers in a single cluster of rack controllers  It will also attempt to automatically connect to them if the one in use becomes inaccessible.  Administrators can alternatively specify multiple region-controller endpoints for a single rack controller by adding entries to `/etc/maas/rackd.conf`.  For example:
[/tab]
[/tabs]
    .
    .
    .
    maas_url:
      - http://<ip 1>:<port>/MAAS/
      - http://<ip 2>:<port>/MAAS/
    .
    .
    .

The setup of highly-available DHCP is now complete.  Note that, for HA purposes, DHCP provisioning will take into account multiple DNS services when there is more than one region controller on a single region.

** How to make region controllers highly available

Implementing highly-available region control is possible when you learn:

- [How to enable highly-available PostgreSQL](#heading--postgresql-ha)
- [How to enable highly-available API services](#heading--secondary-api-servers)
- [How to set up load balancing with HAProxy (optional)](#heading--load-balancing-with-haproxy-optional)

Load balancing is optional, but is highly recommended.

*** How to enable highly-available PostgreSQL

MAAS stores all state information in the PostgreSQL database. It is therefore recommended to run it in HA mode. Configuring HA for PostgreSQL is external to MAAS. You will, therefore, need to study the [PostgreSQL documentation](https://www.postgresql.org/docs/9.5/static/high-availability.html)`↗` and implement the variant of HA that makes you feel most comfortable.

Each region controller uses up to 40 connections to PostgreSQL in high load situations. Running two region controllers requires no modifications to the `max_connections` in `postgresql.conf`. More than two region controllers require that `max_connections` be adjusted to add 40 more connections per added region controller.

*** How to enable highly-available API services

[tabs]
[tab version="v3.4 Snap,v3.4 Packages,v3.3 Snap"]
Setting up high-availability using snaps is relatively easy:

1.  Set up PostgreSQL for high-availability as [explained above](#heading--postgresql-ha). PostgreSQL should run outside of the snap.
2.  [Install](/t/how-to-install-maas/5128#heading--install-from-snap) the MAAS snap on each machine you intend to use as a rack or region controller. You'll need the MAAS shared secret, located here, `/var/snap/maas/common/maas/secret`, on the first region controller you set up.
3.  [Initialise the snap](/t/how-to-install-maas/5128#heading--initialisation) as a `rack` or `region` controller.

Note that if you intend to use a machine as a region controller, you'll need to tell MAAS how to access your PostgreSQL database host with the following arguments:

- `--database-host DATABASE_HOST`
- `--database-name DATABASE_NAME`
- `--database-user DATABASE_USER`
- `--database-pass DATABASE_PASS`
[/tab]
[tab version="v3.3 Packages"]
Please see [Region controllers](/t/how-to-manage-controllers/5172) and [Multiple region endpoints](#heading--multiple-region-endpoints) for more information about how to install and configure rack controllers for multiple region controllers.
[/tab]
[tab version="v3.2 Snap"]
Setting up high-availability using snaps is relatively easy:

1.  Set up PostgreSQL for high-availability as [explained above](#heading--postgresql-ha). PostgreSQL should run outside of the snap.
2.  [Install](/t/how-to-install-maas/5128#heading--install-from-snap) the MAAS snap on each machine you intend to use as a rack or region controller. You'll need the MAAS shared secret, located here, `/var/snap/maas/common/maas/secret`, on the first region controller you set up.
3.  [Initialise the snap](/t/how-to-install-maas/5128#heading--initialisation) as a `rack` or `region` controller.

Note that if you intend to use a machine as a region controller, you'll need to tell MAAS how to access your PostgreSQL database host with the following arguments:

- `--database-host DATABASE_HOST`
- `--database-name DATABASE_NAME`
- `--database-user DATABASE_USER`
- `--database-pass DATABASE_PASS`
[/tab]
[tab version="v3.2 Packages"]
Please see [Region controllers](/t/how-to-manage-controllers/5172) and [Multiple region endpoints](#heading--multiple-region-endpoints) for more information about how to install and configure rack controllers for multiple region controllers.
[/tab]
[tab version="v3.1 Snap"]
Setting up high-availability using snaps is relatively easy:

1.  Set up PostgreSQL for high-availability as [explained above](#heading--postgresql-ha). PostgreSQL should run outside of the snap.
2.  [Install](/t/how-to-install-maas/5128#heading--install-from-snap) the MAAS snap on each machine you intend to use as a rack or region controller. You'll need the MAAS shared secret, located here, `/var/snap/maas/common/maas/secret`, on the first region controller you set up.
3.  [Initialise the snap](/t/how-to-install-maas/5128#heading--initialisation) as a `rack` or `region` controller.

Note that if you intend to use a machine as a region controller, you'll need to tell MAAS how to access your PostgreSQL database host with the following arguments:

- `--database-host DATABASE_HOST`
- `--database-name DATABASE_NAME`
- `--database-user DATABASE_USER`
- `--database-pass DATABASE_PASS`
[/tab]
[tab version="v3.1 Packages"]
Please see [Region controllers](/t/how-to-manage-controllers/5172) and [Multiple region endpoints](#heading--multiple-region-endpoints) for more information about how to install and configure rack controllers for multiple region controllers.
[/tab]
[tab version="v3.0 Snap"]
Setting up high-availability using snaps is relatively easy:

1.  Set up PostgreSQL for high-availability as [explained above](#heading--postgresql-ha). PostgreSQL should run outside of the snap.
2.  [Install](/t/how-to-install-maas/5128#heading--install-from-snap) the MAAS snap on each machine you intend to use as a rack or region controller. You'll need the MAAS shared secret, located here, `/var/snap/maas/common/maas/secret`, on the first region controller you set up.
3.  [Initialise the snap](/t/how-to-install-maas/5128#heading--initialisation) as a `rack` or `region` controller.

Note that if you intend to use a machine as a region controller, you'll need to tell MAAS how to access your PostgreSQL database host with the following arguments:

- `--database-host DATABASE_HOST`
- `--database-name DATABASE_NAME`
- `--database-user DATABASE_USER`
- `--database-pass DATABASE_PASS`
[/tab]
[tab version="v3.0 Packages"]
Please see [Region controllers](/t/how-to-manage-controllers/5172) and [Multiple region endpoints](#heading--multiple-region-endpoints) for more information about how to install and configure rack controllers for multiple region controllers.
[/tab]
[tab version="v2.9 Snap"]
Setting up high-availability using snaps is relatively easy:

1.  Set up PostgreSQL for high-availability as [explained above](#heading--postgresql-ha). PostgreSQL should run outside of the snap.
2.  [Install](/t/how-to-install-maas/5128#heading--install-from-snap) the MAAS snap on each machine you intend to use as a rack or region controller. You'll need the MAAS shared secret, located here, `/var/snap/maas/common/maas/secret`, on the first region controller you set up.
3.  [Initialise the snap](/t/how-to-install-maas/5128#heading--initialisation) as a `rack` or `region` controller.

Note that if you intend to use a machine as a region controller, you'll need to tell MAAS how to access your PostgreSQL database host with the following arguments:

- `--database-host DATABASE_HOST`
- `--database-name DATABASE_NAME`
- `--database-user DATABASE_USER`
- `--database-pass DATABASE_PASS`
[/tab]
[tab version="v2.9 Packages"]
Please see [Region controllers](/t/how-to-manage-controllers/5172) and [Multiple region endpoints](#heading--multiple-region-endpoints) for more information about how to install and configure rack controllers for multiple region controllers.
[/tab]
[/tabs]

*** How to enable load balancing for API services

You can add load balancing with [HAProxy](http://www.haproxy.org/)`↗` load-balancing software to support multiple API servers. In this setup, HAProxy provides access to the MAAS web UI and API.

[note]
If you happen to have Apache running on the same server where you intend to install HAProxy, you will need to stop and disable `apache2`, because HAProxy binds to port 80.
[/note]

**** How to install HAProxy

``` bash
sudo apt install haproxy
```

**** How to configure HAProxy

Configure each API server's load balancer by copying the following into `/etc/haproxy/haproxy.cfg` (see the [upstream configuration manual (external link)](http://cbonte.github.io/haproxy-dconv/1.6/configuration.html)`↗` as a reference). Replace $PRIMARY_API_SERVER_IP and $SECONDARY_API_SERVER_IP with their respective IP addresses:

``` yaml
frontend maas
    bind    *:80
    retries 3
    option  redispatch
    option  http-server-close
    default_backend maas

backend maas
    timeout server 90s
    balance source
    hash-type consistent
    server localhost localhost:5240 check
    server maas-api-1 $PRIMARY_API_SERVER_IP:5240 check
    server maas-api-2 $SECONDARY_API_SERVER_IP:5240 check
```

where `maas-api-1` and `maas-api-2` are arbitrary server labels.

Now restart the load balancer to have these changes take effect:

``` bash
sudo systemctl restart haproxy
```

The configuration of region controller HA is now complete.

**The API server(s) must be now be referenced (e.g. web UI, MAAS CLI) using port 80 (as opposed to port 5240).**

** Move a rack controller from one MAAS instance to another

[tabs]
[tab version="v3.4 Snap,v3.4 Packages,v3.3 Snap,v3.3 Packages,v3.2 Snap,v3.2 Packages,v3.1 Snap,v3.1 Packages,v3.0 Snap,v3.0 Packages,v2.9 Snap,v2.9 Packages" view="UI"]
In effect, there is no such action as moving a rack controller, although you can delete a rack controller from one MAAS and reinstantiate the same controller (binary-wise) on another MAAS instance.  First, delete the rack controller.  In the "Controllers" tab in the UI, select the rack controller you with to delete, choose "Take action" and select "Delete."  You will be asked to confirm with a red button, entitled "Delete 1 controller."
[/tab]
[tab version="v3.4 Snap,v3.4 Packages,v3.3 Snap,v3.3 Packages,v3.2 Snap,v3.2 Packages,v3.1 Snap,v3.1 Packages,v3.0 Snap,v3.0 Packages,v2.9 Snap,v2.9 Packages" view="CLI"]
In effect, there is no such action as moving a rack controller, although you can delete a rack controller from one MAAS and reinstantiate the same controller (binary-wise) on another MAAS instance.  First, delete the rack controller, with the command:

```
maas $PROFILE rack-controller delete $SYSTEM_ID
```

where `$PROFILE` is your admin profile name, and `$SYSTEM_ID` can be found by examining the output of the command:

```
maas $PROFILE rack-controllers read
```

There is no confirmation step, so make sure you have the right rack controller before proceeding.
[/tab]
[/tabs]

Next, you must register a new rack controller, which is always done from the command line.

[tabs]
[tab version="v3.4 Snap,v3.4 Packages,v3.3 Snap,v3.3 Packages,v3.2 Snap,v3.2 Packages,v3.1 Snap,v3.1 Packages,v3.0 Snap,v3.0 Packages,v2.9 Snap,v2.9 Packages" view="UI"]
For this exercise, we're assuming you are using the already installed rack controller code that was previously running on the "from" MAAS instance.  All that's necessary is that you register a new rack controller with the "to" MAAS instance, like this:

```
sudo maas-rack register --url $MAAS_URL_OF_NEW_MAAS --secret $SECRET_FOR_NEW_MAAS
```

where the secret is found in `/var/lib/maas/secret`.

Note that in the UI, if you go to the "Controllers" tab and press the button entitled, "Add rack controller," at the top of the Controllers screen, MAAS will give you a complete command string, including the correct URL and secret values.  Simply cut and paste that string to move the rack controller, paying attention to whether you are using snap or package build modes.
[/tab]
[tab version="v3.4 Snap,v3.4 Packages,v3.3 Snap,v3.3 Packages,v3.2 Snap,v3.2 Packages,v3.1 Snap,v3.1 Packages,v3.0 Snap,v3.0 Packages,v2.9 Snap,v2.9 Packages" view="CLI"]
For this exercise, we're assuming you are using the already installed rack controller code that was previously running on the "from" MAAS instance.  All that's necessary is that you register a new rack controller with the "to" MAAS instance, like this:

```
sudo maas init rack --maas-url $MAAS_URL_OF_NEW_MAAS --secret $SECRET_FOR_NEW_MAAS
```

where the secret is found in `/var/snap/maas/common/maas/secret`.
[/tab]
[/tabs]

*** How to avoid the potential pitfalls of moving a rack controller

There are dangers associate with moving a rack controller -- dangers that may generate errors, get you into a non-working state, or cause you significant data loss.  These dangers are precipitated by one caveat and two potential mistakes:

- **Using the same system as a rack controller and a VM host:** While not forbidden or inherently dangerous, using the same machine as both a rack controller and a VM host may cause resource contention and poor performance.  If the resources on the system are not more than adequate to cover both tasks, you may see slowdowns (or even apparent "freeze" events) on the system.

- **Moving a rack controller from one version of MAAS to another:** MAAS rack controller software is an integral part of each version of MAAS.  If you delete a rack controller from, say, a 2.6 version of MAAS, and attempt to register that 2.6 version of the rack controller code to, say, a 2.9 version of MAAS, you may experience errors and potential data loss.  Using the above example, if you are running both a VM host and a rack controller for MAAS 2.6 on one system, and you suddenly decide to delete that rack controller from 2.6 and attempt to register the same code to a 2.9 MAAS, the VM host may fail or disappear.  This will possibly delete all the VMs you have created or connected to that VM host -- which may result in data loss.  This action is not supported.

- **Connecting one instance of a rack controller to two instances of MAAS, regardless of version:** Trying to connect a single rack controller to two different instances of MAAS can result in all sorts of unpredictable (and potentially catastrophic) behaviour.  It is not a supported configuration.

Take these warnings to heart.  It may seem like a faster approach to "bridge" your existing rack controllers from one MAAS to another -- or from one version of MAAS to another -- while they're running.  Ultimately, though, it will probably result in more work than just following the recommended approach.

* How to enable MAAS native TLS encryption
[tab version="v3.4 Snap,v3.4 Packages,v3.3 Snap,v3.3 Packages,v3.2 Snap,v3.2 Packages"]
TLS can be enabled/disabled with the new `maas config-tls` command:

```nohighlight
usage: maas config-tls [-h] COMMAND ...

Configure MAAS Region TLS.

optional arguments:
  -h, --help  show this help message and exit

drill down:
  COMMAND
    enable    Enable TLS and switch to a secured mode (https).
    disable   Disable TLS and switch to a non-secured mode (http).

the following arguments are required: COMMAND
```

*** How to enable TLS

To enable TLS in MAAS, a private key and a X509 certificate containing the corresponding public key are required. Both key and certificate must be encoded in PEM format.

```nohighlight
usage: maas config-tls enable [-h] [--cacert CACERT] [-p PORT] key cert

positional arguments:
  key                   path to the private key
  cert                  path to certificate in PEM format

optional arguments:
  -h, --help            show this help message and exit
  --cacert CACERT       path to CA certificates chain in PEM format (default: None)
  -p PORT, --port PORT  HTTPS port (default: 5443)

the following arguments are required: key, cert
```

By default, the port for HTTPS traffic will be 5443. It’s possible to specify a different one via the `–port` option.  If your certificate is not self-signed, you can pass a cacert.pem, so that the full chain will be included in the certificate served by MAAS.

If you have HA setup, please note that every MAAS instance will use the same certificate, so you need to create one certificate with multiple domain names or IP addresses; for example:

```nohighlight
X509v3 Subject Alternative Name:
                DNS:example.com, IP Address:10.211.55.9
```

*** How to disable TLS

If for some reason you want to disable TLS, you can do it using the following command:

```nohighlight
usage: maas config-tls disable [-h]

optional arguments:
  -h, --help  show this help message and exit
```

After this, MAAS API and UI will be again reachable on port 5240, over plain HTTP.

** Using the CLI with a TLS-enabled MAAS

To connect to the MAAS API when TLS is enabled, an https URL must be provided to the maas login command, e.g.:

```nohighlight
maas login <profile_name> https://mymaas:5443/MAAS <api_key>

usage: maas login [-h] [--cacerts CACERTS] [-k] profile-name url [credentials]

Log in to a remote API, and remember its description and credentials.

positional arguments:
  profile-name       The name with which you will later refer to this remote server and credentials within this tool.
  url                The URL of the remote API, e.g. http://example.com/MAAS/ or http://example.com/MAAS/api/2.0/ if you wish to specify the API
                     version.
  credentials        The credentials, also known as the API key, for the remote MAAS server. These can be found in the user preferences page in
                     the web UI; they take the form of a long random-looking string composed of three parts, separated by colons.

optional arguments:
  -h, --help         show this help message and exit
  --cacerts CACERTS  Certificate CA file in PEM format
  -k, --insecure     Disable SSL certificate check

If credentials are not provided on the command-line, they will be prompted
for interactively.

the following arguments are required: profile-name, url
```

Certificates provided via `--cacerts` will be stored as a part of your profile and used for next CLI commands invocations.

** Certificate renewal

Once a certificate has expired, you can update it by running the same command used for enabling TLS:

``` nohighlight
$ ​​sudo maas config-tls enable new-server-key.pem new-server.pem --port 5443
```

If you’re using the snap, the certificate and key must be placed in a directory that’s readable by the CLI, such as `/var/snap/maas/common` (e.g., if you're using the snap version).

** How to check whether TLS is enabled

When TLS is enabled, the following certificate information is displayed in the MAAS UI under *Settings >> Configuration >> Security*: 

- CN 
- Expiration date
- Fingerprint
- Certificate

If TLS is disabled, this section will instead show a warning.  We recommend that you enable TLS for secure communication.

*** Notifications

When the specified number of days remain until certificate expiration (as defined in the notification reminder), all administrators will see the certificate expiration notification. This notification enumerates the number of days until certificate expiration.  It is dismissible, but once dismissed, it won't appear again.

A certificate expiration check runs every twelve hours.  When the certificate has expired, the notification will change to “certificate has expired”.

*** How to auto-renew certificates

MAAS does not auto-renew certificates, but there's no reason why we cannot provide a gratuitous example.  Use at your own risk.

**** Set up your own certificate authority

You can setup your own Certificate Authority (CA) server that supports the ACME protocol with these components:

- [step-ca from Smallstep](https://smallstep.com/docs/step-ca)`↗`
- [Caddy server with ACME support](https://caddyserver.com/docs/caddyfile/directives/acme_server)  (available since version 2.5)`↗`

If you have a CA server with ACME protocol support, you can use any ACME client for an automated certificate renewal and use crontab to renew on a desired time interval.  Consider [acme.sh](https://github.com/acmesh-official/acme.sh)`↗`: 

```nohighlight
$> acme.sh --issue -d mymaas.internal --standalone --server https://ca.internal/acme/acme/directory

Your cert is in: /root/.acme.sh/mymaas.internal/mymaas.internal.cer
Your cert key is in: /root/.acme.sh/mymaas.internal/mymaas.internal.key
The intermediate CA cert is in: /root/.acme.sh/mymaas.internal/ca.cer
And the full chain certs is there: /root/.acme.sh/foo/fullchain.cer
```

Once the certificate is issued, you can install it. 

```nohighlight
$> acme.sh --installcert -d maas.internal \
   --certpath /var/snap/maas/certs/server.pem \
   --keypath /var/snap/maas/certs/server-key.pem  \
   --capath  /var/snap/maas/certs/cacerts.pem  \
   --reloadcmd  "(echo y) | maas config-tls enable /var/snap/maas/certs/server-key.pem /var/snap/maas/certs/server.pem --port 5443"
```

Please note that if you have MAAS installed via snap, you need to run above command as root, in order to place cert and key under `/var/snap/maas`.

Another approach would be to write a bash script and pass it to a [`--renew-hook`](https://github.com/acmesh-official/acme.sh/wiki/Using-pre-hook-post-hook-renew-hook-reloadcmd)`↗`.

**** Using certbot

[certbot](https://certbot.eff.org)`↗` can be used to renew certificates and execute a post-renewal hook.  We can use this hook to re-configure MAAS to use fresh certificates.

To create a post-renewal hook, you can put this sample script under `/etc/letsencrypt/renewal-hooks/post/001-update-maas.sh`.

```nohighlight
#!/bin/bash -e

DOMAIN="maas.internal"
CERTSDIR="/etc/letsencrypt/live/$DOMAIN"

cd /var/snap/maas/common

# need to copy certs where the snap can read them
cp "$CERTSDIR"/{privkey,cert,chain}.pem .
yes | maas config-tls enable privkey.pem cert.pem --cacert chain.pem --port 5443

# we don’t want to keep private key and certs around
rm {privkey,cert,chain}.pem
```

Don’t forget to make the script executable:

```nohighlight
chmod +x /etc/letsencrypt/renewal-hooks/post/001-update-maas.sh
```

Of course, you'll first need to obtain a new certificate.  

```nohighlight
sudo REQUESTS_CA_BUNDLE=ca.pem certbot certonly --standalone -d maas.internal     --server https://ca.internal/acme/acme/directory
```

Don't worry, new certs will not run the hook, since hooks are run only on renewal.

To test the renewal process and verify that the hook is executed correctly, you can use the following command with a `--dry-run flag`. Please note, that the hook will be executed and existing certificates will be removed (if you are using an example hook script):

```nohighlight
sudo REQUESTS_CA_BUNDLE=ca.pem certbot renew --standalone --server https://ca.internal/acme/acme/directory --dry-run
```

Please refer to the [cerbot documentation](https://certbot.eff.org/instructions?ws=other&os=ubuntufocal)`↗` for more information.

[/tab]
[tab version="v3.1 Snap,v3.1 Packages,v3.0 Snap,v3.0 Packages,v2.9 Snap,v2.9 Packages"]
MAAS doesn't support TLS encryption natively.  If you are not interested in [setting up an HAProxy](/t/how-to-enable-high-availability/5120#heading--load-balancing-with-haproxy-optional), you can enable TLS independently in the web server software (e.g. Apache, Nginx) which users access directly.  The examples below explain how to create this configuration.

Note that MAAS doesn't bind to port 80; instead, MAAS binds to port 5240.

** nginx example config

    server {
     listen 443 SQL;

     server_name _;
     ssl_certificate /etc/nginx/ssl/nginx.crt;
     ssl_certificate_key /etc/nginx/ssl/nginx.key;

     location / {
      proxy_pass http://localhost:5240;
      include /etc/nginx/proxy_params;
     }

     location /MAAS/ws {
      proxy_pass http://localhost:5240/MAAS/ws;
                    proxy_http_version 1.1;
                    proxy_set_header Upgrade $http_upgrade;
      proxy_set_header Connection "Upgrade";
     }
    }

** apache2 example config

    <VirtualHost *:443>
     SSLEngine On

     SSLCertificateFile /etc/apache2/ssl/apache2.crt
     SSLCertificateKeyFile /etc/apache2/ssl/apache2.key

     RewriteEngine On
            RewriteCond %{REQUEST_URI} ^/MAAS/ws [NC]
            RewriteRule /(.*) ws://localhost:5240/MAAS/ws [P,L]

            ProxyPreserveHost On
            ProxyPass / http://localhost:5240/
            ProxyPassReverse / http://localhost:5240/
    </VirtualHost>
[/tab]
[/tabs]

* How to find machines
The Machines and Devices pages contain a powerful interactive search bar that lets you filter machines and devices.  This article is designed to help you understand how to use this resource. 

**** This article will explain:

- [How to do simple searches](#heading--simple-searches)
- [How to do filtered searches](#heading--filtered-searches)
- [How to create manual filters](#heading--manual-filters)
- [How to set up exact matching](#heading--exact-matching)
- [How to set up partial matching](#heading--partial-matching)
- [How to include multiple search terms](#heading--multiple-search-terms)

This article also includes a [filter properties reference](#heading--filter-properties)

** How to construct a MAAS search parameter

A valid MAAS search parameter looks like this:

<a href="https://discourse.maas.io/uploads/default/original/2X/d/dcf5037cdd886eb85a2d305fd3df111b38865cea.png" target = "_blank"><img src="https://discourse.maas.io/uploads/default/original/2X/d/dcf5037cdd886eb85a2d305fd3df111b38865cea.png"></a>

Each search attribute is placed in the Search box, separated by spaces (if it's an "AND" search operation), or by parentheses and commas (if's it's an "OR" operation).  Specific search parameters use the notation "parameter-name" followed by a colon (":").  For example, here we're searching for "pod:", which equates to the pod name (VM name).  

You can match exactly by using the equals sign ("=") with a filter fragment, or make a partial match by excluding the "=".  You can get an idea how these parameters work by using the filter dropdowns in the UI, which generate a valid search expression in the "Search" box.  There is also a "not" operator ("!") which can be applied to partial or exact matches.

** How to do simple searches

1. Select *Machines*.

2. Enter text in the *Search* box. As you type, MAAS will update search results in real time. You can search across virtually every parameter, including domain, name, tag, power type, IP, status, zone, and so on.

** How to do filtered searches

1. Select *Machines*.

2. Select the *Filters* dropdown.

3. Choose a parameter group.  The group will lock open.

4. Choose one or more parameter values to search against.  MAAS will update the search results in real time.

*** How to create manual filters

Enter filters manually in the search bar to more precisely control your searches:

``` no-highlight
filter-name:([=]val1,...,[=]val2)
```

**Note:** Surround search terms with parentheses when they must occur together, e.g., `status:(failed testing)`.

*** How to set up exact matching

If you need an exact match, preface the search value with an equal sign. For example, to find machines belonging to a pod named `able-cattle`:

``` no-highlight
pod:=able-cattle
```

*** How to set up partial matching

Without an equal sign, MAAS returns partial matches. For example, the following will display all machines belonging to pods with names containing `able` or `cattle`:

``` no-highlight
pod:able,cattle
```

*** Multiple search terms

MAAS uses Boolean AND logic to evaluate multiple search terms. For example, when you type `pod:able,cattle cpu:=5`, MAAS displays machines that belong to pods with names containing `able` OR `cattle` AND having 5 CPU cores. Similarly, if you enter multiple words into the search tool, e.g., `steady able`, MAAS will display machines matching both terms (e.g., `steady` AND `able`).

** Filter properties reference

The following table describes the expanded filter set for the MAAS machine list:

- Items marked "Dyn" are dynamic, populated based on existing data, that is, the "Tags" filter only shows tags that currently exist.  
- Items which are not dynamic present the entire range of possible values, regardless of whether that value currently exists in MAAS; for example, all machine status values are available to be filtered, even if no machines currently have that status.
- Items marked "Grp" can be used to group machines, instead of the default machine status.
- Items marked "Man" must be manually entered, i.e., they are not in the UI filter dropdown, but can be entered in the "Search" box if properly formatted (as in the examples given).

| Parameter (bold) w/example           | Shows nodes...                   | Dyn | Grp | Man |
|--------------------------------------|----------------------------------|-----|-----|-----|
| **arch**:(=architecture)             | with "architecture"              |     | Grp |     |
| arch:(!=architecture)                | NOT with "architecture"          | Dyn |     |     |
| **zone**:(=zone-name)                | in "zone-name"                   | Dyn | Grp |     |
| zone:(!=zone-name)                   | NOT in "zone-name"               | Dyn |     |     |
| **pool**:(=resource-pool)            | in "resource-pool"               | Dyn | Grp |     |
| pool:(!=resource-pool)               | NOT in "resource-pool"           | Dyn |     |     |
| **pod**:(=pod-name)                  | with "pod-name"                  | Dyn | Grp |     |
| pod:(!=pod-name)                     | NOT with "pod-name"              | Dyn |     |     |
| **pod_type**:(=pod-type)             | with power type "pod-type"       | Dyn | Grp | Man |
| pod_type:(!=pod-type)                | NOT with power type "pod-type"   | Dyn |     | Man |
| **domain**:(=domain-name)            | with "domain-name"               | Dyn | Grp | Man |
| domain:(!=domain-name)               | NOT with "domain-name"           | Dyn |     | Man |
| **status**:(=op-status)              | having "op-status"               |     | Grp |     |
| status:(!=op-status)                 | NOT having "op-status"           | Dyn |     |     |
| **owner**:(=user)                    | owned by "user"                  | Dyn | Grp |     |
| owner:(!=user)                       | NOT owned by "user"              | Dyn |     |     |
| **power_state**:(=power-state)       | having "power-state"             |     | Grp | Man |
| power_state:(!=power-state)          | NOT having "power-state"         | Dyn |     | Man |
| **tags**:(=tag-name)                 | with tag "tag-name"              | Dyn |     |     |
| tags:(!=tag-name)                    | NOT with tag "tag-name"          | Dyn |     |     |
| **fabrics**:(=fabric-name)           | in "fabric-name"                 | Dyn |     |     |
| fabrics:(!=fabric-name)              | NOT in "fabric-name"             | Dyn |     |     |
| **fabric_classes**:(=fabric-class)   | in "fabric-class"                | Dyn |     | Man |
| fabric_classes:(!=fabric-class)      | NOT in "fabric-class"            | Dyn |     | Man |
| **fabric_name**:(=fabric-name)       | in "boot-interface-fabric"       | Dyn |     | Man |
| fabric_name:(!=fabric-name)          | NOT in "boot-interface-fabric"   | Dyn |     | Man |
| **subnets**:(=subnet-name)           | attached to "subnet-name"        | Dyn |     |     |
| subnets:(!=subnet-name)              | Not attached to "subnet-name"    | Dyn |     |     |
| **link_speed**:(link-speed)          | having "link-speed"              | Dyn |     | Man |
| link_speed:(!link-speed)             | NOT having "link-speed"          | Dyn |     | Man |
| **vlans**:(=vlan-name)               | attached to "vlan-name"          | Dyn |     |     |
| vlans:(!=vlan-name)                  | NOT attached to "vlan-name"      | Dyn |     |     |
| **storage**:(storage-MB)             | having "storage-MB"              | Dyn |     | Man |
| **total_storage**:(total-stg-MB)     | having "total-stg-MB"            | Dyn |     | Man |
| total_storage:(!total-stg-MB)        | NOT having "total-stg-MB"        | Dyn |     | Man |
| **cpu_count**:(cpu-count)            | having "cpu-count"               | Dyn |     | Man |
| cpu_count:(!cpu-count)               | NOT having "cpu-count"           | Dyn |     | Man |
| **mem**:(ram-in-MB)                  | having "ram-in-MB"               | Dyn |     | Man |
| mem:(!ram-in-MB)                     | NOT having "ram-in-MB"           | Dyn |     | Man |
| **mac_address**:(=MAC)               | having MAC address "MAC"         | Dyn |     | Man |
| mac_address:(!=MAC)                  | NOT having                       | Dyn |     | Man |
| **agent_name**:(=agent-name)         | Include nodes with agent-name    | Dyn |     | Man |
| agent_name:(!=agent-name)            | Exclude nodes with agent-name    | Dyn |     | Man |
| **cpu_speed**:(cpu-speed-GHz)        | CPU speed                        | Dyn |     | Man |
| cpu_speed:(!cpu-speed-GHz)           | CPU speed                        | Dyn |     | Man |
| **osystem**:(=os-name)               | The OS of the desired node       | Dyn |     | Man |
| osystem:(!=os-name)                  | OS to ignore                     | Dyn |     | Man |
| **distro_series**:(=distro-name)     | Include nodes using distro       | Dyn |     | Man |
| distro_series:(!=distro-name)        | Exclude ndoes using distro       | Dyn |     | Man |
| **ip_addresses**:(=ip-address)       | Node's IP address                | Dyn |     | Man |
| ip_addresses:(!=ip-address)          | IP address to ignore             | Dyn |     | Man |
| **spaces**:(=space-name)             | Node's spaces                    | Dyn |     |     |
| spaces:(!=space-name)                | Node's spaces                    | Dyn |     |     |
| **workloads**:(=annotation-text)     | Node's workload annotations      | Dyn |     |     |
| workloads:(!=annotation-text)        | Node's workload annotations      | Dyn |     |     |
| **physical_disk_count**:(disk-count) | Physical disk Count              | Dyn |     | Man |
| physical_disk_count:(!disk-count)    | Physical disk Count              | Dyn |     | Man |
| **pxe_mac**:(=PXE-MAC)               | Boot interface MAC address       | Dyn |     | Man |
| pxe_mac:(!=PXE-MAC)                  | Boot interface MAC address       | Dyn |     | Man |
| **fqdn**:(=fqdn-value)               | Node FQDN                        | Dyn |     | Man |
| fqdn:(!=fqdn-value)                  | Node FQDN                        | Dyn |     | Man |
| **simple_status**:(=status-val)      | Include nodes with simple-status | Dyn |     | Man |
| simple_status:(!=status-val)         | Exclude nodes with simple-status | Dyn |     | Man |
| **devices**:(=)                      | Devices                          | Dyn |     | Man |
| **interfaces**:(=)                   | Interfaces                       | Dyn |     | Man |
| **parent**:(=)                       | Parent node                      | Dyn | Grp | Man |

* How to get started with MAAS

It's relatively easy to get MAAS up and running:

- [Install MAAS](/t/how-to-do-a-fresh-install-of-maas/5128): You can install MAAS from snaps or Debian packages, in two different modes: proof-of-concept or production.

- [Upgrade MAAS](/t/how-to-upgrade-maas/5436): Those already using a previous version of MAAS can easily get up-to-speed with the latest version.

- [Use an Ansible playbook](/t/how-to-spin-up-maas-with-ansible/6367): If you prefer automation, our Ansible playbooks will help you get started with MAAS very quickly.


* How to give and receive help
There are times when interface and documentation aren't enough to figure something out -- that's when you need help.  And given that MAAS is an open-source product, there are also a number of ways you can help us.

** [Use our Discourse forum](/t/how-to-use-our-discourse-forum/6802)

Interact with other users, Canonical support personnel, and the MAAS development team; you can bring us problems, solve problems, or even just catch up on things you didn't know.

** [Get support](https://maas.io/docs/how-to-contact-us)`↗`

We offer paid support for those situations where it's warranted, that is, where it would take a significant amount of time for a Canonical employee to help you with your project, or where your needs are off the MAAS roadmap and require special engineering work.

** [Request new features](/t/how-to-request-a-new-feature/4447)

We do listen to well-spoken feature requests that affect more than two or three users, and we have delivered on these requests in the past.

** [Review and report bugs](/t/how-to-review-and-report-bugs/4446)

Sometimes a bug request is the only way to handle the problem.

** [Contribute documentation](/t/how-to-contribute-documentation/6949)

We originate a lot of documentation, but doc is much better when everybody works on it.
* How to improve MAAS security
As a MAAS administrator, you have the critical responsibility of hardening your installation to help repudiate attacks and malicious actors.  While there are too many variables to make meaningful suggestions for your deployed machines, there are a number of steps you can take to improve the overall security of your MASS setup.  This article provides a few suggestions.

**** This article will help you learn:

- [How to set up a firewall for MAAS](#heading--firewalls)
- [How to configure a TLS-terminating load balancer](#heading--tls)
- [How to use logs to identify security issues](#heading--using-logs-for-security)
- [How to implement PostgreSQL security](#heading--postgres-security)
- [Other things you can do to harden MAAS](#heading--what-else-to-do)
- [Whom to contact for MAAS security consulting](#heading--security-consulting)

** How to set up a firewall for MAAS

<p>Each rack controller must be able to initiate TCP connections on the following ports:</p>
<table>
<thead>
<tr>
<th>Port(s)</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>5240</code></td>
<td>HTTP communication with each region controller. Note that port <code>80</code> is typically used in high-availability environments.
</tr>
<tr>
<td>
<code>5241</code> - <code>5247</code>
</td>
<td>Reserved for internal MAAS services.</td>
</tr>
<tr>
<td><code>5248</code></td>
<td>Reserved for rack HTTP communication.</td>
</tr>
<tr>
<td>
<code>5250</code> - <code>5270</code>
</td>
<td>Reserved for region workers (RPC).</td>
</tr>
</tbody>
</table>

Consider setting your [firewall ](https://help.ubuntu.com/lts/serverguide/firewall.html#firewall-logs)`↗` on your rack and region controllers to disallow communication on all ports except those used by MAAS. For example, assuming you have installed `ufw`, you could execute:

    sudo ufw enable
    sudo ufw default deny incoming

You could then follow that with commands similar to these:

    sudo ufw allow 5240
    sudo ufw allow 5248
    sudo ufw allow 5241:5247/tcp
    sudo ufw allow 5241:5247/udp
    sudo ufw allow 5250:5270/tcp
    sudo ufw allow 5250:5270/udp

Recognise that your particular configuration and version may vary, so consult the appropriate firewall manual pages for your specific MAAS host system.

** How to configure a TLS-terminating load balancer

One of the best steps you can take to improve both security and availability of your MAAS installation is to install TLS-terminating load balancer.  For MAAS, we recommend using [HAProxy ](https://www.haproxy.com)`↗`.  This section explains how to set one up.

<details><summary>What is a TLS-terminated load balancer?</summary>

In the context of MAAS, a [load balancer ](https://www.nginx.com/resources/glossary/load-balancing/)`↗` distributes the incoming Web UI and API requests across multiple region controllers.  This reduces both load on MAAS and wait times for user requests.  Typically, this is known as a high-availability (HA) configuration, although there are two other [HA configurations](/t/how-to-enable-high-availability/5120) that can be enabled for MAAS: one for BMC access (for powering on machines)`↗`, and one for DHCP, which enables primary and secondary DHCP instances that manage the same VLAN.

A TLS-terminated load balancer is a load balancer that carries encryption and decryption as far down the pipe as possible, in this case, all the way to the load balancer itself. Note that, even though the "SSL" keyword may be used to enable operation, the term SSL is considered obsolete.  Hence we choose to use the term "TLS" instead, referring to **Transport Layer Security**.

TLS is meant to provide privacy and data integrity between two or more applications.  Privacy is provided by [symmetric cryptography ](https://en.wikipedia.org/wiki/Symmetric-key_algorithm)`↗`, based on a shared secret and uniquely-generated keys negotiated at the start of a session (during the [TLS handshake ](https://en.wikipedia.org/wiki/Transport_Layer_Security#TLS_handshake)`↗`).  Identity of each app can be authenticated, though this feature can be optional.  Authenticity of messages is ensured by using [message authentication codes ](https://en.wikipedia.org/wiki/Message_authentication_code)`↗` to detect tampering.

</details>

*** PEM file

As a first step, you'll need an SSL certificate (`mysite.com.crt`) with a key pair (`mysite.com.key`), combined into a PEM file:

    cat mysite.com.crt mysite.com.key > mysite.com.pem
    sudo cp mysite.com.pem /etc/ssl/private/

Depending upon your chosen certificate authority, you may also need to copy your CA root certificate and one or more intermediate CA certificates into the same PEM file.

*** Install and configure HA proxy

To install HAProxy, execute the following commands:

    sudo apt-get update
    sudo apt-get install haproxy

Next, edit `/etc/haproxy/haproxy.cfg` as follows.  In the `global` section of the file, add this line:

    maxconn <number of concurrent connections>

Be aware that there's a balance between accepting many connections and overloading the API by trying to serve too many requests.  Also, you should consider adding a line like this one to the same section:

    tune.ssl.default-dh-param 2048

This parameter configures the maximum size of temporary DHE keys that are generated.

Next, in the `defaults` section, add the following lines under `mode http`:

    option forwardfor
    option http-server-close

The option `forwardfor` tells HAProxy to add X-Forward-For headers to each request.  The `http-server-close` option reduces latency between HAProxy and your users by closing connections but maintaining a keep-alive.

Finally, you'll set the frontend and backend parameters that define the connections between HAProxy and MAAS.  For the frontend, you can set parameters this way:

    frontend maas
        bind *:443 ssl crt /etc/ssl/private/mysite.com.pem
        reqadd X-Forwarded-Proto:\ https
        retries 3
        option redispatch
        default_backend maas

This stanza defines a frontend name `maas`; tells HAProxy to handle incoming traffic to port 443, providing SSL encryption, enabled by the certificates and keys you earlier concatenated into your PEM file; allows three retries and redispatch; and forwards these requests to `maas` as the backend server, which is defined something like this:

    backend maas
        timeout server 90s
        balance source
        hash-type consistent
        server localhost localhost:5240 check
        server maas-api-1 <ip-address-of-a-region-controller>:5240 check
        server maas-api-2 <ip-address-of-another-region-controller>:5240 check

This stanza defines a backend server group named `maas`; sets consistent hashing, 90 second timeout, and balanced sources; sets the server address and port (`localhost:5240`); and engages two region controllers, named `maas-api-1` and `maas-api-2`.  Note that the "1" and "2" designations are completely arbitrary, as there is no sense of "primary" or "secondary" associated with this configuration.

Finally, restart the (already-running) load balancer so that these changes can take effect and the HAProxy will begin to forward requests:

    sudo systemctl restart haproxy

Note that you can also [enable HAProxy logging ](https://www.digitalocean.com/community/tutorials/how-to-implement-ssl-termination-with-haproxy-on-ubuntu-14-04)`↗` if desired.  This logging is an optional feature of the HAProxy tool and is thus left to your discretion.  

If desired, you can [bypass the use of SSL](/t/how-to-enable-high-availability/5120#heading--load-balancing-with-haproxy-optional) in your HAProxy.  Alternatively, you can [set up TLS encryption on your MAAS web UI](/t/how-to-enable-maas-native-tls/5116) without implementing HAProxy.

** How to use logs to identify security issues

There are four categories of log files that you can use to help identify security issues:

1. firewall logs
2. Web server logs
3. MAAS log files
4. system log files

This section will offer some advice, as well as links to more detailed information on these categories.

*** Firewall logs

The Ubuntu firewall, [UFW](https://wiki.ubuntu.com/UncomplicatedFirewall)`↗`, is a front-end for [iptables ](https://help.ubuntu.com/community/IptablesHowTo)`↗`, so the UFW log output is very similar to what you'll encounter in iptables itself.  If you want to secure your MAAS installation, it's very important to periodically review your UFW logs, found in `/var/log/ufw*`.

Learning to recognise issues in the UFW/iptables log is an art form, so we're not going to give an extended tutorial here.  Still, there are some key indicators that might help you spot security issues.

You might look for something probing a port that's not supporting an application service.  Attackers use port scanners to look for openings.  You might see entries like these:

    blocked incoming tcp connection request from 96.39.208.43:8240 to 128.17.92.85:6002
    blocked incoming tcp connection request from 96.39.208.43:8240 to 128.17.92.85:6003
    blocked incoming tcp connection request from 96.39.208.43:8240 to 128.17.92.85:6004

You can also compare attempts on unusual port numbers against [well-known hacker tools ](http://www.relevanttechnologies.com/resources_4.asp)`↗`.  For instance, repeated attempts against port 12361 might mean that someone is attempting to attack with the Whack-a-mole exploit.

Also suspicious are repeated, unsuccessful access attempts, against the same port or service, from the same domain, IP address, or subnet. These attempts may be spread out in time (`grep` is your friend, here).  For example, a group of login attempts that look like this may indicate that an attacker is trying to disguise port scans by switching IP addresses within a block of addresses available to them:

    blocked incoming tcp connection request from 96.39.208.43:49343 to 64.242.119.18:31337
    blocked incoming tcp connection request from 96.39.208.62:49343 to 64.242.119.18:31337
    blocked incoming tcp connection request from 96.39.209.243:49343 to 64.242.119.18:31337
    blocked incoming tcp connection request from 96.39.208.135:49343 to 64.242.119.18:31337
    .
    .
    .
    blocked incoming tcp connection request from 96.39.208.208:49343 to 64.242.119.18:31337

Watch out for suspicious messages or connections originating inside your network, which may indicate that you have a Trojan residing inside your UFW:

    blocked outgoing tcp packet from 192.168.23.100:5240 to 96.38.231.18:443 as FIN:ACK received, but there is no active connection.

This message will usually be repeated a number of times, since Trojans are fairly persistent.

Look for source-routed packets, that is, packets with a source address internal to your network, but which originate from outside your network, indicating that someone is trying to spoof one of your internal addresses.

Review the IP addresses that are being rejected and dropped.  Try to identify them with a `ping -a <IP address>`.  Spoofed addresses won't have an owner (and you can block them).  Real addresses have a [whois](https://lookup.icann.org/en)`↗` entry, so it's possible you can contact the ISP to report and resolve this issue.

There are many other firewall log analysis techniques, and a number of good open-source and commercial log analysis programs.  If you decide to analyse directly, though, you're basically looking for blocked connection issues, connections to (potentially) open ports you're not using, and suspicious-looking outbound connections.

*** Web server logs

Detecting malicious activity directed toward your Web server is best done with a log analysis tool.  If you want to review the raw logs directly, you can look for them in two places:

1. `/var/log/httpd/`, `/var/log/apache`, or `/var/log/apache2` (in the case of Apache), or

2. the path given in `/etc/nginx/nginx.conf` or given in your site configuration file, which itself is found at the path `/etc/nginx/sites-available` (in the case of nginx -- look for the `access_log` directive).

Web server log analysis is also an art form, so we don't plan to offer a comprehensive tutorial here, but here are few examples of things to look for in your logs:

- multiple requests in less than one second, or some other appropriate time-frame.

- multiple secure/login page accesses in a one-minute window, especially when they fail.

- attempts to access non-existent pages using different paths or query parameters (e.g., `135.25.48.19:5250/maas/index.html`).

- look out for SQL injection attacks, for example:

    84.55.41.57- - [14/Apr/2016:08:22:13 0100]
    "GET /wordpress/wp-content/plugins/custom_plugin/check_user.php?userid=1 
    AND (SELECT 6810 FROM(SELECT COUNT(*),CONCAT(0x7171787671,(SELECT 
    (ELT(6810=6810,1))),0x71707a7871,FLOOR(RAND(0)*2))x FROM 
    INFORMATION_SCHEMA.CHARACTER_SETS GROUP BY x)a) HTTP/1.1" 200 166 "-" "Mozilla/5.0 
    (Windows; U; Windows NT 6.1; ru; rv:1.9.2.3) Gecko/20100401 Firefox/4.0 (.NET CLR 
    3.5.30729)"

- attempts to run a Web shell, for instance:

    192.168.1.102 29/Oct/2018:14:52:16 GET /b374k.php HTTP/1.1 200 2125 Mozilla/5.0

As mentioned above, there are a large number of Web server exploits, and this document does not propose to enumerate them all.  If you want to ensure secure operation, though, it's useful to familiarise yourself with these kinds of Web server attacks.

*** MAAS log files

[tabs]
[tab version="v3.4 Snap,v3.3 Snap,v3.2 Snap"]
Presently, your primary use of MAAS log files to improve security is to periodically check log files for login failures.  You can check for this activity in the `regiond.log` file, found at `/var/snap/maas/common/log/regiond.log`.  For reference, a valid login request looks like this entry: 
[/tab]
[tab version="v3.4 Packages,v3.3 Packages,v3.2 Packages"]
Presently, your primary use of MAAS log files to improve security is to periodically check log files for login failures.  You can check for this activity in the `regiond.log` file, found at `/var/log/maas/regiond.log`.  For reference, a valid login request looks like this entry: 
[/tab]
[tab version="v3.1 Snap"]
Presently, your primary use of MAAS log files to improve security is to periodically check log files for login failures.  You can check for this activity in the `regiond.log` file, found at `/var/snap/maas/common/log/regiond.log`.  For reference, a valid login request looks like this entry: 
[/tab]
[tab version="v3.1 Packages"]
Presently, your primary use of MAAS log files to improve security is to periodically check log files for login failures.  You can check for this activity in the `regiond.log` file, found at `/var/log/maas/regiond.log`.  For reference, a valid login request looks like this entry: 
[/tab]
[tab version="v3.0 Snap"]
Presently, your primary use of MAAS log files to improve security is to periodically check log files for login failures.  You can check for this activity in the `regiond.log` file, found at `/var/snap/maas/common/log/regiond.log`.  For reference, a valid login request looks like this entry: 
[/tab]
[tab version="v3.0 Packages"]
Presently, your primary use of MAAS log files to improve security is to periodically check log files for login failures.  You can check for this activity in the `regiond.log` file, found at `/var/log/maas/regiond.log`.  For reference, a valid login request looks like this entry: 
[/tab]
[tab version="v2.9 Snap"]
Presently, your primary use of MAAS log files to improve security is to periodically check log files for login failures.  You can check for this activity in the `regiond.log` file, found at `/var/snap/maas/common/log/regiond.log`.  For reference, a valid login request looks like this entry: 
[/tab]
[tab version="v2.9 Packages"]
Presently, your primary use of MAAS log files to improve security is to periodically check log files for login failures.  You can check for this activity in the `regiond.log` file, found at `/var/log/maas/regiond.log`.  For reference, a valid login request looks like this entry: 
[/tab]
[/tabs]


    2020-03-31 21:17:56 regiond: [info] 10.132.172.1 GET /MAAS/accounts/login/ HTTP/1.1
    --> 200 OK (referrer: http://10.132.172.231:5240/MAAS/r/; agent: Mozilla/5.0 (X11;
    Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.149
    Safari/537.36)````

If a login fails due to bad input (username/password), the regiond log will contain an entry something like this one:

    2020-03-31 21:18:08 regiond: [info] 10.132.172.1 POST /MAAS/accounts/login/ HTTP/1.1
    --> 400 BAD_REQUEST (referrer: http://10.132.172.231:5240/MAAS/r/; agent:
    Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko)
    Chrome/80.0.3987.149 Safari/537.36)````

An entry like this one would also be suspect, since it involves omitting username/password entries at the login prompt:

    2020-03-31 21:18:45 regiond: [info] 10.132.172.1 POST /MAAS/accounts/login/ HTTP/1.1
    --> 204 NO_CONTENT (referrer: http://10.132.172.231:5240/MAAS/r/; agent: Mozilla/5.0
    (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.149
    Safari/537.36)````

The key differentiator to distinguish problems from routine failures is the frequency.  If you notice a lot of these last two entries in a given period of time, you may want to investigate more thoroughly.

*** System log files

You can also use the standard system logs to detect malicious activity, though this subject is largely beyond the scope of this document.  As a simple example, consider using `journalctl` to detect and source an SSH brute force attack:

    [root@maasserver ~]# journalctl -u sshd | grep "Failed password"
    Jun 06 13:45:19 router sshd[2487]: Failed password for root from 234.19.184.6 port 42258 ssh2
    Jun 06 13:45:24 router sshd[2487]: Failed password for root from 234.19.184.6 port 42258 ssh2
    Jun 06 13:45:35 router sshd[2487]: Failed password for root from 234.19.184.6 port 38834 ssh2
    Jun 06 13:45:48 router sshd[2487]: Failed password for root from 234.19.184.6 port 35444 ssh2

From here, you can either use `whois` to locate the attacker and work with the ISP to block them, or simply use your UFW firewall to block them directly.

As mentioned, this subject is far too complex for a detailed tutorial in this section.  For more information, try the [Ubuntu journalctl manpage](https://manpages.ubuntu.com/manpages/journalctl.1.html)`↗` or [another, similar source ](https://www.digitalocean.com/community/tutorials/how-to-use-journalctl-to-view-and-manipulate-systemd-logs)`↗`.  

** How to implement PostgreSQL security

PostgreSQL contains secrets, and should be encrypted for maximum protection.  You should consider [full disk encryption ](https://help.ubuntu.com/community/Full_Disk_Encryption_Howto_2019)`↗`.  Also recommended is [TLS encryption between MAAS and PostgreSQL ](https://www.postgresql.org/docs/current/ssl-tcp.html)`↗`.

** Other things you can do to harden MAAS

In addition to the items mentioned above, you should be aware of a few other ways to harden MAAS.

*** Good passwords

You should pick good passwords and store them securely (e.g. in a KeePassX password database). Perform user administration only via the web UI. Only share the `maas` and `root` user passwords with administrators.

*** File permissions

MAAS configuration files should be set to have permission `640`: readable by logins belonging to the `maas` group and writeable only by the `root` user. Currently, the `regiond.conf` file contains the login credentials for the PostgreSQL database used by MAAS to keep track of all machines, networks, and configuration.

[tabs]
[tab version="v3.4 Snap,v3.3 Snap,v3.2 Snap"]
``` bash
chmod 640 /var/snap/maas/current/rackd.conf
chmod 640 /var/snap/maas/current/regiond.conf
```

After:

``` no-highlight
-rw-r----- 1 root maas   90 Sep 27 14:13 rackd.conf
-rw-r----- 1 root maas  157 Sep 27 14:14 regiond.conf
```
*** About snap security

Since snaps are fully confined or "sandboxed," they bring a lot of inherent security to the contained application.  More detailed information can be found in [this snap blog ](https://snapcraft.io/blog/where-eagles-snap-a-closer-look)`↗`.
[/tab]
[tab version="v3.4 Packages,v3.3 Packages,v3.2 Packages"]
``` bash
chmod 640 /etc/maas/rackd.conf
chmod 640 /etc/maas/regiond.conf
```

After:

``` no-highlight
-rw-r----- 1 root maas   90 Sep 27 14:13 rackd.conf
-rw-r----- 1 root maas  157 Sep 27 14:14 regiond.conf
```
[/tab]
[tab version="v3.1 Snap"]
``` bash
chmod 640 /var/snap/maas/current/rackd.conf
chmod 640 /var/snap/maas/current/regiond.conf
```

After:

``` no-highlight
-rw-r----- 1 root maas   90 Sep 27 14:13 rackd.conf
-rw-r----- 1 root maas  157 Sep 27 14:14 regiond.conf
```
*** About snap security

Since snaps are fully confined or "sandboxed," they bring a lot of inherent security to the contained application.  More detailed information can be found in [this snap blog ](https://snapcraft.io/blog/where-eagles-snap-a-closer-look)`↗`.
[/tab]
[tab version="v3.1 Packages"]
``` bash
chmod 640 /etc/maas/rackd.conf
chmod 640 /etc/maas/regiond.conf
```

After:

``` no-highlight
-rw-r----- 1 root maas   90 Sep 27 14:13 rackd.conf
-rw-r----- 1 root maas  157 Sep 27 14:14 regiond.conf
```
[/tab]
[tab version="v3.0 Snap"]
``` bash
chmod 640 /var/snap/maas/current/rackd.conf
chmod 640 /var/snap/maas/current/regiond.conf
```

After:

``` no-highlight
-rw-r----- 1 root maas   90 Sep 27 14:13 rackd.conf
-rw-r----- 1 root maas  157 Sep 27 14:14 regiond.conf
```
*** About snap security

Since snaps are fully confined or "sandboxed," they bring a lot of inherent security to the contained application.  More detailed information can be found in [this snap blog ](https://snapcraft.io/blog/where-eagles-snap-a-closer-look)`↗`.
[/tab]
[tab version="v3.0 Packages"]
``` bash
chmod 640 /etc/maas/rackd.conf
chmod 640 /etc/maas/regiond.conf
```

After:

``` no-highlight
-rw-r----- 1 root maas   90 Sep 27 14:13 rackd.conf
-rw-r----- 1 root maas  157 Sep 27 14:14 regiond.conf
```
[/tab]
[tab version="v2.9 Snap"]
``` bash
chmod 640 /var/snap/maas/current/rackd.conf
chmod 640 /var/snap/maas/current/regiond.conf
```

After:

``` no-highlight
-rw-r----- 1 root maas   90 Sep 27 14:13 rackd.conf
-rw-r----- 1 root maas  157 Sep 27 14:14 regiond.conf
```
*** About snap security

Since snaps are fully confined or "sandboxed," they bring a lot of inherent security to the contained application.  More detailed information can be found in [this snap blog ](https://snapcraft.io/blog/where-eagles-snap-a-closer-look)`↗`.
[/tab]
[tab version="v2.9 Packages"]
``` bash
chmod 640 /etc/maas/rackd.conf
chmod 640 /etc/maas/regiond.conf
```

After:

``` no-highlight
-rw-r----- 1 root maas   90 Sep 27 14:13 rackd.conf
-rw-r----- 1 root maas  157 Sep 27 14:14 regiond.conf
```
[/tab]
[/tabs]

*** Shared secrets

When you add a new rack or region controller, MAAS asks for a shared secret it will use to communicate with the rest of MAAS. This secret is also exposed in the web UI when you click the 'Add rack controller' button on the Controllers page.  MAAS automatically generates this secret when your first region controller installed, and stores the secret in a plain text file.  This file is automatically protected with the correct permissions, so there is no need for any action on your part.

** Whom to contact for MAAS security consulting

If you need help implementing MAAS security, please [contact us](/t/how-to-contact-us/5448).  We will be happy to assist you in arranging security consulting appropriate to your needs.

* How to install and use RBAC with MAAS

Prologue text that explains what this document is about.

** How to set up identity management for RBAC 

As a MAAS RBAC administrator, you want to properly set up the environment for Candid and RBAC to be used. As an RBAC administrator, you need step-by-step procedures that guide you through pre-installation configuration.

*** How to set up and install identity management

*** How to install and enable Candid

A user has detailed instructions for installing and enabling Candid. Provide detailed instructions about Candid installation.

*** How to configure and load Candid

A user has detailed instructions for configuring and loading Candid. Provide detailed instructions for configuring Candid via /var/snap/candid/current/config.yaml, starting Candid, editing /root/admin.keys to have the correct URL for Candid, and creating the necessary credentials, including a self-signed certificate for Candid.

*** How to verify Candid operation

A user has detailed procedures for verifying that a compatible authentication system is installed and working. Provide detailed instructions about which authentication systems are compatible with RBAC, how they must be configured, and how to verify that they are properly configured.  

*** How to ensure proper certificates

A user has detailed instructions for ensuring that valid certificates can be generated and used as needed for Candid to communicate over HTTPS. Provide detailed instructions regarding the CA and certificates needed, as well as instructions for verifying that they are properly configured.

** How to install and configure RBAC

As a MAAS RBAC administrator, you want to properly install and configure RBAC and Candid. As an RBAC administrator, you need step-by-step procedures that guide you through the process of installing RBAC and Candid.

*** How to install RBAC

Provide detailed instructions for obtaining PPA credentials, including necessary contact information and/or links.

*** How to set up RBAC

Provide indirect instructions on how to login to Launchpad, visit your private PPA, set the necessary credentials, add your private PPA, and install the RBAC package.

*** How to log into RBAC and verify RBAC setup

Provide detailed instructions about how to pass the Candid cert to RBAC; define RBAC cert, public key, and CA via SSL; create the appropriate credentials; configure RBAC to use these credentials; verify correct operation of the service-URL; and create an RBAC administrative user.

** How to connect MAAS and RBAC

As a MAAS RBAC administrator, you want to properly connect MAAS to RBAC and establish baseline test users. As an RBAC administrator, you need step-by-step procedures that guide you through the process of making MAAS talk to RBAC successfully.

*** How to configure MAAS with RBAC

Provide detailed instructions about how to configure MAAS to use RBAC, including defining the service name and URL of the RBAC service, along with a login procedure that verifies that the RBAC-maas connection is working correctly.

*** How to use resource pools

Provide detailed instructions for creating new resource pools, or provide a procedure for using existing resource pools.

*** How to create test users

Provide detailed instructions for creating test users, or provide a procedure for using existing test users already established.

*** How to assign test users

Provide instructions on how to associate test users with resource pools.

*** How to verify RBAC with MAAS

Provide detailed instructions for using the RBAC test cases created thus far to verify that RBAC is working correctly with MAAS.

** How to operate RBAC

As a MAAS RBAC administrator, you want to understand how to operate and maintain RBAC. As an RBAC administrator, you need step-by-step procedures that guide you through the various day-to-day aspects of RBAC components, services, groups, roles, and permissions.

*** How to configure services

Provide detailed instructions about MAAS services, as exposed in RBAC, and how they can be manipulated and configured

*** How to configure scopes

Provide detailed instructions about scopes, and how they should be configured and applied

*** How to assign roles

Provide detailed instructions about the way RBAC tabulates roles, services, etc, including the way the hierarchy flows from left to right, how to change views, and how to manipulate items in those views

*** How to set permissions

Provide detailed instructions about the way RBAC tabulates roles, services, etc, including the way the hierarchy flows from left to right, how to change views, and how to manipulate items in those views

*** How to navigate RBAC

Provide detailed instructions about the way RBAC tabulates roles, services, etc, including the way the hierarchy flows from left to right, how to change views, and how to manipulate items in those views

*** How to modify RBAC users

Provide detailed procedures for managing the relationships between roles, permissions, users, groups, and resource pools, including how to add, edit, remove, and reassign them; explain how changes are “buffered”, ie, not implemented until a separate choice is made

*** How to deal with non-machine-related attributes

Provide detailed instructions for address the various non-machine-related components of MAAS that are exposed in RBAC

** How to audit user actions

As a MAAS RBAC administrator, operator, or auditor, you want to understand how to use RBAC logging to review user actions As an RBAC administrator, operator, or auditor, you need step-by-step procedures that guide you through the process of reviewing, analyzing, and correlating log entries

*** How to access RBAC logs 

Provide detailed instructions about RBAC logs, how to find them, how to access them, and how to export them

*** How to post-process RBAC logs

Provide detailed instructions for RBAC logs use built-in filter capability. Provide suggestions and examples for analyz RBAC logs us command line tools on exported log files

*** How to correlate actions, events, and users

Provide detailed procedures for correlating user actions and events with the correct users, roles, and permissions
* How to label devices
MAAS offers a wide range of device labelling: tags that can carry information; static annotations which can identify and describe machines; and dynamic annotations which can describe current workloads for deployed machines.

** [Tag machines](/t/how-to-tag-machines/5928)

The most common tags are machine tags, which can not only help you sort machines, but also help to configure that machine, e.g., with kernel options, when being deployed.

** [Annotate machines](/t/how-to-annotate-machines/5929)

You can also annotate machines.  Static annotations stick with a machine as long as it exists.  Dynamic annotations, on the other hand, last only as long as a specific deployment.

** [Use machine tags](/t/how-to-use-machine-tags/5224)

There are quite a few nuances to machine tags.

** [Use controller tags](/t/how-to-use-controller-tags/5216)

You can also tag controllers to help easily tell them apart.

** [Use storage tags](/t/how-to-use-storage-tags/5232)

Storage tags help you mark and remember specialized storage configurations.

** [Use network tags](/t/how-to-use-network-tags/5228)

Network tags let you remember how you set up specific network interfaces.

* How-to guides
The how-to guides in this section give directions on how to achieve your goals when configuring, managing or using MAAS.

|                                                                          |                                                                                   |
|:-------------------------------------------------------------------------|-----------------------------------------------------------------------------------|
| [How to get started with MAAS](/t/how-to-get-started-with-maas/6202)     | Instructions for installing and upgrading MAAS, including Ansible playbooks       |
| [How to set up networks](/t/how-to-set-up-networks/6742)                 | Instructions for connecting MAAS to your machines and setting up network services |
| [How to use images](/t/how-to-use-images/6192)                           | Instructions for choosing, downloading, customising, and refreshing OS images     |
| [How to manage controllers](/t/how-to-manage-controllers/6498)           | Instructions for adding controllers and creating high-availability configurations |
| [How to manage machines](/t/how-to-manage-machines/6193)                 | Instructions for managing machines and walking them through their lifecycle       |
| [How to manage virtual machines](/t/how-to-manage-virtual-machines/5148) | Instructions for managing, hosting, and connecting virtual machines               |
| [How to label devices](/t/how-to-label-devices/6200)                     | Instructions for labelling machines for yourself and others                       |
| [How to secure MAAS](/t/how-to-secure-maas/6503)                         | Instructions for creating and maintaining a secure MAAS instance                  |
| [How to operate MAAS](/t/how-to-operate-maas/6799)                       | Instructions for the general operation of MAAS                                    |
| [How to give and receive help](/t/how-to-give-and-receive-help/5428)     | Instructions for getting help, support, and technical advice about MAAS           |
|                                                                          |                                                                                   |

Make sure to also check out the [Tutorials](/t/tutorials/6140) for step-by-step instructions that help you get familiar with MAAS, as well as the [Reference](/t/reference/6143) and [Explanation](/t/explanation/6141) sections for other helpful information.

* How to make machines available
This article explains:

[tabs]
[tab version="v3.4 Snap,v3.4 Packages,v3.3 Snap,v3.3 Packages,v3.2 Snap,v3.2 Packages,v3.1 Snap,v3.1 Packages" view="UI"]
- [How to create, delete, and configure machines](#heading--how-to-create-delete-and-configure-machines)
- [How to clone machines](#heading--how-to-clone-machines)
- [How to examine machines and machine details](#heading--how-to-examine-machines-and-machine-details)
- [How to use resource pools](#heading--how-to-use-resource-pools)
- [How to use machine storage](#heading--how-to-use-machine-storage)
[/tab]
[tab version="v3.0 Snap,v3.0 Packages,v2.9 Snap,v2.9 Packages" view="UI"]
- [How to create, delete, and configure machines](#heading--how-to-create-delete-and-configure-machines)
- [How to examine machines and machine details](#heading--how-to-examine-machines-and-machine-details)
- [How to use resource pools](#heading--how-to-use-resource-pools)
- [How to use machine storage](#heading--how-to-use-machine-storage)
[/tab]
[tab version="v3.4 Snap,v3.4 Packages,v3.3 Snap,v3.3 Packages,v3.2 Snap,v3.2 Packages,v3.1 Snap,v3.1 Packages" view="CLI"]
- [How to create, delete, and configure machines](#heading--how-to-create-delete-and-configure-machines)
- [How to clone machines](#heading--how-to-clone-machines)
- [How to examine machines and machine details](#heading--how-to-examine-machines-and-machine-details)
- [How to use resource pools](#heading--how-to-use-resource-pools)
- [How to use annotations](#heading--how-to-use-annotations)
- [How to use machine storage](#heading--how-to-use-machine-storage)
[/tab]
[tab version="v3.0 Snap,v3.0 Packages,v2.9 Snap,v2.9 Packages" view="CLI"]
- [How to create, delete, and configure machines](#heading--how-to-create-delete-and-configure-machines)
- [How to examine machines and machine details](#heading--how-to-examine-machines-and-machine-details)
- [How to use resource pools](#heading--how-to-use-resource-pools)
- [How to use annotations](#heading--how-to-use-annotations)
- [How to use machine storage](#heading--how-to-use-machine-storage)
[/tab]
[/tabs]

This article also provides a [storage layouts reference](#heading--storage-layouts-reference).

Most of the day-to-day work of managing machines is covered here. Utilising machines to do work -- for example, commissioning, testing, and deploying them -- is discussed in [How to deploy machines](/t/how-to-put-machines-to-work/5112).

** How to create, delete, and configure machines

This section shows you:

[tabs]
[tab version="v3.4 Snap,v3.4 Packages,v3.3 Snap,v3.3 Packages,v3.2 Snap,v3.2 Packages,v3.1 Snap,v3.1 Packages,v3.0 Snap,v3.0 Packages"]
- [How to add a machine manually](#heading--how-to-add-a-machine-manually)
- [How to add machines via a chassis](#heading--how-to-add-machines-via-a-chassis)
- [How to manage attached USB and PCI devices](#heading--usb-pci-devices)
[/tab]
[tab version="v2.9 Snap,v2.9 Packages"]
- [How to add a machine manually](#heading--how-to-add-a-machine-manually)
- [How to add machines via a chassis](#heading--how-to-add-machines-via-a-chassis)
[/tab]
[/tabs]

*** How to add a machine manually

[tabs]
[tab version="v3.4 Snap,v3.4 Packages" view="UI"]
To add a machine manually,

1. Select *Machines*.

2. Select *Add hardware > Machine.*

3. Fill in the form and select *Save machine* to register your choice.

4. Alternatively, you can select *Save and add another* to register your choice and repeat the form to add another machine.

The fields on the "Add machine" screen include the following items:

- **Machine name**: This field is used to identify the machine to the user.  It can be set to anything, though it is often set to the MAC address of the machine in question.  This field is optional, in that MAAS will assign a unique, nonsense name if you leave it blank.  You can change this nonsense name later, if desired.

- **Domain**: This field sets the domain name of the domain managed by MAAS.  It can be set to anything; MAAS assigns the domain name "maas" by default.

- **Architecture**: This field refers to the architecture of the machine being added.

- **Minimum Kernel**: This field supplies a drop-down of possible kernels available for deployment on this machine.

- **Zone**: This field allows you to set the availability zone, selected from AZs that you have already created (if any).

- **Resource pool**: This field allows you to set the resource pool for this machine, selected from pools you have already created (if any).

- **MAC Address**: You should fill in this field with the MAC address of the machine you are adding.  Note that the MAC address entered here must use a colon (":") separator, although some MAC addresses are written with dash ("-") separators.

- **Power type**: You must select the power type supported by the machine you are adding, and fill in additional required fields that appear.  See [Power management reference](/t/power-management-reference/5246) for details on the available power types and the relevant parameters for each type.

*** How to add machines via a chassis

You can use the chassis feature to add multiple machines at once. To do this, instead of selecting *Machine* as above, choose *Chassis* from the drop-down menu.  The required fields will change based on the type of chassis you choose.

[note]
As with the manual method, the underlying machines will require netbooting.
[/note]
[/tab]
[tab version="v3.3 Snap,v3.3 Packages,v3.2 Snap,v3.2 Packages,v3.1 Snap,v3.1 Packages,v3.0 Snap,v3.0 Packages,v2.9 Snap,v2.9 Packages" view="UI"]
On the 'Machines' page of the web UI, click the 'Add hardware' button and then select 'Machine'.

Fill in the form and hit 'Save machine'. In this example, you are adding an IPMI machine:

<a href="https://discourse.maas.io/uploads/default/original/1X/faebe2fb37cd73252eaf9521ed1bcf31fb0e76f6.jpeg" target = "_blank"><img src="https://discourse.maas.io/uploads/default/original/1X/faebe2fb37cd73252eaf9521ed1bcf31fb0e76f6.jpeg"></a>

The fields on the "Add machine" screen include the following items:

- **Machine name**: This field is used to identify the machine to the user.  It can be set to anything, though it is often set to the MAC address of the machine in question.  This field is optional, in that MAAS will assign a unique, nonsense name if you leave it blank.  You can change this nonsense name later, if desired.

- **Domain**: This field sets the domain name of the domain managed by MAAS.  It can be set to anything; MAAS assigns the domain name "maas" by default.

- **Architecture**: This field refers to the architecture of the machine being added.

- **Minimum Kernel**: This field supplies a drop-down of possible kernels available for deployment on this machine.

- **Zone**: This field allows you to set the availability zone, selected from AZs that you have already created (if any).

- **Resource pool**: This field allows you to set the resource pool for this machine, selected from pools you have already created (if any).

- **MAC Address**: You should fill in this field with the MAC address of the machine you are adding.  Note that the MAC address entered here must use a colon (":") separator, although some MAC addresses are written with dash ("-") separators.

- **Power type**: You must select the power type supported by the machine you are adding, and fill in additional required fields that appear.  See [Power management reference](/t/power-management-reference/5246) for details on the available power types and the relevant parameters for each type.

*** How to add machines via a chassis

You can use the chassis feature to add multiple machines at once. To do this, instead of selecting 'Machine' as above, choose 'Chassis' from the drop-down menu. In the following example, MAAS will add all available VMs from the given  virsh address:

<a href="https://discourse.maas.io/uploads/default/original/1X/e7f88bce68318cf3c6a8e97b4d31d0b6980e0f32.jpeg" target = "_blank"><img src="https://discourse.maas.io/uploads/default/original/1X/e7f88bce68318cf3c6a8e97b4d31d0b6980e0f32.jpeg"></a>

The required fields will change based on the type of chassis you choose.

[note]
As with the manual method, the underlying machines will require netbooting.
[/note]

[/tab]
[tab version="v3.4 Snap,v3.4 Packages,v3.3 Snap,v3.3 Packages,v3.2 Snap,v3.2 Packages,v3.1 Snap,v3.1 Packages,v3.0 Snap,v3.0 Packages,v2.9 Snap,v2.9 Packages" view="CLI"]
To create a new machine at the command line, enter the following information:

```nohighlight
stormrider@wintermute:~$ maas admin machines create \
> architecture=$ARCH \
> max_addresses=$MAC_ADDRESS \
> power_type=$POWER_TYPE \
> power_parameters_power_id=$POWER_ID \
> power_parameters_power_address=$POWER_ADDRESS \
> power_parameters_power_pass=$POWER_PASSWORD
```

When you enter the command (substituting the `$...` parameters for your own particulars), the screen will pause for a moment, and then return a stream of JSON relating to the added machine.

Here's an example with a local laptop MAAS install, using KVMs as virtual machines:

```nohighlight
stormrider@wintermute:~$ maas admin machines create \
> architecture=amd64 \
> max_addresses=52:54:00:6f:b4:af \
> power_type=virsh \
> power_parameters_power_id=50f6cca2-5d89-43b9-941c-90c9fcd7c156 \
> power_parameters_power_address=qemu+ssh://stormrider@192.168.123.1/system \
> power_parameters_power_pass=xxxxxxx
```

The variable fields in the `machines create` command (the `$...` items) are as follows, in this example: 

```nohighlight
> architecture=$ARCH \
> mac_addresses=$MAC_ADDRESS \
> power_type=$POWER_TYPE \
> power_parameters_power_id=$POWER_ID \
> power_parameters_power_address=$POWER_ADDRESS \
> power_parameters_power_pass=$POWER_PASSWORD
```

- `$ARCH`: This field refers to the architecture of the machine being added, `amd64` in the local laptop example.

- `$MAC_ADDRESS`: This is the MAC address of the boot-enabled NIC for the machine being added.  Note that the MAC address entered here must use a colon (":") separator, although some MAC addresses are written with dash ("-") separators.

- `$POWER_TYPE`: You must select the power type supported by the machine you are adding, and fill in additional required fields that appear.  See [Power management reference](/t/power-management-reference/5246) for details on the available power types and the relevant parameters for each type. In this example, we've used a "virsh" power type (a libvirt KVM), but your choice will depend on your hardware.

- `$POWER_ID`: This is generally the UUID of the machine being added.

- `$POWER_ADDRESS/$POWER_PASSWORD`: In the case of a KVM, these are the only parameters that need to be entered.  See [Power types](https://maas.io/docs/api#power-types)`↗` in the API reference for details on the available power types and the relevant parameters for each type.
[/tab]
[/tabs]

[tabs]
[tab version="v3.4 Snap,v3.4 Packages,v3.3 Snap,v3.3 Packages,v3.2 Snap,v3.2 Packages,v3.1 Snap,v3.1 Packages,v3.0 Snap,v3.0 Packages"]
*** How to manage attached USB and PCI devices

To delete PCI/USB devices from the machine in any machine state, via the CLI only, using the following command:

```nohighlight
maas $PROFILE node-device delete $SYSTEM_ID $DEVICE_ID
```

where:

- $PROFILE   = your user profile (e.g., "admin")
- $SYSTEM_ID = the ID of the machine in question (e.g., "ngx7ry")
- $DEVICE_ID = the ID of the device you want to delete 

If the device is still present in the system, it will be recognised again (and thus "recreated") when the machine is commissioned again.
[/tab]
[tab version="v2.9 Snap,v2.9 Packages"]
MAAS version 2.9 does not recognise PCI and USB devices.  Please upgrade to MAAS version 3.0 or greater to obtain this capability.
[/tab]
[/tabs]

[tabs]
[tab version="v3.4 Snap,v3.4 Packages,v3.3 Snap,v3.3 Packages,v3.2 Snap,v3.2 Packages,v3.1 Snap,v3.1 Packages"]
** How to clone machines

1. Select *Machines*.

2. Checkbox the machine you want to overwrite with a cloned copy of another machine.

3. Select *Actions > Clone from*.

4. *Select the source machine*.

5. *Select what to clone*.

6. Select *Clone to machine*.
[/tab]
[tab version="v3.3 Snap,v3.3 Packages,v3.2 Snap,v3.2 Packages,v3.1 Snap,v3.1 Packages"]
** How to clone machines

Assume you have two machines available, like this:

<a href="https://discourse.maas.io/uploads/default/original/2X/6/6f662618011e3eb1f8e0bfe85748825db4a6ac25.png" target = "_blank"><img src="https://discourse.maas.io/uploads/default/original/2X/6/6f662618011e3eb1f8e0bfe85748825db4a6ac25.png"></a>

Select the machine *to which you want to clone configuration*, and select "Clone from..."

<a href="https://discourse.maas.io/uploads/default/original/2X/b/b4e42a59f1d4bc6d63f2cd24d77316eea3aada1b.png" target = "_blank"><img src="https://discourse.maas.io/uploads/default/original/2X/b/b4e42a59f1d4bc6d63f2cd24d77316eea3aada1b.png"></a>

Under "1. Select the source machine" -- choose a machine from the attached list:

<a href="https://discourse.maas.io/uploads/default/original/2X/2/287bbf3db4bbc3253a976ecde8965c341fc1bee3.png" target = "_blank"><img src="https://discourse.maas.io/uploads/default/original/2X/2/287bbf3db4bbc3253a976ecde8965c341fc1bee3.png"></a>

Under "2. Select what to clone", choose "Network", "Storage", or both (here, we've chosen "Storage"):

<a href="https://discourse.maas.io/uploads/default/original/2X/6/622afe3c0bcd4775ef4c19460cf0f1f480c11efb.png" target = "_blank"><img src="https://discourse.maas.io/uploads/default/original/2X/6/622afe3c0bcd4775ef4c19460cf0f1f480c11efb.png"></a>

Click "Clone to machine". MAAS will report the status of the attempt.

[/tab]
[tab version="v3.0 Snap,v3.0 Packages,v2.9 Snap,v2.9 Packages"]
Cloning capability is available in MAAS from version 3.1 forward.
[/tab]
[/tabs]

** How to examine machines and machine details

This section describes the various ways you can evaluate the health and status of your MAAS machines, using the machine list.  It will show you:

- [How to view the machine list](#heading--how-to-view-the-machine-list)
- [How to view machine details](#heading--how-to-view-machine-details)
- [How to find network info for a machine](#heading--machine-interfaces)
- [How to find storage info for a machine](#heading--how-to-find-machine-storage-info)
- [How to find commissioning logs](#heading--commissioning-log)
- [How to find machine hardware & test logs](#heading--hardware-tests)
- [How to find raw log output for a machine](#heading--raw-log-output)
- [How to find a machine's event logs](#heading--event-logs)
- [How to find machine configuration info](#heading--machine-config)

[tabs]
[tab version="v3.4 Snap,v3.4 Packages,v3.3 Snap,v3.3 Packages,v3.2 Snap,v3.2 Packages,v3.1 Snap,v3.1 Packages,v3.0 Snap,v3.0 Packages,v2.9 Snap,v2.9 Packages" view="CLI"]
*** How to view the machine list

To view the machine list, enter a command similar to this one:

```nohighlight
maas $PROFILE machines read | jq -r '(["HOSTNAME","SYSID",
"POWER","STATUS","OWNER", "TAGS", "POOL","VLAN","FABRIC",
"SUBNET"] | (., map(length*"-"))),(.[] | [.hostname, .system_id, 
.power_state, .status_name, .owner // "-",.tag_names[0] // "-", 
.pool.name,.boot_interface.vlan.name,.boot_interface.vlan.fabric,
.boot_interface.links[0].subnet.name]) | @tsv' | column -t
```

This will return a relatively compact machine listing:

```nohighlight
HOSTNAME      SYSID   POWER  STATUS     OWNER  TAGS                 POOL     VLAN      FABRIC    SUBNET
 --------      -----   -----  ------     -----  ----                 ----     ----      ------    ------
 lxd-vm-1      r8d6yp  off    Deployed   admin  pod-console-logging  default  untagged  fabric-1  10.124.141.0/24
 lxd-vm-2      tfftrx  off    Allocated  admin  pod-console-logging  default  untagged  fabric-1  10.124.141.0/24
 lxd-vm-3      grwpwc  off    Ready      -      pod-console-logging  default  untagged  fabric-1  10.124.141.0/24
 lxd-vm-4      6s8dt4  off    Deployed   admin  pod-console-logging  default  untagged  fabric-1  10.124.141.0/24
 lxd-vm-5      pyebgm  off    Allocated  admin  pod-console-logging  default  untagged  fabric-1  10.124.141.0/24
 lxd-vm-6      ebnww6  off    New        -      pod-console-logging  default  untagged  fabric-1  
 libvirt-vm-1  m7ffsg  off    Ready      -      pod-console-logging  default  untagged  fabric-1  10.124.141.0/24
 libvirt-vm-2  kpawad  off    Ready      -      pod-console-logging  default  untagged  fabric-1  10.124.141.0/24
 libvirt-vm-3  r44hr6  error  Ready      -      pod-console-logging  default  untagged  fabric-1  10.124.141.0/24
 libvirt-vm-4  s3sdkw  off    Ready      -      pod-console-logging  default  untagged  fabric-1  10.124.141.0/24
 libvirt-vm-5  48dg8m  off    Ready      -      pod-console-logging  default  untagged  fabric-1  10.124.141.0/24
 libvirt-vm-6  bacx77  on     Deployed   admin  pod-console-logging  default  untagged  fabric-1  10.124.141.0/24
 ```
 
You can view various details for your machines, by changing the `jq` command as desired:

```nohighlight
maas admin machines read | jq -r '(["HOSTNAME","SYSID","POWER","STATUS",
"OWNER", "TAGS", "POOL", "VLAN","FABRIC","SUBNET"] | (., map(length*"-"))),
(.[] | [.hostname, .system_id, .power_state, .status_name, .owner // "-", 
.tag_names[0] // "-", .pool.name,
.boot_interface.vlan.name, .boot_interface.vlan.fabric,
.boot_interface.links[0].subnet.name]) | @tsv' | column -t
```

This command, for example, will display machine power status, life-cycle status, and various networking parameters:

```nohighlight
HOSTNAME      SYSID   POWER  STATUS     OWNER  TAGS                 POOL     VLAN      FABRIC    SUBNET
--------      -----   -----  ------     -----  ----                 ----     ----      ------    ------
lxd-vm-1      r8d6yp  off    Deployed   admin  pod-console-logging  default  untagged  fabric-1  10.124.141.0/24
lxd-vm-2      tfftrx  off    Allocated  admin  pod-console-logging  default  untagged  fabric-1  10.124.141.0/24
lxd-vm-3      grwpwc  off    Ready      -      pod-console-logging  default  untagged  fabric-1  10.124.141.0/24
lxd-vm-4      6s8dt4  off    Deployed   admin  pod-console-logging  default  untagged  fabric-1  10.124.141.0/24
lxd-vm-5      pyebgm  off    Allocated  admin  pod-console-logging  default  untagged  fabric-1  10.124.141.0/24
lxd-vm-6      ebnww6  off    New        -      pod-console-logging  default  untagged  fabric-1  
libvirt-vm-1  m7ffsg  off    Ready      -      pod-console-logging  default  untagged  fabric-1  10.124.141.0/24
libvirt-vm-2  kpawad  off    Ready      -      pod-console-logging  default  untagged  fabric-1  10.124.141.0/24
libvirt-vm-3  r44hr6  error  Ready      -      pod-console-logging  default  untagged  fabric-1  10.124.141.0/24
libvirt-vm-4  s3sdkw  off    Ready      -      pod-console-logging  default  untagged  fabric-1  10.124.141.0/24
libvirt-vm-5  48dg8m  off    Ready      -      pod-console-logging  default  untagged  fabric-1  10.124.141.0/24
libvirt-vm-6  bacx77  on     Deployed   admin  pod-console-logging  default  untagged  fabric-1  10.124.141.0/24
```
*** How to view machine details

To view output similar to the MAAS UI machine details page, execute the following shell script:

```nohighlight
#!/bin/nohighlight

maas admin machine read r3rd6h | jq '.' > /tmp/machine-json
cat /tmp/machine-json | jq -r '([.hostname,.status_name,"| Power:",.power_state,"| Kernel:",.hwe_kernel,"| Owner:",.owner]) | @tsv' | column -t -o " " > /tmp/machine-details
cat /tmp/machine-json | jq -r '(["CPU:",.architecture,"/",.cpu_count,"core(s) /",.cpu_speed,"Mhz","| Type:",.hardware_info.cpu_model]) | @tsv' | column -t -o " " >> /tmp/machine-details
cat /tmp/machine-json | jq -r '(["Memory:",.memory,"MB | Storage:",.storage,"MB | Power type:",.power_type]) | @tsv' | column -t -o " " >> /tmp/machine-details
cat /tmp/machine-json | jq -r '(["Zone:",.zone.name,"| Resource pool:",.pool.name,"| Domain:",.domain.name]) | @tsv' | column -t -o " " >> /tmp/machine-details
cat /tmp/machine-json | jq -r '(["Tags:",.tag_names[]]) | @tsv' | column -t -o " " >> /tmp/machine-details
cat /tmp/machine-json | jq -r '(["SYSTEM Vendor:",.hardware_info.system_vendor,"| Product:",.hardware_info.system_product]) | @tsv' | column -t -o " " >> /tmp/machine-details
cat /tmp/machine-json | jq -r '([".......Vsn:",.hardware_info.system_version,"| Serial:",.hardware_info.system_serial]) | @tsv' | column -t -o " " >> /tmp/machine-details
cat /tmp/machine-json | jq -r '(["MAINBOARD Vendor:",.hardware_info.mainboard_vendor,"| Product:",.hardware_info.mainboard_product]) | @tsv' | column -t -o " " >> /tmp/machine-details
cat /tmp/machine-json | jq -r '(["..........Firmware:",.hardware_info.mainboard_firmware_vendor]) | @tsv' | column -t -o " " >> /tmp/machine-details
cat /tmp/machine-json | jq -r '(["..........Firmware Vsn:",.hardware_info.mainboard_firmware_version,"| Date:",.hardware_info.mainboard_firmware_date]) | @tsv' | column -t -o " " >> /tmp/machine-details
cat /tmp/machine-json | jq -r '(["NETWORK: Vendor:",.boot_interface.vendor]) | @tsv' | column -t -o " " >> /tmp/machine-details
cat /tmp/machine-json | jq -r '([".........Name:",.boot_interface.name,"| MAC:",.boot_interface.mac_address,"| Link speed:",.boot_interface.link_speed,"Mbps"]) | @tsv' | column -t -o " " >> /tmp/machine-details
cat /tmp/machine-json | jq -r '([".........Fabric:",.boot_interface.vlan.fabric,"| MAAS DHCP:",.boot_interface.vlan.dhcp_on,"| SR-IOV:",.boot_interface.sriov_max_vf]) | @tsv' | column -t -o " " >> /tmp/machine-details
cat /tmp/machine-details
```

This shell script should produce output similar to the following:

```nohighlight
merry-cobra Deployed | Power: on | Kernel: ga-20.04 | Owner: admin
CPU: amd64/generic / 1 core(s) / 1600 Mhz | Type: Intel(R) Core(TM) i5-8265U CPU
Memory: 2048 MB | Storage: 8000.004096 MB | Power type: lxd
Zone: default | Resource pool: default | Domain: maas
Tags: pod-console-logging virtual
SYSTEM Vendor: QEMU | Product: Standard PC (Q35 + ICH9, 2009)
.......Vsn: pc-q35-5.2 | Serial: Unknown
MAINBOARD Vendor: Canonical Ltd. | Product: LXD
..........Firmware: EFI Development Kit II / OVMF
..........Firmware Vsn: 0.0.0 | Date: 02/06/2015
NETWORK: Vendor: Red Hat, Inc.
.........Name: enp5s0 | MAC: 00:16:3e:cc:17:58 | Link speed: 0 Mbps
.........Fabric: fabric-5 | MAAS DHCP: true | SR-IOV: 0
```
[/tab]
[tab version="v3.4 Snap,v3.4 Packages" view="UI"]
*** How to view the machine list

To view the machine list, simply select *Machines*.

*** How to view machine details

To open a detailed view of a machine's status and configuration:

1. Select *Machines*.

2. Click on a machine name.

*** How to find network info for a machine

To find network info for a specific machine:

1. Select *Machines*.

2. Click on a machine name.

3. Select *Network*.

*** How to find machine storage info

To view/edit machine storage info:

1. Select *Machines*.

2. Click on a machine name.

3. Select *Storage*.

*** How to view PCI devices for a given machine

To view the list of PCI devices associated with a given machine:

1. Select *Machines*.

2. Click on a machine name.

3. Select *PCI devices*

*** How to view USB devices for a given machine

To view the list of USB devices associated with a given machine:

1. Select *Machines*.

2. Click on a machine name.

3. Select *USB*.

*** How to find commissioning logs

To view commissioning logs for a given machine:

1. Select *Machines*.

2. Click on a machine name.

3. Select *Commissioning*.

4. To examine an individual log, select *View details* at the end of the row for that log.

5. To review the commissioning history, select *View previous tests* at the end of the row for that log.

*** How to find machine hardware & test logs

To view commissioning logs for a given machine:

1. Select *Machines*.

2. Click on a machine name.

3. Select *Test*.

4. To examine an individual log, select *View details* at the end of the row for that log.

5. To review the commissioning history, select *View previous tests* at the end of the row for that log.

*** How to find raw log output for a machine

1. Select *Machines*.

2. Click on the machine name.

3. Select *Logs*.

4. Select *Installation output*.

*** How to find a machine's event logs

1. Select *Machines*.

2. Click on the machine name.

3. Select *Logs*.

4. Using the *Download* dropdown, you can download logs in various formats.

*** How to find machine configuration info

1. Select *Machines*.

2. Click on the machine name.

3. Select *Configuration*.
[/tab]
[tab version="v3.3 Snap,v3.3 Packages,v3.2 Snap,v3.2 Packages,v3.1 Snap,v3.1 Packages,v3.0 Snap,v3.0 Packages,v2.9 Snap,v2.9 Packages" view="UI"]
*** How to view the machine list

To view the machine list, select "Machines" on the top menu of the MAAS web UI:

<a href="https://discourse.maas.io/uploads/default/original/1X/19e038dbc6e669bfffc0ea5a9946432a75142bfb.jpeg" target = "_blank"><img src="https://discourse.maas.io/uploads/default/original/1X/19e038dbc6e669bfffc0ea5a9946432a75142bfb.jpeg"></a> 

To quickly view more details, roll the cursor over status icons:

<a href="https://discourse.maas.io/uploads/default/original/1X/8f78a8877a029e7a44bcd4cf3d138499637fe790.jpeg" target = "_blank"><img src="https://discourse.maas.io/uploads/default/original/1X/8f78a8877a029e7a44bcd4cf3d138499637fe790.jpeg"></a> 

*** How to view machine details

To open a detailed view of a machine's status and configuration, click a machine's FQDN or MAC address:

<a href="https://discourse.maas.io/uploads/default/original/2X/a/a8ff4caf6362a3d695682499a74d64cb189dfc37.png" target = "_blank"><img src="https://discourse.maas.io/uploads/default/original/2X/a/a8ff4caf6362a3d695682499a74d64cb189dfc37.png"></a>

*** How to find network info for a machine

To find network info for a specific machine, open that machine's "Network" tab in the machine summary:

<a href="https://discourse.maas.io/uploads/default/original/2X/c/c5316db130ae05a9cdabcd49ffaa69f0bb405d1d.png" target = "_blank"><img src="https://discourse.maas.io/uploads/default/original/2X/c/c5316db130ae05a9cdabcd49ffaa69f0bb405d1d.png"></a> 

Options on this tab are described in the introduction to [Networking](/t/how-to-set-up-networks/6174) article in this documentation set.

*** How to find machine storage info

To view/edit machine storage info, click on the "Storage" tab in the machine summary:

<a href="https://discourse.maas.io/uploads/default/original/2X/6/658f4814716a1347fda62ab799ba0d72506c128e.png" target = "_blank"><img src="https://discourse.maas.io/uploads/default/original/2X/6/658f4814716a1347fda62ab799ba0d72506c128e.png"></a> 

See the section about [storage](/t/how-to-deploy-physical-machines/6193#heading--about-storage) for more information.

*** How to find commissioning logs

The "Commissioning" tab brings up a summary log of commissioning events:

<a href="https://discourse.maas.io/uploads/default/original/2X/e/e98766009f32972dfe29293f9bc850b99a9a941f.png" target = "_blank"><img src="https://discourse.maas.io/uploads/default/original/2X/e/e98766009f32972dfe29293f9bc850b99a9a941f.png"></a> 

Click on the dropdown at the end of the row you're interested in, and click on "View details":

<a href="https://discourse.maas.io/uploads/default/original/2X/8/8f7b4c02ce301fb867e7af33267be62498095bb5.png" target = "_blank"><img src="https://discourse.maas.io/uploads/default/original/2X/8/8f7b4c02ce301fb867e7af33267be62498095bb5.png"></a>

This will bring up a detailed log view for that row:

<a href="https://discourse.maas.io/uploads/default/original/2X/4/41a385cdf948dada8bb8d8f94a3137a2b64d46e0.png" target = "_blank"><img src="https://discourse.maas.io/uploads/default/original/2X/4/41a385cdf948dada8bb8d8f94a3137a2b64d46e0.png"></a>

These logs present an extremely detailed, timestamped record of completion and status items from the commissioning process. See the article on [logging](/t/how-to-work-with-log-files/5240) for more details on how to read and interpret these logs.  

*** How to find machine hardware & test logs

This tab presents a summary of tests run against this particular machine.  You can view the summary report, or choose the "View details" dropdown to get details on any particular tests:

<a href="https://discourse.maas.io/uploads/default/original/2X/e/e53a2c01b57df49e56bb4d95552b6a038249aa97.png" target = "_blank"><img src="https://discourse.maas.io/uploads/default/original/2X/e/e53a2c01b57df49e56bb4d95552b6a038249aa97.png"></a> 

The format of these screens is very similar to the Configuration logs shown above.   

*** How to find raw log output for a machine

By choosing "Installation output" on the "Logs" tab, you can see the "raw" log output:

<a href="https://discourse.maas.io/uploads/default/original/2X/d/dc5bb5e6489a382e257dac605f2dbdc6fa1ca630.png" target = "_blank"><img src="https://discourse.maas.io/uploads/default/original/2X/d/dc5bb5e6489a382e257dac605f2dbdc6fa1ca630.png"></a> 

Help interpreting these logs can be found under the [logging](/t/how-to-work-with-log-files/5240) section of this documentation.

*** How to find a machine's event logs

To view the Event log for a machine, choose the "Event" tab under "Logs."  This displays a list of timestamped status updates for events and actions performed on the machine:

<a href="https://discourse.maas.io/uploads/default/original/2X/9/981a1aced2a4c231fa9e4fe1b70e77aeb816f133.png" target = "_blank"><img src="https://discourse.maas.io/uploads/default/original/2X/9/981a1aced2a4c231fa9e4fe1b70e77aeb816f133.png"></a> 

There is a dropdown on the upper right which allows you to choose how many events per page you wish to view. Detailed discussion of this event log can be found under the [logging](/t/how-to-work-with-log-files/5240) section of this documentation.

*** How to find machine configuration info

The final tab from the Machine menu allows you to update machine and power configuration options: 

<a href="https://discourse.maas.io/uploads/default/original/2X/7/7cfd77228a5cf1a6f779897d501f14fbf78fd4b4.png" target = "_blank"><img src="https://discourse.maas.io/uploads/default/original/2X/7/7cfd77228a5cf1a6f779897d501f14fbf78fd4b4.png"></a> 

There are two sections to this tab.  The "Machine configuration" section, shown above, offers some general parameters, mostly related to how this machine is grouped and categorised.  More information on these options are found in the relevant sections of the documentation (e.g., tags, resource pools, and so forth). 

The "Power configuration" supplies the parameters necessary for MAAS to access the machine to PXE-boot it: 

<a href="https://discourse.maas.io/uploads/default/original/2X/1/198898362285e4a1308535a4aa701156a67c9616.png" target = "_blank"><img src="https://discourse.maas.io/uploads/default/original/2X/1/198898362285e4a1308535a4aa701156a67c9616.png"></a> 

More information on Power configuration will be found in the [Power management](/t/power-management-reference/5246) section of this documentation.

[/tab]
[/tabs]

** How to use resource pools

This section will explain:

[tabs]
[tab version="v3.4 Snap,v3.4 Packages" view="UI"]
- [How to add a resource pool](#heading--add-a-resource-pool)
- [How to delete a resource pool](#heading--deleting-a-resource-pool)
- [How to add a node to a resource pool](#heading--add-a-node-to-a-resource-pool)
- [How to remove a node from a resource pool](#heading--removing-a-node-from-a-resource-pool)
- [How to add a VM host to a resource pool](#heading--add-a-vm-host-to-a-resource-pool)
- [How to remove a VM host from a resource pool](#heading--removing-a-vm-host-from-a-resource-pool)

Administrators can manage resource pools on the Machines page in the web UI, under the Resource pools tab.   Also note that all MAAS installations have a resource pool named "default." MAAS automatically adds new machines to the default resource pool.

*** How to add a resource pool

1. Select *Organisation > Pools > Add pool*.

2. Enter a *Name* and -- optionally -- a *Description*.

3. Select *Save pool* to register your changes.

*** How to delete a resource pool

1. Select *Organisation > Pools*.

2. Select the trashcan icon at the end of the pool's row.

3. Select *Delete*.

[note]
If you delete a resource pool, all machines that belong to that resource pool will return to the default pool.
[/note]

*** How to add a machine to a resource pool

1. Select *Machines*.

2. Select the machine(s) you wish to add to a resource pool.

3. Select *Categorise > Set pool*.

4. Choose *Select pool* and choose a pool from the *Resource pool* dropdown.

5. Alternatively, choose *Create pool*, and create a new pool to assign.

6. Select *Set pool...* to register your changes.

*** How to remove a machine from a resource pool

1. Select *Machines*.

2. Select the machine(s) you wish to add to a resource pool.

3. Select *Categorise > Set pool*.

4. Choose *Select pool* and choose "default" from the *Resource pool* dropdown.

5. Select *Set pool...* to register your changes.

*** How to add a VM host to a resource pool

1. Select *KVM > LXD*.

2. Click on the VM host you wish to add to a resource pool.

3. Select *KVM host settings*.

4. Choose a *Resource pool*.

5. Select *Save changes* to register your changes.

*** How to remove a VM host from a resource pool

1. Select *KVM > LXD*.

2. Click on the VM host you wish to add to a resource pool.

3. Select *KVM host settings*.

4. Choose the "default" *Resource pool*.

5. Select *Save changes* to register your changes.

[/tab]
[tab version="v3.3 Snap,v3.3 Packages,v3.2 Snap,v3.2 Packages,v3.1 Snap,v3.1 Packages,v3.0 Snap,v3.0 Packages,v2.9 Snap,v2.9 Packages" view="UI"]
- [How to add a resource pool](#heading--add-a-resource-pool)
- [How to delete a resource pool](#heading--deleting-a-resource-pool)
- [How to add a node to a resource pool](#heading--add-a-node-to-a-resource-pool)
- [How to remove a node from a resource pool](#heading--removing-a-node-from-a-resource-pool)
- [How to add a VM host to a resource pool](#heading--add-a-vm-host-to-a-resource-pool)
- [How to remove a VM host from a resource pool](#heading--removing-a-vm-host-from-a-resource-pool)

Administrators can manage resource pools on the Machines page in the web UI, under the Resource pools tab.   Also note that all MAAS installations have a resource pool named "default." MAAS automatically adds new machines to the default resource pool.

*** How to add a resource pool

Use the Add pool button to add a new resource pool.

After giving your new pool a name and description, click the Add pool button:

<a href="https://assets.ubuntu.com/v1/2f010325-nodes-resource-pools__2.5_add-pool.png" target = "_blank"><img src="https://assets.ubuntu.com/v1/2f010325-nodes-resource-pools__2.5_add-pool.png"></a>

*** How to delete a resource pool

To delete a resource pool, click the trashcan icon next to the pool.

<a href="https://assets.ubuntu.com/v1/630ed938-nodes-resource-pools__2.5_delete-pool.png" target = "_blank"><img src="https://assets.ubuntu.com/v1/630ed938-nodes-resource-pools__2.5_delete-pool.png"></a>

[note]
If you delete a resource pool, all machines that belong to that resource pool will return to the default pool.
[/note]

*** How to add a machine to a resource pool

To add a machine to a resource pool, on the Machines page, select the machine you want to add to the resource pool. Next, select the Configuration tab. Now select the resource pool and click the Save changes button.

<a href="https://assets.ubuntu.com/v1/648e7a8e-nodes-resource-pools__2.5_add-machine.png" target = "_blank"><img src="https://assets.ubuntu.com/v1/648e7a8e-nodes-resource-pools__2.5_add-machine.png"></a>

*** How to remove a machine from a resource pool

To remove a machine from a resource pool, follow the same procedure you would use to add a machine, but select "default" as the new resource pool. This action will return the machine to the default resource pool.

*** How to add a VM host to a resource pool

You can add a VM host to a resource pool when you create a new VM host, or you can edit a VM host's configuration:

<a href="https://assets.ubuntu.com/v1/84a89952-nodes-resource-pools__2.5_pod_to_pool.png" target = "_blank"><img src="https://assets.ubuntu.com/v1/84a89952-nodes-resource-pools__2.5_pod_to_pool.png"></a>

*** How to remove a VM host from a resource pool

To remove a VM host from a resource pool, follow the same procedure you would use to add a VM host to a resource pool, except select "default" as the new resource pool. This action will return the machine to the default resource pool.

[/tab]
[tab version="v3.4 Snap,v3.4 Packages,v3.3 Snap,v3.3 Packages,v3.2 Snap,v3.2 Packages,v3.1 Snap,v3.1 Packages,v3.0 Snap,v3.0 Packages,v2.9 Snap,v2.9 Packages" view="CLI"]
- [How to create a resource pool](#heading--creating-a-resource-pool)
- [How to list available resource pools](#heading--list-available-resource-pools)
- [How to list a single resource pool](#heading--list-a-single-resource-pool)
- [How to update a resource pool](#heading--update-a-resource-pool)
- [How to delete a resource pool](#heading--delete-a-resource-pool)
- [How to add a machine to a resource pool](#heading--add-a-machine-to-a-resource-pool)

*** How to create a resource pool

Here's an example that demonstrates how to create a new resource pool named `myresource`.

``` nohighlight
maas $PROFILE resource-pools create name=myresource description="A new resource pool."
```

[note]
The `description` field is optional.
[/note]

*** How to list available resource pools

``` nohighlight
maas $PROFILE resource-pools read
```
*** How to list a single resource pool

``` nohighlight
maas $PROFILE resource-pool read $RESOURCE_POOL_ID
```

*** How to update a resource pool

``` nohighlight
maas $PROFILE resource-pool update $RESOURCE_POOL_ID name=newname description="A new description."
```

[note]
The `name` and `description` fields are optional.
[/note]

*** How to delete a resource pool

``` nohighlight
maas $PROFILE resource-pool delete $RESOURCE_POOL_ID
```

*** How to add a machine to a resource pool

``` nohighlight
maas $PROFILE machine update $SYSTEM_ID pool=$POOL_NAME
```

** How to use annotations

This section will show you:

- [How to use static annotations](#heading--work-with-static-annotations)
- [How to use dynamic (workload) annotations](#heading--work-with-dynamic-workload-annotations)

*** How to use static annotations

This subsection will show you how to:

- [How to identify your machines](#heading--identify-your-machines)
- [How to set a static annotation for a machine](#heading--set-a-static-annotation-for-a-machine)
- [How to change or clear a static annotation for a machine](#heading--change-or-clear-a-static-annotation-for-a-machine)
- [How to list static annotations for all machines](#heading--list-static-annotations-for-all-machines)
- [How to view a static annotation for one machine](#heading--view-a-static-annotation-for-one-machine)

**** How to identify your machines

To identify your available machines, use a command like this one:

```nohighlight
maas $PROFILE machines read \
| jq -r '(["hostname","system_id"]
|(.,map(length*"-"))),(.[]|[.hostname,.system_id])
|@tsv' | column -t
```

For example:

```nohighlight
maas admin machines read \
| jq -r '(["hostname","system_id"]
|(.,map(length*"-"))),(.[]|[.hostname,.system_id])
|@tsv' | column -t
```

Typical output might look something like this:

```nohighlight
hostname       system_id
--------       ---------
divine-stork   8b3ypp
casual-prawn   4end6r
driven-teal    tgaat6
immune-beetle  43xand
good-osprey    napfxk
smart-hen      c4rwq7
boss-satyr     xn8taa
golden-martin  8fxery
crack-guinea   qk4b3g
finer-leech    cy3dtr
free-mouse     gxtbq4
humble-bunny   srqnnb
wanted-muskox  ekw7fh
one-boa        by477d
great-urchin   srnx4g
ace-frog       g6arwg
alive-marlin   gbwnfb
picked-parrot  am77wn
tough-kit      ke3wc7
legal-whale    8nq3mt
game-sponge    76pdc6
fun-ghoul      qxfm7k
aware-earwig   8m8hs7
chief-crane    7fapx7
select-tapir   4ascbr
on-slug        snfs8d
polite-llama   dbqd4m
frank-coyote   wcmk48
usable-condor  ed8hmy
still-imp      h6ra6d
```

**** How to set a static annotation for a machine

If you want to set the static annotation for a given machine, you can do so with a command that looks like this:

```nohighlight
maas $PROFILE machine update $SYSTEM_ID description="$STATIC_ANNOTATION"
```

For example:

```nohighlight
maas admin machine update ke3wc7 description="kilo-echo-3-whisky-charlie-7"
```

You can check your work by [viewing the static annotations for one machine](#heading--view-a-static-annotation-for-one-machine).

**** How to change or clear a static annotation for a machine

If you want to set the static annotation for a given machine, use the same command you'd use to set a static annotation:

```nohighlight
maas $PROFILE machine update $SYSTEM_ID description="$STATIC_ANNOTATION"
```

The existing annotation will be overwritten by the new one you enter.  For example:

```nohighlight
maas admin machine update ke3wc7 description=""
```

You can check your work by [viewing the static annotations for one machine](#heading--view-a-static-annotation-for-one-machine).


**** How to list static annotations for all machines

To list static annotations for all machines, enter a command similar to this one:

```nohighlight
maas $PROFILE machines read \
| jq -r '(["hostname","system_id","description"]
|(.,map(length*"-"))),(.[]|[.hostname,.system_id,.description])
|@tsv' | column -t
```

For example:

```nohighlight
maas admin machines read \
| jq -r '(["hostname","system_id","description"]
|(.,map(length*"-"))),(.[]|[.hostname,.system_id,.description])
|@tsv' | column -t
```

Output might look something like this:

```nohighlight
hostname       system_id  description
--------       ---------  -----------
driven-teal    tgaat6     tango-golf
humble-bunny   srqnnb     sierra-romeo
tough-kit      ke3wc7     kilo-echo
```

**** How to view a static annotation for one machine

To view a static annotation for one machine, try a command like this:

```nohighlight
 maas $PROFILE machine read $SYSTEM_ID \
| jq -r '(["hostname","system_id","description"]
|(.,map(length*"-"))),([.hostname,.system_id,.description])
|@tsv' | column -t
```

For example:

```nohighlight
 maas admin machine read tgaat6 \
| jq -r '(["hostname","system_id","description"]
|(.,map(length*"-"))),([.hostname,.system_id,.description])
|@tsv' | column -t
```

A command like this might produce output as follows:

```nohighlight
hostname     system_id  description
--------     ---------  -----------
driven-teal  tgaat6     tango-golf
```

*** How to use dynamic (workload) annotations

This section will show you how to:

- [How to identify machines that can receive dynamic annotations](#heading--identify-machines-that-can-receive-dynamic-annotations)
- [How to set dynamic annotations for a machine](#heading--set-dynamic-annotations-for-a-machine)
- [How to clear and change dynamic annotations for a machine](#heading--clear-and-change-dynamic-annotations-for-a-machine)
- [How to list dynamic annotations for all machines](#heading--list-dynamic-annotations-for-all-machines)
- [How to list dynamic allocations for one machine](#heading--list-dynamic-annotations-for-one-machine)

**** How to identify machines that can receive dynamic annotations

You can only set dynamic annotations for machines that are in the "Allocated" or "Deployed" state.  To identify which of your machines are in these states, you can execute the following command:

```nohighlight
maas $PROFILE machines read \
| jq -r '(["hostname","system_id","status"]
|(.,map(length*"-"))),(.[]|[.hostname,.system_id,.status_name])
|@tsv' | column -t
```

For example:

```nohighlight
maas admin machines read \
| jq -r '(["hostname","system_id","status"]
|(.,map(length*"-"))),(.[]|[.hostname,.system_id,.status_name])
|@tsv' | column -t
```

Output should look something like this:

```nohighlight
hostname       system_id  status
--------       ---------  ------
divine-stork   8b3ypp     Deployed
casual-prawn   4end6r     Ready
driven-teal    tgaat6     Allocated
immune-beetle  43xand     Allocated
good-osprey    napfxk     Allocated
smart-hen      c4rwq7     Allocated
boss-satyr     xn8taa     Ready
golden-martin  8fxery     Allocated
crack-guinea   qk4b3g     Allocated
finer-leech    cy3dtr     Deployed
free-mouse     gxtbq4     Allocated
humble-bunny   srqnnb     Allocated
wanted-muskox  ekw7fh     Deployed
one-boa        by477d     Allocated
great-urchin   srnx4g     Allocated
ace-frog       g6arwg     Ready
alive-marlin   gbwnfb     Deployed
picked-parrot  am77wn     Allocated
tough-kit      ke3wc7     Deployed
legal-whale    8nq3mt     Allocated
game-sponge    76pdc6     Allocated
fun-ghoul      qxfm7k     Allocated
aware-earwig   8m8hs7     Deployed
chief-crane    7fapx7     Ready
select-tapir   4ascbr     Allocated
on-slug        snfs8d     Allocated
polite-llama   dbqd4m     Allocated
frank-coyote   wcmk48     Allocated
usable-condor  ed8hmy     Deployed
still-imp      h6ra6d     Allocated
```

**** How to set dynamic annotations for a machine

Dynamic annotations, otherwise known as "workload annotations" or "owner data," can be used to keep track of the runtime status of machines that are allocated or deployed.  These annotations are set using `key=value` pairs.  You can set any `key=value` pair that you wish for any machine, although it's probably more useful if you standardise your key names.

To set a dynamic annotation for a machine, you can enter a command like this:

```nohighlight
maas $PROFILE machine set-owner-data $SYSTEM_ID $KEY=$VALUE
```

For example:

```nohighlight
maas admin machine set-owner-data tgaat6 owner=gsmith@zorko.com
```

This command will return a JSON string representative of the machine's new configuration, including the dynamic annotations you've added. You can check your work by [listing the dynamic annotations for the one machine](#heading--list-dynamic-annotations-for-one-machine) you just edited, or by [listing dynamic annotations for all machines](#heading--list-dynamic-annotations-for-all-machines).

**** How to clear and change dynamic annotations for a machine

You can change dynamic annotations for a machine simply by executing a new `set-owner-data` command:

```nohighlight
maas $PROFILE machine set-owner-data $SYSTEM_ID $KEY=$NEW_VALUE
```

You can clear a dynamic annotation by entering the empty string (`""`) as the $VALUE:

```nohighlight
maas $PROFILE machine set-owner-data $SYSTEM_ID $KEY=""
```

These commands will return a JSON string representative of the machine's new configuration, including the dynamic annotations you've changed or cleared. You can check your work by [listing the dynamic annotations for the one machine](#heading--list-dynamic-annotations-for-one-machine) you just edited, or by [listing dynamic annotations for all machines](#heading--list-dynamic-annotations-for-all-machines).

**** How to list dynamic annotations for all machines

You can list the current dynamic annotations for all machines with a command like this:

```nohighlight
maas $PROFILE machines read \
| jq -r '(["hostname","system_id","owner_data"]
|(.,map(length*"-"))),(.[]|[.hostname,.system_id,.owner_data[]])
|@tsv' | column -t
```

For example:

```nohighlight
maas admin machines read \
| jq -r '(["hostname","system_id","owner_data"]
|(.,map(length*"-"))),(.[]|[.hostname,.system_id,.owner_data[]])
|@tsv' | column -t
```

This command output might look something like this:

```nohighlight
hostname       system_id  owner_data
--------       ---------  ----------
divine-stork   8b3ypp
casual-prawn   4end6r
driven-teal    tgaat6     farquar     foobar
immune-beetle  43xand
good-osprey    napfxk
smart-hen      c4rwq7
```

**** How to list dynamic allocations for one machine

You can list the dynamic annotations for one machine by entering a command of the form:

```nohighlight
maas $PROFILE machine read $SYSTEM_ID \
| jq -r '(["hostname","system_id","owner_data"]
|(.,map(length*"-"))),(.[]|[.hostname,.system_id,.owner_data[]])
|@tsv' | column -t
```

For example:

```nohighlight
maas admin machine read tgaat6 \
| jq -r '(["hostname","system_id","owner_data"]
|(.,map(length*"-"))),(.[]|[.hostname,.system_id,.owner_data[]])
|@tsv' | column -t
```

This will produce output similar to the following:

```nohighlight
hostname     system_id  owner_data
--------     ---------  ----------
driven-teal  tgaat6     farquar     foobar
```
[/tab]
[/tabs]

** How to use machine storage

This section will explain:

[tabs]
[tab version="v3.4 Snap,v3.4 Packages,v3.3 Snap,v3.3 Packages,v3.2 Snap,v3.2 Packages,v3.1 Snap,v3.1 Packages"]
- [How to set global storage layouts](#heading--how-to-set-global-storage-layouts)
- [How to set per-machine storage layouts](#heading--how-to-set-per-machine-storage-layouts)
- [How to specify conditional erasure types](#heading--how-to-specify-conditional-erasure-types)
- [How to erase disks](#heading--how-to-erase-disks)
- [How to manage block devices](#heading--how-to-manage-block-devices)
- [How to manage partitions](#heading--how-to-manage-partitions)
- [How to manage VMFS datastores](#heading--how-to-manage-vmfs-datastores)
- [How to define custom storage layouts](#heading--how-to-define-custom-storage-layouts)
[/tab]
[tab version="v3.0 Snap,v3.0 Packages,v2.9 Snap,v2.9 Packages"]
- [How to set global storage layouts](#heading--how-to-set-global-storage-layouts)
- [How to set per-machine storage layouts](#heading--how-to-set-per-machine-storage-layouts)
- [How to specify conditional erasure types](#heading--how-to-specify-conditional-erasure-types)
- [How to erase disks](#heading--how-to-erase-disks)
- [How to manage block devices](#heading--how-to-manage-block-devices)
- [How to manage partitions](#heading--how-to-manage-partitions)
- [How to manage VMFS datastores](#heading--how-to-manage-vmfs-datastores)
[/tab]
[/tabs]

*** How to set global storage layouts

Layouts can be set globally and on a per-machine basis.

[tabs]
[tab version="v3.4 Snap,v3.4 Packages,v3.3 Snap,v3.3 Packages,v3.2 Snap,v3.2 Packages,v3.1 Snap,v3.1 Packages,v3.0 Snap,v3.0 Packages,v2.9 Snap,v2.9 Packages" view=UI"]
All machines will have a default layout applied when commissioned. An administrator can configure the default layout on the 'Settings' page, under the 'Storage' tab.

<a href="https://discourse.maas.io/uploads/default/original/1X/80de3bc701552cd00bec707830accf380c214b17.png" target = "_blank"><img src="https://discourse.maas.io/uploads/default/original/1X/80de3bc701552cd00bec707830accf380c214b17.png"></a>
[/tab]
[tab version="v3.4 Snap,v3.4 Packages,v3.3 Snap,v3.3 Packages,v3.2 Snap,v3.2 Packages,v3.1 Snap,v3.1 Packages,v3.0 Snap,v3.0 Packages,v2.9 Snap,v2.9 Packages" view="CLI"]
All machines will have a default layout applied when commissioned. To set the default storage layout for all machines:

```nohighlight
maas $PROFILE maas set-config name=default_storage_layout value=$LAYOUT_TYPE
```

For example, to set the default layout to Flat:

```nohighlight
maas $PROFILE maas set-config name=default_storage_layout value=flat
```

Important: The new default will only apply to newly-commissioned machines.

[/tab]
[/tabs]

[note]
The new default will only apply to newly-commissioned machines.
[/note]

*** How to set per-machine storage layouts

[tabs]
[tab version="v3.4 Snap,v3.4 Packages,v3.3 Snap,v3.3 Packages,v3.2 Snap,v3.2 Packages,v3.1 Snap,v3.1 Packages,v3.0 Snap,v3.0 Packages,v2.9 Snap,v2.9 Packages" view="UI"]
An administrator can change the layout for a single machine as well as customise that layout providing this is done while the machine has a status of 'Ready'. This is only possible via the CLI: to see how, click the "CLI" option for your version and delivery method above.
[/tab]
[tab version="v3.4 Snap,v3.4 Packages,v3.3 Snap,v3.3 Packages,v3.2 Snap,v3.2 Packages,v3.1 Snap,v3.1 Packages,v3.0 Snap,v3.0 Packages,v2.9 Snap,v2.9 Packages" view="CLI"]
An administrator can set a storage layout for a machine with a status of ‘Ready’ like this:

```nohighlight
maas $PROFILE machine set-storage-layout $SYSTEM_ID storage_layout=$LAYOUT_TYPE [$OPTIONS]
```

For example, to set an LVM layout where the logical volume has a size of 5 GB:

```nohighlight
maas $PROFILE machine set-storage-layout $SYSTEM_ID storage_layout=lvm lv_size=5368709120
```

You must specify all storage sizes in bytes.

This action will remove the configuration that may exist on any block device.
[/tab]
[/tabs]

[note]
Only an administrator can modify storage at the block device level (providing the machine has a status of 'Ready').
[/note]

[tabs]
[tab version="v3.4 Snap,v3.4 Packages,v3.3 Snap,v3.3 Packages,v3.2 Snap,v3.2 Packages,v3.1 Snap,v3.1 Packages,v3.0 Snap,v3.0 Packages,v2.9 Snap,v2.9 Packages" view="UI"]
*** How to set the default erasure configuration

A default erasure configuration can be set on the 'Settings' page by selecting the 'Storage' tab.

<a href="https://assets.ubuntu.com/v1/4e90c4c7-installconfig-storage-erasure__defaults.png" target = "_blank"><img src="https://assets.ubuntu.com/v1/4e90c4c7-installconfig-storage-erasure__defaults.png"></a>

If option 'Erase machines' disks prior to releasing' is chosen then users will be compelled to use disk erasure. That option will be pre-filled in the machine's view and the user will be unable to remove the option.

With the above defaults, the machine's view will look like this when the Release action is chosen:

<a href="https://assets.ubuntu.com/v1/66e1dcc2-installconfig-storage-erasure__defaults-node.png" target = "_blank"><img src="https://assets.ubuntu.com/v1/66e1dcc2-installconfig-storage-erasure__defaults-node.png"></a>

Where 'secure erase' and 'quick erase' can then be configured by the user.

[/tab]
[tab version="v3.4 Snap,v3.4 Packages,v3.3 Snap,v3.3 Packages,v3.2 Snap,v3.2 Packages,v3.1 Snap,v3.1 Packages,v3.0 Snap,v3.0 Packages,v2.9 Snap,v2.9 Packages" view="CLI"]

*** How to erase disks

When using the [MAAS CLI](/t/maas-cli/5236), you can erase a disk when releasing an individual machine.  Note that this option is not available when releasing multiple machines, so you'll want to make sure you're using:

```nohighlight
maas $PROFILE machine release...
```

and not:

```nohighlight
maas $PROFILE machines release...
```

Note the difference in singular and plural "machine/machines" in the commands.  Releasing a machine requires that you have the `system_id` of the machine to be released, which you can obtain with a command like this one:

```nohighlight
maas admin machines read | jq -r '(["HOSTNAME","SYSID","POWER","STATUS",
"OWNER", "TAGS", "POOL", "VLAN","FABRIC","SUBNET"] | (., map(length*"-"))),
(.[] | [.hostname, .system_id, .power_state, .status_name, .owner // "-", 
.tag_names[0] // "-", .pool.name,
.boot_interface.vlan.name, .boot_interface.vlan.fabric,
.boot_interface.links[0].subnet.name]) | @tsv' | column -t
```

<a href="https://discourse.maas.io/uploads/default/original/1X/a496ac76977909f3403160ca96a1bb7224e785f5.jpeg" target = "_blank"><img src="https://discourse.maas.io/uploads/default/original/1X/a496ac76977909f3403160ca96a1bb7224e785f5.jpeg">
</a>

The basic form of the release command, when erasing disks on releasing, is:

```nohighlight
maas $PROFILE machine release $SYSTEM_ID comment="some comment" erase=true [secure_erase=true ||/&& quick_erase=true]
```

Parameters `secure_erase` and `quick_erase` are both optional, although if you don't specify either of them, the entire disk will be overwritten with null bytes.  Note that this overwrite process is very slow.

Secure erasure uses the drive's secure erase feature, if it has one.  In some cases, this can be much faster than overwriting the entire drive.  Be aware, though, that some drives implement secure erasure as a complete drive overwrite, so this method may still be very slow.  Additionally, if you specify secure erasure and the drive doesn't have this feature, you'll get a complete overwrite anyway -- again, possibly very slow.

Quick erasure wipes 2MB at the start and end of the drive to make recovery both inconvenient and unlikely to happen by accident.  Note, though, that quick erasure is not secure.

*** How to specify conditional erasure types

If you specify both erasure types, like this:

```nohighlight
maas $PROFILE machine release $SYSTEM_ID comment="some comment" erase=true secure_erase=true quick_erase=true
```

then MAAS will perform a secure erasure if the drive has that feature; if not, it will perform a quick erasure.  Of course, if you're concerned about completely erasing the drive, and you're not sure whether the disk has secure erase features, the best way to handle that is to specify nothing, and allow the full disk to be overwritten by null bytes:

```nohighlight
maas $PROFILE machine release $SYSTEM_ID comment="some comment" erase=true
```

*** How to manage block devices

This section will explain:

- [How to list block devices](#heading--how-to-list-block-devices)
- [How to read a block device](#heading--how-to-read-a-block-device)
- [How to create a block device](#heading--how-to-create-a-block-device)
- [How to update a block device](#heading--how-to-update-a-block-device)
- [How to delete a block device](#heading--how-to-delete-a-block-device)
- [How to format Block Device](#heading--format-block-device)
- [How to unformat a block device](#heading--how-to-unformat-a-block-device)
- [How to mount a block device](#heading--how-to-mount-a-block-device)
- [How to unmount a block device](#heading--how-to-unmount-a-block-device)
- [How to set a block device as a boot disk](#heading--how-to-set-a-block-device-as-a-boot-disk)

**** How to list block devices

To view all block devices on a machine use the read operation. This list both physical and virtual block devices, as you can see in the output from the following command:

``` nohighlight
maas admin block-devices read <node-id>
```

Output:

``` nohighlight
Success.
Machine-readable output follows:
[
    {
        "id": 10,
        "path": "/dev/disk/by-dname/vda",
        "serial": "",
        "block_size": 4096,
        "available_size": 0,
        "resource_uri": "/MAAS/api/2.0/nodes/4y3h8a/blockdevices/10/",
        "filesystem": null,
        "id_path": "/dev/vda",
        "size": 5368709120,
        "partition_table_type": "MBR",
        "model": "",
        "type": "physical",
        "uuid": null,
        "used_size": 5365563392,
        "used_for": "MBR partitioned with 1 partition",
        "partitions": [
            {
                "bootable": false,
                "id": 9,
                "resource_uri":"/MAAS/api/2.0/nodes/4y3h8a/blockdevices/10/partition/9",
                "path": "/dev/disk/by-dname/vda-part1",
                "uuid": "aae082cd-8be0-4a64-ab49-e998abd6ea43",
                "used_for": "LVM volume for vgroot",
                "size": 5360320512,
                "type": "partition",
                "filesystem": {
                    "uuid": "a56ebfa6-8ef4-48b5-b6bc-9f9d27065d24",
                    "mount_options": null,
                    "label": null,
                    "fstype": "lvm-pv",
                    "mount_point": null
                }
            }
        ],
        "tags": [
            "rotary"
        ],
        "name": "vda"
    },
    {
        "id": 11,
        "path": "/dev/disk/by-dname/lvroot",
        "serial": null,
        "block_size": 4096,
        "available_size": 0,
        "resource_uri": "/MAAS/api/2.0/nodes/4y3h8a/blockdevices/11/",
        "filesystem": {
            "uuid": "7181a0c0-9e16-4276-8a55-c77364d137ca",
            "mount_options": null,
            "label": "root",
            "fstype": "ext4",
            "mount_point": "/"
        },
        "id_path": null,
        "size": 3221225472,
        "partition_table_type": null,
        "model": null,
        "type": "virtual",
        "uuid": "fc8ba89e-9149-412c-bcea-e596eb7c0d14",
        "used_size": 3221225472,
        "used_for": "ext4 formatted filesystem mounted at /",
        "partitions": [],
        "tags": [],
        "name": "vgroot-lvroot"
    }
]
```

**** How to read a block device

If you want to read just one block device instead of listing all block devices the read operation on the block device endpoint provides that information. To display the details on device '11' from the previous output, for example, we could enter:

``` nohighlight
maas admin block-device read <node-id> 11
```

The above command generates the following output:

``` nohighlight
Success.
Machine-readable output follows:
{
    "available_size": 0,
    "path": "/dev/disk/by-dname/vgroot-lvroot",
    "name": "vgroot-lvroot",
    "used_for": "ext4 formatted filesystem mounted at /",
    "type": "virtual",
    "used_size": 3221225472,
    "filesystem": {
        "uuid": "7181a0c0-9e16-4276-8a55-c77364d137ca",
        "mount_point": "/",
        "mount_options": null,
        "fstype": "ext4",
        "label": "root"
    },
    "id_path": null,
    "id": 11,
    "partition_table_type": null,
    "block_size": 4096,
    "tags": [],
    "resource_uri": "/MAAS/api/2.0/nodes/4y3h8a/blockdevices/11/",
    "uuid": "fc8ba89e-9149-412c-bcea-e596eb7c0d14",
    "serial": null,
    "partitions": [],
    "size": 3221225472,
    "model": null
}
```

It is also possible to use the name of the block device, such as 'sda' or 'vda', instead of its 'id':

``` nohighlight
s admin block-device read <node-id> vda
```

[note]
MAAS allows the name of a block device to be changed. If the block device name has changed then the API call needs to use the new name.
[/note]

    Using the ID is safer as it never changes.

**** How to create a block device

MAAS gathers the required information itself on block devices when re- commissioning a machine. If this doesn't provide the required information, it is also possible - though not recommended - for an administrator to use the API to manually add a physical block device to a machine.

``` nohighlight
maas admin block-devices create <node-id> name=vdb model="QEMU" serial="QM00001" size=21474836480 block_size=4096
```

Depending on your configuration, output should be similar to the following:

``` nohighlight
Success.
Machine-readable output follows:
{
    "available_size": 21474836480,
    "path": "/dev/disk/by-dname/vdb",
    "name": "vdb",
    "used_for": "Unused",
    "type": "physical",
    "used_size": 0,
    "filesystem": null,
    "id_path": "",
    "id": 12,
    "partition_table_type": null,
    "block_size": 4096,
    "tags": [],
    "resource_uri": "/MAAS/api/2.0/nodes/4y3h8a/blockdevices/12/",
    "uuid": null,
    "serial": "QM00001",
    "partitions": [],
    "size": 21474836480,
    "model": "QEMU"
}
```

[note]
The serial number is what MAAS will use when a machine is deployed to find the specific block device. It's important that this be correct. In a rare chance that your block device does not provide a model or serial number you can provide an id_path. The id_path should be a path that is always the same, no matter the kernel version.
[/note]

**** How to update a block device

An administrator can also update the details held on a physical block device, such as its name, from the API:

``` nohighlight
maas admin block-device update <node-id> 12 name=newroot
```

Output from this command will show that the 'name' has changed:

``` nohighlight
Success.
Machine-readable output follows:
{
    "block_size": 4096,
    "size": 21474836480,
    "filesystem": null,
    "model": "QEMU",
    "name": "newroot",
    "partitions": [],
    "tags": [],
    "used_size": 0,
    "path": "/dev/disk/by-dname/newroot",
    "id_path": "",
    "uuid": null,
    "available_size": 21474836480,
    "id": 12,
    "used_for": "Unused",
    "type": "physical",
    "resource_uri": "/MAAS/api/2.0/nodes/4y3h8a/blockdevices/12/",
    "partition_table_type": null,
    "serial": "QM00001"
}
```

**** How to delete a block device

Physical and virtual block devices can be deleted by an administrator, while ordinary users can only delete virtual block devices:

``` nohighlight
maas admin block-device delete <node-id> 12
```

**** How to format a block device

An entire block device can be formatted by defining a filesystem with the 'format' API call:

``` nohighlight
maas admin block-device format <node-id> 11 fstype=ext4
```

Successful output from this command will look similar to this:

``` nohighlight
Success.
Machine-readable output follows:
{
    "block_size": 4096,
    "size": 3221225472,
    "filesystem": {
        "label": "",
        "fstype": "ext4",
        "mount_options": null,
        "uuid": "75e42f49-9a45-466c-8425-87a40e4f4148",
        "mount_point": null
    },
    "model": null,
    "name": "vgroot-lvroot",
    "partitions": [],
    "tags": [],
    "used_size": 3221225472,
    "path": "/dev/disk/by-dname/vgroot-lvroot",
    "id_path": null,
    "uuid": "fc8ba89e-9149-412c-bcea-e596eb7c0d14",
    "available_size": 0,
    "id": 11,
    "used_for": "Unmounted ext4 formatted filesystem",
    "type": "virtual",
    "resource_uri": "/MAAS/api/2.0/nodes/4y3h8a/blockdevices/11/",
    "partition_table_type": null,
    "serial": null
}
```

[note]
You cannot format a block device that contains partitions or is used to make another virtual block device.
[/note]

**** How to unformat a block device

You can remove the filesystem from a block device with the 'unformat' API call:

``` nohighlight
maas admin block-device unformat <node-id> 11
```

The output from this command should show the filesystem is now 'null':

``` nohighlight
Success.
Machine-readable output follows:
{
    "available_size": 3221225472,
    "path": "/dev/disk/by-dname/vgroot-lvroot",
    "name": "vgroot-lvroot",
    "used_for": "Unused",
    "type": "virtual",
    "used_size": 0,
    "filesystem": null,
    "id_path": null,
    "id": 11,
    "partition_table_type": null,
    "block_size": 4096,
    "tags": [],
    "resource_uri": "/MAAS/api/2.0/nodes/4y3h8a/blockdevices/11/",
    "uuid": "fc8ba89e-9149-412c-bcea-e596eb7c0d14",
    "serial": null,
    "partitions": [],
    "size": 3221225472,
    "model": null
}
```

**** How to mount a block device

If a block device has a filesystem, you can use the 'maas' command to mount a block devices at a given mount point:

``` nohighlight
maas admin block-device mount <node-id> 11 mount_point=/srv
```

The mount point is included in the successful output from the command:

``` nohighlight
Success.
Machine-readable output follows:
{
    "available_size": 0,
    "path": "/dev/disk/by-dname/vgroot-lvroot",
    "name": "vgroot-lvroot",
    "used_for": "ext4 formatted filesystem mounted at /srv",
    "type": "virtual",
    "used_size": 3221225472,
    "filesystem": {
        "uuid": "6f5965ad-49f7-42da-95ff-8000b739c39f",
        "mount_point": "/srv",
        "mount_options": "",
        "fstype": "ext4",
        "label": ""
    },
    "id_path": null,
    "id": 11,
    "partition_table_type": null,
    "block_size": 4096,
    "tags": [],
    "resource_uri": "/MAAS/api/2.0/nodes/4y3h8a/blockdevices/11/",
    "uuid": "fc8ba89e-9149-412c-bcea-e596eb7c0d14",
    "serial": null,
    "partitions": [],
    "size": 3221225472,
    "model": null
}
```

**** How to unmount a block device

To remove the mount point from the block device, use the 'unmount' call:

``` nohighlight
maas admin block-device unmount <node-id> 11 mount_point=/srv
```

The previous command will include a nullified 'mount_point' in its output:

``` nohighlight
Success.
Machine-readable output follows:
{
    "available_size": 0,
    "path": "/dev/disk/by-dname/vgroot-lvroot",
    "name": "vgroot-lvroot",
    "used_for": "Unmounted ext4 formatted filesystem",
    "type": "virtual",
    "used_size": 3221225472,
    "filesystem": {
        "uuid": "6f5965ad-49f7-42da-95ff-8000b739c39f",
        "mount_point": null,
        "mount_options": null,
        "fstype": "ext4",
        "label": ""
    },
    "id_path": null,
    "id": 11,
    "partition_table_type": null,
    "block_size": 4096,
    "tags": [],
    "resource_uri": "/MAAS/api/2.0/nodes/4y3h8a/blockdevices/11/",
    "uuid": "fc8ba89e-9149-412c-bcea-e596eb7c0d14",
    "serial": null,
    "partitions": [],
    "size": 3221225472,
    "model": null
}
```

**** How to set a block device as a boot disk

By default, MAAS picks the first added block device to the machine as the boot disk. In most cases this works as expected as the BIOS usually enumerates the boot disk as the first block device. There are cases where this fails and the boot disk needs to be set to another disk. This API allow setting which block device on a machine MAAS should use as the boot disk.:

``` nohighlight
maas admin block-device set-boot-disk <node-id> 10
```

[note]
Only an administrator can set which block device should be used as the boot disk and only a physical block device can be set as as the boot disk. This operation should be done before a machine is allocated or the storage layout will be applied to the previous boot disk.
[/note]

*** How to manage partitions

This section will explain:

- [How to list partitions](#heading--how-to-list-partitions)
- [How to create a partition](#heading--how-to-create-a-partition)
- [How to delete a partition](#heading--how-to-delete-a-partition)
- [How to format a partition](#heading--how-to-format-a-partition)
- [How to unformat a partition](#heading--how-to-unformat-a-partition)
- [How to mount a partition](#heading--how-to-mount-a-partition)
- [How to unmount a partition](#heading--how-to-unmount-a-partition)

**** How to list partitions

To view all the partitions on a block device, use the 'partitions read' API call:

``` nohighlight
maas admin partitions read <node-id> 10
```

``` nohighlight
Success.
Machine-readable output follows:
[
    {
        "bootable": false,
        "id": 9,
        "resource_uri":
"/MAAS/api/2.0/nodes/4y3h8a/blockdevices/10/partition/9",
        "path": "/dev/disk/by-dname/vda-part1",
        "uuid": "aae082cd-8be0-4a64-ab49-e998abd6ea43",
        "used_for": "LVM volume for vgroot",
        "size": 5360320512,
        "type": "partition",
        "filesystem": {
            "uuid": "a56ebfa6-8ef4-48b5-b6bc-9f9d27065d24",
            "mount_options": null,
            "label": null,
            "fstype": "lvm-pv",
            "mount_point": null
        }
    }
]
```

To view the metadata for a specific partition on a block device, rather than all partitions, use the singular 'partition' API call with an endpoint:

``` nohighlight
maas admin partition read <node-id> 10 9
```

**** How to create a partition

To create a new partition on a block device, use the 'create' API call:

``` nohighlight
maas admin partitions create <node-id> 10 size=5360320512
```

In addition to bytes, as shown above, the 'size' of a partition can also be defined with a 'G' for gigabytes or 'M' for megabytes. The output from the previous command will look like this:

``` nohighlight
Success.
Machine-readable output follows:
{
    "bootable": false,
    "path": "/dev/disk/by-dname/vda-part1",
    "filesystem": null,
    "used_for": "Unused",
    "type": "partition",
    "id": 10,
    "size": 5360320512,
    "resource_uri": "/MAAS/api/2.0/nodes/4y3h8a/blockdevices/10/partition/10",
    "uuid": "3d32adbf-9943-4785-ab38-963758338c6c"
}
```

**** How to delete a partition

Partitions can be deleted from a block device with the 'delete' API call. Make sure you double check the partition details as the partition is deleted immediately, with no further confirmation:

``` nohighlight
maas admin partition delete <node-id> 10 9
```

Successful output from the 'delete' command will look like this:

``` nohighlight
Success.
Machine-readable output follows:
```

**** How to format a partition

Partitions can be formatted in a similar way to block devices:

``` nohighlight
maas admin partition format <node-id> 10 9 fstype=ext4
```

The output from the 'format' command will be similar to the following:

``` nohighlight
Success.
Machine-readable output follows:
{
    "id": 9,
    "used_for": "Unmounted ext4 formatted filesystem",
    "resource_uri": "/MAAS/api/2.0/nodes/4y3h8a/blockdevices/10/partition/9",
    "path": "/dev/disk/by-dname/vda-part1",
    "uuid": "aae082cd-8be0-4a64-ab49-e998abd6ea43",
    "size": 5360320512,
    "bootable": false,
    "type": "partition",
    "filesystem": {
        "uuid": "ea593366-be43-4ea3-b2d5-0adf82085a62",
        "mount_point": null,
        "mount_options": null,
        "fstype": "ext4",
        "label": ""
    }
}
```

[note]
You cannot format partitions that are used to make another virtual block device.
[/note]

**** How to unformat a partition

You can also remove the filesystem from a partition with the 'unformat' API call:

``` nohighlight
maas admin partition unformat <node-id> 10 10 fstype=ext4
```

``` nohighlight
Success.
Machine-readable output follows:
{
    "bootable": false,
    "path": "/dev/disk/by-dname/vda-part1",
    "filesystem": null,
    "used_for": "Unused",
    "type": "partition",
    "id": 10,
    "size": 5360320512,
    "resource_uri": "/MAAS/api/2.0/nodes/4y3h8a/blockdevices/10/partition/10",
    "uuid": "3d32adbf-9943-4785-ab38-963758338c6c"
}
```

**** How to mount a partition

A formatted partition can be mounted at a given mount point with the 'mount' command.

``` nohighlight
maas admin partition mount <node-id> 10 10 mount_point=/srv
```

The mount point and the filesystem is visible in the output from the command:

``` nohighlight
Success.
Machine-readable output follows:
{
    "bootable": false,
    "id": 10,
    "resource_uri": "/MAAS/api/2.0/nodes/4y3h8a/blockdevices/10/partition/10",
    "path": "/dev/disk/by-dname/vda-part1",
    "uuid": "3d32adbf-9943-4785-ab38-963758338c6c",
    "used_for": "ext4 formatted filesystem mounted at /srv",
    "size": 5360320512,
    "type": "partition",
    "filesystem": {
        "uuid": "1949a5fb-f7bd-4ada-8ba5-d06d3f5857a8",
        "mount_options": "",
        "label": "",
        "fstype": "ext4",
        "mount_point": "/srv"
    }
}
```

**** How to unmount a partition

A previous mounted partition can be unmounted with the 'unmount' command:

``` nohighlight
maas admin partition unmount 4y3h8a 10 10
```

After successfully running this command, the mount point will show as 'null' in the output:

``` nohighlight
Success.
Machine-readable output follows:
{
    "bootable": false,
    "id": 10,
    "resource_uri": "/MAAS/api/2.0/nodes/4y3h8a/blockdevices/10/partition/10",
    "path": "/dev/disk/by-dname/vda-part1",
    "uuid": "3d32adbf-9943-4785-ab38-963758338c6c",
    "used_for": "Unmounted ext4 formatted filesystem",
    "size": 5360320512,
    "type": "partition",
    "filesystem": {
        "uuid": "1949a5fb-f7bd-4ada-8ba5-d06d3f5857a8",
        "mount_options": null,
        "label": "",
        "fstype": "ext4",
        "mount_point": null
    }
    "type": "partition",
    "id": 3,
    "size": 2000003072
}
```

**** How to manage VMFS datastores

This section will explain:

- [How to list VMFS datastores](#heading--how-to-list-vmfs-datastores)
- [How to view a VMFS datastore](#heading--how-to-view-a-vmfs-datastore)
- [How to create a VMFS datastore](#heading--how-to-create-a-vmfs-datastore)
- [How to edit a VMFS datastore](#heading--how-to-edit-a-vmfs-datastore)
- [How to delete a VMFS datastore](#heading--how-to-delete-a-vmfs-datastore)

**** How to list VMFS datastores

To view all VMFS Datastores on a machine, use the 'vmfs-datastores read' API call:

``` nohighlight
maas $PROFILE vmfs-datastores read $SYSTEM_ID
```

``` nohighlight
[
    {
        "human_size": "45.8 GB",
        "filesystem": {
            "fstype": "vmfs6",
            "mount_point": "/vmfs/volumes/datastore1"
        },
        "uuid": "2779a745-1db3-4fd7-b06e-455b728fffd4",
        "name": "datastore1",
        "system_id": "4qxaga",
        "devices": [
            {
                "uuid": "c55fe657-689d-4570-8492-683dd5fa1c40",
                "size": 35026632704,
                "bootable": false,
                "tags": [],
                "used_for": "VMFS extent for datastore1",
                "filesystem": {
                    "fstype": "vmfs6",
                    "label": null,
                    "uuid": "55ac6422-68b5-440e-ba65-153032605b51",
                    "mount_point": null,
                    "mount_options": null
                },
                "type": "partition",
                "device_id": 5,
                "path": "/dev/disk/by-dname/sda-part3",
                "system_id": "4qxaga",
                "id": 71,
                "resource_uri": "/MAAS/api/2.0/nodes/4qxaga/blockdevices/5/partition/71"
            },
            {
                "uuid": "5182e503-4ad4-446e-9660-fd5052b41cc5",
                "size": 10729029632,
                "bootable": false,
                "tags": [],
                "used_for": "VMFS extent for datastore1",
                "filesystem": {
                    "fstype": "vmfs6",
                    "label": null,
                    "uuid": "a5949b18-d591-4627-be94-346d0fdaf816",
                    "mount_point": null,
                    "mount_options": null
                },
                "type": "partition",
                "device_id": 6,
                "path": "/dev/disk/by-dname/sdb-part1",
                "system_id": "4qxaga",
                "id": 77,
                "resource_uri": "/MAAS/api/2.0/nodes/4qxaga/blockdevices/6/partition/77"
            }
        ],
        "id": 17,
        "size": 45755662336,
        "resource_uri": "/MAAS/api/2.0/nodes/4qxaga/vmfs-datastore/17/"
    }
]
```

**** How to view a VMFS datastore

To view a specific VMFS Datastores on a machine, use the 'vmfs-datastore read' API call:

``` nohighlight
maas $PROFILE vmfs-datastore read $SYSTEM_ID $VMFS_DATASTORE_ID
```

``` nohighlight
{
    "uuid": "fb6fedc2-f711-40de-ab83-77eddc3e19ac",
    "name": "datastore1",
    "system_id": "b66fn6",
    "id": 18,
    "filesystem": {
        "fstype": "vmfs6",
        "mount_point": "/vmfs/volumes/datastore1"
    },
    "human_size": "2.8 GB",
    "devices": [
        {
            "uuid": "b91df576-ba02-4acb-914f-03ba9a2865b7",
            "size": 2814377984,
            "bootable": false,
            "tags": [],
            "system_id": "b66fn6",
            "used_for": "VMFS extent for datastore1",
            "type": "partition",
            "id": 80,
            "filesystem": {
                "fstype": "vmfs6",
                "label": null,
                "uuid": "4a098d71-1e59-4b5f-932d-fc30a1c0dc96",
                "mount_point": null,
                "mount_options": null
            },
            "device_id": 1,
            "path": "/dev/disk/by-dname/vda-part3",
            "resource_uri": "/MAAS/api/2.0/nodes/b66fn6/blockdevices/1/partition/80"
        }
    ],
    "size": 2814377984,
    "resource_uri": "/MAAS/api/2.0/nodes/b66fn6/vmfs-datastore/18/"
}
```

**** How to create a VMFS datastore

A VMware VMFS datastore is created on one or more [block devices](#heading--about-block-devices) or [partitions](#heading--about-partitions).

To create a VMFS Datastores on a machine use the 'vmfs-datastores create' API call:

``` nohighlight
maas $PROFILE vmfs-datastores create $SYSTEM_ID name=$VMFS_NAME block_devices=$BLOCK_ID_1,$BLOCK_ID_2 partitions=$PARTITION_ID_1,$PARTITION_ID_2
```

``` nohighlight
{
    "system_id": "b66fn6",
    "devices": [
        {
            "uuid": "b91df576-ba02-4acb-914f-03ba9a2865b7",
            "size": 2814377984,
            "bootable": false,
            "tags": [],
            "device_id": 1,
            "system_id": "b66fn6",
            "type": "partition",
            "used_for": "VMFS extent for datastore42",
            "filesystem": {
                "fstype": "vmfs6",
                "label": null,
                "uuid": "fc374367-a2fb-4e50-9377-768bfe9705b6",
                "mount_point": null,
                "mount_options": null
            },
            "path": "/dev/disk/by-dname/vda-part3",
            "id": 80,
            "resource_uri": "/MAAS/api/2.0/nodes/b66fn6/blockdevices/1/partition/80"
        }
    ],
    "name": "datastore42",
    "filesystem": {
        "fstype": "vmfs6",
        "mount_point": "/vmfs/volumes/datastore42"
    },
    "id": 19,
    "size": 2814377984,
    "uuid": "2711566c-2df4-4cc4-8c06-7392bb1f9532",
    "human_size": "2.8 GB",
    "resource_uri": "/MAAS/api/2.0/nodes/b66fn6/vmfs-datastore/19/"
}
```

**** How to edit a VMFS datastore

To edit an existing VMFS Datastores on a machine use the 'vmfs-datastore update' API call:

``` nohighlight
maas $PROFILE vmfs-datastore update $SYSTEM_ID $VMFS_ID name=$NEW_VMFS_NAME add_block_devices=$NEW_BLOCK_ID_1,$NEW_BLOCK_ID_2 add_partitions=$NEW_PARTITION_ID_1,$NEW_PARTITION_ID_2 remove_partitions=$EXISTING_PARTITION_ID1,$EXISTING_PARTITION_ID2
```

``` nohighlight
{
    "uuid": "2711566c-2df4-4cc4-8c06-7392bb1f9532",
    "name": "datastore42",
    "system_id": "b66fn6",
    "id": 19,
    "filesystem": {
        "fstype": "vmfs6",
        "mount_point": "/vmfs/volumes/datastore42"
    },
    "human_size": "13.5 GB",
    "devices": [
        {
            "uuid": "b91df576-ba02-4acb-914f-03ba9a2865b7",
            "size": 2814377984,
            "bootable": false,
            "tags": [],
            "system_id": "b66fn6",
            "used_for": "VMFS extent for datastore42",
            "type": "partition",
            "id": 80,
            "filesystem": {
                "fstype": "vmfs6",
                "label": null,
                "uuid": "fc374367-a2fb-4e50-9377-768bfe9705b6",
                "mount_point": null,
                "mount_options": null
            },
            "device_id": 1,
            "path": "/dev/disk/by-dname/vda-part3",
            "resource_uri": "/MAAS/api/2.0/nodes/b66fn6/blockdevices/1/partition/80"
        },
        {
            "uuid": "f21fe54e-b5b1-4562-ab6b-e699e99f002f",
            "size": 10729029632,
            "bootable": false,
            "tags": [],
            "system_id": "b66fn6",
            "used_for": "VMFS extent for datastore42",
            "type": "partition",
            "id": 86,
            "filesystem": {
                "fstype": "vmfs6",
                "label": null,
                "uuid": "f3d9b6a3-bab3-4677-becb-bf5a421bfcc2",
                "mount_point": null,
                "mount_options": null
            },
            "device_id": 2,
            "path": "/dev/disk/by-dname/vdb-part1",
            "resource_uri": "/MAAS/api/2.0/nodes/b66fn6/blockdevices/2/partition/86"
        }
    ],
    "size": 13543407616,
    "resource_uri": "/MAAS/api/2.0/nodes/b66fn6/vmfs-datastore/19/"
}
```

**** How to delete a VMFS datastore

To delete a VMFS Datastores on a machine use the 'vmfs-datastore delete' API call:

``` nohighlight
maas $PROFILE vmfs-datastore delete $SYSTEM_ID $VMFS_ID
```
[/tab]
[/tabs]

** How to define custom storage layouts

[tabs]
[tab version="v3.4 Snap,v3.4 Packages,v3.3 Snap,v3.3 Packages,v3.2 Snap,v3.2 Packages,v3.1 Snap,v3.1 Packages"]
MAAS 3.1 allows you to define a custom storage layout for a machine, via a custom commissioning script.  You must upload a script which conforms to the following rules:

- it must run after the `40-maas-01-machine-resources` script and before the `50-maas-01-commissioning` one, so it should have a name that starts with anything between `41-` and `49-`. This ensures the script can access the JSON file created by the former which provides info about the machine hardware and network resources. In addition, the custom script can directly inspect the machine it's running on to determine how to configure storage.
- it can read machine hardware/network information from the JSON file at the path specified by `$MAAS_RESOURCES_FILE`
- it must output a JSON file at the path specified by `$MAAS_STORAGE_CONFIG_FILE` with the desired storage layout
- names of disks provided in the custom layout must match the ones detected by MAAS and provided in the resources file.

**# Configuration format

The configuration contains two main sections:
- `layout`, which lists the desired storage layout in terms of disks and volumes, along with their setup (partitions, volumes, ...).
  This consists of a dictionary of device names and their configuration. Each device must have a `type` property (see below for supported types).
- `mounts`, which lists the desired filesystem mount points.
  As an example:

```json
"mounts": {
  "/": {
    "device": "sda2",
    "options": "noatime"
  },
  "/boot/efi": {
    "device": "sda1"
  },
  "/data": {
    "device": "raid0"
  }     
}
```

A complete `$MAAS_STORAGE_CONFIG_FILE` would look like this:

```json
{
    "layouts": {
        "sda": {
           ...
        },
        "raid0": {
           ...
        },
        ...
    },
    "mounts": {
       "/": {
           ...
       },
       ...
    }
}
```



The following device types are supported in the `"layout"` section:

**** Disk

```json
"sda": {
  "type": "disk",
  "ptable": "gpt",
  "boot": true,
  "partitions": [
    {
      "name": "sda1",
      "fs": "vfat",
      "size": "100M"
      "bootable": true,
    }
  ]
}
```
A `disk` entry defines a physical disk.
The following details can be specified:
- the partition table type (`ptable`), which can be `gpt` or `mbr`
- whether it should be selected as `boot` disk
- optionally, a list of partitions to create, with their `size` and filesystem type (`fs`)

**** LVM

```json
"lvm0": {
  "type": "lvm",
  "members": [
    "sda1",
    "sdb1",
  ],
  "volumes": [
    {
      "name": "data1",
      "size": "5G",
      "fs": "ext4"
    },
    {
      "name": "data2",
      "size": "7G",
      "fs": "btrfs"
    }
  ]
}
```

An `lvm` entry defines a VG (volume group) composed by a set of disks or partitions (listed as `members`). Optionally it's possible to specify the the LVs (logical volumes) to create.
Those are defined similarly to partitions, with a name and size (and optionally the filesystem).

**** Bcache

```json
"bcache0": {
  "type": "bcache",
  "cache-device": "sda",
  "backing-device": "sdf3",
  "cache-mode": "writeback",
  "fs": "ext4"
}
```

A `bcache`  entry must specify a device to use as cache and one to use as storage. Both can be either a partition or a disk.
Optionally the `cache-mode` for the Bcache can be specified.

**** RAID

```json
"myraid": {
  "type": "raid",
  "level": 5,
  "members": [
    "sda",
    "sdb",
    "sdc",
  ],
  "spares": [
    "sdd",
    "sde"
  ],
  "fs": "btrfs"
```

A `raid` entry defines a RAID with a set of member devices.
Spare devices can also be specified.



**# Configuration examples

Here's a few examples of custom storage layout configurations that a script could output to the `$MAAS_STORAGE_CONFIG_FILE`. The examples assumes that the machine has 5 disks (named `sda` to `sde`).

Note that there's no need to add entries for those devices in the `layout` section if the disks are not explicitly partitioned, but just used by other devices (e.g. RAID or LVM).

**** Simple single-disk layout with GPT partitioning
```json
{
  "layout": {
    "sda": {
      "type": "disk",
      "ptable": "gpt",
      "boot": true,
      "partitions": [
        {
          "name": "sda1",
          "fs": "vfat",
          "size": "500M",
          "bootable": true
        },
        {
          "name": "sda2",
          "size": "5G",
          "fs": "ext4"
        },
        {
          "name": "sda3",
          "size": "2G",
          "fs": "swap"
        },
        {
          "name": "sda4",
          "size": "120G",
          "fS": "ext4"
        }
      ]
    }
  },
  "mounts": {
    "/": {
      "device": "sda2",
      "options": "noatime"
    },
    "/boot/efi": {
      "device": "sda1"
    },
    "/data": {
      "device": "sda4"
    },
    "none": {
      "device": "sda3"
    }
  }
}
```
In the `mounts` section, options for mount points can be specified. For swap, an entry must be present (with any unique name that doesn't start with a `/`), otherwise the swap will be created but not activated.

**** RAID 5 setup (with spare devices)
```json
{
  "layout": {
    "storage": {
      "type": "raid",
      "level": 5,
      "members": [
        "sda",
        "sdb",
        "sdc"
      ],
      "spares": [
        "sdd",
        "sde"
      ],
      "fs": "btrfs"
    }
  },
  "mounts": {
    "/data": {
      "device": "storage"
    }
  }
}
```
Both full disks and partitions can be used as RAID members.

**** LVM with pre-defined volumes
```json
{
  "layout": {
    "storage": {
      "type": "lvm",
      "members": [
        "sda",
        "sdb",
        "sdc",
        "sdd"
      ],
      "volumes": [
        {
          "name": "data1",
          "size": "1T",
          "fs": "ext4"
        },
        {
          "name": "data2",
          "size": "2.5T",
          "fs": "btrfs"
        }
      ]
    }
  },
  "mounts": {
    "/data1": {
      "device": "data1"
    },
    "/data2": {
      "device": "data2"
    }
  }
}
```
If no volumes are specified, the volume group is still created.

**** Bcache
```json
{
  "layout": {
     "data1": {
      "type": "bcache",
      "cache-device": "sda",
      "backing-device": "sdb",
      "cache-mode": "writeback",
      "fs": "ext4"
    },
    "data2": {
      "type": "bcache",
      "cache-device": "sda",
      "backing-device": "sdc",
      "fs": "btrfs"
    }
  },
  "mounts": {
    "/data1": {
      "device": "data1"
    },
    "/data2": {
      "device": "data2"
    }
  }
}
```
The same cache set can be used by different bcaches by specifying the same `backing-device` for them.

**** LVM on top of RAID with Bcache
```json
{
  "layout": {
    "bcache0": {
      "type": "bcache",
      "backing-device": "sda",
      "cache-device": "sdf"
    },
    "bcache1": {
      "type": "bcache",
      "backing-device": "sdb",
      "cache-device": "sdf"
    },
    "bcache2": {
      "type": "bcache",
      "backing-device": "sdc",
      "cache-device": "sdf"
    },
    "bcache3": {
      "type": "bcache",
      "backing-device": "sdd",
      "cache-device": "sdf"
    },
    "bcache4": {
      "type": "bcache",
      "backing-device": "sde",
      "cache-device": "sdf"
    },
    "raid": {
      "type": "raid",
      "level": 5,
      "members": [
        "bcache0",
        "bcache1",
        "bcache2"
      ],
      "spares": [
        "bcache3",
        "bcache4"
      ]
    },
    "lvm": {
      "type": "lvm",
      "members": [
        "raid"
      ],
      "volumes": [
        {
          "name": "root",
          "size": "10G",
          "fs": "ext4"
        },
        {
          "name": "data",
          "size": "3T",
          "fs": "btrfs"
        }
      ]
    }
  },
  "mounts": {
   "/": {
      "device": "root"
    },
    "/data": {
      "device": "data"
    }
  }
}
```
The RAID is created by using 5 bcache devices, each one using a different disk and the same SSD cache device. LVM is created on top of the RAID device and volumes are then created in it, to provide partitions.

[/tab]
[tab version="v3.0 Snap,v3.0 Packages,v2.9 Snap,v2.9 Packages"]
Custom storage layouts are available in MAAS version 3.1 and greater.
[/tab]
[/tabs]

** Storage layouts reference

There are three layout types:

1.   Flat layout
2.   LVM layout
3.   bcache layout

The layout descriptions below will include the EFI partition. If your system is not using UEFI, regard `sda2` as `sda1` (with an additional 512 MB available to it).

*** Flat layout storage reference

With the Flat layout, a partition spans the entire boot disk. The partition is formatted with the ext4 filesystem and uses the `/` mount point:

| Name | Size        | Type | Filesystem | Mount point |
|:----:|------------:|:----:|:----------:|:------------|
| sda  | -           | disk |            |             |
| sda1 | 512 MB      | part | FAT32      | /boot/efi   |
| sda2 | rest of sda | part | ext4       | /           |

The following three options are supported:

1. `boot_size`: Size of the boot partition on the boot disk. Default is 0, meaning not to create the boot partition. The '/boot' will be placed on the root filesystem.

2. `root_device`: The block device on which to place the root partition. The default is the boot disk.

3. `root_size`: Size of the root partition. Default is 100%, meaning the entire size of the root device.

*** LVM storage layout reference

The LVM layout creates the volume group `vgroot` on a partition that spans the entire boot disk. A logical volume `lvroot` is created for the full size of the volume group; is formatted with the ext4 filesystem; and uses the `/` mount point:

| Name   | Size        | Type | Filesystem     | Mount point |
|:----:|------------:|:----:|:----------:|:------------|
| sda    | -           | disk |                |             |
| sda1   | 512 MB      | part | FAT32          | /boot/efi   |
| sda2   | rest of sda | part | lvm-pv(vgroot) |             |
| lvroot | rest of sda | lvm  | ext4           | /           |
| vgroot | rest of sda | lvm  |                |             |

The following six options are supported:

1. `boot_size`: Size of the boot partition on the boot disk. Default is 0, meaning not to create the boot partition. The '/boot' will be placed on the root filesystem.
2. `root_device`: The block device on which to place the root partition. The default is the boot disk.
3. `root_size`: Size of the root partition. Default is 100%, meaning the entire size of the root device.
4. `vg_name`: Name of the created volume group. Default is `vgroot`.
5. `lv_name`: Name of the created logical volume. Default is `lvroot`.
6. `lv_size`: Size of the created logical volume. Default is 100%, meaning the entire size of the volume group.

*** bcache storage layout reference

A bcache layout will create a partition that spans the entire boot disk as the backing device. It uses the smallest block device tagged with 'ssd' as the cache device. The bcache device is formatted with the ext4 filesystem and uses the `/` mount point. If there are no 'ssd' tagged block devices on the machine, then the bcache device will not be created, and the Flat layout will be used instead:

| Name      | Size        | Type | Filesystem | Mount point |
|:----:|------------:|:----:|:----------:|:------------|
| sda       | -           | disk |            |             |
| sda1      | 512 MB      | part | FAT32      | /boot/efi   |
| sda2      | rest of sda | part | bc-backing |             |
| sdb (ssd) | -           | disk |            |             |
| sdb1      | 100% of sdb | part | bc-cache   |             |
| bcache0   | per sda2    | disk | ext4       | /           |

The following seven options are supported:

1. `boot_size`: Size of the boot partition on the boot disk. Default is 0, meaning not to create the boot partition. The '/boot' will be placed on the root filesystem.
2. `root_device`: The block device upon which to place the root partition. The default is the boot disk.
3. `root_size`: Size of the root partition. Default is 100%, meaning the entire size of the root device.
4. `cache_device`: The block device to use as the cache device. Default is the smallest block device tagged ssd.
5. `cache_mode`: The cache mode to which MAAS should set the created bcache device. The default is `writethrough`.
6. `cache_size`: The size of the partition on the cache device. Default is 100%, meaning the entire size of the cache device.
7. `cache_no_part`: Whether or not to create a partition on the cache device. Default is false, meaning to create a partition using the given `cache_size`. If set to true, no partition will be created, and the raw cache device will be used as the cache.

vmfs6 storage layout reference
*** VMFS6 storage layout reference

The VMFS6 layout is used for VMware ESXi deployments only. It is required when configuring VMware VMFS Datastores. This layout creates all operating system partitions, in addition to the default datastore. The datastore may be modified.  New datastores may be created or extended to include other storage devices. The base operating system partitions may not be modified because VMware ESXi requires them. Once applied another storage layout must be applied to remove the operating system partitions.

| Name | Size      | Type    | Use               |
|:-----|------------:|:----:|:----------|
| sda  | -         | disk    |                   |
| sda1 | 3 MB      | part    | EFI               |
| sda2 | 4 GB      | part    | Basic Data        |
| sda3 | Remaining | part    | VMFS Datastore 1  |
| sda4 | -         | skipped |                   |
| sda5 | 249 MB    | part    | Basic Data        |
| sda6 | 249 MB    | part    | Basic Data        |
| sda7 | 109 MB    | part    | VMware Diagnostic |
| sda8 | 285 MB    | part    | Basic Data        |
| sda9 | 2.5 GB    | part    | VMware Diagnostic |

The following options are supported:

- `root_device`: The block device upon which to place the root partition. Default is the boot disk.

- `root_size`: Size of the default VMFS Datastore. Default is 100%, meaning the remaining size of the root disk.

*** Blank storage layout reference

The blank layout removes all storage configuration from all storage devices. It is useful when needing to apply a custom storage configuration.

[note]
Machines with the blank layout applied are not deployable; you must first configure storage manually.
[/note]

* How to manage controllers
Controllers are the backbone of MAAS.

** [Configure controllers](/t/how-to-configure-controllers/5172)

MAAS initializes with one rack and one region controller.  Your local environment, though, may require more controllers -- or even specialised controller configurations.

** [Enable high availability](/t/how-to-enable-high-availability/5120)

Just by adding rack and region controllers, you create high-availability, but there are some specific choices you can make that help to tailor your HA environment.
* How to manage machines

With your network properly configured, and suitable images selected and downloaded, you're ready to begin deploying machines.
 
- [Make machines available](/t/how-to-make-machines-available/5160): Before deploying machines, you need to discover, enlist, or add them manually to MAAS.
 
- [Customise machines](/t/how-to-customise-machines/5108): In many cases, you'll want to customise the hardware and software of your machines as they are deployed.
 
- [Put machines to work](/t/how-to-put-machines-to-work/5112): The real purpose of MAAS is to deploy machines to fulfill the goals of your organization.
 

* How to manage networking 
Networking lies right at the heart of MAAS. 

- [Connect MAAS networks](/t/how-connect-maas-networks/5164): The first step is to set up connections within MAAS, so that MAAS nodes will be able to talk to the MAAS controllers. 

- [Enable DHCP](/t/how-to-enable-dhcp/5132): In MAAS, DHCP is required for node discovery - unless DHCP is set up correctly, MAAS will not be able to discover the machines.

- [Manage availability zones](/t/how-to-manage-availability-zones/5152): Machines can be allocated to different availability zones for fault-tolerance.

* How to manage user accounts
** How to add a user

[tabs]
[tab version="v3.4 Snap,v3.4 Packages" view="UI"]
To add a user:

1. Go to *Settings >> Users*.

2. Select *Add user*.

3. Fill in the fields in the displayed form.

4. Optionally make the user a *MAAS administrator* by checking the box.

4. Select *Save user* to register your changes.

** How to change a user's preferences

1. Select the MAAS username near the bottom of the left-hand navigation panel.

2. Under *Details*, change the *Username*, *Full name*, or *Email address* as desired.

3. Under *Details*, select *Change password...* to change the user's password.

4. Under *API key*, edit, delete, or *Generate MAAS API keys* as desired.

5. Under *SSH keys*, delete or *Import SSH keys* as desired.

6. Under *SSL keys*, delete or *Add SSL keys* as desired.
[/tab]
[tab version="v3.3 Snap,V3.3 Packages,v3.2 Snap,v3.2 Packages,v3.1 Snap,v3.1 Packages,v3.0 Snap,v3.0 Packages,v2.9 Snap,v2.9 Packages" view="UI"]
To add a user:

1. Go to *Settings >> Users*.

2. Select *Add user*.

3. Fill in the fields in the displayed form.

4. Optionally make the user a *MAAS administrator* by checking the box.

4. Select *Add user* to register your changes.

** How to change a user's preferences

Clicking the MAAS username in the top right corner will show that user's preferences.
[/tab]
[tab version="v3.4 Snap,v3.4 Packages,v3.3 Snap,V3.3 Packages,v3.2 Snap,v3.2 Packages,v3.1 Snap,v3.1 Packages,v3.0 Snap,v3.0 Packages,v2.9 Snap,v2.9 Packages" view="CLI"]
To add a regular user, enter the following command:

```
maas $PROFILE users create username=$USERNAME \
    email=$EMAIL_ADDRESS password=$PASSWORD is_superuser=0
```

All the options are necessary. Note that stipulating a password on the CLI may be a security hazard, depending on your environment.
[/tab]
[/tabs]

*** How to add an SSH key for a user

Assuming a public key exists in `/home/ubuntu/.ssh/id_rsa.pub` - add a key with the following command:
```
ubuntu@maas:~$ maas $PROFILE sshkeys create key="$(cat /home/ubuntu/.ssh/id_rsa.pub)"
Success.
```

[note]
The user normally imports their initial SSH key on the first login to the web UI.
[/note]

[tabs]
[tab version="v3.4 Snap,v3.4 Packages,v3.3 Snap,V3.3 Packages,v3.2 Snap,v3.2 Packages,v3.1 Snap,v3.1 Packages,v3.0 Snap,v3.0 Packages,v2.9 Snap,v2.9 Packages" view="UI"]
To add a locally-generated public key to a user:

1. Select *Settings > Users*.

2. Select the pencil icon to the right of the user that needs the additional key.

3. Select *Import SSH key*.

4. Select *Upload* from the *Source* dropdown and paste the complete contents of the key file, usually called `id_rsa.pub`.

5. Alternatively, you can select *Launcpad* or *GitHub*, if you want to load a SSH key from there.  You will need to enter the user ID associated with the key.

6. Select *Import SSH key* to register your changes.

*** How to add an API key for a user

To add an API key for a user:

1. Select *Settings > Users*.

2. Select the pencil icon to the right of the user that needs an API key.

3. Select *API keys*.

4. Select *Generate MAAS API key*.

5. Optionally enter a name for the API key.

6. Select *Generate API key* to create the new key.
[/tab]
[tab version="v3.4 Snap,v3.4 Packages,v3.3 Snap,V3.3 Packages,v3.2 Snap,v3.2 Packages,v3.1 Snap,v3.1 Packages,v3.0 Snap,v3.0 Packages,v2.9 Snap,v2.9 Packages" view="CLI"]
We recommend that you use the web UI to set or change a user's API key.  To see how, select the "UI" choice in the dropdown above.
[/tab]
[/tabs]

*** How users can change their password

[tabs]
[tab version="v3.4 Snap,v3.4 Packages,v3.3 Snap,V3.3 Packages,v3.2 Snap,v3.2 Packages,v3.1 Snap,v3.1 Packages,v3.0 Snap,v3.0 Packages,v2.9 Snap,v2.9 Packages" view="UI"]
The current user can change their password with the following procedure:

1. Select *Settings > Users*.

2. Select the pencil icon to the far right of the username.  

3. Select *Details*.

3. Select *Change password...* at the bottom of the screen. The screen will extend.

4. Enter the *Current password*.

5. Enter the *New password*.

6. Enter the *New password (again)*.  Note that both new password entries must match.

7. Select *Save* to register the changes.

[note]
An administrator can change any user's password from *Settings > Users*.
[/note]
[/tab]
[tab version="v3.4 Snap,v3.4 Packages,v3.3 Snap,V3.3 Packages,v3.2 Snap,v3.2 Packages,v3.1 Snap,v3.1 Packages,v3.0 Snap,v3.0 Packages,v2.9 Snap,v2.9 Packages" view="CLI"]
We recommend that you use the web UI to change user passwords.  To see how, select the "UI" choice in the dropdown above.
[/tab]
[/tabs]

* How to manage virtual machines
If you have already created a [VM host](/t/vm-hosting/6705), you will want to create and delete [virtual machines](/t/virtual-machines/6704) (VMs).

[tabs]
[tab version="v3.4 Snap,v3.4 Packages" view="UI"]
** How to add a VM from the MAAS UI

To add a virtual machine from the MAAS UI:

1. Select *KVM > <vm-hosting-solution>*.

2. Select the desired VM host by clicking on its name.

3. Select *Add VM".

4. Optionally enter the *VM name*.

5. Select *Use any available core(s)* or *Pin VM to specific core(s)*, as desired.  Enter specific core identities as appropriate.

6. Enter the *RAM* desired.

7. Select *Show advanced* if you want to edit the *Domain*, *Zone*, *Resource pool*, or *Architeture*.  Make those changes as desired.

8. Optionally *Define* interfaces.

9. Optionally *Add disks*.

10. Select *Compose machine* to create the virtual machine.
[/tab]
[tab version="v3.4 Snap,v3.4 Packages,v3.3 Snap,v3.3 Packages,v3.2 Snap,v3.2 Packages,v3.1 Snap,v3.1 Packages,v3.0 Snap,v3.0 Packages,v2.9 Snap,v2.9 Packages" view="UI"]
** How to add a VM from the MAAS UI

To add a virtual machine from the MAAS UI:

1. While on VM host's details view, select 'Compose' from the 'Take action' drop-down menu to compose a machine.

2. You can choose which storage pool to use from a drop-down list. 

**Note** that when adding more than one disk, the boot disk will be the last disk in the list.

3. Click the 'Compose machine' button when you're finished. MAAS will present the VM host detail view. 

In a few moments, your new machine will be auto-commissioned. The 'Machines' page will reflect this as well. MAAS will deduct the new machine's resources from the VM host's resources.

** How to delete a VM

To delete a VM, delete it as you would any other MAAS machine:

1. Select the desired machine from the list of machines. 

2. Select 'Delete' from the 'Take Action' menu.

[/tab]
[tab version="v3.4 Snap,v3.4 Packages,v3.3 Snap,v3.3 Packages,v3.2 Snap,v3.2 Packages,v3.1 Snap,v3.1 Packages,v3.0 Snap,v3.0 Packages,v2.9 Snap,v2.9 Packages" view="CLI"]
- [How to add a VM](#heading--adding-a-vm-from-the-cli)
- [How to set resources while adding a VM](#heading--set-resources)
- [How to set the architecture while adding a VM](#heading--architecture)
- [How to set storage parameters while adding a VM](#heading--storage)
- [How to specify interfaces while adding a VM](#heading--interfaces)
- [How to find a VM host ID](#heading--find-vm-host-ids)
- [How to delete a VM](#heading--delete-a-vm)

There is also some [background material on VM hosting](/t/how-to-deploy-virtual-machines/6500#heading--MAAS-VM-hosting) available, if you need it.

** How to add a VM

To compose a basic VM:

    maas $PROFILE vm-host compose $VM_HOST_ID

Example output for default composing:

    {
        "system_id": "73yxmc",
        "resource_uri": "/MAAS/api/2.0/machines/73yxmc/"
    }

*** How to set resources while adding a VM

Compose with resources specified:

    maas $PROFILE vm-host compose $VM_HOST_ID $RESOURCES

Where $RESOURCES is a space-separated list of six constraints:

1. *cores=* requested cores
2. *cpu_speed=* requested minimum cpu speed in MHz
3. *memory=* requested memory in MB
4. *architecture=* See [Architecture](#heading--architecture) below 
5. *storage=* See [Storage](#heading--storage) below
6. *interfaces=* See [Interfaces](#heading--interfaces) below

*** How to set the architecture while adding a VM

To list available architectures:

    maas $PROFILE boot-resources read

Then, for example:

    maas $PROFILE vm-host compose $VM_HOST_ID \
        cores=40 cpu_speed=2000 memory=7812 architecture="amd64/generic"

*** How to set storage parameters while adding a VM


Storage parameters look like this:

    storage="<label>:<size in GB>(<storage pool name>),<label>:<size in GB>(<storage pool name>)"

For example, let's examine how to compose a machine with the following two disks:

1.   32 GB disk from storage pool `pool1`
2.   64 GB disk from storage pool `pool2`

where we want the first disk to be a bootable root partition `/` and the second to be a home directory.

First, create the VM:

    maas $PROFILE vm-host compose $VM_HOST_ID "storage=mylabel:32(pool1),mylabel:64(pool2)"

Note that the labels, here `mylabel`, are an ephemeral convenience that you might find useful in scripting MAAS actions.

MAAS will create a VM with 2 disks, `/dev/vda` (32 GB) and `/dev/vdb` (64 GB). After MAAS enlists, commissions and allocates the machine, you can edit the disks before deploying to suit your needs. For example, we'll set a boot, root, and home partition.

We'll start by deleting the `/` partition MAAS created because we want a separate `/boot` partition to demonstrate how yo.

    maas admin partition delete $VM_HOST_ID $DISK1_ID $PARTITION_ID

[note]
To find `$DISK1_ID` and `$PARTITION_ID`, use `maas admin machine read $VM_HOST_ID`.
[/note]

Now, create a boot partition (~512MB):

    maas admin partitions create $VM_HOST_ID $DISK1_ID size=512000000 bootable=True

We'll use the remaining space for the root partition, so create another without specifying size:

    maas admin partitions create $VM_HOST_ID $DISK1_ID

Finally, create a partition to use as the home directory. Here we'll use the entire space:

    maas admin partitions create $VM_HOST_ID $DISK2_ID

[note]
To find `$DISK2_ID`, use `maas admin machine read $VM_HOST_ID`.
[/note]

Now, format the partitions. This requires three commands:

    maas admin partition format $VM_HOST_ID $DISK1_ID $BOOT_PARTITION_ID fstype=ext2
    maas admin partition format $VM_HOST_ID $DISK1_ID $ROOT_PARTITION_ID fstype=ext4
    maas admin partition format $VM_HOST_ID $DISK2_ID $HOME_PARTITION_ID fstype=ext4

[note]
To find the partition IDs, use `maas admin partitions read $VM_HOST_ID $DISK1_ID` and `maas admin partitions read $VM_HOST_ID $DISK2_ID`
[/note]

Before you can deploy the machine with our partition layout, you need to mount the new partitions. Here, we'll do that in three commands:

    maas admin partition mount $SYSTEM_ID $DISK1_ID $BOOT_PARTITION_ID     "mount_point=/boot"
    maas admin partition mount $SYSTEM_ID $DISK1_ID $ROOT_PARTITION_ID "mount_point=/"
    maas admin partition mount $SYSTEM_ID $DISK2_ID $HOME_PARTITION_ID "mount_point=/home"

Finally, we deploy the machine. MAAS will use the partitions as we have defined them, similar to a normal Ubuntu desktop install:

    maas admin machine deploy $SYSTEM_ID

*** How to specify interfaces while adding a VM

Using the `interfaces` constraint, you can compose virtual machines with interfaces, allowing the selection of VM host NICs.

If you don't specify an `interfaces` constraint, MAAS maintains backward compatibility by checking for a `maas` network, then a `default` network to which to connect the virtual machine.

If you specify an `interfaces` constraint, MAAS creates a `bridge` or `macvlan` attachment to the networks that match the given constraint. MAAS prefers `bridge` interface attachments when possible since this typically results in successful communication.

Consider the following interfaces constraint:

    interfaces=eth0:space=maas;eth1:space=storage

Assuming you deploy the VM host on a machine or controller with access to the `maas` and `storage` [spaces](/t/maas-glossary/5416#heading--spaces), MAAS will create an `eth0` interface bound to the `maas` space and an `eth1` interface bound to the `storage` space.

Another example tells MAAS to assign unallocated IP addresses:

    interfaces=eth0:ip=172.16.99.42

MAAS automatically converts the `ip` constraint to a VLAN constraint (matching the VLAN which corresponds to the subnet can be found -- e.g. `172.16.99.0/24`.) and assigns the IP address to the newly-composed machine upon allocation.

See the Machines [MAAS API documentation](https://maas.io/docs/api#machines)`↗` for a list of all constraint keys.

*** How to find a VM host ID

Here's a simple way to find a VM host's ID by name using `jq`:

    maas $PROFILE vm-hosts read | jq '.[] | select (.name=="MyVMHost") | .name, .id'

Example output:

    "MyVMHost"
    1

** How to delete a VM

    maas $PROFILE machine delete $SYSTEM_ID

After you delete a machine, its resources will be available for other VMs.
[/tab]
[/tabs]

* How to manage VM hosts
In order to  deploy a VM host in your MAAS network, you first need to set up a bridge to connect between your VM host and MAAS itself. Once that's done, you can add and manage VM hosts -- and subsequently, create VMs to act as MAAS machines.  This article explains:

[tabs]
[tab version="v3.4 Snap,v3.4 Packages,v3.3 Snap,v3.3 Packages,v3.2 Snap,v3.2 Packages,v3.1 Snap,v3.1 Packages" view="UI"]
- [How to set up a VM host bridge with the web UI](#heading--maas-bridge-web-ui)
- [How to set up a VM host bridge with netplan](#heading--maas-bridge-netplan)
- [How to set up a VM host bridge with libvirt](#heading--maas-bridge-libvirt)
- [How to set up SSH for use by libvirt](#heading--set-up-ssh)
- [How to add a VM host](#heading--adding-a-vm-host)
- [How to delete a VM host](#heading--deleting-a-vm-host)
- [How to configure a VM host](#heading--configuration)
 - [How to use LXD clusters](#heading--lxd-clusters)
[/tab]
[tab version="v3.4 Snap,v3.4 Packages,v3.3 Snap,v3.3 Packages,v3.2 Snap,v3.2 Packages,v3.1 Snap,v3.1 Packages" view="CLI"]
- [How to set up a VM host bridge with the MAAS CLI/API](#heading--maas-bridge-cli)
- [How to set up a VM host bridge with netplan](#heading--maas-bridge-netplan)
- [How to set up a VM host bridge with libvirt](#heading--maas-bridge-libvirt)
- [How to set up SSH for use by libvirt](#heading--set-up-ssh)
- [How to add a VM host](#heading--adding-a-vm-host)
- [How to configure a VM host](#heading--configuration)
- [How to list VM-hosts](#heading--list-vm-hosts)
- [How to list configurable VM host parameters](#heading--list-config-params)
- [How to change a VM host's name](#heading--change-vm-host-name)
- [How to change a VM host's pool](#heading--change-vm-host-pool)
- [How to list the resources of all VM hosts](#heading--list-resources-of-all-vm-hosts)
- [How to list the resources of a single VM host](#heading--list-resources-of-a-vm-host)
- [How to update a VM host's configuration](#heading--update-vm-host-configuration)
- [How to list a VM host's connection parameters](#heading--list-vm-host-connection-parameters)
 - [How to use LXD clusters](#heading--lxd-clusters)
[/tab]
[tab version="v3.0 Snap,v3.0 Packages,v2.9 Snap,v2.9 Packages" view="UI"]
- [How to set up a VM host bridge with the web UI](#heading--maas-bridge-web-ui)
- [How to set up a VM host bridge with netplan](#heading--maas-bridge-netplan)
- [How to set up a VM host bridge with libvirt](#heading--maas-bridge-libvirt)
- [How to set up SSH for use by libvirt](#heading--set-up-ssh)
- [How to add a VM host](#heading--adding-a-vm-host)
- [How to configure a VM host](#heading--configuration)
[/tab]
[tab version="v3.0 Snap,v3.0 Packages,v2.9 Snap,v2.9 Packages" view="CLI"]
- [How to set up a VM host bridge with the MAAS CLI/API](#heading--maas-bridge-cli)
- [How to set up a VM host bridge with netplan](#heading--maas-bridge-netplan)
- [How to set up a VM host bridge with libvirt](#heading--maas-bridge-libvirt)
- [How to set up SSH for use by libvirt](#heading--set-up-ssh)
- [How to add a VM host](#heading--adding-a-vm-host)
- [How to configure a VM host](#heading--configuration)
- [How to list VM-hosts](#heading--list-vm-hosts)
- [How to list configurable VM host parameters](#heading--list-config-params)
- [How to change a VM host's name](#heading--change-vm-host-name)
- [How to change a VM host's pool](#heading--change-vm-host-pool)
- [How to list the resources of all VM hosts](#heading--list-resources-of-all-vm-hosts)
- [How to list the resources of a single VM host](#heading--list-resources-of-a-vm-host)
- [How to update a VM host's configuration](#heading--update-vm-host-configuration)
- [How to list a VM host's connection parameters](#heading--list-vm-host-connection-parameters)
[/tab]
[/tabs]

To enable VM host networking features, MAAS must match the VM host IP address of a potential VM host with a known device (a machine or controller). For example, if a machine not known to MAAS is set up as a VM host, enhanced interface selection features will not be available.

[note]
It's essential to enforce usage of IP addresses to avoid domain name conflicts, should different controllers resolve the same domain name with different IP addresses. You should also avoid using 127.0.0.1 when running multiple controllers, as it would confuse MAAS.
[/note]

If you need some background on VM hosting, we have a [refresher](/t/how-to-deploy-virtual-machines/6500#heading--MAAS-VM-hosting) available.

[tabs]
[tab version="v3.4 Snap,v3.4 Packages,v3.3 Snap,v3.3 Packages,v3.2 Snap,v3.2 Packages,v3.1 Snap,v3.1 Packages,v3.0 Snap,v3.0 Packages,v2.9 Snap,v2.9 Packages" view="UI"]
** How to set up a VM host bridge with the web UI

To set up a VM host bridge with the Web UI:

1. Select *Machines*.

2. Select the machine you want to use as a VM host.

3. Select *Network*.

4. Select the network where you want to create the bridge.

5. Select *Create bridge*.

6. Configure the bridge on a subnet MAAS controls.  You may use any IP mode for the bridge.

7. Register your changes with *Create bridge*.

Then you can deploy Ubuntu.

[/tab]
[tab version="v3.4 Snap,v3.4 Packages,v3.3 Snap,v3.3 Packages,v3.2 Snap,v3.2 Packages,v3.1 Snap,v3.1 Packages,v3.0 Snap,v3.0 Packages,v2.9 Snap,v2.9 Packages" view="CLI"]
** How to use the MAAS API to configure a bridge

You can also use the MAAS CLI/API to configure a VM host bridge, with the following procedure:

1. Select the interface you wish to configure the bridge on. This example uses the boot interface, since the boot interface must be connected to a MAAS controlled network -- but any interface is allowed:

        INTERFACE_ID=$(maas $PROFILE machine read $SYSTEM_ID | jq .boot_interface.id)

2. Create the bridge:

         BRIDGE_ID=$(maas $PROFILE interfaces create-bridge $SYSTEM_ID name=br0 parent=$INTERFACE_ID | jq .id)

3. Select the subnet where you want the bridge (this should be a MAAS controlled subnet):

        SUBNET_ID=$(maas $PROFILE subnets read | jq -r '.[] | select(.cidr == "10.0.0.0/24" and .managed == true).id')

4. Connect the bridge to the subnet:

          maas $PROFILE interface link-subnet $SYSTEM_ID $BRIDGE_ID subnet=$SUBNET_ID mode="STATIC" ip_address="10.0.0.101"

[/tab]
[/tabs]

** How to set up a VM host bridge with netplan

You can also use netplan to configure a VM host bridge:

Open your netplan configuration file.  This should be in `/etc/netplan`.  It could be called `50-cloud-init.yaml`, `netplan.yaml`, or something else.  Modify the file to add a bridge, using the example below to guide you:

```nohighlight
network:
    bridges:
        br0:
            addresses:
            - 10.0.0.101/24
            gateway4: 10.0.0.1
            interfaces:
            - enp1s0
            macaddress: 52:54:00:39:9d:f9
            mtu: 1500
            nameservers:
                addresses:
                - 10.0.0.2
                search:
                - maas
            parameters:
                forward-delay: 15
                stp: false
    ethernets:
        enp1s0:
            match:
                macaddress: 52:54:00:39:9d:f9
            mtu: 1500
            set-name: enp1s0
        enp2s0:
            match:
                macaddress: 52:54:00:df:87:ac
            mtu: 1500
            set-name: enp2s0
        enp3s0:
            match:
                macaddress: 52:54:00:a7:ac:46
            mtu: 1500
            set-name: enp3s0
    version: 2
```

Apply the new configuration with `netplan apply`.

** How to set up a VM host bridge with libvirt

It is also possible to use [libvirt](https://ubuntu.com/server/docs/virtualization-libvirt)`↗` to configure a virtual bridge.  This method will also work for LXD VM hosts running on Ubuntu.  Be aware that other methods may be required if you are configuring LXD on an OS other than Ubuntu.

By default, libvirt creates a virtual bridge, `virbr0`, through which VMs communicate with each other and the Internet. DHCP, supplied by libvirt, automatically assigns an IP address to each VM.  However, to enable network booting in MAAS, you’ll need to provide DHCP in MAAS and either:

- Disable DHCP on libvirt’s `default` network, or
- Create a new libvirt network `maas` with DHCP disabled.

You can set up such a `maas` network like this:

```nohighlight
cat << EOF > maas.xml
<network>
 <name>maas</name>
 <forward mode='nat'>
   <nat>
     <port start='1024' end='65535'/>
   </nat>
 </forward>
 <dns enable="no" />
 <bridge name='virbr1' stp='off' delay='0'/>
 <domain name='testnet'/>
 <ip address='172.16.99.1' netmask='255.255.255.0'>
 </ip>
</network>
EOF
virsh net-define maas.xml
```

Note that this network also has NAT port forwarding enabled to allow VMs to communicate with the Internet at large. Port forwarding is very useful in test environments.

** How to set up SSH for use by libvirt

For MAAS to successfully communicate with libvirt on your VM host machine -- whether you're running from snap or package, or running rack controllers in LXD containers or on localhost -- this example command must succeed from every rack controller:

```nohighlight
virsh -c qemu+ssh://$USER@$VM_HOST_IP/system list --all
```

Here, `$USER` is a user on your VM host who is a member of the `libvirtd` Unix group on the VM host, and `$VM_HOST_IP` is the IP of your VM host.  **Note** that insufficient permissions for `$USER` may cause the `virsh` command to fail with an error such as `failed to connect to the hypervisor`. Check the `$USER` group membership to make sure `$USER` is a member of the `libvirtd` group.

[tabs]
[tab version="v3.4 Snap,v3.3 Snap"]
*** How to set up SSH (libvirt only)

If you installed MAAS via snap, then create the needed SSH keys this way:

```nohighlight
sudo mkdir -p /var/snap/maas/current/root/.ssh
cd /var/snap/maas/current/root/.ssh
sudo ssh-keygen -f id_rsa
```

Finally, on the VM host, you'll need to add `id_rsa.pub` to the `authorized_keys` file in `/home/<vm-host-user-homedir-name>/.ssh/`,  where `<vm-host-user-homedir-name>` is the name of your VM host user.
[/tab]
[tab version="v3.4 Packages,v3.3 Packages"]
*** How to set up SSH (libvirt only)

The `maas` user on your rack controllers will issue all libvirt commands. Therefore, you'll need to set up SSH public keys on every rack controller for user `maas`.  First create SSH keys on all rack controllers:

```nohighlight
$ sudo -i
root@maas:~$ mkdir -p /var/snap/maas/current/root/.ssh
root@maas:~$ cd /var/snap/maas/current/root/.ssh
root@maas:~$ ssh-keygen -f id_rsa
```

Next, add the contents of `~maas/.ssh/id_rsa.pub` to the VM host user's `~$USER/.ssh/authorized_keys`. To accomplish this, log into your VM host node, via SSH, from a host for which MAAS has a matching public SSH key.
[/tab]
[tab version="v3.2 Snap"]
*** How to set up SSH (libvirt only)

If you installed MAAS via snap, then create the needed SSH keys this way:

```nohighlight
sudo mkdir -p /var/snap/maas/current/root/.ssh
cd /var/snap/maas/current/root/.ssh
sudo ssh-keygen -f id_rsa
```

Finally, on the VM host, you'll need to add `id_rsa.pub` to the `authorized_keys` file in `/home/<vm-host-user-homedir-name>/.ssh/`,  where `<vm-host-user-homedir-name>` is the name of your VM host user.
[/tab]
[tab version="v3.2 Packages"]
*** How to set up SSH (libvirt only)

The `maas` user on your rack controllers will issue all libvirt commands. Therefore, you'll need to set up SSH public keys on every rack controller for user `maas`.  First create SSH keys on all rack controllers:

```nohighlight
$ sudo -i
root@maas:~$ mkdir -p /var/snap/maas/current/root/.ssh
root@maas:~$ cd /var/snap/maas/current/root/.ssh
root@maas:~$ ssh-keygen -f id_rsa
```

Next, add the contents of `~maas/.ssh/id_rsa.pub` to the VM host user's `~$USER/.ssh/authorized_keys`. To accomplish this, log into your VM host node, via SSH, from a host for which MAAS has a matching public SSH key.
[/tab]
[tab version="v3.1 Snap"]
*** How to set up SSH (libvirt only)

If you installed MAAS via snap, then create the needed SSH keys this way:

```nohighlight
sudo mkdir -p /var/snap/maas/current/root/.ssh
cd /var/snap/maas/current/root/.ssh
sudo ssh-keygen -f id_rsa
```

Finally, on the VM host, you'll need to add `id_rsa.pub` to the `authorized_keys` file in `/home/<vm-host-user-homedir-name>/.ssh/`,  where `<vm-host-user-homedir-name>` is the name of your VM host user.
[/tab]
[tab version="v3.1 Packages"]
*** How to set up SSH (libvirt only)

The `maas` user on your rack controllers will issue all libvirt commands. Therefore, you'll need to set up SSH public keys on every rack controller for user `maas`.  First create SSH keys on all rack controllers:

```nohighlight
$ sudo -i
root@maas:~$ mkdir -p /var/snap/maas/current/root/.ssh
root@maas:~$ cd /var/snap/maas/current/root/.ssh
root@maas:~$ ssh-keygen -f id_rsa
```

Next, add the contents of `~maas/.ssh/id_rsa.pub` to the VM host user's `~$USER/.ssh/authorized_keys`. To accomplish this, log into your VM host node, via SSH, from a host for which MAAS has a matching public SSH key.
[/tab]
[tab version="v3.0 Snap"]
*** How to set up SSH (libvirt only)

If you installed MAAS via snap, then create the needed SSH keys this way:

```nohighlight
sudo mkdir -p /var/snap/maas/current/root/.ssh
cd /var/snap/maas/current/root/.ssh
sudo ssh-keygen -f id_rsa
```

Finally, on the VM host, you'll need to add `id_rsa.pub` to the `authorized_keys` file in `/home/<vm-host-user-homedir-name>/.ssh/`,  where `<vm-host-user-homedir-name>` is the name of your VM host user.
[/tab]
[tab version="v3.0 Packages"]
*** How to set up SSH (libvirt only)

The `maas` user on your rack controllers will issue all libvirt commands. Therefore, you'll need to set up SSH public keys on every rack controller for user `maas`.  First create SSH keys on all rack controllers:

```nohighlight
$ sudo -i
root@maas:~$ mkdir -p /var/snap/maas/current/root/.ssh
root@maas:~$ cd /var/snap/maas/current/root/.ssh
root@maas:~$ ssh-keygen -f id_rsa
```

Next, add the contents of `~maas/.ssh/id_rsa.pub` to the VM host user's `~$USER/.ssh/authorized_keys`. To accomplish this, log into your VM host node, via SSH, from a host for which MAAS has a matching public SSH key.
[/tab]
[tab version="v2.9 Snap"]
*** How to set up SSH (libvirt only)

If you installed MAAS via snap, then create the needed SSH keys this way:

```nohighlight
sudo mkdir -p /var/snap/maas/current/root/.ssh
cd /var/snap/maas/current/root/.ssh
sudo ssh-keygen -f id_rsa
```

Finally, on the VM host, you'll need to add `id_rsa.pub` to the `authorized_keys` file in `/home/<vm-host-user-homedir-name>/.ssh/`,  where `<vm-host-user-homedir-name>` is the name of your VM host user.
[/tab]
[tab version="v2.9 Packages"]
*** How to set up SSH (libvirt only)

The `maas` user on your rack controllers will issue all libvirt commands. Therefore, you'll need to set up SSH public keys on every rack controller for user `maas`.  First create SSH keys on all rack controllers:

```nohighlight
$ sudo -i
root@maas:~$ mkdir -p /var/snap/maas/current/root/.ssh
root@maas:~$ cd /var/snap/maas/current/root/.ssh
root@maas:~$ ssh-keygen -f id_rsa
```

Next, add the contents of `~maas/.ssh/id_rsa.pub` to the VM host user's `~$USER/.ssh/authorized_keys`. To accomplish this, log into your VM host node, via SSH, from a host for which MAAS has a matching public SSH key.
[/tab]
[/tabs]

[tabs]
[tab version="v3.4 Snap,v3.4 Packages,v3.3 Snap,v3.3 Packages,v3.2 Snap,v3.2 Packages,v3.1 Snap,v3.1 Packages" view="UI"]
** How to add a LXD VM host with a MAAS-generated certificate

To add a LXD VM host with a MAAS-generated certificate:

1. Select *KVM > LXD*.

2. Select *Add KVM*.

3. Enter a *Name* for the KVM host.

4. Optionally, select a non-default *Zone*.

5. Optionally, select a non-default *Resource pool*.

6. Enter the *LXD address* as the gateway address of the bridge for that LXD instance.  For example, if `lxdbr0` has address `10.4.241.0`, the default gateway address is `10.4.241.1`.

7. Select *Generate new certificate*.

8. Select *Next*.

9. Select *Add trust to LXD via command line*.

10. Copy the bash command and certificate from the text box.

11. In a terminal, paste the copied command and make sure that it runs.

12. Select *Check authentication*.  You'll switch screens; if all goes well, you'll see *Connected* with a green checkmark.

13. Select *Add new project* or *Select existing project*.  Be aware that if you select an existing project, any VMs already in that project will begin to commission.

14. Select *Next*.  You will drop out to a dashboard for the VM host.

You can then add virtual machines to this new VM host as desired.

**** How to add a LXD VM host using an existing certificate

To use your own existing certificate with a LXD VM host:

1. Select *KVM*.

2. Select *Add KVM*.

3. Enter a *Name* for the KVM host.

4. Optionally, select a non-default *Zone*.

5. Optionally, select a non-default *Resource pool*.

6. Enter the *LXD address* as the gateway address of the bridge for that LXD instance.  For example, if `lxdbr0` has address `10.4.241.0`, the default gateway address is `10.4.241.1`.

7. Select *Provide certificate and private key*.  The screen will extend.

8. *Upload a certificate* or paste one in the certificate text box.

9. *Upload a private key* or paste on in the private key text box.

10. Select *Next*.

11. Select *Add trust to LXD via command line*.

12. Copy the bash command and certificate from the text box.

13. In a terminal, paste the copied command and make sure that it runs.

14. Select *Check authentication*.  You'll switch screens; if all goes well, you'll see *Connected* with a green checkmark.

15. Select *Add new project* or *Select existing project*.  Be aware that if you select an existing project, any VMs already in that project will begin to commission.

16. Select *Next*.  You will drop out to a dashboard for the VM host.

You can then add virtual machines to this new VM host as desired.

** How to delete a VM host

To delete a VM host:

1. Select *KVM*.

2. Select the VM host you wish to configure.

3. Select *KVM host settings*.

4. Select *Danger zone >> Remove KVM host*.  You will need to confirm this choice.

There is no way to recover the VM host after confirming.
[/tab]
[tab version="v3.0 Snap,v3.0 Packages" view="UI"]
** How to add VM host

To add a VM host:

1. Select *KVM*.

2. Select *Add KVM*.

3. Choose the *KVM host type*.

3. Enter a *Name* for the KVM host.

4. Optionally, select a non-default *Zone*.

5. Optionally, select a non-default *Resource pool*.

6. If you chose the *LXD* host type, enter the *LXD address* as the gateway address of the bridge for that LXD instance.  For example, if `lxdbr0` has address `10.4.241.0`, the default gateway address is `10.4.241.1`.

7. If you chose the *virsh* host type, enter the *Virsh address*, which is of the form `qemu+ssh://<vm host IP>/system`.

8. Enter any requested passwords, if needed.

9. Select *Authenticate*.  MAAS will drop to a KVM host screen.

You can then add virtual machines to this new VM host as desired.
[/tab]
[tab version="v3.4 Snap,v3.4 Packages,v3.3 Snap,v3.3 Packages,v3.2 Snap,v3.2 Packages,v3.1 Snap,v3.1 Packages,v3.0 Snap,v3.0 Packages" view="CLI"]
To add a VM host:

```nohighlight
maas $PROFILE vm-host create type=$VM_HOST_TYPE power_address=$POWER_ADDRESS \
    [power_user=$USERNAME] power_pass=$PASSWORD {project=$PROJECT} \
    [zone=$ZONE] [tags=$TAG1,$TAG2,...]
```

$VM_HOST_TYPE can currently take two values: `virsh` and `lxd`.

$POWER_ADDRESS typically looks like the following for libvirt:

    qemu+ssh://<vm host IP>/system

of like this for LXD (Beta):

    https://10.0.0.100:8443

Both $USERNAME and $PASSWORD are optional for the virsh power type. $ZONE and $TAGS are optional for all VM hosts.

The `power_...` parameters will vary with power type.  See the [API reference](/docs/api#power-types) for a listing of available power types.

For example, to create a LXD VM host, enter the following:

```nohighlight
maas $PROFILE vm-hosts create type=lxd power_address=$LXD_BRIDGE_ADDRESS \
   power_pass=$LXD_TRUST_PASSWORD project=$PROJECT_NAME
```

Note that for LXD VM hosts, a project name is not optional.  Project names cannot contain spaces or special characters. If you enter a project name which doesn't exist, MAAS will create the LXD project for you.
[/tab]
[tab version="v2.9 Snap,v2.9 Packages" view="CLI"]
To add a VM host:

```nohighlight
maas $PROFILE vm-host create type=$VM_HOST_TYPE power_address=$POWER_ADDRESS \
    [power_user=$USERNAME] [power_pass=$PASSWORD] [zone=$ZONE] \
    [tags=$TAG1,$TAG2,...]
```

$VM_HOST_TYPE can currently take three values: `rsd`, `virsh`, and `lxd`.

$POWER_ADDRESS typically looks like the following for libvirt:

    qemu+ssh://<vm host IP>/system

of like this for LXD (Beta):

    https://10.0.0.100:8443

Both $USERNAME and $PASSWORD are optional for the virsh power type. $ZONE and $TAGS are optional for all VM hosts.

The `power_...` parameters will vary with power type.  See the [API reference](/docs/api#power-types) for a listing of available power types.

*** Some examples

For example, to create an RSD VM host, enter:

```nohighlight
maas $PROFILE vm-hosts create type=rsd power_address=10.3.0.1:8443 \
    power_user=admin power_pass=admin
```

To create a KVM host, enter the following:

```nohighlight
maas $PROFILE vm-hosts create type=virsh power_address=qemu+ssh://ubuntu@192.168.1.2/system
```
[/tab]
[/tabs]

[note]
MAAS will automatically discover and store the resources your VM host contains. Any existing machines will also appear on the 'Machines' page, and MAAS will automatically attempt to commission them.
[/note]

** How to configure a VM host

[tabs]
[tab version="v3.4 Snap,v3.4 Packages,v3.3 Snap,v3.3 Packages,v3.2 Snap,v3.2 Packages,v3.1 Snap,v3.1 Packages,v3.0 Snap,v3.0 Packages,v2.9 Snap,v2.9 Packages" view="UI"]
To configure a VM host:

1. Select *KVM > LXD*.

2. Select the VM host you wish to configure.

3. Select *KVM host settings*.

4. Optionally set *KVM configuration >> Zone* by selecting from the dropdown.

5. Optionally set the *KVM configuration >> Resource pool* by selecting from the dropdown.

6. Optionally add or change *KVM configuration >> Tags*.

7. Optionally change the *KVM configuration >> CPU overcommit* ratio by moving the slider.

8. Optionally change the *KVM configuration >> Memory overcommit* ratio by moving the slider.

9. If you've made changes to this point, select *KVM configuration >> Save changes*.  MAAS will save the *KVM configuration* changes, but will not switch screens.

10. If you need to change the *Authentication >> Certificate*, you may do so.  Make sure to choose *Update certificate* to register your changes.

11. If you want to remove this KVM host, choose *Danger zone >> Remove KVM host*.  You will need to confirm this choice.
[/tab]
[tab version="v3.4 Snap,v3.4 Packages,v3.3 Snap,v3.3 Packages,v3.2 Snap,v3.2 Packages,v3.1 Snap,v3.1 Packages,v3.0 Snap,v3.0 Packages,v2.9 Snap,v2.9 Packages" view="CLI"]
Using the CLI, it's possible to update the configuration of a VM host.  You can change these configurable parameters with an `update` command -- but first, you'll want to know how to check the values of configurable parameters, both before and after the change.

*** How to list VM-hosts

To begin, you can list your available KVM-hosts with the following command:

```nohighlight
maas admin vm-hosts read | jq -r '(["ID, "VM-HOST","SYSID","CORES",
"USED","RAM", "USED","STORAGE", "USED"] | (., map(length*"-"))),
(.[]| [.id,.name,.host.system_id,.total.cores, .used.cores, .total.memory, .used.memory,.total.local_storage, .used.local_storage])
| @tsv' | column -t
```

*** How to list configurable VM host parameters

There are just a few parameters that you can change for a VM host.  You can list these, on a per-host basis, using the following two-step procedure:

1. Run the command above to get the VM host ID (different from the System ID, see the first column in the listing).

2. Enter the following command to list configurable parameters:

```nohighlight
maas admin vm-host read $ID | jq -r '(["ID","NAME","POOL","ZONE",
"CPU-O/C", "RAM-O/C", "TAGS"] | (., map(length*"-"))), (.| [.id,.name,
.pool.name, .zone.name,.cpu_over_commit_ratio, 
.memory_over_commit_ratio, .tags[]]) | @tsv' | column -t
```

where $ID is the ID (not System ID) of the VM-host.

*** How to change a VM host's name

You can change the VM host's name very simply, with this command:

    maas admin vm-host update $ID name=$NEW_NAME

where $ID is the VM host's ID (not System ID), and $NEW_NAME is the new name you want to assign.  You can check that the change was successful by just printing out the ID and name, like this:

```nohighlight
maas admin vm-host read $ID | jq -r '(["ID","NAME"] 
| (., map(length*"-"))), (.| [.id,.name]) 
| @tsv' | column -t
```

*** How to change a VM host's pool

You can also change the VM host's pool with a simple command:

```nohighlight
maas admin vm-host update $ID pool=$VALID_POOL
```

where $ID is the VM host's ID (not System ID), and $VALID_POOL is the name of a pool that already exists.  If you mention a pool you haven't created yet, you'll get an error like this:

```nohighlight
{"pool": ["Select a valid choice. That choice is not one of the available choices."]}
```

```nohighlight
maas admin resource-pools read | jq -r '.[] | (.name)'
```

If you really want to set your VM host to a new one, you just need to create a new one with this command:

```nohighlight
maas admin resource-pools create name=$NEW_POOL_NAME
```

Then double-check it with `catvmpools`, and assign your VM host to it using the earlier command. 

** How to list the resources of all VM hosts

```nohighlight
maas $PROFILE vm-hosts read
```

A portion of the sample output:

``` no-highlight
        "id": 93,
        "capabilities": [
            "composable",
            "fixed_local_storage",
            "iscsi_storage"
        ],
        "name": "civil-hermit",
```

** How to list the resources of a single VM host

To list an individual VM host's resources:

```nohighlight
maas $PROFILE vm-host read $VM_HOST_ID
```

** How to update a VM host's configuration

Update overcommit ratios for a KVM host:

```nohighlight
maas $PROFILE vm-host update $VM_HOST_ID power_address=qemu+ssh://ubuntu@192.168.1.2/system \
        power_pass=example cpu_over_commit_ratio=2.5 memory_over_commit_ratio=10.0
```

Update the default storage pool used by a KVM host:

```nohighlight
maas $PROFILE vm-host update $VM_HOST_ID power_address=qemu+ssh://ubuntu@192.168.1.2/system \
        power_pass=example default_storage_pool=pool2
```

** How to list a VM host's connection parameters

To list a VM host's connection parameters:

```nohighlight
maas $PROFILE vm-host parameters $VM_HOST_ID
```

Example output:

```no-highlight
{
    "power_address": "10.3.0.1:8443",
    "power_pass": "admin",
    "power_user": "admin"
}
```
[/tab]
[/tabs]

*** LXD clusters

[tabs]
[tab version="v3.4 Snap,v3.4 Packages,v3.3 Snap,v3.3 Packages,v3.2 Snap,v3.2 Packages,v3.1 Snap,v3.1 Packages"]
MAAS takes advantage of the existing LXD clustering capability.

**** About LXD clusters

LXD clusters within the context of MAAS are a way of viewing and managing existing VM host clusters and composing VMs within said cluster.  MAAS will not create a new cluster, but will discover an existing cluster when you provide the info for adding a single clustered host.

**** How to add LXD clusters

MAAS assumes you have already configured a cluster within the context of LXD. You then need to configure this cluster with a single trust MAAS will use to communicate with said cluster. 

The process of adding a LXD cluster is [identical to the procedure for adding a LXD VM host](#heading--adding-a-vm-host).  The only difference is that the name you provide will be used for the cluster instead of the individual host. 

MAAS will then connect to the provided host and discover the other hosts within the cluster, and rename the initially defined host with the cluster member name configured in LXD. The VM host will show up as a *Cluster* on the dashboard. 

**** How to compose VMs in LXD clusters

To compose VMs in a LXD cluster, follow the procedure for [adding a VM to a VM host](/t/how-to-create-and-manage-vms/5148). 

*** How to delete LXD clusters

To delete a LXD cluster, [delete any VM host within the cluster](#heading--deleting-a-vm-host).  

[note]
This will delete the cluster and **all** members within the cluster.  Make sure that's what you want to do.
[/note]
[/tab]
[tab version="v3.0 Snap,v3.0 Packages,v2.9 Snap,v2.9 Packages"]
LXD clusters are available to MAAS starting with MAAS version 3.1.
[/tab]
[/tabs]

* How to mirror images locally
Mirroring images is worthy of consideration.  The SimpleStreams protocol delivers Images to MAAS, which is especially useful when your Internet link is slow or unreliable. In this mirrored configuration, images will be instantly available when MAAS requests them.

Canonical provides two SimpleStreams for MAAS images: candidate and stable. Both streams contain Ubuntu images, CentOS images, bootloaders extracted from the Ubuntu archive, and release notifications. Either stream can be used in any version of MAAS greater than 2.1, but not all images are supported in older versions.

This article will tell you:

- [How to change the stream with the UI](#heading--changing-the-stream)
- [How to change the stream with the CLI](#heading--changing-stream-with-cli)
- [How to set up a local image mirror](#heading--set-up-local-mirror)

** How to change the stream with the UI

To switch to the candidate stream:

1. Select *Images*.

2. Select *Change source*.

3. Select *Custom*.

4. Set the *URL* to `http://images.maas.io/ephemeral-v3/candidate`.

5. Select *Connect*.

MAAS uses the stable stream by default. To switch back to the stable stream, simply repeat the above procedure, but set the *URL* to `maas.io`.

** How to change the stream with the CLI

To switch to a stream with the CLI, enter the following commands:

```
BOOT_SOURCE_ID=$(maas $PROFILE boot-sources read | jq '.[] | select(.url | contains("images.maas.io/ephemeral-v3")) | .id')
maas $PROFILE boot-source update $BOOT_SOURCE_ID url=$STREAM_URL
```
** How to set up a local mirror

To use mirroring, you begin by installing the necessary software on the host that will house the mirror:

``` bash
sudo apt install simplestreams
```

First define some variables to unclutter eventual CLI commands:

``` bash
KEYRING_FILE=/usr/share/keyrings/ubuntu-cloudimage-keyring.gpg
IMAGE_SRC=https://images.maas.io/ephemeral-v3/stable
IMAGE_DIR=/var/www/html/maas/images/ephemeral-v3/stable
```

The below example selects all available kernels that are compatible with either Ubuntu 18.04 (Bionic) and Ubuntu 20.04 (Focal) for the amd64 architecture, resulting in a download of approximately 3.1 GB. The second command mirrors the bootloaders.

``` bash
sudo sstream-mirror --keyring=$KEYRING_FILE $IMAGE_SRC $IMAGE_DIR \
    'arch=amd64' 'release~(bionic|focal)' --max=1 --progress
sudo sstream-mirror --keyring=$KEYRING_FILE $IMAGE_SRC $IMAGE_DIR \
    'os~(grub*|pxelinux)' --max=1 --progress
```

To know in advance what the `sstream-mirror` command will grab, or if you want to save bandwidth and time by avoiding bad selections, include the `--dry-run` option. When you are satisfied, remove that option to initiate the download.

MAAS will write the images to disk in the directory defined by the variable 'IMAGE_DIR' above, and the 'location' of the new boot source will be:

`URL=http://<myserver>/maas/images/ephemeral-v3/stable/`

Where `<myserver>` identifies your server's hostname or IP address.

Verify the availability of the images by visiting the above URL.

The final `sstream-mirror` command should be invoked at regular intervals (i.e. with `cron`) to ensure the mirror contains the latest images.

* How to observe a live MAAS
[tabs]
[tab version="v3.3 Snap,v3.3 Packages,v3.2 Snap,v3.2 Packages"]
We aim to make MAAS observable, a system in which the internal state can be estimated using only telemetry data. We now offer easier integration with Prometheus and Loki, which are the data ingestion components of the popular Grafana / Prometheus / Loki / AlarmManager stack. This data should be consumed by a stack composed of off-the-shelf open source software, provided by either Juju (for example with the Canonical Observability Stack) or third-parties (SaaS, self-managed).

In this document, you will learn:

- [About MAAS observability](#heading--about-maas-observability)
- [How to configure and use MAAS observability features](#heading--how-to-use-maas-observability-features)
 
** About MAAS observability

Depicted below we have a reference observability stack composed of Prometheus (metrics ingestion and alerting based on metrics), Loki (log ingestion and alerting based on logs), Grafana (visualisation), Alertmanager (notification routing and dispatching) and Grafana Agent (telemetry collector).

<a href="https://discourse.maas.io/uploads/default/optimized/2X/d/d6f66cbb3ea314818894b4f07ca8037628993ae2_2_690x437.png" target = "_blank">![](upload://eGnGAB4W9qzA8wgGzGaWozgmMTl.png)</a>

This document shows how to configure this stack to consume telemetry and to raise alerts of failures.

*** MAAS observability requirements

- a Ubuntu host with MAAS 3.2+ running
- a Ubuntu host with enough storage capacity to hold logs and metrics' time-series

Both hosts require Internet access during the install process. We use LXD to create this setup in a single host, but it's optional. When configuring the stack for a production environment, we advise you to read the Prometheus and Loki documentation to improve security and performance.

** How to use MAAS observability features

Observing MAAS requires three steps: configuring the tool stack, exporting the telemetry, and then verifying that everything is working properly.  This section will show you:

- [How to configure the observability stack](#heading--configuring-the--observability-stack)
- [How to export MAAS controller telemetry](#heading--how-to-export-maas-controller-telemetry)
- [How to verify correct operation](#heading--how-to-verify-correct-operation)

*** How to configure the observability stack

Create a VM with the following script to install all required software.

```bash
export LXD_NET=virbr0
export GRAFANA_REPOS=https://packages.grafana.com/oss/deb
export GRAFANA_KEY=https://packages.grafana.com/gpg.key
export LOKI_PKG=https://github.com/grafana/loki/releases/download/v2.4.2/loki-linux-amd64.zip
export PROM_PKG=https://github.com/prometheus/prometheus/releases/download/v2.31.1/prometheus-2.31.1.linux-amd64.tar.gz
export PROM_ALERT_PKG=https://github.com/prometheus/alertmanager/releases/download/v0.23.0/alertmanager-0.23.0.linux-amd64.tar.gz

cat <<EOF | lxc launch ubuntu: o11y
config:
    user.user-data: |
        #cloud-config
        apt:
            sources:
                grafana:
                    source: 'deb ${GRAFANA_REPOS} stable main'
                    key: |
$(wget -qO- ${GRAFANA_KEY} | sed 's/^/                        /')
        packages:
        - unzip
        - grafana
        - make
        - git
        - python3-pip
        runcmd:
        - mkdir -p /opt/prometheus /opt/loki /opt/alertmanager
        - wget -q "${LOKI_PKG}" -O /tmp/loki-linux-amd64.zip
        - unzip /tmp/loki-linux-amd64.zip -d /opt/loki
        - chmod a+x /opt/loki/loki-linux-amd64
        - wget -qO- "${PROM_PKG}" | tar xz --strip-components=1 -C /opt/prometheus
        - wget -qO- "${PROM_ALERT_PKG}" | tar xz --strip-components=1 -C /opt/alertmanager
        - cat /dev/zero | sudo -u ubuntu -- ssh-keygen -q -N ""
        ssh_authorized_keys:
        - $(cat ${HOME}/.ssh/id_rsa.pub | cut -d' ' -f1-2)
description: O11y stack
devices:
    eth0:
        type: nic
        name: eth0
        network: ${LXD_NET}
EOF

# log into the VM
lxc shell 011y
```

Next, you have to configure and start four services, include Prometheus, Loki, AlertManager, and Grafana.  This subsection will teach you:

- [How to configure and start the Prometheus service](#heading--how-to-configure-and-start-the-prometheus-service)
- [How to configure and start the Loki service](#heading--how-to-configure-and-start-the-loki-service)
- [How to start the AlertManager](#heading--how-to-start-the-alertmanager)
- [How to start Grafana](#heading--how-to-start-grafana)

Once these services are started, you can proceed to export telemetry data and see how your observability tools are working.

**** How to configure and start the Prometheus service

Create the Prometheus configuration.

```bash
cat > /opt/prometheus/prometheus.yaml <<EOF
global:
  evaluation_interval: 1m
rule_files:
  - /var/lib/prometheus/rules/maas/*.yml
alerting:
  alertmanagers:
    - static_configs:
      - targets:
        - localhost:9093
EOF
```

MAAS has a git repository of curated alert rules for Prometheus. Checkout this repository, compile the rules and copy them to prometheus directory.

```bash
git clone https://github.com/canonical/maas-prometheus-alert-rules.git
cd maas-prometheus-alert-rules
make python-deps groups

mkdir -p /var/lib/prometheus/rules/maas
cp group.yml /var/lib/prometheus/rules/maas/
```

Start the Prometheus service. You should enable the *Remote-Write Receiver* function.

```bash
systemd-run -u prometheus /opt/prometheus/prometheus \
    --config.file=/opt/prometheus/prometheus.yaml \
    --enable-feature=remote-write-receiver
```

**** How to configure and start the Loki service

Create the Loki configuration.

```bash
cat > /opt/loki/loki.yaml <<EOF
auth_enabled: false
server:
  http_listen_port: 3100
  grpc_listen_port: 9096
common:
  path_prefix: /var/lib/loki/
  storage:
    filesystem:
      chunks_directory: /var/lib/loki/chunks
      rules_directory: /var/lib/loki/rules
  replication_factor: 1
  ring:
    instance_addr: 127.0.0.1
    kvstore:
      store: inmemory
schema_config:
  configs:
    - from: 2020-10-24
      store: boltdb-shipper
      object_store: filesystem
      schema: v11
      index:
        prefix: index_
        period: 24h
ruler:
  alertmanager_url: http://localhost:9093
  evaluation_interval: 15s
  poll_interval: 1m
  storage:
    type: local
    local:
      directory: /var/lib/loki/rules
  enable_api: true
EOF
```

MAAS has a git repository of curated alert rules for Loki. Checkout this repository, compile the rules and copy them to Loki directory.

```bash
git clone https://github.com/canonical/maas-loki-alert-rules.git
cd maas-loki-alert-rules
make groups

mkdir -p /var/lib/loki/rules/fake
cp rules/bundle.yml /var/lib/loki/rules/fake/
```

Start the Loki service.

```bash
systemd-run -u loki /opt/loki/loki-linux-amd64 \
    --config.file=/opt/loki/loki.yaml
```

**** How to start the AlertManager

The default configuration is enough for receiving alerts from Prometheus and Loki. You should read the project documentation to change it to forward notifications to somewhere useful.

```bash
systemd-run -u alertmanager /opt/alertmanager/alertmanager \
    --config.file=/opt/alertmanager/alertmanager.yml
```

You can access the AlertManager dashboard at `http://<VM_IP>:9093`

**** How to start Grafana

Grafana works out-of-the-box with the default configuration.

```bash
systemctl enable grafana-server
systemctl start grafana-server
```

You can access the dashboard at `http://<VM_IP>:3000`, the default user/password is "admin".

*** How to export MAAS controller telemetry

The Grafana Agent should be installed in the same host as MAAS.

```bash
# Set this to the address of the VM running Loki and Prometheus
export O11y_IP=<VM_IP>
export GRAFANA_AGENT_PKG=https://github.com/grafana/agent/releases/download/v0.22.0/agent-linux-amd64.zip

wget -q "${GRAFANA_AGENT_PKG}" -O /tmp/agent.zip
unzip /tmp/agent.zip -d /opt/agent
chmod a+x /opt/agent/agent-linux-amd64
```

Copy the agent example configuration from MAAS and start the agent. Adapt the environment variable values to your setup.  For example, if you're using a snap, the `MAAS_LOGS` variable would be as shown (`/var/snap/maas/common/log`):

```bash
mkdir -p /var/lib/grafana-agent/positions \
         /var/lib/grafana-agent/wal
cp /snap/maas/current/usr/share/maas/grafana_agent/agent.yaml.example /opt/agent/agent.yml

systemd-run -u telemetry \
    -E HOSTNAME="$(hostname)" \
    -E AGENT_WAL_DIR="/var/lib/grafana-agent/wal" \
    -E AGENT_POS_DIR="/var/lib/grafana-agent/positions" \
    -E PROMETHEUS_REMOTE_WRITE_URL="http://${O11y_IP}:9090/api/v1/write" \
    -E LOKI_API_URL="http://${O11y_IP}:3100/loki/api/v1/push" \
    -E MAAS_LOGS="/var/snap/maas/common/log/" \
    -E MAAS_IS_REGION="true" \
    -E MAAS_IS_RACK="true" \
    -E MAAS_AZ="default" \
    /opt/agent/agent-linux-amd64 \
        -config.expand-env \
        -config.file=/opt/agent/agent.yml
```

On the other hand, if you're using packages, the `MAAS_LOGS` would be `/var/log/maas`, as shown below:

```bash
    ...
    -E MAAS_LOGS="/var/log/maas" \
    ...
```

Be sure to adjust the values of the other environment variables to suit your situation, where applicable.

Next, enable log forwarding in MAAS.

```bash
# set the TCP port the Grafana Agent is listening for syslog messages
# this port must match the one in /opt/agent/agent.yml
maas $ADMIN maas set-config name=promtail_port value=5238

# enable syslog forwarding
maas $ADMIN maas set-config name=promtail_enabled value=true
```

*** How to verify correct operation

You should be able to add Loki and Prometheus as datasources in Grafana and to create dashboards for visualising MAAS metrics, and to receive alerts through Alertmanager.
[/tab]
[tab version="v3.1 Snap,v3.1 Packages,v3.0 Snap,v3.0 Packages,v2.9 Snap,v2.9 Packages"]
MAAS services can provide [Prometheus](https://prometheus.io/)`↗` endpoints for collecting performance metrics.

** How to set up Prometheus for MAAS

MAAS can provide five endpoints of particular interest to MAAS users:

1.   TFTP server file transfer latency
2.   HTTP requests latency
3.   Websocket requests latency
4.   RPC calls (between MAAS services) latency
5.   Per request DB queries counts

All available metrics are prefixed with `maas_`, to make it easier to look them up in Prometheus and Grafana UIs.

**** This article will help you learn:

- [How do I enable Prometheus endpoints?](#heading--enabling-prometheus-endpoints)
- [How do I configure Prometheus endpoints?](#heading--configuring-prometheus)
- [How can I deploy Prometheus and Grafana?](#heading--deploying-prometheus-and-grafana)

*** Enabling Prometheus endpoints

Whenever you install the `python3-prometheus-client` library, Prometheus endpoints are exposed over HTTP by the `rackd` and `regiond` processes under the default `/metrics` path.

[note]
Currently, prometheus metrics are shared when rack and region controllers are running on the same machine, even though each service provides its own port.  You can safely only query one of the two ports if you're running both controllers.
[/note]

[tabs]
[tab version="v3.1 Snap,v3.0 Snap,v2.9 Snap"]
For a vb Snap-based MAAS installation, the libraries already included in the snap so that metrics will be available out of the box.
[/tab]
[tab version="v3.1 Packages,v3.0 Packages,v2.9 Packages"]
For a Debian-based MAAS installation, install the library and restart MAAS services as follows:

    sudo apt install python3-prometheus-client
    sudo systemctl restart maas-rackd
    sudo systemctl restart maas-regiond

MAAS also provides optional stats about resources registered with the MAAS server itself.  These include four broad categories of information:

1.   The number of nodes by type, arch, ...
2.   Number of networks, spaces, fabrics, VLANs and subnets
3.   Total counts for machines CPU cores, memory and storage
4.   Counters for VM host resources

After installing the `python3-prometheus-client` library as describe above, run the following to enable stats:

    maas $PROFILE maas set-config name=prometheus_enabled value=true
[/tab]
[/tabs]

*** Configuring Prometheus

Once the `/metrics` endpoint is available in MAAS services, Prometheus can be configured to scrape metric values from these. You can configure this by adding a stanza like the following to the [prometheus configuration](https://prometheus.io/docs/prometheus/latest/configuration/configuration/)`↗`:

``` yaml
    - job_name: maas
      static_configs:
        - targets:
          - <maas-host1-IP>:5239  # for regiond
          - <maas-host1-IP>:5249  # for rackd
          - <maas-host2-IP>:5239  # regiond-only
          - <maas-host3-IP>:5249  # rackd-only
```

If the MAAS installation includes multiple nodes, the `targets` entries must be adjusted accordingly, to match services deployed on each node.

If  you have enabled MAAS stats,  you must add an additional Prometheus job to the config:

``` yaml
    - job_name: maas
      metrics_path: /MAAS/metrics
      static_configs:
        - targets:
          - <maas-host-IP>:5240
```

In case of a multi-host deploy, adding a single IP for any of the MAAS hosts running `regiond` will suffice.

*** Deploying Prometheus and Grafana

[Grafana](https://grafana.com/)`↗` and Prometheus can be easily deployed using Juju.

The [MAAS performance repo](https://git.launchpad.net/~maas-committers/maas/+git/maas-performance)`↗` repository provides a sample `deploy-stack` script that will deploy and configure the stack on LXD containers.

First, you must install juju via:

    sudo snap install --classic juju

Then you can run the script from the repo:

    grafana/deploy-stack <MAAS-IP>

To follow the progress of the deployment, run the following:

    watch -c juju status --color

Once you deploy everything, the Grafana UI is accessible on port `3000` with the credentials `admin`/`grafana`. The Prometheus UI will be available on port `9090`.

The repository also provides some sample dashboard covering the most common use cases for graphs. These are available under `grafana/dashboards`.  You can import them from the Grafana UI or API.
[/tab]
[/tabs]
* How to operate MAAS
This section provides some best practices for working with MAAS in your enterprise.

- [How to find machines](/t/how-to-find-machines/5192): When you have a large number of MAAS machines, you can search this list with filtering and matching.

- [Back up MAAS instances](/t/how-to-back-up-maas/5096): You should regularly back up your MAAS instances and keep back-ups handy to restore when things go wrong.

- [Monitor MAAS](/t/how-to-monitor-maas/5204): You can observe a running MAAS to assure that everything's running properly.

- [Audit MAAS](/t/how-to-audit-maas/5987): MAAS can be audited to trace user actions over the life of the system.

- [Troubleshoot MAAS](/t/how-to-troubleshoot-maas/5333): Here, you'll find instructions for sorting out common issues that come up with MAAS.

* How to put machines to work
The ultimate purpose of MAAS is to deploy and manage machines.  It's important to understand [how images get deployed](/t/how-to-acquire-images/6192#heading--how-images-deploy) -- as explained in [About the machine life-cycle](/t/how-to-deploy-physical-machines/6193#heading--about-the-machine-life-cycle), machines must first be enlisted or commissioned, then allocated, then deployed.  This article will help you build on that understanding by explaining:

- [How to commission a machine](#heading--how-to-commission-a-machine)
- [How to test machines](#heading--how-to-test-machines)
- [How to allocate machines](#heading--allocate-machines)
- [How to deploy machines](#heading--deploy)

** How to commission a machine

Note that if you are using your own commissioning scripts, and you do not want them to automatically run every time, you must specify `noauto`, as in this script snippet:

```
#!/bin/bash
#
# --- Start MAAS 1.0 script metadata ---
# name: 50-script-example
# title: Example
# description: Just an example
# script_type: commissioning
# tags: noauto
```

If you do not specify `noauto`, your custom commissioning scripts will run every time commissioning is attempted.

[tabs]
[tab version="v3.2 Snap,v3.3 Snap,v3.2 Packages,v3.3 Packages,v3.1 Snap,v3.1 Packages,v3.0 Snap,v3.0 Packages,v2.9 Snap,v2.9 Packages" view="UI"]
To commission a machine:

- Go to the "Machines" page.

- Select the machine(s) you want to commission.

- Choose "Commission" under the "Take action" drop-down menu:

<a href="https://discourse.maas.io/uploads/default/original/1X/5f196ca5e175e3f37d7cffbb2341fb0ee9cee16a.png" target = "_blank"><img src="https://discourse.maas.io/uploads/default/original/1X/5f196ca5e175e3f37d7cffbb2341fb0ee9cee16a.png"></a>

You have the option of selecting some extra parameters (checkboxes) and performing hardware tests.

These options include:

- **Allow SSH access and prevent machine powering off**: Machines are normally powered off after commissioning. This option keeps the machine on and enables SSH so you can access the machine.

- **Retain network configuration**: When enabled, preserves any custom network settings previously configured for the machine. See [Networking](/t/how-to-set-up-networks/6174) for more information.

- **Retain storage configuration**: When enabled, preserves any storage settings previously configured for the machine. See [Storage](/t/how-to-deploy-physical-machines/6193#heading--about-storage) for more details.

- **Update firmware**: Runs scripts tagged with `update_firmware`.

- **Configure HBA**: Runs scripts tagged with `configure_hba`.

<a href="https://discourse.maas.io/uploads/default/original/1X/5f196ca5e175e3f37d7cffbb2341fb0ee9cee16a.png" target = "_blank"><img src="https://discourse.maas.io/uploads/default/original/1X/5f196ca5e175e3f37d7cffbb2341fb0ee9cee16a.png"></a>

- Click the Hardware tests field to reveal a drop-down list of tests to add and run during commissioning. See [Hardware testing](/t/how-to-deploy-physical-machines/6193#heading--about-testing-hardware) for more information on hardware testing scripts.

- Finalise the directive by hitting "Commission machine".

While commissioning, the machine status will change to reflect this state (Commissioning).  MAAS discovers the machine's network topology.  MAAS then prompts a machine network interface to connect to the fabric, VLAN, and subnet combination for configuration. Usually, MAAS assigns a static IP address out of the reserved IP range for the subnet ("Auto assign" mode). The next section details several assignment modes.

Once commissioned, a machine's status will change to Ready, and an extra tab for the machine called "Commissioning" will become available. This tab contains the results of the scripts executed during the commissioning process.
[/tab]
[tab version="v3.2 Snap,v3.3 Snap,v3.2 Packages,v3.3 Packages,v3.1 Snap,v3.1 Packages,v3.0 Snap,v3.0 Packages,v2.9 Snap,v2.9 Packages" view="CLI"]
To commission a machine that's in the "Ready" state, via the CLI, use the following command:

```nohighlight
maas $PROFILE machine commission $SYSTEM_ID
```
<details><summary>Typical JSON output (long listing)</summary>

```nohighlight
Success.
Machine-readable output follows:
{
    "storage_test_status_name": "Pending",
    "bcaches": [],
    "cpu_count": 1,
    "interface_set": [
        {
            "params": "",
            "numa_node": 0,
            "tags": [],
            "id": 10,
            "mac_address": "52:54:00:15:36:f2",
            "vendor": "Red Hat, Inc.",
            "children": [],
            "effective_mtu": 1500,
            "discovered": [],
            "links": [],
            "link_speed": 0,
            "link_connected": true,
            "system_id": "bhxws3",
            "enabled": true,
            "interface_speed": 0,
            "firmware_version": null,
            "name": "ens3",
            "sriov_max_vf": 0,
            "product": null,
            "vlan": {
                "vid": 0,
                "mtu": 1500,
                "dhcp_on": true,
                "external_dhcp": null,
                "relay_vlan": null,
                "fabric": "fabric-2",
                "primary_rack": "8dwnne",
                "name": "untagged",
                "id": 5003,
                "space": "undefined",
                "secondary_rack": null,
                "fabric_id": 2,
                "resource_uri": "/MAAS/api/2.0/vlans/5003/"
            },
            "parents": [],
            "type": "physical",
            "resource_uri": "/MAAS/api/2.0/nodes/bhxws3/interfaces/10/"
        }
    ],
    "network_test_status_name": "Unknown",
    "numanode_set": [
        {
            "index": 0,
            "memory": 985,
            "cores": [
                0
            ]
        }
    ],
    "locked": false,
    "hardware_uuid": "F677A842-571C-4E65-ADC9-11E2CF92D363",
    "default_gateways": {
        "ipv4": {
            "gateway_ip": null,
            "link_id": null
        },
        "ipv6": {
            "gateway_ip": null,
            "link_id": null
        }
    },
    "status_action": "",
    "status_message": "Commissioning",
    "cpu_test_status_name": "Unknown",
    "memory_test_status": -1,
    "virtualblockdevice_set": [],
    "pool": {
        "name": "default",
        "description": "Default pool",
        "id": 0,
        "resource_uri": "/MAAS/api/2.0/resourcepool/0/"
    },
    "current_testing_result_id": 9,
    "current_installation_result_id": null,
    "netboot": true,
    "description": "",
    "special_filesystems": [],
    "testing_status": 0,
    "memory": 1024,
    "current_commissioning_result_id": 8,
    "storage": 5368.70912,
    "commissioning_status": 0,
    "cpu_test_status": -1,
    "tag_names": [
        "virtual"
    ],
    "memory_test_status_name": "Unknown",
    "swap_size": null,
    "status_name": "Commissioning",
    "other_test_status": -1,
    "pod": null,
    "storage_test_status": 0,
    "blockdevice_set": [
        {
            "id_path": "/dev/disk/by-id/ata-QEMU_HARDDISK_QM00001",
            "size": 5368709120,
            "block_size": 512,
            "tags": [
                "ssd"
            ],
            "serial": "QM00001",
            "uuid": null,
            "numa_node": 0,
            "available_size": 5368709120,
            "id": 3,
            "partition_table_type": null,
            "model": "QEMU HARDDISK",
            "path": "/dev/disk/by-dname/sda",
            "storage_pool": null,
            "used_for": "Unused",
            "filesystem": null,
            "system_id": "bhxws3",
            "used_size": 0,
            "partitions": [],
            "name": "sda",
            "type": "physical",
            "resource_uri": "/MAAS/api/2.0/nodes/bhxws3/blockdevices/3/"
        }
    ],
    "other_test_status_name": "Unknown",
    "distro_series": "",
    "testing_status_name": "Pending",
    "ip_addresses": [],
    "address_ttl": null,
    "system_id": "bhxws3",
    "physicalblockdevice_set": [
        {
            "firmware_version": "2.5+",
            "serial": "QM00001",
            "uuid": null,
            "numa_node": 0,
            "available_size": 5368709120,
            "size": 5368709120,
            "tags": [
                "ssd"
            ],
            "id": 3,
            "partition_table_type": null,
            "id_path": "/dev/disk/by-id/ata-QEMU_HARDDISK_QM00001",
            "model": "QEMU HARDDISK",
            "path": "/dev/disk/by-dname/sda",
            "storage_pool": null,
            "used_for": "Unused",
            "filesystem": null,
            "system_id": "bhxws3",
            "used_size": 0,
            "partitions": [],
            "name": "sda",
            "block_size": 512,
            "type": "physical",
            "resource_uri": "/MAAS/api/2.0/nodes/bhxws3/blockdevices/3/"
        }
    ],
    "fqdn": "ace-swan.maas",
    "osystem": "",
    "domain": {
        "authoritative": true,
        "ttl": null,
        "resource_record_count": 0,
        "name": "maas",
        "id": 0,
        "is_default": true,
        "resource_uri": "/MAAS/api/2.0/domains/0/"
    },
    "boot_interface": {
        "params": "",
        "numa_node": 0,
        "tags": [],
        "id": 10,
        "mac_address": "52:54:00:15:36:f2",
        "vendor": "Red Hat, Inc.",
        "children": [],
        "effective_mtu": 1500,
        "discovered": [],
        "links": [],
        "link_speed": 0,
        "link_connected": true,
        "system_id": "bhxws3",
        "enabled": true,
        "interface_speed": 0,
        "firmware_version": null,
        "name": "ens3",
        "sriov_max_vf": 0,
        "product": null,
        "vlan": {
            "vid": 0,
            "mtu": 1500,
            "dhcp_on": true,
            "external_dhcp": null,
            "relay_vlan": null,
            "fabric": "fabric-2",
            "primary_rack": "8dwnne",
            "name": "untagged",
            "id": 5003,
            "space": "undefined",
            "secondary_rack": null,
            "fabric_id": 2,
            "resource_uri": "/MAAS/api/2.0/vlans/5003/"
        },
        "parents": [],
        "type": "physical",
        "resource_uri": "/MAAS/api/2.0/nodes/bhxws3/interfaces/10/"
    },
    "hostname": "ace-swan",
    "network_test_status": -1,
    "min_hwe_kernel": "",
    "power_state": "off",
    "interface_test_status_name": "Unknown",
    "owner_data": {},
    "volume_groups": [],
    "power_type": "virsh",
    "node_type": 0,
    "owner": "admin",
    "cache_sets": [],
    "architecture": "amd64/generic",
    "hwe_kernel": null,
    "zone": {
        "name": "default",
        "description": "",
        "id": 1,
        "resource_uri": "/MAAS/api/2.0/zones/default/"
    },
    "disable_ipv4": false,
    "boot_disk": {
        "firmware_version": "2.5+",
        "serial": "QM00001",
        "uuid": null,
        "numa_node": 0,
        "available_size": 5368709120,
        "size": 5368709120,
        "tags": [
            "ssd"
        ],
        "id": 3,
        "partition_table_type": null,
        "id_path": "/dev/disk/by-id/ata-QEMU_HARDDISK_QM00001",
        "model": "QEMU HARDDISK",
        "path": "/dev/disk/by-dname/sda",
        "storage_pool": null,
        "used_for": "Unused",
        "filesystem": null,
        "system_id": "bhxws3",
        "used_size": 0,
        "partitions": [],
        "name": "sda",
        "block_size": 512,
        "type": "physical",
        "resource_uri": "/MAAS/api/2.0/nodes/bhxws3/blockdevices/3/"
    },
    "status": 1,
    "iscsiblockdevice_set": [],
    "raids": [],
    "node_type_name": "Machine",
    "hardware_info": {
        "system_vendor": "QEMU",
        "system_product": "Standard PC (i440FX + PIIX, 1996)",
        "system_family": "Unknown",
        "system_version": "pc-i440fx-focal",
        "system_sku": "Unknown",
        "system_serial": "Unknown",
        "cpu_model": "Intel Core Processor (Skylake, IBRS)",
        "mainboard_vendor": "Unknown",
        "mainboard_product": "Unknown",
        "mainboard_serial": "Unknown",
        "mainboard_version": "Unknown",
        "mainboard_firmware_vendor": "SeaBIOS",
        "mainboard_firmware_date": "04/01/2014",
        "mainboard_firmware_version": "1.13.0-1ubuntu1",
        "chassis_vendor": "QEMU",
        "chassis_type": "Other",
        "chassis_serial": "Unknown",
        "chassis_version": "pc-i440fx-focal"
    },
    "commissioning_status_name": "Pending",
    "bios_boot_method": "pxe",
    "interface_test_status": -1,
    "cpu_speed": 0,
    "resource_uri": "/MAAS/api/2.0/machines/bhxws3/"
}
```

</details>

If you need to find the machine's $SYSTEM_ID, you can use a command like this one:

```nohighlight
maas $PROFILE machines read | jq '.[] | .hostname, .system_id'
"ace-swan"
"bhxws3"
```
[/tab]
[/tabs]

Once commissioned, you may consider adding a tag to this machine.  The next step is testing and deployment.

** How to test machines

This section explains:

[tabs]
[tab version="v3.2 Snap,v3.3 Snap,v3.2 Packages,v3.3 Packages,v3.1 Snap,v3.1 Packages,v3.0 Snap,v3.0 Packages,v2.9 Snap,v2.9 Packages" view="UI"]
- [How to download built-in scripts](#heading--how-to-download-built-in-scripts)
- [How to upload scripts](#heading--how-to-upload-scripts)
- [How to debug script failures](#heading--how-to-debug-script-failures)
- [How to locate script files](#heading--how-to-locate-script-files)
- [How to locate log files](#heading--how-to-locate-log-files)
- [How to run all scripts manually](#heading--how-to-run-all-scripts-manually)
- [How to apply a hardware test](#heading--apply-a-hardware-test)
- [How to test network links](#heading--network-link-testing)
- [How to detect slow network links](#heading--slow-link-detection)
- [How to configure network validation and testing scripts](#heading--network-validation-scripts-and-testing)
- [How to customise network testing](#heading--customisable-network-testing)
[/tab]
[tab version="v3.2 Snap,v3.3 Snap,v3.2 Packages,v3.3 Packages,v3.1 Snap,v3.1 Packages,v3.0 Snap,v3.0 Packages,v2.9 Snap,v2.9 Packages" view="CLI"]
- [How to locate script files](#heading--how-to-locate-script-files)
- [How to locate log files](#heading--how-to-locate-log-files)
- [How to run all scripts manually](#heading--how-to-run-all-scripts-manually)
- [How to apply a hardware test](#heading--apply-a-hardware-test)
- [How to test network links](#heading--network-link-testing)
- [How to detect slow network links](#heading--slow-link-detection)
- [How to configure network validation and testing scripts](#heading--network-validation-scripts-and-testing)
- [How to customise network testing](#heading--customisable-network-testing)
- [How to download built-in scripts](#heading--how-to-download-built-in-scripts)
- [How to upload hardware testing scripts](#heading--how-to-upload-hardware-testing-scripts)
- [How to list all uploaded hardware testing scripts](#heading--how-to-list-all-uploaded-hardware-testing-scripts)
- [How to update hardware testing scripts](#heading--how-to-update-hardware-testing-scripts)
- [How to revert hardware testing scripts](#heading--how-to-revert-hardware-testing-scripts)
- [How to download a script](#heading--how-to-download-a-script)
- [How to delete a script](#heading--how-to-delete-a-script)
- [How to view script results](#heading--how-to-view-script-results)
- [How to filter script results](#heading--how-to-filter-script-results)
- [How to suppress failed results](#heading--how-to-suppress-failed-results)
- [How to upload hardware test scripts](#heading--upload-test-scripts)
- [How to use tags to group commissioning and testing scripts](#heading--tags-group-scripts)
- [How to view testing results](#heading--results)
- [How to test network links](#heading--network-link-testing)
- [How to detect slow network links](#heading--slow-link-detection)
- [How to configure network validation and testing scripts](#heading--network-validation-scripts-and-testing)
- [How to customise network testing](#heading--customisable-network-testing)
[/tab]
[/tabs]

You can also refer to technical details and examples for [commissioning scripts](/t/commissioning-scripts-reference/5375) and [testing scripts](/t/hardware-test-scripts-reference/5392) as needed.

*** How to download built-in scripts

You can download the source for all commissioning and test scripts via the API with the following command:

``` nohighlight
maas $PROFILE node-script download $SCRIPT_NAME
```

The source code to all built-in scripts is available on [launchpad](https://git.launchpad.net/maas/tree/src/metadataserver/builtin_scripts)`↗`.

[tabs]
[tab version="v3.2 Snap,v3.3 Snap,v3.2 Packages,v3.3 Packages,v3.1 Snap,v3.1 Packages,v3.0 Snap,v3.0 Packages,v2.9 Snap,v2.9 Packages" view="UI"]
*** How to upload scripts

Scripts can be uploaded to MAAS using the web UI. Select the 'User scripts' tab of the 'Settings' page - the 'Commissioning scripts' section is near the top. Within the Commissioning scripts section, click the Upload script button followed by 'Choose file' to open a requester, locate the script, and select Upload to upload it to MAAS.

A status message of Commissioning script created will appear.  You'll then be able to select your script after selecting [Test hardware](/t/how-to-deploy-physical-machines/6193#heading--about-testing-hardware) from a machine's 'Take action' menu.

<a href="https://assets.ubuntu.com/v1/50e08fdf-nodes-hw-scripts__2.4_select.png" target = "_blank"><img src="https://assets.ubuntu.com/v1/50e08fdf-nodes-hw-scripts__2.4_select.png"></a>

[note]
MAAS executes scripts in lexicographical order. This order allows you to control when your scripts execute, and whether they run before or after the standard MAAS scripts.
[/note]

*** How to debug script failures

Clicking on the title of a completed or failed script will reveal the output from that specific script.

<a href="https://assets.ubuntu.com/v1/855015e5-nodes-hw-scripts__2.2_fail.png" target = "_blank"><img src="https://assets.ubuntu.com/v1/855015e5-nodes-hw-scripts__2.2_fail.png"></a>

If you need further details, especially when writing and running your own scripts, you can connect to a machine and examine its logs and environment.

To do this, enable Allow SSH access and prevent the machine from powering off when selecting 'Test hardware' from the machine 'Take action' menu.

<a href="https://assets.ubuntu.com/v1/da793c67-nodes-hw-scripts__2.4_ssh.png" target = "_blank"><img src="https://assets.ubuntu.com/v1/da793c67-nodes-hw-scripts__2.4_ssh.png"></a>

Because scripts operate within an ephemeral version of Ubuntu, enabling this option stops the machine from shutting down, allowing you to connect and probe a script's status.

As long as you've added your [SSH key](/t/how-to-manage-user-accounts/5184) to MAAS, you can connect with SSH to the machine's IP with a username of `ubuntu`. Type `sudo -i` to get root access.
[/tab]
[tab version="v3.2 Snap,v3.3 Snap,v3.2 Packages,v3.3 Packages,v3.1 Snap,v3.1 Packages,v3.0 Snap,v3.0 Packages,v2.9 Snap,v2.9 Packages" view="CLI"]
*** How to upload hardware testing scripts

To upload a hardware testing script to MAAS, enter the following:

```nohighlight
maas $PROFILE node-scripts create name=$SCRIPT_NAME name> \
 script=$PATH_TO_SCRIPT type=testing
```

Changing the type to commissioning adds the test script to the commissioning process.

*** How to list all uploaded hardware testing scripts

You can list all uploaded scripts with the following command:

```nohighlight
maas $PROFILE node-scripts read type=testing filters=$TAG
```

The optional filters argument lets you search for tags assigned to a script, such as using `TAG=cpu` with the above example.

*** How to update hardware testing scripts

A script's metadata, and even the script itself, can be updated from the command line:

```nohighlight
maas $PROFILE node-script update \
 $SCRIPT_NAME script=$PATH_TO_SCRIPT comment=$COMMENT
```

The JSON formatted output to the above command will include 'history' dictionary entries, detailing script modification times and associated comments:

```nohighlight
"history": [
    {
        "id": 40,
        "created": "Tue, 12 Sep 2017 12:12:08 -0000",
        "comment": "Updated version"
    },
    {
        "id": 34,
        "created": "Fri, 08 Sep 2017 17:07:46 -0000",
        "comment": null
    }
]
```

*** How to revert hardware testing scripts

MAAS keeps a history of all uploaded script versions, allowing you to easily revert to a previous version, using the `id` of the desired version:

```nohighlight
maas $PROFILE node-script revert $SCRIPT_NAME to=$VERSION_ID
```

[note]
The history for later modifications will be lost when reverting to an earlier version of the script.
[/note]

*** How to download a script

To download a script, enter the following:

```nohighlight
maas $PROFILE node-script download $SCRIPT_NAME > $LOCAL_FILENAME
```
*** How to delete a script

To delete a script, use `delete`:

```nohighlight
maas $PROFILE node-script delete $SCRIPT_NAME
```

*** How to view script results

The command line allows you to not only view the current script's progress but also retrieve the verbatim output from any previous runs too.

If you only want to see the latest or currently-running result, you can use `current-commissioning`, `current-testing`, or `current-installation` instead of an id:

```nohighlight
maas $PROFILE node-script-result read $SYSTEM_ID $RESULTS
```
**** How to filter script results

You can also limit which results are returned by type (commissioning, testing, or installation), script name, or script run:

```nohighlight
maas $PROFILE node-script-results read \
 $SYSTEM_ID type=$SCRIPT_TYPE filters=$SCRIPT_NAME,$TAGS
```
*** How to suppress failed results

You can also suppress failed results, which is useful if you want to ignore a known failure:

```nohighlight
maas $PROFILE node-script-results update \
 $SYSTEM_ID type=$SCRIPT_TYPE filters=$SCRIPT_NAME,$TAGS suppressed=$SUPPRESSED
```

where `$SUPPRESSED` is either `True` or `False`. The JSON formatted output to the above command will include 'results' dictionary with an entry for `suppressed`:

```nohighlight
"results": [
    {
        "id": 21,
        "created": "Tue, 02 Apr 2019 17:00:36 -0000",
        "updated": "Tue, 02 Apr 2019 20:56:41 -0000",
        "name": "smartctl-validate",
        "status": 5,
        "status_name": "Aborted",
        "exit_status": null,
        "started": "Tue, 02 Apr 2019 20:56:41 -0000",
        "ended": "Tue, 02 Apr 2019 20:56:41 -0000",
        "runtime": "0:00:00",
        "starttime": 1554238601.765214,
        "endtime": 1554238601.765214,
        "estimated_runtime": "0:00:00",
        "parameters": {
            "storage": {
                "argument_format": "{path}",
                "type": "storage",
                "value": {
                    "id_path": "/dev/vda",
                    "model": "",
                    "name": "sda",
                    "physical_blockdevice_id": 1,
                    "serial": ""
                }
            }
        },
        "script_id": 1,
        "script_revision_id": null,
        "suppressed": true
    }
]
```

Finally, results can be downloaded, either to stdout, stderr, as combined output or as a tar.xz:

```nohighlight
maas $PROFILE node-script-result download $SYSTEM_ID $RUN_ID output=all \
 filetype=tar.xz > $LOCAL_FILENAME
```

[note]
**$RUN_ID** is labelled `id` in the verbose result output.
[/note]

[/tab]
[/tabs]

*** How to locate script files

Commissioning and testing script files may be found in the following directories:

- `/tmp/user_data.sh.*/scripts/commissioning/`: Commissioning scripts
- `/tmp/user_data.sh.*/scripts/testing/`: Hardware testing scripts

*** How to locate log files

Commissioning and testing log files may be found in the following directories:

- `/tmp/user_data.sh*/out/`
- `/var/log/cloud-init-output.log`
- `/var/log/cloud-init.log`

*** How to run all scripts manually

You can also run all commissioning and hardware-testing scripts on a machine for debugging.

```nohighlight
/tmp/user_data.sh.*/bin/maas-run-remote-scripts \
    [--no-download] \
    [--no-send] \
    /tmp/user_data.sh.*
```

Where:

- `--no-download`: Optional. Do not download the scripts from MAAS again.
- `--no-send`: Optional. Do not send the results to MAAS.

For example, to run all the scripts again without downloading again from MAAS:

```nohighlight
/tmp/user_data.sh.*/bin/maas-run-remote-scripts --no-download /tmp/user_data.sh.*
```

Here, all the scripts are run again after downloading from MAAS, but no output data is sent to MAAS:

```nohighlight
/tmp/user_data.sh.*/bin/maas-run-remote-scripts --no-send /tmp/user_data.sh.*
```

[tabs]
[tab version="v3.2 Snap,v3.3 Snap,v3.2 Packages,v3.3 Packages,v3.1 Snap,v3.1 Packages,v3.0 Snap,v3.0 Packages,v2.9 Snap,v2.9 Packages" view="UI"]
*** How to apply a hardware test

To launch a test, select the target machine from the 'Machines' page and use the 'Take action' drop-down menu to select 'Test hardware'. When ready, hit the 'Test machine' button. Here, a test is applied to a deployed machine:

<a href="https://assets.ubuntu.com/v1/8e876889-nodes-hw-testing__2.4_deployed.png" target = "_blank"><img src="https://assets.ubuntu.com/v1/8e876889-nodes-hw-testing__2.4_deployed.png"></a>

There is the option of not powering off the machine and to allow SSH access.

A default test will be selected (`smartctl-validate`, a hard drive test) but you can choose others by clicking the 'Select scripts' label. Doing so will reveal the following choices:

<a href="https://assets.ubuntu.com/v1/ccfefe25-nodes-hw-testing__2.4_deployed-choices.png" target = "_blank"><img src="https://assets.ubuntu.com/v1/ccfefe25-nodes-hw-testing__2.4_deployed-choices.png"></a>

See [Commissioning scripts reference](/t/commissioning-scripts-reference/5375) for more details on how these scripts work and how you can write your own.

[/tab]
[tab version="v3.2 Snap,v3.3 Snap,v3.2 Packages,v3.3 Packages,v3.1 Snap,v3.1 Packages,v3.0 Snap,v3.0 Packages,v2.9 Snap,v2.9 Packages" view="CLI"]
*** How to upload hardware test scripts

To upload a hardware testing script to MAAS, enter the following:

``` bash
maas $PROFILE node-scripts create name=$SCRIPT_NAME name> \
 script=$PATH_TO_SCRIPT type=testing
```

Changing the type to commissioning adds the test script to the commissioning process.

You can list all uploaded scripts with the following command:

``` bash
maas $PROFILE node-scripts read type=testing filters=$TAG
```

The optional filters argument lets you search for tags assigned to a script, such as using `TAG=cpu` with the above example.

A script's metadata, and even the script itself, can be updated from the command line:

``` bash
maas $PROFILE node-script update \
 $SCRIPT_NAME script=$PATH_TO_SCRIPT comment=$COMMENT
```

The JSON formatted output to the above command will include 'history' dictionary entries, detailing script modification times and associated comments:

``` json
"history": [
    {
        "id": 40,
        "created": "Tue, 12 Sep 2017 12:12:08 -0000",
        "comment": "Updated version"
    },
    {
        "id": 34,
        "created": "Fri, 08 Sep 2017 17:07:46 -0000",
        "comment": null
    }
]
```

MAAS keeps a history of all uploaded script versions, allowing you to easily revert to a previous version using the `id` of the version you wish to revert to:

``` bash
maas $PROFILE node-script revert $SCRIPT_NAME to=$VERSION_ID
```

[note]
The history for later modifications will be lost when reverting to an earlier version of the script.
[/note]

To download a script, enter the following:

``` bash
maas $PROFILE node-script download $SCRIPT_NAME > $LOCAL_FILENAME
```

To delete a script, use `delete`:

``` bash
maas $PROFILE node-script delete $SCRIPT_NAME
```

*** How to use tags to group commissioning and testing scripts

Tags make scripts easier to manage; grouping scripts together for commissioning and testing, for example:

``` bash
maas $PROFILE node-script add-tag $SCRIPT_NAME tag=$TAG
maas $PROFILE node-script remove-tag $SCRIPT_NAME tag=$TAG
```

MAAS runs all commissioning scripts by default. However, you can select which custom scripts to run during commissioning by name or tag:

``` bash
maas $PROFILE machine commission \
 commissioning_scripts=$SCRIPT_NAME,$SCRIPT_TAG
```

You can also select which testing scripts to run by name or tag:

``` bash
maas $PROFILE machine commission \
 testing_scripts=$SCRIPT_NAME,$SCRIPT_TAG
```

Any testing scripts tagged with commissioning will also run during commissioning.

*** How to view testing results

The command line allows you to not only view the current script's progress but also retrieve the verbatim output from any previous runs too.

If you only want to see the latest or currently-running result, you can use `current-commissioning`, `current-testing`, or `current-installation` instead of an id:

``` bash
maas $PROFILE node-script-result read $SYSTEM_ID $RESULTS
```

You can also limit which results are returned by type (commissioning, testing, or installation), script name, or script run:

``` bash
maas $PROFILE node-script-results read \
 $SYSTEM_ID type=$SCRIPT_TYPE filters=$SCRIPT_NAME,$TAGS
```

You can also suppress failed results, which is useful if you want to ignore a known failure:

``` bash
maas $PROFILE node-script-results update \
 $SYSTEM_ID type=$SCRIPT_TYPE filters=$SCRIPT_NAME,$TAGS suppressed=$SUPPRESSED
```

where `$SUPPRESSED` is either `True` or `False`. The JSON formatted output to the above command will include 'results' dictionary with an entry for `suppressed`:

``` json
"results": [
    {
        "id": 21,
        "created": "Tue, 02 Apr 2019 17:00:36 -0000",
        "updated": "Tue, 02 Apr 2019 20:56:41 -0000",
        "name": "smartctl-validate",
        "status": 5,
        "status_name": "Aborted",
        "exit_status": null,
        "started": "Tue, 02 Apr 2019 20:56:41 -0000",
        "ended": "Tue, 02 Apr 2019 20:56:41 -0000",
        "runtime": "0:00:00",
        "starttime": 1554238601.765214,
        "endtime": 1554238601.765214,
        "estimated_runtime": "0:00:00",
        "parameters": {
            "storage": {
                "argument_format": "{path}",
                "type": "storage",
                "value": {
                    "id_path": "/dev/vda",
                    "model": "",
                    "name": "sda",
                    "physical_blockdevice_id": 1,
                    "serial": ""
                }
            }
        },
        "script_id": 1,
        "script_revision_id": null,
        "suppressed": true
    }
]
```

Finally, results can be downloaded, either to stdout, stderr, as combined output or as a tar.xz:

``` bash
maas $PROFILE node-script-result download $SYSTEM_ID $RUN_ID output=all \
 filetype=tar.xz > $LOCAL_FILENAME
```

[note]
**$RUN_ID** is labelled `id` in the verbose result output.
[/note]
[/tab]
[/tabs]

*** How to test network links

MAAS can check whether links are connected or disconnected, so that you can detect unplugged cables.  If you are not running MAAS 2.7 or higher, you must first upgrade and then recommission your machines to find disconnected links.  MAAS not only reports unplugged cables, but also gives a warning when trying to configure a disconnected interface.  In addition, administrators can change the cable connection status after manually resolving the issue.

[tabs]
[tab version="v3.2 Snap,v3.3 Snap,v3.2 Packages,v3.3 Packages,v3.1 Snap,v3.1 Packages,v3.0 Snap,v3.0 Packages,v2.9 Snap,v2.9 Packages" view="UI"]
When MAAS detects a broken network link, users will see a screen similar to this one: 

<a href="https://discourse.maas.io/uploads/default/original/1X/687feb2ddea8b317f0deba239bcb1779fd5f33d3.jpeg" target = "_blank"><img src="https://discourse.maas.io/uploads/default/original/1X/687feb2ddea8b317f0deba239bcb1779fd5f33d3.jpeg"></a> 

If you're already using a version of MAAS less than 2.7, you will want to upgrade and recommission your existing machines to check link status.  Note that you will also receive a warning from MAAS when trying to configure a disconnected interface.
[/tab]
[tab version="v3.2 Snap,v3.3 Snap,v3.2 Packages,v3.3 Packages,v3.1 Snap,v3.1 Packages,v3.0 Snap,v3.0 Packages,v2.9 Snap,v2.9 Packages" view="CLI"]
To check network testing results, enter the following command:

```
maas $PROFILE interfaces read $SYSTEM_ID \
| jq -r '(["LINK_NAME","LINK_CONNECTED?","LINK_SPEED", "I/F_SPEED"]
| (., map(length*"-"))), (.[] | [.name, .link_connected, .link_speed, .interface_speed])
| @tsv' | column -t
```

which produces an output similar to this:

```
LINK_NAME  LINK_CONNECTED?  LINK_SPEED  I/F_SPEED
---------  ---------------  ----------  ---------
ens3       false            -           1 Gpbs
```

From this screen, you can see that the `ens3` link is not connected (hence an unreported link speed). 
[/tab]
[/tabs]

Once you have manually repaired the broken connection, an administrator can change cable connection status:

[tabs]
[tab version="v3.2 Snap,v3.3 Snap,v3.2 Packages,v3.3 Packages,v3.1 Snap,v3.1 Packages,v3.0 Snap,v3.0 Packages,v2.9 Snap,v2.9 Packages" view="UI"]
<a href="https://discourse.maas.io/uploads/default/original/1X/b8b24a2e5fbc40b6469a24733a518b510cf0d955.jpeg" target = "_blank"><img src="https://discourse.maas.io/uploads/default/original/1X/b8b24a2e5fbc40b6469a24733a518b510cf0d955.jpeg"></a> 
[/tab]
[tab version="v3.2 Snap,v3.3 Snap,v3.2 Packages,v3.3 Packages,v3.1 Snap,v3.1 Packages,v3.0 Snap,v3.0 Packages,v2.9 Snap,v2.9 Packages" view="CLI"]
```
maas $PROFILE interface update $SYSTEM_ID $INTERFACE_ID link_connected=true
```
[/tab]
[/tabs]

** How to detect slow network links

[tabs]
[tab version="v3.2 Snap,v3.3 Snap,v3.2 Packages,v3.3 Packages,v3.1 Snap,v3.1 Packages,v3.0 Snap,v3.0 Packages,v2.9 Snap,v2.9 Packages" view="UI"]
As servers and hardware get faster, the chances increase that you might encounter a speed mismatch when connecting your NIC to a network device.  MAAS can warn you if your interface is connected to a link slower than what the interface supports, by automatically detecting link and interface speed and reporting them via the UI:

<a href="https://discourse.maas.io/uploads/default/original/1X/e73a81df222f44c0b364eefcd0880e2a84c7303b.jpeg" target = "_blank"><img src="https://discourse.maas.io/uploads/default/original/1X/e73a81df222f44c0b364eefcd0880e2a84c7303b.jpeg"></a>  

Depending on your physical hardware, the problem may not be repairable, but once you identify a slow link, you can replace a slow switch without recommissioning.  
[/tab]
[tab version="v3.2 Snap,v3.3 Snap,v3.2 Packages,v3.3 Packages,v3.1 Snap,v3.1 Packages,v3.0 Snap,v3.0 Packages,v2.9 Snap,v2.9 Packages" view="CLI"]
As servers and hardware get faster, the chances increase that you might encounter a speed mismatch when connecting your NIC to a network device.  MAAS can warn you if your interface is connected to a link slower than what the interface supports, when you run the above command:

```
maas $PROFILE interfaces read $SYSTEM_ID \
| jq -r '(["LINK_NAME","LINK_CONNECTED?","LINK_SPEED", "I/F_SPEED"]
| (., map(length*"-"))), (.[] | [.name, .link_connected, .link_speed, .interface_speed])
| @tsv' | column -t
```

From the resulting output, you can detect when your link/interface speeds are slower than expected. Depending on your physical hardware, the problem may not be repairable, but once you identify a slow link, you can replace a slow switch without recommissioning.  

[/tab]
[/tabs]

Administrators can change or update the link and interface speeds after manual changes to the connection:

```
maas $PROFILE interface update $SYSTEM_ID $INTERFACE_ID link_speed=$NEW_LINK_SPEED \
interface_speed=$NEW_INTERFACE_SPEED
```

This functionality is only available through the MAAS CLI.

*** How to configure network validation and testing scripts

MAAS allows you to configure network connectivity testing in a number of ways. If MAAS can’t connect to the rack controller, deployment can’t complete.  MAAS can check connectivity to the rack controller and warn you if there’s no link, long before you have to try and debug it. For example, if you can’t connect to your gateway controller, traffic can’t leave your network. 

Via the Web UI only, MAAS can check this link and recognise that there’s no connectivity, which alleviates hard-to-detect network issues:

<a href="https://discourse.maas.io/uploads/default/original/1X/c4f81cb3ef1a90f0a46fb62c893a4cc9f7e5f45a.jpeg" target = "_blank"><img src="https://discourse.maas.io/uploads/default/original/1X/c4f81cb3ef1a90f0a46fb62c893a4cc9f7e5f45a.jpeg"></a> 

Users can now test their network configuration to check for:

- Interfaces which have a broken network configuration
- Bonds that are not fully operational
- Broken gateways, rack controllers, and Internet links

[tabs]
[tab version="v3.2 Snap,v3.3 Snap,v3.2 Packages,v3.3 Packages,v3.1 Snap,v3.1 Packages,v3.0 Snap,v3.0 Packages,v2.9 Snap,v2.9 Packages" view="UI"]
In addition, MAAS can comprehensively test Internet connectivity testing. You can give a list of URLs or IP addresses to check from the network testing screen:

<a href="https://discourse.maas.io/uploads/default/original/1X/12dd87ce0bffd54c2e459c4dea850af5fcbe14d0.jpeg" target = "_blank"><img src="https://discourse.maas.io/uploads/default/original/1X/12dd87ce0bffd54c2e459c4dea850af5fcbe14d0.jpeg"></a> 
[/tab]
[tab version="v3.2 Snap,v3.3 Snap,v3.2 Packages,v3.3 Packages,v3.1 Snap,v3.1 Packages,v3.0 Snap,v3.0 Packages,v2.9 Snap,v2.9 Packages" view="CLI"]
In addition, MAAS can comprehensively test Internet connectivity testing. You can give a list of URLs or IP addresses to check:

<a href="https://discourse.maas.io/uploads/default/original/1X/b92a8ca1821bc1ccf60cf7fddcb57f3fbeda4408.jpeg" target = "_blank"><img src="https://discourse.maas.io/uploads/default/original/1X/b92a8ca1821bc1ccf60cf7fddcb57f3fbeda4408.jpeg"></a> 
[/tab]
[/tabs]

In the ephemeral environment, standard DHCP is still applied, but when network testing runs, MAAS can apply your specific configuration for the duration of the test.  While all URLs / IPs are tested with all interfaces, MAAS can test each of your interfaces individually, including breaking apart bonded NICS and testing each side of your redundant interfaces. You can also run different tests on each pass, e.g., a different set of URLs, although each run would be a different testing cycle.

To test individual interfaces, for example, you could issue the following CLI command:

<a href="https://discourse.maas.io/uploads/default/original/1X/7fadb56a2939f7a781510a55813141de03521e0d.jpeg" target = "_blank"><img src="https://discourse.maas.io/uploads/default/original/1X/7fadb56a2939f7a781510a55813141de03521e0d.jpeg"></a> 

Note that in this command, we are testing internet connectivity to the single interface "br0."

*** How to customise network testing

MAAS allow you to customise network testing according to your needs.  You can create your own commissioning scripts and tests related to networking, and you can run them during the network testing portion of the MAAS workflow.

<a href="https://discourse.maas.io/uploads/default/original/1X/0dcf089dbd8efc2fc9d0782d3b15f47647e950b8.jpeg" target = "_blank"><img src="https://discourse.maas.io/uploads/default/original/1X/0dcf089dbd8efc2fc9d0782d3b15f47647e950b8.jpeg"></a> 

There are no particular restrictions on these scripts, so you can test a wide variety of possible conditions and situations.  Administrators can upload network tests and test scripts.  Administrators can also create tests which accept an interface parameter, or scripts which apply custom network configurations.  

Users can specify unique parameters using the API, override machines which fail network testing (allowing their use), and suppress individual failed network tests.  Users can also review the health status from all interface tests, even sorting them by interface name and MAC.  In addition, MAAS can report the overall status of all interfaces.

** How to allocate machines

[tabs]
[tab version="v3.2 Snap,v3.3 Snap,v3.2 Packages,v3.3 Packages,v3.1 Snap,v3.1 Packages,v3.0 Snap,v3.0 Packages,v2.9 Snap,v2.9 Packages" view="UI"]
To allocate a node with the web UI, select a machine which is in the "Ready" state, and drop down the "Take action" menu:

<a href="https://discourse.maas.io/uploads/default/original/1X/3724346e052c865f4e865d1caf2778b115f0798f.jpeg" target = "_blank"><img src="https://discourse.maas.io/uploads/default/original/1X/3724346e052c865f4e865d1caf2778b115f0798f.jpeg"></a>

Select "Allocate" from the drop-down menu, which will bring you to a confirmation screen:

<a href="https://discourse.maas.io/uploads/default/original/1X/a0ece8bf58c03db3c89ad71afcaeb9101bd34e24.jpeg" target = "_blank"><img src="https://discourse.maas.io/uploads/default/original/1X/a0ece8bf58c03db3c89ad71afcaeb9101bd34e24.jpeg"></a>

MAAS will allocate the selected machine; you can now find it in the list of "Allocated" machines:

<a href="https://discourse.maas.io/uploads/default/original/1X/a2bdb8b7b7c5705daee14bdea5caed223537917d.jpeg" target = "_blank"><img src="https://discourse.maas.io/uploads/default/original/1X/a2bdb8b7b7c5705daee14bdea5caed223537917d.jpeg"></a>
[/tab]
[tab version="v3.2 Snap,v3.3 Snap,v3.2 Packages,v3.3 Packages,v3.1 Snap,v3.1 Packages,v3.0 Snap,v3.0 Packages,v2.9 Snap,v2.9 Packages" view="CLI"]

To allocate a random node:

``` bash
maas $PROFILE machines allocate
```

To allocate a specific node:

``` bash
maas $PROFILE machines allocate system_id=$SYSTEM_ID
```

[/tab]
[/tabs]

[note]
To allocate a node, it must have a status of 'Ready'.
[/note]

** How to deploy machines

[tabs]
[tab version="v3.2 Snap,v3.3 Snap,v3.2 Packages,v3.3 Packages,v3.1 Snap,v3.1 Packages,v3.0 Snap,v3.0 Packages,v2.9 Snap,v2.9 Packages" view="UI"]
To deploy directly from MAAS, select one or more machine(s) and press the 'Deploy' button.

<a href="https://assets.ubuntu.com/v1/56958753-nodes-deploy__2.4_deploy.png" target = "_blank"><img src="https://assets.ubuntu.com/v1/56958753-nodes-deploy__2.4_deploy.png"></a>

You then have the option of deviating from the default OS, release, and kernel. When ready, press 'Deploy X machine(s)' (where 'X' is the number of machines selected).

<a href="https://assets.ubuntu.com/v1/d65b9884-nodes-deploy__2.6-deploy-confirm.png" target = "_blank"><img src="https://assets.ubuntu.com/v1/d65b9884-nodes-deploy__2.6-deploy-confirm.png"></a>

While a machine is deploying its status will change to Deploying to 'OS', where 'OS' is the name of the OS being deployed (e.g. 'Deploying to Ubuntu 16.04 LTS').

Once a machine has finished deploying its status will change to just the name of the OS (e.g. 'Ubuntu 18.04 LTS').
[/tab]
[tab version="v3.2 Snap,v3.3 Snap,v3.2 Packages,v3.3 Packages,v3.1 Snap,v3.1 Packages,v3.0 Snap,v3.0 Packages,v2.9 Snap,v2.9 Packages" view="CLI"]
To deploy a node:

``` bash
maas $PROFILE machine deploy $SYSTEM_ID
```

To deploy a node as a KVM host:

``` bash
maas $PROFILE machine deploy $SYSTEM_ID install_kvm=True
```

[note]
To deploy with the CLI, the node must have a status of 'Allocated'. See 'Allocate a node' above.
[/note]

*** Configure deployment timeout

By default, when you deploy a machine, MAAS will consider the deployment a failure if it doesn't complete within 30 minutes.  You can configure this timeout, if you wish, with the command:

```
maas $PROFILE maas set-config name=node-timeout value=$NUMBER_OF_MINUTES
```

*** How to enlist a machine that’s already running a workload

In order to add machine that’s already running a workload, there are currently two options:

Via the API/CLI, you can create a machine, passing the deployed flag:

```
$ maas $profile machines create deployed=true hostname=mymachine \   
architecture=amd64 mac_addresses=00:16:3e:df:35:bb power_type=manual
```

On the machine itself (the recommended way, if the machine is running Ubuntu), you can download a helper script from MAAS and create the machine that way:

```
$ wget http://$MAAS_IP:5240/MAAS/maas-run-scripts
$ chmod 755 maas-run-scripts
$ ./maas-run-scripts register-machine --hostname mymachine \
 > http://$MAAS_IP:5240/MAAS $MAAS_API_TOKEN
```

Now you have a machine in MAAS that’s in the deployed state, with no hardware information yet.

*** How to update hardware information for a deployed machine

The recommended way of updating the hardware information for a deployed machine is to download the maas-run-scripts script and run it on the machine itself:

```
$ wget http://$MAAS_IP:5240/MAAS/maas-run-scripts
$ chmod 755 maas-run-scripts
$ ./maas-run-scripts report-results --config mymachine-creds.yaml
```

If you created the machine with the maas-run-scripts, you should have such a mymachine-creds.yaml file already. If not, it should look like this:

```
reporting:
          maas:
            consumer_key: $CONSUMER_KEY
            endpoint: http://$MAAS_IP:5240/MAAS/metadata/status/$SYSTEM_ID
            token_key: $TOKEN_KEY
            token_secret: $TOKEN_SECRET
```

You may get the needed credentials from the MAAS API, for example:

```
$ maas $profile machine get-token wxwwga
Success.
Machine-readable output follows:
{
        "token_key": "Lyy9BS4tKsQakDQScy",
        "token_secret": "V8vta8Azwn6FZVkfHnuTvLGLScAvEufB",
        "consumer_key": "YGT6QKSH65aap4tGnw"
}
```

[/tab]
[/tabs]
* How to report a bug
MAAS bugs are reported via Launchpad. We'll provide the link in a moment, but first, some necessary prerequisites.

Filing a good bug report makes all the difference in how quickly we can triage and address your problem.  This brief how-to guide will walk you through the steps of filing a bug.  Some of these steps are necessary preparation, and the better you prepare before you file, the better the bug report.

[note]
Note that in the examples that follow, a `>>` symbol indicates the newest line added to the draft report.  It isn't necessary for you to type these characters.
[/note]

** Step 1: Understand what's required

You need to gather or prepare some information before you file your bug report.  Here is the short list of pre-filing tasks that will smooth the process:

- [Prepare a concise summary](#heading--concise-summary)
- [Identify your version and build first](#heading--version-and-build)
- [Explain whether you're using the UI, CLI, or API](#heading--which-interface)
- [Explain what happens](#heading--what-happens)
- [Explain how to reproduce your issue](#heading--reproducing-your-issue)
- [Take screenshots, if relevant](#heading--screenshots)
- [Locate and capture logfiles, if at all possible](#heading--logfiles)

Open a text editor with a new file, so you can capture and refine this information in advance.

*** Prepare a concise summary

Keep your bug summary short and concise.  We recommend something of the form:

```text
<something specific happens> with MAAS <involved feature(s)>
```

or

```text
MAAS <behaves some unexpected way> when I <try to use a particular feature>
```

For example:

```
>>MAAS fails to PXE boot IBM LPAR machines as KVM hosts
```

Add this information to your text file.

*** Identify your version and build

We need to know the version and build (and packaging format) that you're running.

<a href="#heading--If-you're-using-a-snap">**** id="heading--If-you're-using-a-snap">If you're using a snap

If you're using a snap, execute `snap list maas` at the command line, which will return some lines like this:

```text
Name  Version                       Rev    Tracking     Publisher   Notes
maas  3.0.0~beta2-9796-g.2182ab55f  13292  latest/edge  canonical✓  -
```

We want the know the `Version` field; in this case, that's `3.0.0~beta2-9796-g.2182ab55f`. Add a line like this to your text file:

```text
MAAS fails to PXE boot IBM LPAR machine as VM host

>>I'm using snap version/build 3.0.0~beta2-9796-g.2182ab55f.
```

<a href="#heading--If-you're-using-a-debian-package">**** id="heading--If-you're-using-a-debian-package">If you're using a debian package

If you're using a deb, execute `apt list maas` at the command line, and enter whatever it returns into your text file, as in the snap example above, being sure to specify:

```text
I'm using debian package version/build...
```

*** Using CLI, UI, or API?

Next, you'll need to specify which interface you're using, and generally what command(s) you were attempting.  For example:

```text
MAAS fails to PXE boot IBM LPAR machine as VM host

I'm using snap version/build 3.0.0~beta2-9796-g.2182ab55f.

>>I tried to create a VM host using a previously discovered IBM LPAR machine,
using the MAAS UI.
```

*** Explain what happens

Being as concise and specific as you can, explain what seemed to go wrong.  For example:

```text
MAAS fails to PXE boot IBM LPAR machine as VM host

I'm using snap version/build 3.0.0~beta2-9796-g.2182ab55f.

I tried to create a VM host using a previously discovered IBM LPAR machine,
using the MAAS UI.

>>I was able to select the machine under "Add KVM," define its parameters,
and select a project.  I was also able to push "Authenticate," and the
expected commissioning process began.  After the machine powered on, though,
the commissioning process timed out trying to PXE boot the machine.  Looking
at the machine, it had indeed been powered on, but nothing happened after that.
```

*** Explain how to reproduce your issue

In addition to the explanation above, you should create a step-by-step list of what you did to reproduce the problem.  For example:

```text
MAAS fails to PXE boot IBM LPAR machine as VM host

I'm using snap version/build 3.0.0~beta2-9796-g.2182ab55f.

I tried to create a VM host using a previously discovered IBM LPAR machine, using the MAAS UI.

I was able to select the machine under "Add KVM," define its parameters,
and select a project.  I was also able to push "Authenticate," and the
expected commissioning process began.  After the machine powered on, though,
the commissioning process timed out trying to PXE boot the machine.  Looking
at the machine, it had indeed been powered on, but nothing happened after that.

>>Steps to reproduce:
1. Confirm that at least one IBM LPAR 700 model is discovered and in the
Ready state (see screenshot-1).

2. Select the KVM tab (see screenshot-2).

3. Press the "Add KVM" button.

4. Select the KVM host type according to your network (I selected "<selection>").

5. Enter a name for the KVM host (I entered "<entered-name>").

6. Enter a bridge address to reach the KVM host (I entered "<power-address>").

7. Enter the appropriate password.

8. Press the "Authenticate" button.

9. On the project screen which pops up, select the "default" project.

10. Press the "Add KVM" button.

11. Return to the machine list and find the machine you just added as a KVM.
It should show "Powering on."

12. Watch until the machine enters the "Performing PXE boot" stage.

13. Wait for the machine to time out without reaching "Loading ephemeral"
stage, as normal.

14. Examine the logfiles (see attached logfiles).
```

*** Take relevant screenshots

If you think it will help -- especially when using the UI -- try and capture screenshots of any unexpected results or ambiguous actions.  Your goal isn't to document your experience in pictures, but to provide a visual reference where verbal descriptions fall short.  Name these so you can sync them with your explanation (e.g., "screenshot-1"). You'll attach them later on in the process.

*** Locate and capture logfiles

If at all possible, capture at least the following logfiles, for the time period surrounding your error situation:

1. maas.log
2. regiond.log
3. rackd.log
4. the rsyslog file of the affected machine(s), if it exists.

On snap, these files are located as follows:

- /var/snap/maas/common/log/maas.log
- /var/snap/maas/common/log/regiond.log
- /var/snap/maas/common/log/rackd.log
- /var/snap/maas/common/log/rsyslog/$MACHINE_NAME/$RELEVANT_DATE/messages

If you're using packages, you'll find the files in these locations:

- /var/log/maas/maas.log
- /var/log/maas/regiond.log
- /var/log/maas/rackd.log
- /var/log/maas/rsyslog/$MACHINE_NAME/$RELEVANT_DATE/messages

**** How to compress these logfiles into a tar.gz file for efficient upload

The following animation shows a recommended process for creating a `tar.gz` file.  The mistakes are intentional, so that you'll know what to do if you forget an option now and then, or forget to use `sudo` in a step or two:

<a href="https://discourse.maas.io/uploads/default/original/2X/c/c8e05533fc4f076774d26e592fe02418bc70ecf7.gif" target = "_blank"><img src="https://discourse.maas.io/uploads/default/original/2X/c/c8e05533fc4f076774d26e592fe02418bc70ecf7.gif"></a>

** Step 2: Filing your bug

Have your prepared text file handy.  In our example, it looks like this:

```text
MAAS fails to PXE boot IBM LPAR machine as VM host

I'm using snap version/build 3.0.0~beta2-9796-g.2182ab55f.

I tried to create a VM host using a previously discovered IBM LPAR machine, using the MAAS UI.

I was able to select the machine under "Add KVM," define its parameters,
and select a project.  I was also able to push "Authenticate," and the
expected commissioning process began.  After the machine powered on, though,
the commissioning process timed out trying to PXE boot the machine.  Looking
at the machine, it had indeed been powered on, but nothing happened after that.

Steps to reproduce:
1. Confirm that at least one IBM LPAR 700 model is discovered and in the
Ready state (see screenshot-1).

2. Select the KVM tab (see screenshot-2).

3. Press the "Add KVM" button.

4. Select the KVM host type according to your network (I selected "<selection>").

5. Enter a name for the KVM host (I entered "<entered-name>").

6. Enter a bridge address to reach the KVM host (I entered "<power-address>").

7. Enter the appropriate password.

8. Press the "Authenticate" button.

9. On the project screen which pops up, select the "default" project.

10. Press the "Add KVM" button.

11. Return to the machine list and find the machine you just added as a KVM.
It should show "Powering on."

12. Watch until the machine enters the "Performing PXE boot" stage.

13. Wait for the machine to time out without reaching "Loading ephemeral"
stage, as normal.

14. Examine the logfiles (see attached logfiles).
```

To file a bug, go to the [launchpad bug report page](https://bugs.launchpad.net/maas/+filebug)`↗`. You'll see a screen similar to this one:

<a href="https://discourse.maas.io/uploads/default/original/2X/f/f445ad576553b45468775c1de98fdaf075bd03a0.png" target = "_blank"><img src="https://discourse.maas.io/uploads/default/original/2X/f/f445ad576553b45468775c1de98fdaf075bd03a0.png"></a>

Enter your concise summary in the "Summary:" line:

<a href="https://discourse.maas.io/uploads/default/original/2X/7/7cb71e56284024d72d46c05767df1080e099b329.png" target = "_blank"><img src="https://discourse.maas.io/uploads/default/original/2X/7/7cb71e56284024d72d46c05767df1080e099b329.png"></a>

Press enter to get to the next screen:

<a href="https://discourse.maas.io/uploads/default/original/2X/6/609dc8a082bafc1c3e12a60dbfe19920307ecdb6.png" target = "_blank"><img src="https://discourse.maas.io/uploads/default/original/2X/6/609dc8a082bafc1c3e12a60dbfe19920307ecdb6.png"></a>

Paste the rest of your text file into the "Further information" box:

<a href="https://discourse.maas.io/uploads/default/original/2X/c/c6ab47abae93d7bc2a0143472a517911e346f864.png" target = "_blank"><img src="https://discourse.maas.io/uploads/default/original/2X/c/c6ab47abae93d7bc2a0143472a517911e346f864.png"></a>

Attach your screenshots and logfiles by opening the "Extra options" section and browsing for your attachments.  Ideally, you'd attach the files one at a time, with a comprehensible description.

When you're done, simply choose "Submit Bug Report" and check your work on the screen that follows:

<a href="https://discourse.maas.io/uploads/default/original/2X/a/a2d7f7207d92891d237b852fc67f201a37993973.png" target = "_blank"><img src="https://discourse.maas.io/uploads/default/original/2X/a/a2d7f7207d92891d237b852fc67f201a37993973.png"></a>

If you want, you can view a [sample bug](https://bugs.launchpad.net/maas/+bug/1923516)`↗` with precisely these parameters.

* How to request a feature

Sometimes, what starts out as a bug report turns into a feature request, when someone realises that MAAS is not broken, just designed differently than you expected.  At other times, there are capabilities you'd like to see that simply haven't been added to MAAS yet -- or that we wouldn't have considered.  In either case, a feature request is the right answer.  

If you're pretty sure what you want to see, you can skip on to [discourse](https://discourse.maas.io/c/features/15)`↗` and open a new Features post.  

If you've got the idea, but you aren't sure how to frame it, the rest of this article will help.

** First, some reassurance

We do deliver the most-requested features when we can.  In our last [feature poll](https://discourse.maas.io/t/-/4865), the feature that got the most votes -- [enlisting deployed machines](/t/what-is-new-with-maas-3-1/5964#heading--enlist-deployed-machines) -- was actually delivered as part of MAAS version 3.1.  This doesn't mean we'll build every requested feature, or that we'll get to them as quickly as you might need them.  The point is that we listen and try to respond, so it's worth your time to let us know what you need.

** What will help us

That said, there are some key points of information that will help us bring you the features you're requesting.  Remember that any feature we deliver has to be part of our roadmap.  The roadmap combines ideas from our own experience, company priorities for MAAS, paying customer needs, known issues, obvious things that need doing, and your feature requests.  Capabilties that make it to the top of that roadmap have some common characteristics:

- They solve a specific problem that's experienced by more than two or three users.
- They are based on clearly-stated needs that are easy for us to understand and relate to real user stories and use cases.
- Better yet, they are accompanied by detailed user stories and use cases (anonymized, of course).
- They are also well-explained, in that we can understand how you think MAAS should operate and what it should do differently than what's in the current codebase.  It's one thing to tell us you need MAAS to solve a problem; it's a different matter entirely for us to understand how you need it to solve that problem.
- They are accompanied by contact information, even if it's indirect or semi-anonymous, so that we can get back to you with questions.

The better the case, the better the chances we'll consider picking up the feature in a future release.  As stated above, there are no guarantees, but we do listen and consider well-spoken arguments.


* How to secure MAAS

There are many things you can do to secure your MAAS instance.

** [Improve MAAS security](/t/how-to-improve-maas-security/5196)

Simple, everyday steps go a long way toward securing MAAS.

** [Manage user accounts](/t/how-to-manage-user-accounts/5184)

Assign and manage access rights with user accounts.

** [Enable MAAS native TLS](/t/how-to-enable-maas-native-tls/5116)

Beyond standard TLS, later versions of MAAS provide a native TLS capability.

** [Use Vault with MAAS](/t/how-to-use-hashicorp-vault-with-maas/6942)

HashiCorp Vault can be used with MAAS to secure your secrets, such as the region-rack secret.

** [Set up an air-gapped MAAS](/t/how-to-set-up-an-air-gapped-maas/5212)

You can successfully isolate MAAS from the Internet, using proxies and mirrors.
* How to set up an air-gapped MAAS
Many MAAS users maintain their data centres in an air-gapped environment that does not have an external Internet connection. MAAS runs well in this configuration, though keeping MAAS supplied with updates and images requires a bit of extra effort.

There are essentially four things that must be available to an air-gapped MAAS for smooth operation:

1. Snap updates (via the snap proxy)
2. Packages (via a local repo, possibly with a transparent proxy)
3. MAAS-maintained images (via  local mirror, possibly with a transparent proxy)
4. Other OS images (various methods)

There is at least one way to make each of these things available in an air-gapped environment.  Some of these can be set up to use a transparent proxy, which minimises changes to other components of the MAAS environment.

**** This article will help you learn:

- [How to use the snap proxy to refresh snaps in an air-gapped environment](#heading--How-to-use-the-snap-proxy-to-refresh-snaps-in-an-air-gapped-environment)
- [How to make package updates available in an air-gapped environment](#heading--apt-mirror)
- [How to retrieve or update MAAS images in an air-gapped environment](#heading--local-image-mirroring)
- [How to retrieve or update non-MAAS-maintained images in an air-gapped environment](#heading--non-maas-images)
- [How to use user_data to access non-MAAS-maintained images](#heading--other-os-user-data)
- [How to set up a transparent proxy](#heading--transparent-proxy)

** How to use the snap proxy to refresh snaps in an air-gapped environment

Using snaps in an air-gapped environment is possible with the Snap Store Proxy, which can be deployed in networks that are disconnected from the Internet.  Currently, the features required to use this proxy in an [air-gapped](https://docs.ubuntu.com/snap-store-proxy/en/airgap)`↗` mode are part of a password-protected internal Beta.  

Client devices connect to the air-gapped proxy and never contact the general Snap Store nor the Internet.  Proxy operators will need to side-load all needed snaps and updates into the proxy. 

There are three main steps to setting up this proxy:

1. Register an offline Snap Store Proxy on an Internet-connected machine.
2. Set up HTTPS access to ensure adequate security.
3. Fetch the necessary snaps as needed by your MAAS environment (on the Internet-connected machine).

This proxy requires a properly configured PostgreSQL database -- see the [setup instructions](https://docs.ubuntu.com/snap-store-proxy/en/airgap)`↗` for the Snap Store Proxy for more details.

** How to make package updates available in an air-gapped environment

The simplest way to use local package repos is via the [reprepro](https://manpages.ubuntu.com/manpages/focal/man1/reprepro.1.html)`↗` command.  There is an older command, `apt-mirror`, which is no longer maintained; it's not recommended.

The `reprepro` command manages a local repository of Debian packages.  You can add files manually or download them from some other repository.  It does not require an external database.  This command also handles signatures of mirrored repos, and can create signatures for the generated package indices, if desired.  

You may wish to create a [transparent proxy](#heading--transparent-proxy) to make using your local repo easier.

** How to retrieve or update MAAS images in an air-gapped environment
 
MAAS has an [established process](/t/how-to-mirror-images-locally/5927) for mirroring images locally.   The steps are relatively simple:

1. Install the `simplestreams` package.
2. Define some variables to simplify CLI usage.
3. Create the desired mirrors, specifying where you want your images stored.
4. Set up a new boot source on your local server, referring to the local mirror.

See the [local image mirror](/t/how-to-mirror-images-locally/5927) for details.  Note that you can use the menu at the top of that page to switch to specific instructions for the version, build-type, and interface you prefer.

** How to retrieve or update non-MAAS-maintained images in an air-gapped environment

MAAS allows you to deploy many types of OSes, and, once deployed, install specific software.  MAAS can configure a user specified repository for Ubuntu, so a user can mirror the Ubuntu apt repositories and point MAAS at those repos. When Ubuntu deploys apt will automatically be configured to use the user defined apt mirrors. 

MAAS only does this for Ubuntu, not CentOS or RHEL. If you deploy CentOS or RHEL with MAAS, the repos that built the image will be deployed.  But this won't work in an air \-gapped environment. To access non-MAAS-maintained images in an air-gapped environment, you will need to use one of two methods:

- Use `user_data`.
- Create custom images and store them in your local mirror.

Here's a thumbnail sketch of both of these methods.

*** How to use user_data to access non-MAAS-maintained images

A user can create custom `user_data` which will configure CentOS or RHEL to use a specific mirror.  Check out the [machine customisation](/t/how-to-customise-machines/5108) page for details on how to make this work.

*** Storing customer images for non-MAAS-maintained images

You can also [create custom images](/t/how-to-build-custom-images/5104) and store them in your local mirror.  Once you have the image built, consult the page on [local image mirrors](/t/how-to-mirror-images-locally/5927) to see how to incorporate your newly-built image into the local stash.

** How to set up a transparent proxy

If you don't wish to disturb the default configurations for Ubuntu and MAAS, you can create a transparent proxy for Debian packages and images, via the following general steps:

1. Configure Ubuntu to get packages via HTTP.
2. Configure MAAS to get packages via HTTP.
3. Create a local mirror repo for `archive.ubuntu.com`.
4. Create a local image mirror for `images.maas.io`.
5. Configure DNS to point to the local mirrors for both of those URLs.

This avoids any need to change the default settings for MAAS or Ubuntu.

* How to set up LXD
LXD and MAAS are separate products, and it's useful to allow them to interact as equals, covering a much wider range of use cases.  To allow each of them to follow their own operational models, but still allow them to work together, we've taken advantage of LXD projects.

There is a [refresher on VM hosting](/t/how-to-deploy-virtual-machines/6500#heading--MAAS-VM-hosting) available, if you need it.

[tabs]
[tab version="v3.3 Snap,v3.3 Packages,v3.2 Snap,v3.2 Packages,v3.1 Snap,v3.1 Packages,v3.0 Snap,v3.0 Packages"]
- [How to make LXD available for hosting](#heading--lxd-setup)
- [About LXD projects and MAAS](#heading--projects-explanation)
- [An LXD project tutorial](#heading--projects-tutorial)
- [How to use LXD projects with MAAS](#heading--projects-step-2-lxd-and-maas)
[/tab]
[tab version="v2.9 Snap,v2.9 Packages"]
- [How to make LXD available for hosting](#heading--lxd-setup)
[/tab]
[/tabs]

** How to make LXD available for VM hosting

Assuming that you want to use LXD [VM hosts](/t/how-to-deploy-virtual-machines/6500#heading--about-vm-hosts), you need to install the correct version of LXD. Prior to the release of Ubuntu 20.04 LXD was installed using Debian packages. The Debian packaged version of LXD is too old to use with MAAS. If this is the case, you’ll need to remove the LXD Debian packages and install the Snap version.  Note that you cannot install both Debian and snap versions, as this creates a conflict.

*** How to remove older versions of LXD

If you're on a version of Ubuntu older than 20.04, or you have the Debian version of LXD, start the uninstall process with the following command:

```nohighlight
sudo apt-get purge -y *lxd* *lxc*
```

This command should result in output that looks something like this:

```nohighlight
Reading package lists... Done
Building dependency tree      
Reading state information... Done
Note, selecting 'lxde-core' for glob '*lxd*'
Note, selecting 'python-pylxd-doc' for glob '*lxd*'
Note, selecting 'python3-pylxd' for glob '*lxd*'
Note, selecting 'python-nova-lxd' for glob '*lxd*'
Note, selecting 'lxde-common' for glob '*lxd*'
Note, selecting 'lxde-icon-theme' for glob '*lxd*'
Note, selecting 'lxde-settings-daemon' for glob '*lxd*'
Note, selecting 'lxde' for glob '*lxd*'
Note, selecting 'lxdm' for glob '*lxd*'
Note, selecting 'lxd' for glob '*lxd*'
Note, selecting 'lxd-tools' for glob '*lxd*'
Note, selecting 'python-pylxd' for glob '*lxd*'
Note, selecting 'lxdm-dbg' for glob '*lxd*'
Note, selecting 'lxde-session' for glob '*lxd*'
Note, selecting 'nova-compute-lxd' for glob '*lxd*'
Note, selecting 'openbox-lxde-session' for glob '*lxd*'
Note, selecting 'python-nova.lxd' for glob '*lxd*'
Note, selecting 'lxd-client' for glob '*lxd*'
Note, selecting 'openbox-lxde-session' instead of 'lxde-session'
Note, selecting 'lxctl' for glob '*lxc*'
Note, selecting 'lxc-common' for glob '*lxc*'
Note, selecting 'python3-lxc' for glob '*lxc*'
Note, selecting 'libclxclient-dev' for glob '*lxc*'
Note, selecting 'lxc-templates' for glob '*lxc*'
Note, selecting 'lxc1' for glob '*lxc*'
Note, selecting 'lxc-dev' for glob '*lxc*'
Note, selecting 'lxc' for glob '*lxc*'
Note, selecting 'liblxc1' for glob '*lxc*'
Note, selecting 'lxc-utils' for glob '*lxc*'
Note, selecting 'vagrant-lxc' for glob '*lxc*'
Note, selecting 'libclxclient3' for glob '*lxc*'
Note, selecting 'liblxc-dev' for glob '*lxc*'
Note, selecting 'nova-compute-lxc' for glob '*lxc*'
Note, selecting 'python-lxc' for glob '*lxc*'
Note, selecting 'liblxc-common' for glob '*lxc*'
Note, selecting 'golang-gopkg-lxc-go-lxc.v2-dev' for glob '*lxc*'
Note, selecting 'lxcfs' for glob '*lxc*'
Note, selecting 'liblxc-common' instead of 'lxc-common'
Package 'golang-gopkg-lxc-go-lxc.v2-dev' is not installed, so not removed
Package 'libclxclient-dev' is not installed, so not removed
Package 'libclxclient3' is not installed, so not removed
Package 'lxc-templates' is not installed, so not removed
Package 'lxctl' is not installed, so not removed
Package 'lxde' is not installed, so not removed
Package 'lxde-common' is not installed, so not removed
Package 'lxde-core' is not installed, so not removed
Package 'lxde-icon-theme' is not installed, so not removed
Package 'lxde-settings-daemon' is not installed, so not removed
Package 'lxdm' is not installed, so not removed
Package 'lxdm-dbg' is not installed, so not removed
Package 'openbox-lxde-session' is not installed, so not removed
Package 'python-lxc' is not installed, so not removed
Package 'python3-lxc' is not installed, so not removed
Package 'vagrant-lxc' is not installed, so not removed
Package 'liblxc-dev' is not installed, so not removed
Package 'lxc-dev' is not installed, so not removed
Package 'nova-compute-lxc' is not installed, so not removed
Package 'nova-compute-lxd' is not installed, so not removed
Package 'python-nova-lxd' is not installed, so not removed
Package 'python-pylxd' is not installed, so not removed
Package 'python-pylxd-doc' is not installed, so not removed
Package 'lxc' is not installed, so not removed
Package 'lxc-utils' is not installed, so not removed
Package 'lxc1' is not installed, so not removed
Package 'lxd-tools' is not installed, so not removed
Package 'python-nova.lxd' is not installed, so not removed
Package 'python3-pylxd' is not installed, so not removed
The following packages were automatically installed and are no longer required:
  dns-root-data dnsmasq-base ebtables libuv1 uidmap xdelta3
Use 'sudo apt autoremove' to remove them.
The following packages will be REMOVED:
  liblxc-common* liblxc1* lxcfs* lxd* lxd-client*
0 upgraded, 0 newly installed, 5 to remove and 21 not upgraded.
After this operation, 34.1 MB disk space will be freed.
(Reading database ... 67032 files and directories currently installed.)
Removing lxd (3.0.3-0ubuntu1~18.04.1) ...
Removing lxd dnsmasq configuration
Removing lxcfs (3.0.3-0ubuntu1~18.04.2) ...
Removing lxd-client (3.0.3-0ubuntu1~18.04.1) ...
Removing liblxc-common (3.0.3-0ubuntu1~18.04.1) ...
Removing liblxc1 (3.0.3-0ubuntu1~18.04.1) ...
Processing triggers for man-db (2.8.3-2ubuntu0.1) ...
Processing triggers for libc-bin (2.27-3ubuntu1) ...
(Reading database ... 66786 files and directories currently installed.)
Purging configuration files for liblxc-common (3.0.3-0ubuntu1~18.04.1) ...
Purging configuration files for lxd (3.0.3-0ubuntu1~18.04.1) ...
Purging configuration files for lxcfs (3.0.3-0ubuntu1~18.04.2) ...
Processing triggers for systemd (237-3ubuntu10.40) ...
Processing triggers for ureadahead (0.100.0-21) ...
```

You should also autoremove packages no longer needed by LXD:

```nohighlight
$ sudo apt-get autoremove -y
```

Output from this command should be similar to:

```nohighlight
Reading package lists... Done
Building dependency tree      
Reading state information... Done
The following packages will be REMOVED:
  dns-root-data dnsmasq-base ebtables libuv1 uidmap xdelta3
0 upgraded, 0 newly installed, 6 to remove and 21 not upgraded.
After this operation, 1860 kB disk space will be freed.
(Reading database ... 66769 files and directories currently installed.)
Removing dns-root-data (2018013001) ...
Removing dnsmasq-base (2.79-1) ...
Removing ebtables (2.0.10.4-3.5ubuntu2.18.04.3) ...
Removing libuv1:amd64 (1.18.0-3) ...
Removing uidmap (1:4.5-1ubuntu2) ...
Removing xdelta3 (3.0.11-dfsg-1ubuntu1) ...
Processing triggers for man-db (2.8.3-2ubuntu0.1) ...
Processing triggers for libc-bin (2.27-3ubuntu1) ...
```

Now install LXD from the Snap:

```nohighlight
$ sudo snap install lxd
2020-05-20T22:02:57Z INFO Waiting for restart...
lxd 4.1 from Canonical✓ installed
```

*** R\How to refresh LXD on 20.04

If you are on 20.04 or above LXD should be installed by default, but it's a good idea to make sure it's up to date:

```nohighlight
$ sudo snap refresh
All snaps up to date.
```

*** How to initialise LXD prior to use

Once LXD is installed it needs to be configured with `lxd init` before first use:

```nohighlight
$ sudo lxd init
```

Your interactive output should look something like the following. Note a few points important points about these questions:

1. `Would you like to use LXD clustering? (yes/no) [default=no]: no` - support for LXD clusters was added in MAAS v3.1.

2. `Name of the storage back-end to use (btrfs, dir, lvm, zfs, ceph) [default=zfs]: dir` - testing has primarily been with dir; other options should work, but less testing has been done, so use at your own risk.

3. `Would you like to connect to a MAAS server? (yes/no) [default=no]: no` - When LXD is connected to MAAS containers or virtual machines created by LXD will be automatically added to MAAS as devices.  This feature should work, but has limited testing thus far.

4. `Would you like to configure LXD to use an existing bridge or host interface? (yes/no) [default=no]: yes` - The bridge LXD creates is isolated and not managed by MAAS. If this bridge is used, you would be able to add the LXD VM host and compose virtual machines, but commissioning, deploying, and any other MAAS action which uses the network will fail -- so `yes` is the correct answer here.

5. `Name of the existing bridge or host interface: br0` - br0 is the name of the bridge the user configured (see sections above) which is connected to a MAAS-managed network.

6. `Trust password for new clients:` - This is the password the user will enter when connecting with MAAS.


```nohighlight
Would you like to use LXD clustering? (yes/no) [default=no]: no
Do you want to configure a new storage pool? (yes/no) [default=yes]: yes
Name of the new storage pool [default=default]:  
Name of the storage back-end to use (btrfs, dir, lvm, zfs, ceph) [default=zfs]: dir
Would you like to connect to a MAAS server? (yes/no) [default=no]: no
Would you like to create a new local network bridge? (yes/no) [default=yes]: no
Would you like to configure LXD to use an existing bridge or host interface? (yes/no) [default=no]: yes
Name of the existing bridge or host interface: br0
Would you like LXD to be available over the network? (yes/no) [default=no]: yes
Address to bind LXD to (not including port) [default=all]:
Port to bind LXD to [default=8443]:
Trust password for new clients:
Again:
Would you like stale cached images to be updated automatically? (yes/no) [default=yes]
Would you like a YAML "lxd init" preseed to be printed? (yes/no) [default=no]:
```

After initialising LXD, you will also want to make sure that LXD is not trying to provide DHCP for the new local network bridge.  You can check this with the following command:

```nohighlight
lxc network show lxdbr0
```

If you didn't accept the default bridge name (lxdbr0), substitute your name for that new bridge in the command above. This will produce output something like this:

```nohighlight
config:
  dns.mode: managed
  ipv4.address: 10.146.214.1/24
  ipv4.dhcp: "true"
  ipv4.nat: "true"
  ipv6.address: fd42:c560:ee59:bb2::1/64
  ipv6.dhcp: "true"
  ipv6.nat: "true"
description: ""
name: lxdbr0
type: bridge
used_by:
- /1.0/profiles/default
managed: true
status: Created
locations:
- none
```

There is a [quick tutorial](https://github.com/lxc/lxd/blob/master/doc/networks.md)`↗` on the possible settings here.  For simplicity, to turn off LXD-provided DHCP, you need to change three settings, as follows:

```nohighlight
lxc network set lxdbr0 dns.mode=none
lxc network set lxdbr0 ipv4.dhcp=false
lxc network set lxdbr0 ipv6.dhcp=false
```

You can check your work by repeating the `show` command:

```nohighlight
$ lxc network show lxdbr0
config:
  dns.mode: none
  ipv4.address: 10.146.214.1/24
  ipv4.dhcp: "false"
  ipv4.nat: "true"
  ipv6.address: fd42:c560:ee59:bb2::1/64
  ipv6.dhcp: "false"
  ipv6.nat: "true"
description: ""
name: lxdbr0
type: bridge
used_by:
- /1.0/profiles/default
managed: true
status: Created
locations:
- none
```

Once that's done, the LXD host is now ready to be added to MAAS as an LXD VM host. Upon adding the VM host, its own commissioning information will be refreshed.

When composing a virtual machine with LXD, MAAS uses either the 'maas' LXD profile, or (if that doesn't exist) the 'default' LXD profile. The profile is used to determine which bridge to use. Users may also add additional LXD options to the profile which are not yet supported in MAAS.

[tabs]
[tab version="v3.3 Snap,v3.3 Packages,v3.2 Snap,v3.2 Packages,v3.1 Snap,v3.1 Packages,v3.0 Snap,v3.0 Packages"]
** About LXD projects and MAAS

It may be beneficial to understand how LXD projects fit into the overall MAAS ecosystem.  LXD projects are not intended to be MAAS projects; they are only intended to limit which LXD containers and VMs are available to MAAS. This section will tell you:

- [About LXD projects](#heading--projects-big-picture)
- [About design choices](#heading--projects-history)
- [About alternatives to LXD projects in MAAS](#heading--projects-alternatives)

*** About LXD projects

[LXD projects](https://ubuntu.com/tutorials/introduction-to-lxd-projects#1-overview) are a feature of the [LXD lightweight container hypervisor](https://ubuntu.com/server/docs/containers-lxd)`↗` -- a next-generation container manager which makes containers as easy to manage as virtual machines.  With LXD, you can create lots of containers, providing different services across many different use cases.  As a result of this flexibility, it can become confusing to keep track of exactly which containers are providing what services to answer which use case.

To help with this potential confusion, LXD provides a "projects" feature, which allows you to group one or more containers together into related projects.  These projects can be manipulated and managed with the same `lxc` tool used to manage the containers and virtual machines themselves.

Since MAAS makes use of LXD as a VM host, it's useful to be able to manipulate and manage projects not only through MAAS, but also directly through LXD.  In fact, to properly scope MAAS access to your LXD resources, it's essential that you understand how to use LXD projects in some detail.

*** About design choices

Prior versions of MAAS implemented LXD VM hosts, but did so in a strongly-coupled way; that is, MAAS essentially took control of the VMs, disallowing or overriding some direct LXD controls.  In a sense, MAAS became the owner, rather than simply a separate program, communicating interdependently with LXD to accomplish user purposes.

This was less than ideal.  For example, MAAS would discover all running LXD VMs and immediately commission them, essentially performing a "clean install" and wiping out their contents.  This behaviour isn't suitable when MAAS is connected to a deployed LXD with VMs already running services that may not have been targeted for enlistment by MAAS.

With the release of MAAS 3.0 version, MAAS now becomes a LXD tenant, rather than an owner.

As you can infer from some of the discussion above, MAAS and LXD are separate products.  It's beneficial for them to interact, but not if one product cannot be controlled and managed independently of the other.  The combination of MAAS and LXD can cover a much broader set of use cases between them if they act as independent tools.  While the most loosely-coupled model would have MAAS selecting individual VMs, that model isn't compatible with how MAAS works with other networks and devices, so we chose to implement projects as a way to section off LXD VMs for normal use by MAAS.

**** MAAS now controls VMs at a project level

Essentially, MAAS still commissions any VMs it discovers within a VM host, but the scope of the discovery is now limited to a single project.  That project can contain any subset of the existing VMs, from zero to all of them.  Any VM that's already in a project assigned to MAAS will still be re-commissioned, which is the normal behaviour of MAAS upon network discovery.

Here are the criteria which drove this decision:

- It should be possible for an administrator to select which project(s) MAAS manages, and thus which machines will be automatically enlisted and commissioned.
- It should be possible for MAAS to create and self-assign a new (empty) project, so that the user can compose new VMs within LXD from within the MAAS interface.
- No per-project features should be enabled by default in MAAS-created projects.
- The project must be explicitly specified when creating the LXD VM host.
- When a LXD VM host is added, only the existing VMs assigned to the selected project (if any) should be visible to MAAS (and thus, automatically commissioned), and VMs in non-MAAS projects are not affected in any way.
- When a LXD VM host is deleted, the default behaviour should be for MAAS to leave existing VMs, rather than automatically deleting them; VMs in non-MAAS projects are not affected in any way.
- MAAS will not create VMs in projects that it doesn't own.
- When a VM is composed in MAAS, it's created in the target LXD project.

In addition, a MAAS administrator should be able to:

- refresh information about a specific LXD VM host, and receive correct, up-to-date, and timely status.
- explicitly specify connections between networks and VM interfaces.
- deploy a machine directly as an LXD VM host, connected to MAAS from the outset.

These criteria were fully met in the MAAS LXD tenant implementation released as part of MAAS 3.0.

*** About alternatives to LXD projects in MAAS

You can see from the discussion above that LXD projects were used primarily to cordon off some LXD VMs from MAAS, to avoid them from being enlisted and commissioned by MAAS (and thus, essentially, destroyed, from an operational perspective).  These LXD project features provide some limited capabilities to group machines. Because LXD projects only deal with LXD VMs and VM hosts, they are not a complete or comprehensive set of project tools.  MAAS already has those machine grouping capabilities in the form of resource pools.

We realised, though, as we were working with LXD projects, that we could vastly improve the documentation for resource pools in the area of projects and project management.  You'll find significant new material in the resource pools section of the doc.  We also realised that it would be helpful to have "real-time tags" for machines, that is, annotations that persist only while a machine is allocated or deployed.  These new "tags" are known as workload annotations, and they have also been given a thorough treatment, also with their own page in the left-hand navigation.

** An LXD project tutorial

A good understanding of LXD projects is essential for those using LXD VM hosts, especially if you plan to include non-MAAS-controlled VMs in your LXD instance.  Normally, we wouldn't revisit instructions [found elsewhere](https://ubuntu.com/tutorials/introduction-to-lxd-projects#1-overview)`↗`, but because the discussion flows quickly and naturally into MAAS-related usage, it seemed prudent to give a light overview of some basic feature information.

*** How to list LXD projects

Before you try to manipulate projects, it's useful to understand how to list them, so that you can check your results as you go.  If you've successfully [installed and initialised lxd](https://linuxcontainers.org/lxd/getting-started-cli/)`↗`, you should be able to list projects.  A basic project list can be obtained with the following command:

```nohighlight
lxc project list
```

You should get a listing something like this:

```nohighlight
+---------------------+--------+----------+-----------------+----------+-------------------------+---------+
|        NAME         | IMAGES | PROFILES | STORAGE VOLUMES | NETWORKS |       DESCRIPTION       | USED BY |
+---------------------+--------+----------+-----------------+----------+-------------------------+---------+
| default (current)   | YES    | YES      | YES             | YES      | Default LXD project     | 7       |
+---------------------+--------+----------+-----------------+----------+-------------------------+---------+
| pg-basebackup-tests | NO     | YES      | NO              | NO       | Project managed by MAAS | 16      |
+---------------------+--------+----------+-----------------+----------+-------------------------+---------+
```

Note that this particular instantiation of LXD has two projects: the default project (which generally always exists in LXD), and a project called `pg-basebackup-tests` which is already managed by MAAS.

There is a column labelled `USED BY`, which tabulates the number of entities contained within the project. There isn't a project-related command to get a list of the containers and VMs within a project.  Instead, you use the LXC command `lxc list`:

```text
lxc list
```

which yields something like the following tabulated listing:

```nohighlight
+-----------------+---------+------+---------------------------------------------+-----------------+-----------+
|      NAME       |  STATE  | IPV4 |                    IPV6                     |      TYPE       | SNAPSHOTS |
+-----------------+---------+------+---------------------------------------------+-----------------+-----------+
| trusty-drake    | STOPPED |      |                                             | VIRTUAL-MACHINE | 0         |
+-----------------+---------+------+---------------------------------------------+-----------------+-----------+
| upward-stallion | RUNNING |      | fd42:ec:5a53:59d2:216:3eff:febf:7fa7 (eth0) | CONTAINER       | 0         |
+-----------------+---------+------+---------------------------------------------+-----------------+-----------+
| witty-lizard    | STOPPED |      |                                             | VIRTUAL-MACHINE | 0         |
+-----------------+---------+------+---------------------------------------------+-----------------+-----------+
| crazy-goose     | STOPPED |      |                                             | VIRTUAL-MACHINE | 0         |
+-----------------+---------+------+---------------------------------------------+-----------------+-----------+
| dirty-horse     | STOPPED |      |                                             | VIRTUAL-MACHINE | 0         |
+-----------------+---------+------+---------------------------------------------+-----------------+-----------+
| confused-mouse  | STOPPED |      |                                             | VIRTUAL-MACHINE | 0         |
+-----------------+---------+------+---------------------------------------------+-----------------+-----------+
| uplifting-dog   | STOPPED |      |                                             | VIRTUAL-MACHINE | 0         |
+-----------------+---------+------+---------------------------------------------+-----------------+-----------+
```

How do you know which project you're listing?  The most reliable way is to first list projects and see which one is marked `(current)`, like this:

```nohighlight
lxc project list
```

As you see in the sample output, the currently visible and accessible project is listed as `(current)` in the project listing:

```text
+---------------------+--------+----------+-----------------+----------+-------------------------+---------+
|        NAME         | IMAGES | PROFILES | STORAGE VOLUMES | NETWORKS |       DESCRIPTION       | USED BY |
+---------------------+--------+----------+-----------------+----------+-------------------------+---------+
| default (current)   | YES    | YES      | YES             | YES      | Default LXD project     | 7       |
+---------------------+--------+----------+-----------------+----------+-------------------------+---------+
| pg-basebackup-tests | NO     | YES      | NO              | NO       | Project managed by MAAS | 16      |
+---------------------+--------+----------+-----------------+----------+-------------------------+---------+
```

We'll show you how to switch to a different project further along in this tutorial.

*** How to create a LXD project

Suppose that you're about to create a MAAS VM host, and you want a specific project named "maas-vm-host-1" for this particular situation.  You can create that project with the following command:

```nohighlight
$ lxc project create maas-vm-host-1
Project maas-vm-host-1 created
```

When you check your work with `lxc project list`, you'll see that LXD did not automatically switch to the new project:

```nohighlight
+---------------------+--------+----------+-----------------+----------+-------------------------+---------+
|        NAME         | IMAGES | PROFILES | STORAGE VOLUMES | NETWORKS |       DESCRIPTION       | USED BY |
+---------------------+--------+----------+-----------------+----------+-------------------------+---------+
| default (current)   | YES    | YES      | YES             | YES      | Default LXD project     | 7       |
+---------------------+--------+----------+-----------------+----------+-------------------------+---------+
| maas-vm-host-1      | YES    | YES      | YES             | NO       |                         | 1       |
+---------------------+--------+----------+-----------------+----------+-------------------------+---------+
| pg-basebackup-tests | NO     | YES      | NO              | NO       | Project managed by MAAS | 16      |
+---------------------+--------+----------+-----------------+----------+-------------------------+---------+
```

The `lxc` tool generally does only what you ask, nothing more.  

*** How to delete a LXD project

Now, suppose that you decide you don't need this project yet.  No worries: you can easily delete it like this:

```nohighlight
$ lxc project delete maas-vm-host-1
Project maas-vm-host-1 deleted
```

You can check that it was successfully deleted with the `lxc project list` command:

```text
+---------------------+--------+----------+-----------------+----------+-------------------------+---------+
|        NAME         | IMAGES | PROFILES | STORAGE VOLUMES | NETWORKS |       DESCRIPTION       | USED BY |
+---------------------+--------+----------+-----------------+----------+-------------------------+---------+
| default (current)   | YES    | YES      | YES             | YES      | Default LXD project     | 7       |
+---------------------+--------+----------+-----------------+----------+-------------------------+---------+
| pg-basebackup-tests | NO     | YES      | NO              | NO       | Project managed by MAAS | 16      |
+---------------------+--------+----------+-----------------+----------+-------------------------+---------+
```

*** How to rename a LXD project

On the other hand, maybe you didn't need to actually delete that project, just change the name to `maas-vm-host-001`, which is what you really wanted in the first place.  Consider your original project name, `maas-vm-host-1`:

```nohighlight
+---------------------+--------+----------+-----------------+----------+-------------------------+---------+
|        NAME         | IMAGES | PROFILES | STORAGE VOLUMES | NETWORKS |       DESCRIPTION       | USED BY |
+---------------------+--------+----------+-----------------+----------+-------------------------+---------+
| default (current)   | YES    | YES      | YES             | YES      | Default LXD project     | 7       |
+---------------------+--------+----------+-----------------+----------+-------------------------+---------+
| maas-vm-host-1      | YES    | YES      | YES             | NO       |                         | 1       |
+---------------------+--------+----------+-----------------+----------+-------------------------+---------+
| pg-basebackup-tests | NO     | YES      | NO              | NO       | Project managed by MAAS | 16      |
+---------------------+--------+----------+-----------------+----------+-------------------------+---------+
```

You can quickly and easily change the project name like this:

```nohighlight
$ lxc project rename maas-vm-host-1 maas-vm-host-001
Project maas-vm-host-1 renamed to maas-vm-host-001
$ lxc project list
+---------------------+--------+----------+-----------------+----------+-------------------------+---------+
|        NAME         | IMAGES | PROFILES | STORAGE VOLUMES | NETWORKS |       DESCRIPTION       | USED BY |
+---------------------+--------+----------+-----------------+----------+-------------------------+---------+
| default (current)   | YES    | YES      | YES             | YES      | Default LXD project     | 7       |
+---------------------+--------+----------+-----------------+----------+-------------------------+---------+
| maas-vm-host-001    | YES    | YES      | YES             | NO       |                         | 1       |
+---------------------+--------+----------+-----------------+----------+-------------------------+---------+
| pg-basebackup-tests | NO     | YES      | NO              | NO       | Project managed by MAAS | 16      |
+---------------------+--------+----------+-----------------+----------+-------------------------+---------+
```

From now on, we'll be combining command output with the command invocation, most of the time.

*** How to switch between LXD projects

You can choose which LXD project is currently visible and accessible, that is, you can choose which project will be acted on by most of the other project commands.  Let's begin by listing the current projects:

```nohighlight
$ lxc project list
+---------------------+--------+----------+-----------------+----------+-------------------------+---------+
|        NAME         | IMAGES | PROFILES | STORAGE VOLUMES | NETWORKS |       DESCRIPTION       | USED BY |
+---------------------+--------+----------+-----------------+----------+-------------------------+---------+
| default (current)   | YES    | YES      | YES             | YES      | Default LXD project     | 7       |
+---------------------+--------+----------+-----------------+----------+-------------------------+---------+
| maas-vm-host-001    | YES    | YES      | YES             | NO       |                         | 1       |
+---------------------+--------+----------+-----------------+----------+-------------------------+---------+
| pg-basebackup-tests | NO     | YES      | NO              | NO       | Project managed by MAAS | 16      |
+---------------------+--------+----------+-----------------+----------+-------------------------+---------+
```

Only the project marked `(current)` in the project listing can be manipulated, for the most part, with the obvious exceptions of command that take project names (like "create," "delete," and so forth).  For example, using `lxc list` to enumerate the names of containers and VMs limits its scope to the current project, which is till "default" at this point:

```nohighlight
$ lxc list
+-----------------+---------+------+---------------------------------------------+-----------------+-----------+
|      NAME       |  STATE  | IPV4 |                    IPV6                     |      TYPE       | SNAPSHOTS |
+-----------------+---------+------+---------------------------------------------+-----------------+-----------+
| trusty-drake    | STOPPED |      |                                             | VIRTUAL-MACHINE | 0         |
+-----------------+---------+------+---------------------------------------------+-----------------+-----------+
| upward-stallion | RUNNING |      | fd42:ec:5a53:59d2:216:3eff:febf:7fa7 (eth0) | CONTAINER       | 0         |
+-----------------+---------+------+---------------------------------------------+-----------------+-----------+
| witty-lizard    | STOPPED |      |                                             | VIRTUAL-MACHINE | 0         |
+-----------------+---------+------+---------------------------------------------+-----------------+-----------+
| crazy-goose     | STOPPED |      |                                             | VIRTUAL-MACHINE | 0         |
+-----------------+---------+------+---------------------------------------------+-----------------+-----------+
| dirty-horse     | STOPPED |      |                                             | VIRTUAL-MACHINE | 0         |
+-----------------+---------+------+---------------------------------------------+-----------------+-----------+
| confused-mouse  | STOPPED |      |                                             | VIRTUAL-MACHINE | 0         |
+-----------------+---------+------+---------------------------------------------+-----------------+-----------+
| uplifting-dog   | STOPPED |      |                                             | VIRTUAL-MACHINE | 0         |
+-----------------+---------+------+---------------------------------------------+-----------------+-----------+
```

Suppose I want to know what all those "USE BY" things are in that `pg-basebackup-tests` project?  Well, the most straightforward way to get that list is to first switch projects, like this:

```nohighlight
lxc project switch pg-basebackup-tests
```

This command returns nothing if successful (following the old UNIX rule of "no news is good news").  If you now repeat the project list command, like this:

```nohighlight
$ lxc project list
+-------------------------------+--------+----------+-----------------+----------+-------------------------+---------+
|             NAME              | IMAGES | PROFILES | STORAGE VOLUMES | NETWORKS |       DESCRIPTION       | USED BY |
+-------------------------------+--------+----------+-----------------+----------+-------------------------+---------+
| default                       | YES    | YES      | YES             | YES      | Default LXD project     | 7       |
+-------------------------------+--------+----------+-----------------+----------+-------------------------+---------+
| maas-vm-host-001              | YES    | YES      | YES             | NO       |                         | 1       |
+-------------------------------+--------+----------+-----------------+----------+-------------------------+---------+
| pg-basebackup-tests (current) | NO     | YES      | NO              | NO       | Project managed by MAAS | 16      |
+-------------------------------+--------+----------+-----------------+----------+-------------------------+---------+
```

You can see in the above listing that we've switched to the "...-tests" project.  Now when we do a container list, we'll see a different set:

```nohighlight
$ lxc list
+-----------------+---------+------+---------------------------------------------+-----------------+-----------+
|      NAME       |  STATE  | IPV4 |                    IPV6                     |      TYPE       | SNAPSHOTS |
+-----------------+---------+------+---------------------------------------------+-----------------+-----------+
| whacky-moose    | STOPPED |      |                                             | VIRTUAL-MACHINE | 0         |
+-----------------+---------+------+---------------------------------------------+-----------------+-----------+
| peeved-gerbil   | STOPPED |      |                                             | VIRTUAL-MACHINE | 0         |
+-----------------+---------+------+---------------------------------------------+-----------------+-----------+
| hairy-nutria    | STOPPED |      |                                             | VIRTUAL-MACHINE | 0         |
+-----------------+---------+------+---------------------------------------------+-----------------+-----------+
| mad-crocodile   | STOPPED |      |                                             | VIRTUAL-MACHINE | 0         |
+-----------------+---------+------+---------------------------------------------+-----------------+-----------+
| flirty-possum   | STOPPED |      |                                             | VIRTUAL-MACHINE | 0         |
+-----------------+---------+------+---------------------------------------------+-----------------+-----------+
| angry-armadillo | STOPPED |      |                                             | VIRTUAL-MACHINE | 0         |
+-----------------+---------+------+---------------------------------------------+-----------------+-----------+
| sneaky-snake    | STOPPED |      |                                             | VIRTUAL-MACHINE | 0         |
+-----------------+---------+------+---------------------------------------------+-----------------+-----------+
| happy-catfish   | STOPPED |      |                                             | VIRTUAL-MACHINE | 0         |
+-----------------+---------+------+---------------------------------------------+-----------------+-----------+
| cute-kitten     | STOPPED |      |                                             | VIRTUAL-MACHINE | 0         |
+-----------------+---------+------+---------------------------------------------+-----------------+-----------+
| bombastic-dog   | STOPPED |      |                                             | VIRTUAL-MACHINE | 0         |
+-----------------+---------+------+---------------------------------------------+-----------------+-----------+
| articulate-eel  | STOPPED |      |                                             | VIRTUAL-MACHINE | 0         |
+-----------------+---------+------+---------------------------------------------+-----------------+-----------+
| mobid-owl       | STOPPED |      |                                             | VIRTUAL-MACHINE | 0         |
+-----------------+---------+------+---------------------------------------------+-----------------+-----------+
| drunk-crow      | STOPPED |      |                                             | VIRTUAL-MACHINE | 0         |
+-----------------+---------+------+---------------------------------------------+-----------------+-----------+
| spicy-alligator | STOPPED |      |                                             | VIRTUAL-MACHINE | 0         |
+-----------------+---------+------+---------------------------------------------+-----------------+-----------+
| nice-crawfish   | STOPPED |      |                                             | VIRTUAL-MACHINE | 0         |
+-----------------+---------+------+---------------------------------------------+-----------------+-----------+
```

It's good practice to always switch projects carefully, so you're not operating in some other project and creating chaos by accident.

*** How to get a summary of LXD project resources

We said that `lxc` commands operate on the current project most of the time.  We gave that caveat because of commands like `lxc project info`, which requires a project name to get any usable output.  For example, if you just type `lxc project info`, you'll just get some "help" output:

```nohighlight
$ lxc project info
Description:
  Get a summary of resource allocations

Usage:
  lxc project info [<remote>:]<project> <key> [flags]

Flags:
      --format   Format (csv|json|table|yaml) (default "table")

Global Flags:
      --debug            Show all debug messages
      --force-local      Force using the local unix socket
  -h, --help             Print help
      --project string   Override the source project
  -q, --quiet            Don't show progress information
  -v, --verbose          Show all information messages
      --version          Print version number
```

You can see from the help listing that a project name is required.  Let's try that again with a fairly large project:

```nohighlight
$ lxc project info pg-basebackup-tests
+------------------+-----------+----------+
|     RESOURCE     |   LIMIT   |  USAGE   |
+------------------+-----------+----------+
| CONTAINERS       | UNLIMITED | 0        |
+------------------+-----------+----------+
| CPU              | UNLIMITED | 15       |
+------------------+-----------+----------+
| DISK             | UNLIMITED | 120.00GB |
+------------------+-----------+----------+
| INSTANCES        | UNLIMITED | 15       |
+------------------+-----------+----------+
| MEMORY           | UNLIMITED | 32.21GB  |
+------------------+-----------+----------+
| NETWORKS         | UNLIMITED | 0        |
+------------------+-----------+----------+
| PROCESSES        | UNLIMITED | 0        |
+------------------+-----------+----------+
| VIRTUAL-MACHINES | UNLIMITED | 15       |
+------------------+-----------+----------+
```

Here we see that the `pg-basebackup-tests` file has no containers, 15 VMs, 120GB of disk space used, etc.  You can do this for any project, even if it's not the current project, so from where we are here (in the `pg-basebackup-tests` project), we can still check resources in the `default` project:

```nohighlight
$ lxc project info default
+------------------+-----------+---------+
|     RESOURCE     |   LIMIT   |  USAGE  |
+------------------+-----------+---------+
| CONTAINERS       | UNLIMITED | 1       |
+------------------+-----------+---------+
| CPU              | UNLIMITED | 2       |
+------------------+-----------+---------+
| DISK             | UNLIMITED | 16.00GB |
+------------------+-----------+---------+
| INSTANCES        | UNLIMITED | 3       |
+------------------+-----------+---------+
| MEMORY           | UNLIMITED | 4.29GB  |
+------------------+-----------+---------+
| NETWORKS         | UNLIMITED | 2       |
+------------------+-----------+---------+
| PROCESSES        | UNLIMITED | 0       |
+------------------+-----------+---------+
| VIRTUAL-MACHINES | UNLIMITED | 2       |
+------------------+-----------+---------+
```

Note that `lxc project info` requires a project name.  As mentioned earlier, typing the command without a project name just gives you a help message, not the stats for the default or current projects.

*** How to show LXD project options

You'll remember that the "USED BY" column seemed to list more entities than there were containers or VMs.  For example, the `default` project is "USED BY" seven entities, but only shows three containers or VMs:

```nohighlight
$ lxc project list
+---------------------+--------+----------+-----------------+----------+-------------------------+---------+
|        NAME         | IMAGES | PROFILES | STORAGE VOLUMES | NETWORKS |       DESCRIPTION       | USED BY |
+---------------------+--------+----------+-----------------+----------+-------------------------+---------+
| default (current)   | YES    | YES      | YES             | YES      | Default LXD project     | 7       |
+---------------------+--------+----------+-----------------+----------+-------------------------+---------+
| maas-vm-host-001    | YES    | YES      | YES             | NO       |                         | 1       |
+---------------------+--------+----------+-----------------+----------+-------------------------+---------+
| pg-basebackup-tests | NO     | YES      | NO              | NO       | Project managed by MAAS | 16      |
+---------------------+--------+----------+-----------------+----------+-------------------------+---------+
```

You can make more sense of the "USED BY" column, and get a lot more information about your project, by using the `lxc project show` command:

```nohighlight
$ lxc project show default
config:
  features.images: "true"
  features.networks: "true"
  features.profiles: "true"
  features.storage.volumes: "true"
description: Default LXD project
name: default
used_by:
- /1.0/images/9a30ffb2faeea61cce6012c63071a1f1504a76e1dbbe03e575cc313170fdaf43
- /1.0/instances/trusty-drake
- /1.0/instances/upward-stallion
- /1.0/instances/witty-lizard
- /1.0/networks/lxdbr0
- /1.0/networks/lxdbr1
- /1.0/profiles/default
```

Here you'll see several categories of information, notably as list of entities that are using this project.  For example, there are three VMs/containers, two networks, one image, and the default profile.

What's really interesting, though, is that the `pg-basebackup-tests` project is only used by 16 entities -- but there are 15 VMs in that project.  What's that discrepancy about?  Well, we can find out by showing the options for that project:

```text
$ lxc project show pg-basebackup-tests
config:
  features.images: "false"
  features.profiles: "true"
  features.storage.volumes: "false"
description: Project managed by MAAS
name: pg-basebackup-tests
used_by:
- /1.0/instances/whacky-moose?project=pg-basebackup-tests
- /1.0/instances/peeved-gerbil?project=pg-basebackup-tests
- /1.0/instances/hairy-nutria?project=pg-basebackup-tests
- /1.0/instances/mad-crocodile?project=pg-basebackup-tests
- /1.0/instances/flirty-possum?project=pg-basebackup-tests
- /1.0/instances/angry-armadillo?project=pg-basebackup-tests
- /1.0/instances/sneaky-snake?project=pg-basebackup-tests
- /1.0/instances/happy-catfish?project=pg-basebackup-tests
- /1.0/instances/cute-kitten?project=pg-basebackup-tests
- /1.0/instances/bombastic-dog?project=pg-basebackup-tests
- /1.0/instances/articulate-eel?project=pg-basebackup-tests
- /1.0/instances/morbid-owl?project=pg-basebackup-tests
- /1.0/instances/drunk-crow?project=pg-basebackup-tests
- /1.0/instances/spicy-alligator?project=pg-basebackup-tests
- /1.0/instances/nice-crawfish?project=pg-basebackup-tests
- /1.0/profiles/default?project=pg-basebackup-tests
```

Here you can see that the non-default project contains only a default profile for itself, and the 15 VMs added there.  The other entities aren't needed here, or can be accessed in the	`default` project if required.

*** How to use LXD projects with MAAS

This subsection will show you:

- [How to create a new project for MAAS when instantiating a VM host](#heading--projects-s2-create-with-vm-host)
- [How to create a new VM in the LXD project associated with a VM host](#heading--projects-s2-create-vm-in-vm-host-project)
- [How to move an existing VM into the LXD project associated with a VM host](#heading--projects-s2-move-vm-into-vm-host-project)
- [How to delete the VM host](#heading--projects-s2-delete-vm-host)
- [How to move LXD entities to another project to hide them from MAAS](#heading--projects-s2-move-non-maas-items)

*** How to create a new project for MAAS when instantiating a VM host

If you're using MAAS from the CLI, you'll want to make sure you've generated an API key and logged in before you attempt to create a VM host.  These steps are fairly simple; first, you'll need the MAAS URL, which for this example, is `http://192.168.33.91:5240/MAAS`.   You can find this URL by typing:

```nohighlight
$ maas --help
```

This will return a help string.  The correct MAAS API URL is shown in the last entry, "admin:"

```nohighlight
usage: maas [-h] COMMAND ...

optional arguments:
  -h, --help      show this help message and exit

drill down:
  COMMAND
    login         Log in to a remote API, and remember its description and credentials.
    logout        Log out of a remote API, purging any stored credentials.
    list          List remote APIs that have been logged-in to.
    refresh       Refresh the API descriptions of all profiles.
    init          Initialise MAAS in the specified run mode.
    config        View or change controller configuration.
    status        Status of controller services.
    migrate       Perform migrations on connected database.
    apikey        Used to manage a user's API keys. Shows existing keys unless --generate or --delete
                  is passed.
    configauth    Configure external authentication.
    createadmin   Create a MAAS administrator account.
    changepassword
                  Change a MAAS user's password.
    admin         Interact with http://192.168.33.91:5240/MAAS/api/2.0/
```

Next, you'll need to generate the API key for your administrative user.  You can this by entering the following at the command line:

```nohighlight
$ sudo maas apikey --generate --username admin
[sudo] password for $USERNAME:
```

This will return only the API key, which looks something like this:

```nohighlight
PPWQWHs75G6rRmhgdQ:mskfQUYsSqBQnfCYC8:ZruUD3EmnQyhRLapR5whY4bV4h8n7zr7
```

Having both of these, you can login with the following command:

```nohighlight
$ maas login admin http://192.168.33.91:5240/MAAS/api/2.0
API key (leave empty for anonymous access): PPWQWHs75G6rRmhgdQ:mskfQUYsSqBQnfCYC8:ZruUD3EmnQyhRLapR5whY4bV4h8n7zr7
```

Note in this example, you could cut and paste both the MAAS API URL and the API key into the command, at appropriate points.  When you log in successfully, you will obtain a help listing something like this:

```nohighlight

You are now logged in to the MAAS server at
http://192.168.33.91:5240/MAAS/api/2.0/ with the profile name 'admin'.

For help with the available commands, try:

  maas admin --help
```

Now that you're logged in, you can create a new KVM with the following `maas $PROFILE vmhosts create` command:

```nohighlight
$ maas admin vmhosts create --help
usage: maas admin vmhosts create [--help] [-d] [-k] [data [data ...]]

Create a VM host


This method accepts keyword arguments.  Pass each argument as a
key-value pair with an equals sign between the key and the value:
key1=value1 key2=value key3=value3.  Keyword arguments must come after
any positional arguments.

Create or discover a new VM host.

:param type: Required.  The type of VM host to create:
``lxd`` or ``virsh``.
:type type: String

 :param power_address: Required.  Address that gives
MAAS access to the VM host power control. For example, for virsh
``qemu+ssh:-172.16.99.2#,system``
For ``lxd``, this is just the address of the host.
:type power_address: String

 :param power_user: Required.  Username to use for
power control of the VM host. Required for ``virsh``
VM hosts that do not have SSH set up for public-key authentication.
:type power_user: String

 :param power_pass: Required.  Password to use for
power control of the VM host. Required ``virsh`` VM hosts that do
not have SSH set up for public-key authentication and for ``lxd``
if the MAAS certificate is not registered already in the LXD server.
:type power_pass: String

 :param name: Optional.  The new VM host's name.
:type name: String

 :param zone: Optional.  The new VM host's zone.
:type zone: String

 :param pool: Optional.  The name of the resource
pool the new VM host will belong to. Machines composed from this VM host
will be assigned to this resource pool by default.
:type pool: String

 :param tags: Optional.  A tag or list of tags (
comma delimited) to assign to the new VM host.
:type tags: String

 :param project: Optional.  For ``lxd`` VM hosts, the
project that MAAS will manage. If not provided, the ``default`` project
will be used. If a nonexistent name is given, a new project with that
name will be created.
:type project: String


Common command-line options:
    --help, -h
	Show this help message and exit.
    -d, --debug
	Display more information about API responses.
    -k, --insecure
	Disable SSL certificate check
stormrider@wintermute:~$ maas admin vmho
```

In the case of our example server, you'd type:

```nohighlight
$ maas admin vmhosts create type=lxd power_address=10.196.199.1:8443 project=keystone name=foo
```

You'd be greeted with a success result that looks something like this:

```nohighlight
Success.
Machine-readable output follows:
{
    "host": {
        "system_id": "hybned",
        "__incomplete__": true
    },
    "storage_pools": [
        {
            "id": "default",
            "name": "default",
            "type": "dir",
            "path": "/var/snap/lxd/common/lxd/storage-pools/default",
            "total": 248618848256,
            "used": 0,
            "available": 248618848256,
            "default": true
        },
        {
            "id": "default2",
            "name": "default2",
            "type": "dir",
            "path": "/var/snap/lxd/common/lxd/storage-pools/default2",
            "total": 248618848256,
            "used": 0,
            "available": 248618848256,
            "default": false
        }
    ],
    "type": "lxd",
    "used": {
        "cores": 0,
        "memory": 0,
        "local_storage": 0
    },
    "zone": {
        "name": "default",
        "description": "",
        "id": 1,
        "resource_uri": "/MAAS/api/2.0/zones/default/"
    },
    "total": {
        "cores": 0,
        "memory": 0,
        "local_storage": 497237696512
    },
    "tags": [
        "pod-console-logging"
    ],
    "architectures": [
        "amd64/generic"
    ],
    "available": {
        "cores": 0,
        "memory": 0,
        "local_storage": 497237696512
    },
    "pool": {
        "name": "default",
        "description": "Default pool",
        "id": 0,
        "resource_uri": "/MAAS/api/2.0/resourcepool/0/"
    },
    "default_macvlan_mode": null,
    "name": "foo",
    "version": "4.13",
    "id": 26,
    "memory_over_commit_ratio": 1.0,
    "cpu_over_commit_ratio": 1.0,
    "capabilities": [
        "composable",
        "dynamic_local_storage",
        "over_commit",
        "storage_pools"
    ],
    "resource_uri": "/MAAS/api/2.0/vm-hosts/26/"
}
```

Note that we specified the project `keystone` as part of this creation step.  We can now check the LXD project list and see if we did, in fact, create that project:

```nohighlight
$ lxc project list
+---------------------+--------+----------+-----------------+----------+-------------------------+---------+
|        NAME         | IMAGES | PROFILES | STORAGE VOLUMES | NETWORKS |       DESCRIPTION       | USED BY |
+---------------------+--------+----------+-----------------+----------+-------------------------+---------+
| default             | YES    | YES      | YES             | YES      | Default LXD project     | 5       |
+---------------------+--------+----------+-----------------+----------+-------------------------+---------+
| keystone            | NO     | YES      | NO              | NO       | Project managed by MAAS | 1       |
+---------------------+--------+----------+-----------------+----------+-------------------------+---------+
| maas_vm_host_001    | YES    | YES      | YES             | NO       |                         | 1       |
+---------------------+--------+----------+-----------------+----------+-------------------------+---------+
| new_project         | NO     | YES      | NO              | NO       | Project managed by MAAS | 1       |
+---------------------+--------+----------+-----------------+----------+-------------------------+---------+
| not-maas (current)  | YES    | YES      | YES             | NO       |                         | 3       |
+---------------------+--------+----------+-----------------+----------+-------------------------+---------+
| pg-basebackup-tests | NO     | YES      | NO              | NO       | Project managed by MAAS | 16      |
+---------------------+--------+----------+-----------------+----------+-------------------------+---------+
```

Finally, we can switch to the project to get a detailed look:

```nohighlight
$ lxc project switch keystone
$ lxc list
+------+-------+------+------+------+-----------+
| NAME | STATE | IPV4 | IPV6 | TYPE | SNAPSHOTS |
+------+-------+------+------+------+-----------+
```

You'll note that, since we just created the VM host, without adding any VMs, the `keystone` project will be empty.

*** How to create a new VM in the LXD project associated with a VM host

Let's say that you have created your VM host (called `foo`, in this case) with a new, empty project called `keystone`.  Now you want to create (that is, compose) a VM is this project.  You can accomplish this with a command similar to the following:

```nohighlight
maas admin vmhost compose 26
```

The VM host ID is found from the `resource_uri` line of the JSON output that was returned when you created the VM host; in this case, that line looks like this, yielding the ID number `26`:

```nohighlight
    "resource_uri": "/MAAS/api/2.0/vm-hosts/26/"
```

MAAS will create (compose) the VM and immediately commission it.  You can see this by executing the following command:

```nohighlight
maas admin machines read wx8xcr | grep status_name
```

In this example, we're using the system ID returned as the `resource_uri` of the composed VM that was returned in the JSON from the `maas admin vmhost compose` command above.  We recieve output similar to the following:

```nohighlight
    "network_test_status_name": "Unknown",
    "testing_status_name": "Passed",
    "status_name": "Ready",
    "commissioning_status_name": "Passed",
    "other_test_status_name": "Unknown",
    "storage_test_status_name": "Passed",
    "interface_test_status_name": "Unknown",
    "cpu_test_status_name": "Unknown",
    "memory_test_status_name": "Unknown",
```

You can see by the several status messages that this machine was successfully commissioned, sitting now in the ready state.

So from this experiment, we can see that creating (composing) a VM in a VM host causes MAAS to automatically commission the VM.

*** How to move an existing VM into the LXD project associated with a VM host

We've seen what happens if we compose a VM in a VM host -- it's automatically commissioned.  But what if we move an existing VM into a LXD project that's associated with a MAAS VM host?  Let's try it and see.

First, let's check on an existing VM in one of our other projects:

```nohighlight
$ lxc project switch not-maas
$ lxc list
+-----------------+---------+------+------+-----------------+-----------+
|      NAME       |  STATE  | IPV4 | IPV6 |      TYPE       | SNAPSHOTS |
+-----------------+---------+------+------+-----------------+-----------+
| trusty-drake    | STOPPED |      |      | VIRTUAL-MACHINE | 0         |
+-----------------+---------+------+------+-----------------+-----------+
| upward-stallion | STOPPED |      |      | CONTAINER       | 0         |
+-----------------+---------+------+------+-----------------+-----------+
$ lxc move trusty-drake trusty-drake --project not-maas --target-project keystone
$ lxc project switch keystone
$ lxc list
+--------------+---------+------+------+-----------------+-----------+
|     NAME     |  STATE  | IPV4 | IPV6 |      TYPE       | SNAPSHOTS |
+--------------+---------+------+------+-----------------+-----------+
| handy-sloth  | STOPPED |      |      | VIRTUAL-MACHINE | 0         |
+--------------+---------+------+------+-----------------+-----------+
| trusty-drake | STOPPED |      |      | VIRTUAL-MACHINE | 0         |
+--------------+---------+------+------+-----------------+-----------+
```

We can check the status with MAAS, but we'll find that the machine isn't recognised.  If we turn it on, it will be enlisted by MAAS.  Since MAAS doesn't know about it yet, we need to turn it on with the following command:

```nohighlight
lxc start trusty-drake
```

Nothing happens for a while, but eventually MAAS will discover the machine and attempt to commission it.  In fact, since MAAS doesn't know what power type to use, it completes all the commissioning scripts except `30-maas-01-bmc-config`:

| Name | Tags | Result | Date | Runtime |
|:-----|:----:|:------:|:-----|--------:|
| 20-maas-01-install-lldpd |	node|	Passed|	Mon, 19 Apr. 2021 21:42:22|	0:00:00|
| 20-maas-02-dhcp-unconfigured-ifaces|	node|	Passed|	Mon, 19 Apr. 2021 21:42:22|	0:00:00|
| 30-maas-01-bmc-config|	bmc-config, node|	Skipped|	Mon, 19 Apr. 2021 21:42:22|	0:00:00|
50-maas-01-commissioning| 	node|	Passed|	Mon, 19 Apr. 2021 21:42:23|	0:00:00|
maas-capture-lldpd| 	node|	Passed|	Mon, 19 Apr. 2021 21:43:17|	0:00:53|
maas-get-fruid-api-data| 	node|	Passed|	Mon, 19 Apr. 2021 21:42:26|	0:00:00|
maas-kernel-cmdline| 	node|	Passed|	Mon, 19 Apr. 2021 21:42:25|	0:00:01|
maas-list-modaliases| 	node|	Passed|	Mon, 19 Apr. 2021 21:42:25|	0:00:00|
maas-lshw| 	node|	Passed|	Mon, 19 Apr. 2021 21:42:26|	0:00:02|
maas-serial-ports| 	node|	Passed|	Mon, 19 Apr. 2021 21:42:24|	0:00:00|
maas-support-info| 	node|	Passed|	Mon, 19 Apr. 2021 21:42:25|	0:00:01|

This machine will sit in the "New" state until you assign it a power type, and enter the correct power parameters.

For example, to get this new (moved) VM ready to be fully commissioned, you'll need to first find it in the machine list:

```nohighlight
maas admin machines read
(lots of JSON output, down to the last line)
  "resource_uri": "/MAAS/api/2.0/machines/r3mmsh/"
'''

*** How to delete the VM host

At some point, you may want to delete your MAAS VM host.  You can do so in the following way:

*** How to move LXD entities to another project to hide them from MAAS

Suppose that want to use MAAS with your default LXD project, and that you have a couple of LXD entities in your default project that you don't want to use with MAAS:

```nohighlight
$ lxc list
+-----------------+---------+-----------------------+---------------------------------------------+-----------------+-----------+
|      NAME       |  STATE  |         IPV4          |                    IPV6                     |      TYPE       | SNAPSHOTS |
+-----------------+---------+-----------------------+---------------------------------------------+-----------------+-----------+
| trusty-drake    | STOPPED |                       |                                             | VIRTUAL-MACHINE | 0         |
+-----------------+---------+-----------------------+---------------------------------------------+-----------------+-----------+
| upward-stallion | RUNNING | 10.196.199.194 (eth0) | fd42:ec:5a53:59d2:216:3eff:febf:7fa7 (eth0) | CONTAINER       | 0         |
+-----------------+---------+-----------------------+---------------------------------------------+-----------------+-----------+
| witty-lizard    | STOPPED |                       |                                             | VIRTUAL-MACHINE | 0         |
+-----------------+---------+-----------------------+---------------------------------------------+-----------------+-----------+
```

In the above example, you want to use `witty-lizard` with MAAS, but you want to move the other two entities to a project called `not-maas`.  To accomplish this, you first need to create the `not-maas` project if it doesn't exist:

```nohighlight
$ lxc project create not-maas
Project not-maas created
$ lxc project list
+---------------------+--------+----------+-----------------+----------+-------------------------+---------+
|        NAME         | IMAGES | PROFILES | STORAGE VOLUMES | NETWORKS |       DESCRIPTION       | USED BY |
+---------------------+--------+----------+-----------------+----------+-------------------------+---------+
| default (current)   | YES    | YES      | YES             | YES      | Default LXD project     | 7       |
+---------------------+--------+----------+-----------------+----------+-------------------------+---------+
| maas_vm_host_001    | YES    | YES      | YES             | NO       |                         | 1       |
+---------------------+--------+----------+-----------------+----------+-------------------------+---------+
| not-maas            | YES    | YES      | YES             | NO       |                         | 1       |
+---------------------+--------+----------+-----------------+----------+-------------------------+---------+
| pg-basebackup-tests | NO     | YES      | NO              | NO       | Project managed by MAAS | 16      |
+---------------------+--------+----------+-----------------+----------+-------------------------+---------+
```

Having done so, you now want to move `trusty-drake` and `upward-stallion` to a new project.  Let's tackle `trusty-drake` first:

```nohighlight
$ lxc move trusty-drake trusty-drake --project default --target-project not-maas --verbose
$ lxc list
+-----------------+---------+-----------------------+---------------------------------------------+-----------------+-----------+
|      NAME       |  STATE  |         IPV4          |                    IPV6                     |      TYPE       | SNAPSHOTS |
+-----------------+---------+-----------------------+---------------------------------------------+-----------------+-----------+
| upward-stallion | RUNNING | 10.196.199.194 (eth0) | fd42:ec:5a53:59d2:216:3eff:febf:7fa7 (eth0) | CONTAINER       | 0         |
+-----------------+---------+-----------------------+---------------------------------------------+-----------------+-----------+
| witty-lizard    | STOPPED |                       |                                             | VIRTUAL-MACHINE | 0         |
+-----------------+---------+-----------------------+---------------------------------------------+-----------------+-----------+
$ lxc project switch not-maas
$ lxc list
+--------------+---------+------+------+-----------------+-----------+
|     NAME     |  STATE  | IPV4 | IPV6 |      TYPE       | SNAPSHOTS |
+--------------+---------+------+------+-----------------+-----------+
| trusty-drake | STOPPED |      |      | VIRTUAL-MACHINE | 0         |
+--------------+---------+------+------+-----------------+-----------+
```

It's important to note that the `move` step may take 30 seconds or more; that's normal.

Next, let's try moving `upward-stallion`, which is a running container:

```nohighlight
$ lxc move upward-stallion upward-stallion --project default --target-project not-maas --verbose
Error: Failed creating instance record: Failed initialising instance: Invalid devices: Failed detecting root disk device: No root device could be found
$ lxc list
+-----------------+---------+-----------------------+---------------------------------------------+-----------------+-----------+
|      NAME       |  STATE  |         IPV4          |                    IPV6                     |      TYPE       | SNAPSHOTS |
+-----------------+---------+-----------------------+---------------------------------------------+-----------------+-----------+
| upward-stallion | RUNNING | 10.196.199.216 (eth0) | fd42:ec:5a53:59d2:216:3eff:fe64:a206 (eth0) | CONTAINER       | 0         |
+-----------------+---------+-----------------------+---------------------------------------------+-----------------+-----------+
| witty-lizard    | STOPPED |                       |                                             | VIRTUAL-MACHINE | 0         |
+-----------------+---------+-----------------------+---------------------------------------------+-----------------+-----------+
```

Hmm, what's that error message about?  Well, you actually need to add the default storage pool to the mix, with this command:

```nohighlight
lxc profile device add default root disk path=/ pool=default
```

Having done so, you can try the move again:

```nohighlight
$ lxc move upward-stallion upward-stallion --project default --target-project not-maas --verbose
$ lxc list                            
+--------------+---------+------+------+-----------------+-----------+
|     NAME     |  STATE  | IPV4 | IPV6 |      TYPE       | SNAPSHOTS |
+--------------+---------+------+------+-----------------+-----------+
| witty-lizard | STOPPED |      |      | VIRTUAL-MACHINE | 0         |
+--------------+---------+------+------+-----------------+-----------+
$ lxc project switch not-maas
$ lxc list
+-----------------+---------+------+------+-----------------+-----------+
|      NAME       |  STATE  | IPV4 | IPV6 |      TYPE       | SNAPSHOTS |
+-----------------+---------+------+------+-----------------+-----------+
| trusty-drake    | STOPPED |      |      | VIRTUAL-MACHINE | 0         |
+-----------------+---------+------+------+-----------------+-----------+
| upward-stallion | STOPPED |      |      | CONTAINER       | 0         |
+-----------------+---------+------+------+-----------------+-----------+
```

The move succeeds this time -- with an important distinction: the compartment `upward-stallion` was `STOPPED` by `lxc` during the move.  This is an important planning consideration when you're trying to create MAAS VMs & VM hosts in an already-active LXD instantiation. 
[/tab]
[tab version="v2.9 Snap,v2.9 Packages"]

MAAS 2.9 does not support LXD projects; you must upgrade to MAAS version 3.0 or greater to access this feature.
[/tab]
[/tabs]
* How to spin up MAAS with Ansible
[tabs]
[tab version="v3.4 Snap,v3.4 Packages,v3.3 Snap,v3.3 Packages,v3.2 Snap,v3.2 Packages"]
Ansible playbooks make it easy to install and configure MAAS.

** How to install a region controller with Ansible

As an operator, you want to install a MAAS region controller onto a given host using Ansible. To accomplish this, you must:

1. Attach the `maas_region_controller` role to your region controller host by adding the following to the [Inventory file](https://docs.ansible.com/ansible/latest/user_guide/intro_inventory.html#inventory-basics-formats-hosts-and-groups)`↗`.   In the example below, we've attached the region controller role to a host running on `10.10.0.20` with the user `ubuntu`:

INI:

```INI
[maas_region_controller]
10.10.0.20 ansible_user=ubuntu
```

YAML:

```YAML
all:
  maas_region_controller:
    hosts:
      10.10.0.20:
        ansible_user: ubuntu
```

2. Set the following Ansible variables in the `hosts` file:

`[maas_region_controller]` variables:
```
maas_version: "latest"          # The version of MAAS to install on the host
maas_installation_type: "snap"  # The installation manager to use (snap or deb)
maas_snap_channel: "stable"     # The snap channel, if using snap
maas_url: $Ip_Address           # The url of the database for this MAAS
enable_tls: false               # Whether TLS should be enabled for this MAAS
o11y_enable: false              # Whether observability should be enabled for this MAAS

# Details for the administrative account
admin_username: "admin"
admin_password: "admin"
admin_email: "admin@email.com"
admin_id: "lp:admin"            # Either lp:user-id (Launchpad) or gh:user-id (Github)
```

3. Run the playbook to install the region controller.  A successful run of the playbook should give the operator an accessible and ready MAAS instance.

Some important notes on installation:

- The default installation is a snap.
- The installed region controller is used to set a `maas_url` variable when there is not one already set for later Rack Controller configuration use.
- The operator can optionally enable TLS.
- The playbook sets up the admin user.
- The playbook adds any provided preseeds.
- The playbook only installs the `maas-region-api` deb if the operator chooses the `deb` installation.
- Once the region controller is installed, the playbook will run migrations using the configured postgresql primary instance.
- Running on an already configured machine -- but with a new version -- should upgrade the instance.
- The operator can override the postgres DSN variable on any machine (hence not defining any `maas_postgres` host) to use an existing PostgreSQL instance not managed by this playbook.
- These variables can be also be defined at the Ansible command line using the `--extra_vars` argument.
- The playbook uses an ansible variable to determine what version of MAAS to deploy.  The playbook won’t execute (i.e “skipped” in the context of Ansible) if `host_vars` show the Ubuntu version is incompatible with the version and install method.  
- The Region Controller tasks should be able to execute on multiple hosts in a single execution if the target is an Ansible Group rather than a single host.

*** Finding the new region controller

You can find the newly-installed region controller at the specified MAAS host IP address, as though the controller had been installed manually.

** How to install a rack controller with Ansible

As an operator, you want to install a MAAS rack controller to a given host, using Ansible. To accomplish this, you must: 

1. Assign a host to the `maas_rack_controller` role in the Ansible `hosts` file:

INI
```INI
[maas_rack_controller]
$Host_Ip_Address extra_variable=$Variable_Value
$Second_Host_Ip
```
YAML
```
all:
  maas_rack_controller:
    hosts:
      $Host_Ip_Address:
        extra_variable: $Variable_Value
      $Second_Host_Ip
```

2. Set the following Ansible variables in the `Hosts` file:

`[maas_rack_controller]` Variables
```bash
maas_version: "latest"          # The version of MAAS to install on the host
maas_installation_type: "snap"  # The installation manager to use
maas_snap_channel: "stable"     # The snap channel, if using snap
maas_url: $Ip_Address           # The url of the region controller for this MAAS
maas_rack_secret:               # The secret used to enroll a MAAS rack
enable_tls: false               # Whether TLS should be enabled for this MAAS
o11y_enable: false              # Whether observability should be enabled for this MAAS
```

3. Run the Ansible playbook to install the region controller.

Some notes about installation:

- When running the playbook for a host with the `maas_rack_controller` role, the playbook installs the MAAS Rack Controller on the specified hosts.
- The `maas_url` variable is used to connect the Region Controller(s), either previously configured from a Region Controller install task, or provided by the user. 
- If the `maas_url` variable is not set, the Rack Controller tasks are “skipped”.  Some notes about the installation:
- The operator can optionally enable TLS.
- The rack controller tasks should be able to execute on multiple hosts in a single execution if an Ansible Group is targeted rather than a single host.
- Only install the maas-rack-controller deb if using the deb installation.
- Running on an already configured machine but with a new version should upgrade the instance.
- You can optionally install a grafana agent by setting the `o11y_enable` variable to `true` either in the hosts file or at the command line.

*** Finding the new rack controller

The rack controller should be accessible at the specified host IP address, just as if you had installed it there manually.

*** How to uninstall MAAS with Ansible

As an operator, you want to be able to revert the MAAS setup installed by this playbook, such that the machine is clean of all MAAS packages or snaps.  In order to teardown a MAAS deployment, do the following:

1. Find the entry-point within the playbook to teardown the installed MAAS packages or snaps.  

2. Back up the database and MAAS configuration, if desired.  Note that the target machine is restored state prior to installation, with no MAAS, directories, or files present on the system.

3. Run the playbook from this entry-point to remove the installation.

Running this playbook with the default configuration with perfectly undo the default installation.

*** How to configure MAAS HA with Ansible

As an operator, you want to install a reverse proxy and configure high-availability region controllers for a given host using Ansible.  Note that HA region controllers require an HAProxy configuration.

You can accomplish this with the following steps:

1. Set the following in the `hosts` file to set the `maas_proxy` role:

```nohighlight
[maas_proxy]
my.host ansible_user=ssh_user
```
2. Run the full playbook, or add `--tags maas_proxy` to run only the tasks for this role.

3. Verify that the HAProxy is forwarding traffic by running the following if HAProxy is on a separate host from the region controller:

```nohighlight
curl -L http://<haproxy host>:5240/MAAS`
```

4. If HAProxy is not on a separate host, change the port number to 5050 when you run the command, like this:

```nohighlight
curl -L http://<haproxy host>:5050/MAAS
```

Note that the playbook configures the HAProxy instance for optimal use, such that OS images can be uploaded (for example). An unresponsive Region Controller is taken out of the upstream pool quickly. The HAProxy instance does not interfere with Nginx/MAAS TLS configuration

** How to install HA PostgreSQL

As an operator, you want to install a HA Postgresql database cluster to a given set of hosts using Ansible. You can accomplish this with the following steps:

1. Set the following in the `hosts` file to set the `maas_postgres` and `maas_corosync` roles:

```nohighlight
[maas_corosync]
my.db1 ansible_user=ssh_user
my.db2 ansible_user=ssh_user
my.db3 ansible_user=ssh_user

[maas_pacemaker:children]
maas_corosync

[maas_postgres]
my.db1 ansible_user=ssh_user
my.db2 ansible_user=ssh_user
my.db3 ansible_user=ssh_user
```

2. Set the following Ansible variables in the `Hosts` file:

`[maas_pacemaker]` Variables
```bash
# Fencing configuration
maas_pacemaker_fencing_driver: $stonith_driver
maas_pacemaker_stonith_params: $stonith_parameters
```

`[maas_postgres]` HA-related variables
```bash
maas_postgres_floating_ip: $vIP
maas_postgres_floating_ip_prefix_len: $vIP_masklen
```

3. Run the full playbook, or add `--tags maas_ha_postgres` to run only the tasks for this roles.

4. Verify the primary by running `sudo -u postgres psql` and making sure you get a prompt.

Note that Ansible installs the latest supported version of PostgreSQL supported for the given MAAS version. If the playbook runs with other roles set on targeted hosts / groups, the tasks associated with the `maas_postgresql` role runs first. If the operator sets a variable for importing a backup, the backup is loaded into PostgreSQL.

** How to enable Observability capabilities

As an operator, you want to export metrics and logs to your observability stack using Ansible. You can accomplish this with the following steps:

1. Set the following Ansible variables in the `Hosts` file:

`[all]` Variables
```bash
o11y_enable: true
o11y_prometheus_url: http://$prometheus_ip:9090/api/v1/write
o11y_loki_url: http://$loki:3100/loki/api/v1/push
```

Optionally you can set `o11y_enable` only on hosts of interest.

2. Run the playbook

This installs and configures the `grafana-agent` service on all roles that support it. You can disable either metrics or logs export by omitting the respective endpoint definition. You need to run the Prometheus server with the `remote-write-receiver` feature enabled in order to receive metrics pushed by the agents.

MAAS has a curated collection of alert rules for Prometheus and Loki. You can export these rules using the following command, where `o11y_alertrules_dest` is the directory where the files should me placed.

```bash
ansible-playbook --extra-vars="o11y_alertrules_dest=/tmp" ./alertrules.yaml
```

The resulting files (`loki-alert-rules.yml` and `prometheus-alert-rules.yml`) should be installed in the Loki and Prometheus servers respectively. See https://maas.io/docs/how-to-monitor-maas for a basic observability stack setup.

<!--
** Firewall rules

As a operator, you want to be able to setup MAAS in a secure way, following best practices and operational guidance on securing MAAS. In order to make a MAAS setup secure, I would Ansible playbooks to configure firewalls and file permissions based on https://maas.io/docs/how-to-secure-maas.

 
** PostgreSQL role bundling scripts -->
[/tab]
[tab version="v3.1 Snap,v3.1 Packages,v3.0 Snap,v3.0 Packages,v2.9 Snap,v2.9 Packages"]
Ansible makes it easy to install and configure MAAS 3.2 and above.  Our Ansible playbooks have not been tested or vetted with MAAS versions 3.1 or lower.  If you want to take advantage of Ansible, we strongly recommend upgrading to MAAS 3.2 or higher.
[/tab]
[/tabs]
* How to tag machines
[Tags](/t/how-to-label-and-find-machines/6200#heading--about-tags) for different objects have similar purposes, but they aren't necessarily administered in the same way -- so we've included detailed articles for each tag type.  That said, many of the common operations regarding tags are performed in the same way.  This article will present some general, explanatory information, and then look at tag management steps that are the same (or very similar) across all types of MAAS tags.

** How to name tags

When working with tags, there are some universal rules you need to follow:

1. Tag names can include any combination of alphabetic letters (a-zA-Z), numbers (0-9), dashes (-) and underscores (_).
2. Tag names can be a maximum of 256 characters in length.
3. Tag names *cannot* include spaces.

In general, names that do not conform to these rules cannot be created.

** How to download hardware configuration information

To download hardware configuration information in XML format:

1. Select *Machines*.

2. Select a machine which is allocated or deployed.

3. Select *Logs >> Installation output >> Download >> Machine output (XML)*.

You can [learn more about these attributes](https://ezix.org/project/wiki/HardwareLiSter)`↗` if desired.  Note that:

- Size and capacity can have various meanings depending on the device
- The size of a node is always equal to its capacity
- Serial refers to the device’s serial number, but is used to report the MAC address for network devices, GUID for disk partition.

You can also find device classes from the same sources. 

** Automatic tags

MAAS 3.2 and above provide greatly expanded tagging capability. You can auto-apply tags to machines that match a custom XPath expression. Setting up an automatic tag lets you recognise special hardware characteristics and settings, e.g., the gpu passthrough.

[tabs]
[tab version="v3.4 Snap,v3.4 Packages" view="UI"]
** How to create automatic tags

To create automatic tags:

1. Select *Organisation > Tags*.

2. Select *Create new tag*.

3. Enter the *Tag name*.

4. Optionally enter a *Comment*.

5. Optionally enter *Kernel options*.

6. Enter an XPath-based *Definition*.  A tag is considered automatic when the definition field is filled with an XPath expression. The current version of our UI will only validate if your XPath expression is valid or not, but it will not show you which machines it will apply to before you create the tag.

7. Select *Save* to register your changes.

Once an automatic tag is created the screen will initially show that 0 machines are tagged. That is because MAAS is running a background task to auto-apply the tag to matching machines. It can take some time to see that the number of machines tagged is populating. 

[note]
Kernel options will be applied at boot time. So by default kernel options will not be applied to any machines until they are deployed. If machines are deployed before they are tagged, the kernel option will be applied when these machines are redeployed.
[/note]

** How to update the definition of a tag

1. Select *Organisation > Tags*.

2. Select the pencil icon on the right end of the tag's row.

3. Edit the *Definition*.

4. Select *Save* to register your changes.

Keep in mind that when a new definition is updated, MAAS will re-tag all the machines that match with the new definition. This can take some time, since it is a background process. 
[/tab]
[tab version="v3.3 Snap,v3.3 Packages,v3.2 Snap,v3.2 Packages" view="UI"]
** How to create automatic tags

To create automatic tags:

1. Select *Machines*.

2. Select *Tags*.

3. Select *Create new tag*.

4. Enter the *Tag name*.

5. Optionally enter a *Comment*.

6. Optionally enter *Kernel options*.

7. Enter an XPath-based *Definition*.  A tag is considered automatic when the definition field is filled with an XPath expression. The current version of our UI will only validate if your XPath expression is valid or not, but it will not show you which machines it will apply to before you create the tag.

8. Select *Save* to register your changes.

Once an automatic tag is created the screen will initially show that 0 machines are tagged. That is because MAAS is running a background task to auto-apply the tag to matching machines. It can take some time to see that the number of machines tagged is populating. 

[note]
Kernel options will be applied at boot time. So by default kernel options will not be applied to any machines until they are deployed. If machines are deployed before they are tagged, the kernel option will be applied when these machines are redeployed.
[/note]

** How to update the definition of a tag

1. Select *Machines*.

2. Select *Tags*.

3. Select the pencil icon on the right end of the tag's row.

4. Edit the *Definition*.

5. Select *Save* to register your changes.

Keep in mind that when a new definition is updated, MAAS will re-tag all the machines that match with the new definition. This can take some time, since it is a background process. 
[/tab]
[tab version="v3.1 Snap,v3.1 Packages,v3.0 Snap,v3.0 Packages,v2.9 Snap,v2.9 Packages" view="UI"]
Automatic tags are only available in MAAS 3.2 and above.
[/tab]
[tab version="v3.4 Snap,v3.4 Packages,v3.3 Snap,v3.3 Packages,v3.2 Snap,v3.2 Packages" view="CLI"]
Automatic tags are only available via the MAAS UI.
[/tab]
[tab version="v3.1 Snap,v3.1 Packages,v3.0 Snap,v3.0 Packages,v2.9 Snap,v2.9 Packages" view="CLI"]
Automatic tags are only available in MAAS 3.2 and above.
[/tab]
[/tabs]

** How to update the kernel options on a tag

To update the kernel options on a tag:

1. Select *Machines*.

2. Select *Tags*.

3. Select the pencil icon on the right end of the tag's row.

4. Edit the *Kernel options*.

5. Select *Save* to register your changes.

Kernel options can exist for both manual and automatic tags. However, they will be applied during boot time (commissioning and deploying).

[note]
If the tagged machines are deployed, the updated kernel option won’t apply until the machines are redeployed. We suggest that you release those machines prior to the update, then redeploy those machines when the kernel options of the tag are updated.
[/note]

** How to unassign tags from machines

To unassign tags from machines:

1. Select *Machines*.

2. Select the checkbox(es) next to the machine(s) you wish to untag.

3. Select *Take action >> Tag*.  A table of tags appears at the top of the screen.

4. For each tag you wish to unassign, select *Remove*.  The text will change to *Discard* with an *X* to the right.

5. If you want to undo a choice before saving, click the *X* to right of *Discard* to undo the proposed change.

6. When you're satisfied with your new tag configuration, select *Save* to finalize and register your choice(s).

[note]
Automatic tags cannot be unassigned manually. You can either update or delete automatic tags.
[/note]

You can also unassign tags individually by going to *Machines >> {machine-name} >> Configuration >> Tags >> Edit*. The *Tags* table functions exactly the same as what's described above.

** How to see all tagged nodes

To see how many nodes (Machines, controllers, devices) are tagged, search for GRUB_CMDLINE_LINUX_DEFAULT in the "Installation output" tab of the machine details page. That log should stay around for the lifetime of the deployment of the machine.  The log gets overwritten when you redeploy the machine.  For example:

```nohighlight
GRUB_CMDLINE_LINUX_DEFAULT="sysrq_always_enabled dyndbg='file drivers/usb/* +p' console=tty1 console=ttyS0"
```



[tabs]
[tab version="v3.4 Snap,v3.4 Packages,v3.3 Snap,v3.3 Packages,v3.2 Snap,v3.2 Packages" view="UI"]
** How to create and assign tags

In the MAAS UI, creating and assigning tags is a combined operation; that is, you create tags as you assign them, rather than creating them first.  Creating tags in the UI is a little different user experience: there is a self-loading completion menu that collects all tags of a similar type.  This completion menu helps you avoid misspelling tags when entering them more than once; otherwise, you might not be able to group and filter tags properly. It also makes tag entry more efficient.

To create and assign a tag to specific machines:

1. Select *Machines*.

2. Select the checkbox next to the machines you wish to tag.

3. Select *Take action >> Tag*.  A *Tag* dialogue will pop up at the top of the screen.

4. In the box labeled *Search existing or add new tags*, enter the name for your proposed tag (e.g., *{tag-name}*).

5. Select *Create tag {tag-name}*, under the *Search...* box.  A modal dialogue box pops up; *{tag-name}* will automatically populate the *Tag name* field.

6. Optionally enter a *Comment*.

7. Optionally enter some *Kernal options*.

8. Select *Create and add to tag changes*.  You will be returned to the previous screen; note that your changes have not yet been registered with MAAS.

9. If you wish to abaondon this new tag without assigning it, select *Discard X* to the right of the new tag name.  It will disappear from the list.

10. If you're happy with the new tag(s), select *Save* to apply your changes to the selected machines.  You'll be returned to *Machines*.

You can confirm your changes by hovering over the *Tags* list in the *Machines* screen.

** How to delete and remove tags

You have two choices when it comes to eliminating tags from machines in your MAAS instance: you can delete them from all machines, or simply remove them from specific machines.

*** Deleting tags from all machines at once

To delete tags from all machines:

1. Select *Machines*.

2. Select *Tags*.

3. Select the trash can icon to the right of the tag you'd like to delete.  A warning dialogue will pop up at the top of the screen.

4. Select *Delete*.

The tag will be unassigned from all machines and deleted.  There is no undo.

*** Removing a tag from specific machines

To remove a tag only from specific machines:

1. Select *Machines*.

2. Select the checkbox next to each machine from which you want the tag removed.

3. Select *Take action >> Tag*.

4. For each tag you wish to unassign, select *Remove*.  The text will change to *Discard* with an *X* to the right.

5. If you want to undo a choice before saving, click the *X* to right of *Discard* to undo the proposed change.

6. When you're satisfied with your new tag configuration, select *Save* to finalize and register your choice(s).

[/tab]
[tab version="v3.1 Snap,v3.1 Packages,v3.0 Snap,v3.0 Packages,v2.9 Snap,v2.9 Packages" view="UI"]
** How to create and assign tags

In the MAAS UI, creating and assigning tags is a combined operation; that is, you create tags as you assign them, rather than creating them first.  Creating tags in the UI is a little different user experience: there is a self-loading completion menu that collects all tags of a similar type.  This completion menu helps you avoid misspelling tags when entering them more than once; otherwise, you might not be able to group and filter tags properly. It also makes tag entry more efficient.

The process for creating and assigning tags in the UI is generally the same for all tag types:

1. Place the cursor in the *Tags* box.

2. Type the name of the new tag.

3. Press the return key to add the new tag.  The auto complete list will re-appear after you've entered the tag, in case you'd like to enter another tag.

4. When you're done, select the appropriate completion button to register your changes.

The tag you just entered will now be added to the tag auto complete list, in alphabetical order, for re-use with other machines.

** How to delete and remove tags

With the MAAS UI, you remove tags, rather than explicitly deleting them.  Tags are "deleted" when you have removed them from all machines.

To remove (unassign) a tag:

1. Find the *Tags* box.

2. Click the *X* next to the tag you wish to remove.

3. When you're done, select the appropriate completion button to register your changes.

Note that the tag you just removed will be deleted from  the tag auto complete list when it is no longer assigned to any  machines.
[/tab]
[tab version="v3.4 Snap,v3.4 Packages,v3.3 Snap,v3.3 Packages,v3.2 Snap,v3.2 Packages,v3.1 Snap,v3.1 Packages,v3.0 Snap,v3.0 Packages,v2.9 Snap,v2.9 Packages" view="CLI"]
** How to create a tag

With the CLI, you can create a tag with the following command:

```nohighlight
maas $PROFILE tags create name=$TAG_NAME comment='$TAG_COMMENT'
```

For example, depending upon your system configuration, you might type a command similar to this one:

```nohighlight
maas admin tags create name="new_tag" comment="a new tag for test purposes"
```

When the command is successful, you should see output similar to this:

```nohighlight
Success.
Machine-readable output follows:
{
    "name": "new_tag",
    "definition": "",
    "comment": "a new tag for test purposes",
    "kernel_opts": "",
    "resource_uri": "/MAAS/api/2.0/tags/new_tag/"
}
```

You can verify your work by [listing all the tags on this MAAS](#heading--list-all-tags-available-on-this-maas).

** How to create tags with built-in kernel options

You can create tags with embedded kernel boot options.  When you apply such tags to a machine, those kernel boot options will be applied to that machine on the next deployment.

To create a tag with embedded kernel boot options, use the following command:

```nohighlight
maas $PROFILE tags create name='$TAG_NAME' \
    comment='$TAG_COMMENT' kernel_opts='$KERNEL_OPTIONS'
```

For example:

```nohighlight
maas admin tags create name='nomodeset_tag' \
    comment='nomodeset_kernel_option' kernel_opts='nomodeset vga'
```

This command yields the following results:

```nohighlight
Success.
Machine-readable output follows:
{
    "name": "nomodeset_tag",
    "definition": "",
    "comment": "nomodeset_kernel_option",
    "kernel_opts": "nomodeset vga",
    "resource_uri": "/MAAS/api/2.0/tags/nomodeset_tag/"
}
```

You can check your work with a modified form of the listing command:

```nohighlight
maas admin tags read | jq -r \
'(["tag_name","tag_comment","kernel_options"]
|(.,map(length*"-"))),(.[]|[.name,.comment,.kernel_opts]) 
| @tsv' | column -t
```

This should give you results something like this:

```nohighlight
tag_name             tag_comment                  kernel_options                     
--------             -----------                  --------------                     
virtual                                                                              
new_tag              a-new-tag-for-test-purposes                                     
pod-console-logging  console=tty1                 console=ttyS0                      
nomodeset_tag        nomodeset_kernel_option      nomodeset       vga
```

** How to delete a tag

With the CLI, you can delete a tag with the following command:

```nohighlight
maas $PROFILE tag delete $TAG_NAME
```

For example, depending upon your system configuration, you might type a command similar to this one:

```nohighlight
maas admin tag delete zorko
```

When the command is successful, you should see output similar to this:

```nohighlight
Success.
Machine-readable output follows:
```

Note that there is no actual "Machine-readable output" produced by this command, in most cases. Also note that remove a tag removes it from any nodes where you may have assigned it, but does not affect those nodes in any other way.

You can check your work by [listing all the tags on this MAAS](#heading--list-all-tags-available-on-this-maas).

** How to update a tag

You can update a tag (e.g., a tag comment) like this:

```nohighlight
maas $PROFILE tag update $TAG_NAME comment='$TAG_COMMENT'
```

For example:

```nohighlight
maas admin tag update new_tag comment="a-new-tag-for-test-purposes"
```

This should return an output similar to this one:

```nohighlight
Success.
Machine-readable output follows:
{
    "name": "new_tag",
    "definition": "",
    "comment": "a-new-tag-for-test-purposes",
    "kernel_opts": "",
    "resource_uri": "/MAAS/api/2.0/tags/new_tag/"
}
```

You can always verify by [listing all the tags on this MAAS](#heading--list-all-tags-available-on-this-maas).

** How to list all tags available on this MAAS

You can list all tags that currently exist in this MAAS with a command of the form:

```nohighlight
maas $PROFILE tags read | jq -r '(["tag_name","tag_comment"]|(.,map(length*"-"))),(.[]|[.name,.comment]) | @tsv' | column -t
```

For example:

```nohighlight
maas admin tags read | jq -r '(["tag_name","tag_comment"]|(.,map(length*"-"))),(.[]|[.name,.comment]) | @tsv' | column -t
```

Your output might look like this:

```nohighlight
tag_name  tag_comment
--------  -----------
virtual   
new_tag   a-new-tag-for-test-purposes
```

** How to rebuild a tag

If you need to update tags for all machines – without having to recommission them – you can accomplish this with the rebuild command:

```nohighlight
maas $PROFILE tag rebuild $TAG
```

This command automatically applies the tag to all machines regardless of state, even machines that are actively deployed. For example:

```nohighlight
maas admin tag rebuild virtual
```

This command would produce output similar to the following:

```nohighlight
Success.
Machine-readable output follows:
{
    "rebuilding": "virtual"
}
```
[/tab]
[/tabs]

* How to troubleshoot MAAS
This article may help you deal with some common problems.  It is organised by topic:

- [Find and fix a leaked MAAS admin API key](#heading--Find-and-fix-a-leaked-MAAS-admin-API-key)
- [Networking issues](#heading--networking-issues)
- [Machine life-cycle failures](#heading--machine-life-cycle-failures)
- [Custom image creation problems](#heading--custom-image-creation-problems)

** Find and fix a leaked MAAS admin API key

MAAS hardware sync may leak the MAAS admin API key.  The simple solution for this is to:

- Rotate all admin tokens
- Re-deploy all machines that have hardware sync enabled

For users who don’t want to re-deploy, the following instructions explain how to manually swap the token.

*** Manually swapping the MAAS admin API token

Check if you have any machines with Hardware Sync enabled.  The easiest way to do this is a database query:

```nohighlight
select system_id 
from maasserver_node 
where enable_hw_sync = true;
```

On each of the reported machines there might be a leaked API key that belongs to the user with admin permissions. This will show only machines that do exist now. It is possible that such machines existed before, but were removed. We still do recommend you to rotate API keys.

Here, on one of the machines, we have a leaked API key `PMmKvCw26reY7SaDet:g5rY7FNDu2ZDKER5zL:pNAHKcpR7eLWA6g2RSxrqdgSXEKgTAMT`:

```nohighlight
cat /lib/systemd/system/maas_hardware_sync.service

[Unit]
Description=MAAS Hardware Sync Service
Documentation=<https://maas.io>
Wants=maas_hardware_sync.timer
After=network.target

[Service]
Type=oneshot
ExecStartPre=/usr/bin/wget -O /usr/bin/maas-run-scripts <http://10.100.0.10:5248/MAAS/maas-run-scripts>
ExecStartPre=/bin/chmod 0755 /usr/bin/maas-run-scripts
ExecStartPre=/usr/bin/maas-run-scripts get-machine-token\
    '<http://10.100.0.10:5248/MAAS'\>
    'PMmKvCw26reY7SaDet:g5rY7FNDu2ZDKER5zL:pNAHKcpR7eLWA6g2RSxrqdgSXEKgTAMT'\
    knt4rm\
    /tmp/maas-machine-creds.yml
ExecStart=/usr/bin/maas-run-scripts report-results --config /tmp/maas-machine-creds.yml

[Install]
WantedBy=multi-user.target
```

Just to ensure this token actually belongs to an admin account, we can do another database query:

```nohighlight
select u.username, u.email 
from auth_user u
left join piston3_consumer c 
on u.id = c.user_id
-- we need only the consumer key of the token. token.split(":")[0]
where key = 'PMmKvCw26reY7SaDet';
```

You should login into MAAS UI with an account owning a leaked API key, find a leaked API key and remove it. This is the most convinient way; it guarantees that all steps will be audited and all caches will be reset.  After API key is removed, MAAS CLI will stop working (if you were using the same token), so you will need to go through setting up the CLI credentials again.

The hardware sync feature will stop working as well.  Here are two options:

- Redeploy the machine, so it will use the new systemd template 
- Manually create a credentials file and modify `/lib/systemd/system/maas_hardware_sync.service` to match


** Networking issues

The following networking issues may be creating problems for you:

- [Adding overlapping subnets in fabric can break deployments](#heading--overlapping-subnets-can-break-deployments)
- [Need to reconfigure server IP address](#heading--need-to-reconfigure-server-ip-address)
- [Network boot an IBM Power server](#heading--ibm-power-server-pxe-boot)
- [Resolve MAAS/LXD DNS & DHCP conflicts/network issues](#heading--maas-lxd-network-conflicts)

Please feel free to add other issues and solutions, if you have them.

*** Adding overlapping subnets in fabric can break deployments

**Characteristic failure**: A machine performs PXE boot, then gets trapped in a boot loop, causing deployment to fail.

MAAS does not currently prevent you from creating overlapping subnets, for example:

- subnet 1 = 192.168.48.0/24
- subnet 2 = 192.168.48.0/22

This can break deployments, because the controllers can't reliably determine which subnet should get a packet destined for one of the overlapping addresses. The IP range of one subnet should be unique compared to every other subnet on the same segment.

At least one way to cause this error is to edit a subnet in the `netplan` file.  MAAS will add the updated subnet, but may not drop the existing subnet, causing overlap.  You can fix this by deleting the subnet you do not want from the Web UI.

If you have a machine that PXE boots, but then fails deployment, either in an infinite boot loop or some unspecified failure, check your subnets to be sure you do not have overlap.  If so, delete the outdated subnet.

*** Need to reconfigure server IP address

If you made a mistake during setup or you just need to reconfigure your MAAS server, you can simply run the setup again:

``` bash
sudo dpkg-reconfigure maas-region-controller
```

*** Network booting IBM Power servers

Some IBM Power server servers have OPAL firmware which uses an embedded Linux distribution as the boot environment. All the PXE interactions are handled by **Petitboot**, which runs in the user space of this embedded Linux rather than a PXE ROM on the NIC itself.

When no specific interface is assigned as the network boot device, petitboot has a known issue which is detailed in [LP#1852678](https://bugs.launchpad.net/ubuntu-power-systems/+bug/1852678)`↗`, specifically comment #24, that can cause issues when deploying systems using MAAS, since in this case all active NICs are used for PXE boot with the same address.

So, when using IBM Power servers with multiple NICs that can network boot, it's strongly recommended to configure just a single <specific> NIC as the network boot device via **Petitboot**.

*** Resolve DNS conflicts between LXD and MAAS

If you get into a situation where MAAS and LXD are both managing DNS on your MAAS network, there's a simple fix. You can turn off LXD's DNS management with the following command:

````bash
lxc network set $LXD_BRIDGE_NAME dns.mode=none
````

You should also disable DHCP on IPv4 and IPv6 withing LXD:

````bash
lxc network set $LXD_BRIDGE_NAME ipv4.dncp=false
lxc network set $LXD_BRIDGE_NAME ipv6.dhcp=false
````

Once you've done this, you can check your work with the following command:

````bash
lxc network show $LXD_BRIDGE_NAME
````

** Machine life-cycle failures

When attempting to run a machine through its life-cycle, you may have encountered one of these issues:

- [Nodes hang on "Commissioning"](#heading--nodes-hang-on-commissioning)
- [Node deployment fails](#heading--node-deployment-fails)
- [Nodes fail to PXE boot](#heading--nodes-fail-to-pxe-boot)
- [Can't log in to node](#heading--cant-log-in-to-node)
- [\"File not found\" when creating commissioning or node script with MAAS CLI](#heading--commissioning-script-file-not-found)
- [Can't login to machine after deployment](#heading--machine-login-issues)

Please feel free to add other issues and solutions, if you have them.

*** Nodes hang on "Commissioning"

**** Possible Cause: Timing issues

Various parts of MAAS rely on OAuth to negotiate a connection to nodes. If the current time reported by the hardware clock on your node differs significantly from that on the MAAS server, the connection will not be made.

**SOLUTION:** Check that the hardware clocks are consistent, and if necessary, adjust them. This can usually be done from within the system BIOS, without needing to install an OS.

**** Possible Cause: Network drivers

Sometimes the hardware can boot from PXE, but fail to load correct drivers when booting the received image. This is sometimes the case when no open source drivers are available for the network hardware.

**SOLUTION:** The best fix for this problem is to install a Linux-friendly network adaptor. It *is* theoretically possible to modify the boot image to include proprietary drivers, but it is not a straightforward task.

*** Node deployment fails

When deployment fails the [Rescue mode](/t/maas-glossary/5416#heading--rescue-mode) action can be used to boot ephemerally into the node, followed by an investigation.

As an example, an improperly configured PPA was added to MAAS which caused nodes to fail deployment. After entering Rescue mode and connecting via SSH, the following was discovered in file `/var/log/cloud-init-output.log`:

``` no-highlight
2016-11-28 18:21:48,982 - cc_apt_configure.py[ERROR]: failed to add apt GPG Key
to apt keyring
Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/cloudinit/config/cc_apt_configure.py",
line 540, in add_apt_key_raw
    util.subp(['apt-key', 'add', '-'], data=key.encode(), target=target)
  File "/usr/lib/python3/dist-packages/cloudinit/util.py", line 1836, in subp
    cmd=args)
cloudinit.util.ProcessExecutionError: Unexpected error while running command.
Command: ['apt-key', 'add', '-']
Exit code: 2
Reason: -
Stdout: ''
Stderr: 'gpg: no valid OpenPGP data found.\n'
```

In this instance, the GPG fingerprint was used instead of the GPG key. After rectifying this oversight, nodes were again able to successfully deploy.

*** Nodes fail to PXE boot

**** Possible Cause: Using an incorrectly</a> configured VM</h4>

Some virtual machine setups include emulation of network hardware that does not support PXE booting, and in most setups, you will need to explicitly set the VM to boot via PXE.

**SOLUTION**: Consult the VM docs for details on PXE booting.

**** Possible Cause: DHCP conflict

If you are using MAAS in a setup with an existing DHCP, *DO NOT SET UP THE MAAS DHCP SERVER* as this will cause no end of confusion to the rest of your network and most likely won't discover any nodes either.

**SOLUTION**: You will need to configure your existing DHCP server to point to the MAAS server.

*** Can't log in to node

Sometimes you may wish to log in directly to a node on your system. If you have set up Juju and MAAS, the node will automatically have SSH authentication enabled (and public keys installed) allowing you to log in. There is also an option in the MAAS web interface to add new SSH keys to the nodes (via Preferences in the drop down menu which appears when clicking your username in the top-right of the page).

*** \"File not found\" when creating commissioning or node script with MAAS CLI

When creating a commissioning script with the MAAS CLI, like this:

```nohighlight
maas $PROFILE commissioning-scripts create name=scriptname content@=/tmp/filename
```

you may receive a "file not found" error:

```nohighlight
[Errno 2] No such file or directory: '/tmp/filename'
```

There are two possible sources of the error:

- You did not actually type the filename correctly, or the file does not exist in `/tmp`.  Check the spelling and make sure the file is actually present in `/tmp` (for example).

- You are using the snap version of MAAS.  When using the MAAS snap, you may not use `/tmp` due to confinement rules.  Move the file to `/opt` or `/home/myhomdir` and try again.

In fact, trying to upload the script from any directory owned by `root` will give a similar error.

Also note that `commissioning-scripts` is deprecated and may be removed at some future time.  Use the form `node-scripts` instead; consult the MAAS CLI built-in help for details.

*** Can't login to machine after deployment

When everything seems to be right about your machine deployment, but you can't login, there's a chance you might not be using the right username.  You may have added your personal SSH key to MAAS, but your corresponding login doesn't seem to work; that's because the logins for the machines are generally related to the operating system, e.g.:

- For machines deploying Ubuntu, the username is `ubuntu`, and the login would be `ubuntu@$MACHINE_IP`.

- For machines deploying CentOS 7, the username is `centos`, and the login would be `centos@$MACHINE_IP`.

- For machines deploying CentOS 8, the username is `cloud-user`, and the login would be `cloud-user@$MACHINE_IP`.

Note there is a trick for determining the correct machine login, which works on many different versions of Linux.  If you attempt to `ssh root@$MACHINE_IP`, this will fail, but often tells you which user you should be using. 

** Custom image creation problems

You may have experienced these errors when trying to create custom images for MAAS:

- [Command 'packer' not found](#heading--packer-not-found)
- [No rule to make target ...OVMF_VARS.fd](#heading--no-rule-for-ovmf)
- [Failure to create QEMU driver](#heading--failed-creating-qemu-driver)

Please feel free to add other issues and solutions, if you have them.

*** Command 'packer' not found

You might attempt to run `packer` and receive the following error:

```nohighlight
stormrider@neuromancer:~$ packer
Command 'packer' not found, but can be installed with:
sudo snap install packer  # version 1.0.0-2, or
sudo apt  install packer  # version 1.6.6+ds1-4
See 'snap info packer' for additional versions.
```

More likely, you attempt a `make` and receive this error:

```nohighlight
stormrider@neuromancer:~/mnt/Dropbox/src/git/packer-maas/ubuntu$ make
sudo rm -f -rf output-qemu custom-ubuntu*.gz
cp -v /usr/share/OVMF/OVMF_VARS.fd OVMF_VARS.fd
'/usr/share/OVMF/OVMF_VARS.fd' -> 'OVMF_VARS.fd'
sudo PACKER_LOG=1 packer build ubuntu-lvm.json && reset
sudo: packer: command not found
make: *** [Makefile:21: custom-ubuntu-lvm.dd.gz] Error 1
rm OVMF_VARS.fd
```

In both cases, the problem is the same: `packer` has not been installed. You can fix it by [following these instructions](https://maas.io/docs/how-to-build-custom-images#heading--how-to-install-packer)`↗`.

*** No rule to make target ...OVMF_VARS.fd

If you encounter an error like this:

```nohighlight
stormrider@neuromancer:~/mnt/Dropbox/src/git/packer-maas/ubuntu$ make
sudo rm -f -rf output-qemu custom-ubuntu*.gz
make: *** No rule to make target '/usr/share/OVMF/OVMF_VARS.fd', needed by 'OVMF_VARS.fd'.  Stop.
```

then you have forgotten to [install a needed dependency](https://maas.io/docs/how-to-build-custom-images#heading--how-to-install-packer)`↗`.

*** Failure to create QEMU driver

If you encounter an error such as this one:

```nohighlight
2022/06/04 17:04:47 machine readable: error-count []string{"1"}
==> Some builds didn't complete successfully and had errors:
2022/06/04 17:04:47 machine readable: qemu,error []string{"Failed creating Qemu driver: exec: \"qemu-img\": executable file not found in $PATH"}
==> Builds finished but no artefacts were created.
Build 'qemu' errored after 880 microseconds: Failed creating Qemu driver: exec: "qemu-img": executable file not found in $PATH
```

then you have forgotten to [install a needed dependency](https://maas.io/docs/how-to-build-custom-images#heading--how-to-install-packer)`↗`.

** Miscellaneous issues

Finally, you may be facing an issue which doesn't fit into any category, such as one of these:

- [Subarchitecture error thrown by django](#heading--django-subarch-error)
- [Forgot MAAS administrator password](#heading--forgot-maas-administrator-password)
- [Can't find MAAS web UI](#heading--cant-find-maas-web-ui)
- [Backdoor image login](#heading--backdoor-image-login)
- [Migrate an existing snap installation to use a local PostgreSQL server](#heading--migrating-maas)
- [Manually export the MAAS database](#heading--manual-export)
- [Try jq recipes using the CLI](#heading--jq-machine-list)

Please feel free to add other issues and solutions, if you have them.

*** Subarchitecture error thrown by django

Occasionally, you may encounter an error similar to this one:

```
django.core.exceptions.ValidationError: ['Subarchitecture(<value>) must be generic when setting hwe_kernel.']
```

One potential solution for this problem is to specify a different commissioning kernel, such as upgrading from Xenial to Focal, etc.  

*** Forgot MAAS administrator password

As long as you have sudo privileges the `maas` command can be used to change the password for a MAAS administrator on the MAAS region controller:

``` bash
sudo maas changepassword $PROFILE
```

where $PROFILE is the name of the user.


*** Can't find MAAS web UI

By default, the web UI is located at `http://<hostname>:5240/MAAS/`. If you can't access it, there are a few things to try:

- Check that the web server is running - By default the web interface uses Apache, which runs under the service name *apache2*. To check it, on the MAAS server box you can run `sudo /etc/init.d/apache2 status`.
- Check that the hostname is correct - It may seem obvious, but check that the hostname is being resolved properly. Try running a browser (even a text mode one like `elinks`) on the same box as the MAAS server and navigating to the page. If that doesn't work, try `http://127.0.0.1:5240/MAAS/`, which will always point at the local server.
- If you are still getting "404 - Page not found" errors, check that the MAAS web interface has been installed in the right place. There should be a file present called `/usr/share/maas/maas/urls.py`.

*** Backdoor image login

Ephemeral images are used by MAAS to boot nodes during commissioning, as well as during deployment. By design, these images are not built to be edited or tampered with, instead they're used to probe the hardware and launch [cloud-init](https://launchpad.net/cloud-init)`↗`.

However, if you find yourself with no other way to access a node, especially if a node fails during commissioning, Linux-based ephemeral images can be modified to enable a *backdoor* that adds or resets a user's password. You can then login to check the **cloud-init** logs, for example, and troubleshoot the problem.

As images are constantly updated and refreshed, the backdoor will only ever be temporary, but it should help you login to see what may be going wrong with your node.

**** Extract the cloud image

First, download the cloud image that corresponds to the architecture of your node. The *Images* page of the web UI lists the images currently being cached by MAAS. Images can be downloaded from [https://cloud-images.ubuntu.com/](https://cloud-images.ubuntu.com/)`↗`.

For example:

``` bash
wget https://cloud-images.ubuntu.com/xenial/current/xenial-server-cloudimg-amd64-root.tar.gz
```

With the image downloaded, extract its contents so that the *shadow* password file can be edited:

``` bash
mkdir xenial
sudo tar -C xenial -xpSf xenial-server-cloudimg-amd64-root.tar.gz --numeric-owner --xattrs "--xattrs-include=*"
```

[note]
`sudo` is required when extracting the image filesystem and when making changes to the files extracted from the image filesystem.
[/note]

**** Generate password hash

Now generate a hashed password. Use the following Python 3 command, replacing **ubuntu** with the password you wish to use:

``` bash
python3 -c 'import crypt; print(crypt.crypt("ubuntu", crypt.mksalt(crypt.METHOD_SHA512)))'
```

Output from the previous command looks like the following:

``` no-highlight
$6$AaHblHl5KGrWBmPV$20ssynyY0EhcT9AwZgA2sTdYt4Bvd97bX7PjeyqVLKun2Hk3NBa8r7efM2duK7pi2dlnd5tG76I0dTUvjb6hx0
```

Open the `xenial/etc/shadow` file extracted from the image with a text editor and insert the password hash into the *root* user line of `etc/shadow`, between the first and second colons:

``` no-highlight
root:$6$AaHblHl5KGrWBmPV$20ssynyY0EhcT9AwZgA2sTdYt4Bvd97bX7PjeyqVLKun2Hk3NBa8r7efM2duK7pi2dlnd5tG76I0dTUvjb6hx0:17445:0:99999:7:::
```

Save the file and exit the text editor.

**** Rebuild SquashFS image

Recent versions of MAAS use SquashFS to hold the ephemeral image filesystem. The final step is to use the following command to create a SquashFS file called `xenial-customized.squashfs` that contains the modified shadow file:

``` bash
sudo mksquashfs xenial/ xenial-customized.squashfs -xattrs -comp xz
```

The output should look like the following:

``` no-highlight
Parallel mksquashfs: Using 2 processors
Creating 4.0 filesystem on xenial-customized.squashfs, block size 131072.
[=======]  2516/26975   9%
```

You now have an ephemeral image with a working root login that can replace an image locally cached by MAAS.

**** Use the custom image

Images are synchronised by the region controller and stored on the rack controller in `/var/lib/maas/boot-resources/`, with the *current* directory linking to the latest synchronised images.

For example, the latest low-latency Ubuntu 16.04 image can be found in the following directory:

``` bash
cd /var/lib/maas/boot-resources/current/ubuntu/amd64/ga-16.04-lowlatency/xenial/stable
```

To replace the original, substitute the *squashfs* file with the custom image generated earlier, making sure the new owner is *maas*:

``` bash
mv squashfs squashfs_original
cp /home/ubuntu/xenial-customized.squashfs .
chown maas:maas squashfs
```

You can now use this image to commission or deploy a node and access the root account with the backdoor password, such as by deploying the same specific image from the web UI to the node you wish to troubleshoot.

*** Migrating an existing snap installation

If you're currently running MAAS from a snap in `all` mode, you can easily migrate your database to a local PostgreSQL server with the following command:

    sudo /snap/maas/current/helpers/migrate-vd Snapatabase

This will install PostgreSQL from the archive and migrate the MAAS database to it. 

**Note** that if PostgreSQL is already installed on the machine, the script will move the current datadir out of the way and replace it with the one from the snap, after  confirmation with the user. If you want to keep the current database set and just import the MAAS database, you'll need to perform a manual dump/restore of the MAAS database, explained below.

The migration script will automatically adjust the snap configuration to use the new database.  Note, though, that the target database must be at least the same version level as the one currently used in the snap (PostgreSQL 10).  Consequently, the migration script only supports Ubuntu 18.04 (bionic) or later.

*** Manually exporting the MAAS database

If you want to export the database from the snap to an already setup PostgreSQL server, possibly on a different machine, you can manually export it from MAAS as follows. With MAAS running (as this ensures access to the database), run:

    export PGPASS=$(sudo awk -F':\\s+' '$1 == "database_pass" {print $2}' \
        /var/snap/maas/current/regiond.conf)
    sudo pg_dump -U maas -h /var/snap/maas/common/postgres/sockets \
        -d maasdb -F t -f maasdb-dump.tar

This will produce a binary dump in `maasdb-dump.tar`.  You can then stop the MAAS snap via

    sudo snap stop maas

Before importing it to the new database server, you need to create a user and database for MAAS:

``` nohighlight
sudo -u postgres \
    psql -c "CREATE USER maas WITH ENCRYPTED PASSWORD '<password>'"
sudo -u postgres createdb maasdb -O maas
```

Also, make sure that remote access is set up for the newly created `maas` user in `/etc/postgresql/10/main/pg_hba.conf`.  The file should contain a line similar to:

    host    maasdb  maas    0/0     md5

Be sure to replace `0/0`, above, with the proper CIDR to restrict access to a specific subnet.  Finally, you can import the database dump with:

    sudo -u postgres pg_restore -d maasdb maasdb-dump.tar

To finish the process, you'll need to update the MAAS snap config to:

- update the database config in `/var/snap/maas/current/regiond.conf` with the proper `database_host` and `database_pass`
- change the content of `/var/snap/maas/common/snap_mode` from `all` to `region+rack`

Using a local PostgreSQL server is a little bit of work, but it provides great benefits in terms of MAAS scalability and performance.


*** jq recipes using the CLI

Here are some `jq` recipes to get some human-readable output from the MAAS CLI.

**** Basic machine list

This recipe, which we keep in a file called `jqml.sh`, prints a basic machine list

    #!/bin/bash
    maas admin machines read | jq -r '(["HOSTNAME","SYSID","POWER","STATUS",
    "OWNER", "POOL", "VLAN","FABRIC","SUBNET"] | (., map(length*"-"))),
    (.[] | [.hostname, .system_id, .power_state, .status_name, .owner, .pool.name,
    .boot_interface.vlan.name, .boot_interface.vlan.fabric,
    .boot_interface.links[0].subnet.name]) | @tsv' | column -t

For this to work, you need to **only** break lines in the jq string ('...') or add backslashes if you break outside that boundary.

**** Machine list with first tag added

It's a good idea to keep your most important machine tag first, as it's the first one you'll see.  It makes scanning your list (UI or CLI/jq) much more efficient.  Here's a recipe that adds the first tag to the console-printed machine list.  We keep it in `jqmltag.sh`, but of course, you can call it whatever you want.

     #!/bin/bash
     maas admin machines read | jq -r '(["HOSTNAME","SYSID","POWER","STATUS",
     "OWNER", "TAGS", "POOL", "VLAN","FABRIC","SUBNET"] | (., map(length*"-"))),
     (.[] | [.hostname, .system_id, .power_state, .status_name, .owner // "-", 
     .tag_names[0] // "-", .pool.name,
     .boot_interface.vlan.name, .boot_interface.vlan.fabric,
     .boot_interface.links[0].subnet.name]) | @tsv' | column -t

* How to upgrade MAAS
[tabs]
[tab version="v3.3 Snap"]
[note]
PostgreSQL 12 is deprecated with the release of MAAS 3.3, in favour of PostgreSQL 14. Support for PostgreSQL 12 will be discontinued in MAAS 3.4.
[/note]

** How to upgrade a snap to MAAS 3.3

To upgrade from a earlier snap version to the 3.3 snap (using a `region+rack` configuration):

1. Enter the following command:

```nohighlight
sudo snap refresh --channel=3.3 maas
```

2. Enter your account password.

The snap will refresh from the 3.3 channel.  You will not need to re-initialise MAAS.

If you are using a multi-node maas deployment with separate regions and racks, you should first run the upgrade command above for rack nodes, then for region nodes.
[/tab]
[tab version="v3.3 Packages"] 
** How to upgrade MAAS 2.9++ to MAAS 3.3

If you are running MAAS 3.2 through MAAS 2.9, you can upgrade directly to MAAS 3.3 with the following procedure:

1. Check whether the target system is running Ubuntu 22.04 LTS:

```nohighlight
lsb_release -a
```

The response should look something like this:

```nohighlight
Distributor ID:	Ubuntu
Description:	Ubuntu xx.yy
Release:	xx.yy
Codename:	$RELEASE_NAME
```

The required “xx.yy” for MAAS 3.3 is “22.04,” code-named “Jammy”.

2. If you are currently running Ubuntu focal 20.04 LTS, Upgrade to Jammy 22.04 LTS:

```nohighlight
sudo do-release-upgrade --allow-third-party
```

3. Accept the defaults for any questions asked by the upgrade script.

4. Reboot the machine when requested.

5. Check whether the upgrade was successful:

```nohighlight
lsb_release -a
```

A successful upgrade should respond with output similar to the following:

```nohighlight
Distributor ID:	Ubuntu
Description:	Ubuntu 22.04(.nn) LTS
Release:	22.04
Codename:	jammy
```

** How to upgrade MAAS 2.8-- to MAAS 3.3

If you’re upgrading from MAAS version 2.8 or lower to version 3.3, try the fooling procedure.  While the this procedure should work, note that they it's untested. Use at your own risk. 

1. Back up your MAAS server completely with your favorite tools and media.

2. Add the MAAS 3.3 PPA to your repository list; ignore any apparent error messages:

```nohighlight
sudo apt-add-repository ppa:maas/3.3
```

3. Upgrade the release; answer any questions with the default values:

```nohighlight
sudo do-release-upgrade --allow-third-party
```

4. Check whether your upgrade has been successful:

```nohighlight
lsb_release -a
```

If the ugprade was successful, this command should yield output similar to the following:

```nohighlight
No LSB modules are available.
Distributor ID:	Ubuntu
Description:	Ubuntu 22.04(.nn) LTS
Release:	22.04
Codename:	jammy
```

5. Check your running MAAS install (by looking at the information on the bottom of the machine list) to make sure you’re running the 3.3 release.

If this didn’t work, you will need to restore from the backup you made in step 1, and consider obtaining separate hardware to install MAAS 3.3.
[/tab]
[tab version="v3.2 Snap"] 
** How to upgrade a snap to MAAS 3.2

To upgrade from a earlier snap version to the 3.2 snap (using a `region+rack` configuration):

1. Enter the following command:

```nohighlight
sudo snap refresh --channel=3.2 maas
```

2. Enter your account password.

The snap will refresh from the 3.2 channel; you will not need to re-initialise MAAS.

If you are using a multi-node maas deployment with separate regions and racks, you should first run the upgrade command above for rack nodes, then for region nodes.
[/tab]
[tab version="v3.2 Packages"] 
** How to ugprade MAAS 2.9++ to MAAS 3.2

To upgrade from MAAS 2.9 - 3.1 to MAAS 3.2, follow these steps:

1. Back up your MAAS server completely; the tools and media are left entirely to your discretion.

2. Add the MAAS 3.2 PPA to your repository list with the following command, ignoring any apparent error messages:

```
sudo apt-add-repository ppa:maas/3.2
```

3. Run the MAAS upgrade:

```
sudo apt update
sudo apt upgrade maas
```

4. Check your running MAAS install (by looking at the information on the bottom of the machine list) to make sure you're running the 3.2 release.

5. If this didn't work, you will need to restore from the backup you made in step 1, and consider obtaining separate hardware to install MAAS 3.2.

** How to upgrade MAAS 2.8-- to MAAS 3.2

If you are running MAAS 2.8 or lower, you can't upgrade directly to MAAS 3.2: 

1. Make sure that the target system is running Ubuntu 20.04 LTS or higher:

```
lsb_release -a
```

The response should look something like this:

```
Distributor ID:	Ubuntu
Description:	Ubuntu xx.yy
Release:	xx.yy
Codename:	$RELEASE_NAME
```

The minimum "xx.yy" required for MAAS 3.2 is "20.04," code-named "focal."

2. If you're not running focal, pgrade the release:

```
sudo do-release-upgrade --allow-third-party
```

3. Accept the defaults for any questions asked by the upgrade script.

4. Reboot the machine when requested.

5. Check whether the upgrade was successful:

```
lsb_release -a
```

A successful upgrade should respond with output similar to the following:

```
Distributor ID:	Ubuntu
Description:	Ubuntu 20.04(.nn) LTS
Release:	20.04
Codename:	focal
```
[/tab]
[tab version="v3.1 Snap"] 
** How to upgrade a snap to MAAS 3.1

To upgrade from a earlier snap version to the 3.1 snap (using a `region+rack` configuration):

1. Refresh the snap:

```nohighlight
sudo snap refresh --channel=3.1 maas
```
2. Enter your account password.

The snap will refresh from the 3.1 channel.  You will not need to re-initialise MAAS.

If you are using a multi-node maas deployment with separate regions and racks, you should first run the upgrade command above for rack nodes, then for region nodes.
[/tab]
[tab version="v3.1 Packages"] 
** How to ugprade MAAS 2.9++ to MAAS 3.1

You can upgrade from MAAS 2.9 or MAAS 3.0 to MAAS 3.1:

1. Back up your MAAS server completely; the tools and media are left entirely to your discretion.  Just be sure that you can definitely restore your previous configuration, should this procedure fail to work correctly.

2. Add the MAAS 3.1 PPA to your repository list with the following command, ignoring any apparent error messages:

```
sudo apt-add-repository ppa:maas/3.1
```

3. Run the MAAS upgrade like this:

```
sudo apt update
sudo apt upgrade maas
```

4. Check your running MAAS install (by looking at the information on the bottom of the machine list) to make sure you're running the 3.1 release.

5. If this didn't work, you will need to restore from the backup you made in step 1, and consider obtaining separate hardware to install MAAS 3.1.

** How to upgrade MAAS 2.8-- to MAAS 3.1

If you are running MAAS 2.8 or lower, you can also upgrade directly to MAAS 3.1, but it requires some extra steps.  You must first make sure that the target system is running Ubuntu 20.04 LTS or higher, by executing the following command:

```
lsb_release -a
```

The response should look something like this:

```
Distributor ID:	Ubuntu
Description:	Ubuntu xx.yy
Release:	xx.yy
Codename:	$RELEASE_NAME
```

The minimum "xx.yy" required for MAAS 3.0 is "20.04," code-named "focal."

If you are currently running Ubuntu bionic 18.04 LTS, you can upgrade to focal 20.04 LTS with the following procedure:

1. Upgrade the release:

```
sudo do-release-upgrade --allow-third-party
```

2. Accept the defaults for any questions asked by the upgrade script.

3. Reboot the machine when requested.

4. Check whether the upgrade was successful:

```
lsb_release -a
```

A successful upgrade should respond with output similar to the following:

```
Distributor ID:	Ubuntu
Description:	Ubuntu 20.04(.nn) LTS
Release:	20.04
Codename:	focal
```
[/tab]
[tab version="v3.0 Snap"] 
** How to upgrade a snap to MAAS 3.0

To upgrade from a earlier snap version to the 3.0 snap (using a `region+rack` configuration), do the following:

1. Refresh the snap:

```nohighlight
sudo snap refresh --channel=3.0 maas
```

2. Enter your user password.

The snap will refresh from the 3.0 channel.  You will not need to re-initialise MAAS.

If you are using a multi-node maas deployment with separate regions and racks, you should first run the upgrade command above for rack nodes, then for region nodes.
[/tab]
[tab version="v3.0 Packages"] 
** How to ugprade MAAS 2.9 to MAAS 3.0

To upgrade a working MAAS 2.9 instance to MAAS 3.0, follow these steps:

1. Back up your MAAS server completely; the tools and media are left entirely to your discretion.

2. Add the MAAS 3.0 PPA to your repository list:

```
sudo apt-add-repository ppa:maas/3.0
```

3. Run the MAAS upgrade:

```
sudo apt update
sudo apt upgrade maas
```

4. Check your running MAAS install (by looking at the information on the bottom of the machine list) to make sure you're running the 3.0 release.

5. If this didn't work, you will need to restore from the backup you made in step 1, and consider obtaining separate hardware to install MAAS 3.0.

** How to upgrade MAAS 2.8-- to MAAS 3.0

If you are running MAAS 2.8, you can upgrade directly to MAAS 3.0 with the following procedure:

1. Check your release:

```
lsb_release -a
```

The response should look something like this:

```
Distributor ID:	Ubuntu
Description:	Ubuntu xx.yy
Release:	xx.yy
Codename:	$RELEASE_NAME
```

The minimum "xx.yy" required for MAAS 3.0 is "20.04," code-named "focal."

If you are currently running Ubuntu bionic 18.04 LTS, you can upgrade to focal 20.04 LTS with the following procedure:

2. Upgrade the release:

```
sudo do-release-upgrade --allow-third-party
```

3. Accept the defaults for any questions asked by the upgrade script.

4. Reboot the machine when requested.

5. Check whether the upgrade was successful:

```
lsb_release -a
```

A successful upgrade should respond with output similar to the following:

```
Distributor ID:	Ubuntu
Description:	Ubuntu 20.04(.nn) LTS
Release:	20.04
Codename:	focal
```
[/tab]
[tab version="v2.9 Snap"] 
** How to upgrade a snap to MAAS 2.9

To upgrade from an earlier snap version to the 2.9 snap (using a `region+rack` configuration), do the following:

1. Refresh the snap:

```nohighlight
sudo snap refresh --channel=3.0/stable maas
```

2. Enter your user password.

The snap will refresh from the 3.0 channel.  You will not need to re-initialise MAAS.

If you are using a multi-node maas deployment with separate regions and racks, you should first run the upgrade command above for rack nodes, then for region nodes.
[/tab]
[tab version="v2.9 Packages"] 
** How to upgrade MAAS 2.8-- to MAAS 2.9

MAAS 2.8 is the last supported version for Ubuntu 18.04 LTS.  Newer versions of MAAS will not be back-portable, and consequently, to upgrade to MAAS 2.9 and all future versions, you will also need to upgrade the base operating system to Ubuntu 20.04.  

You do these two operations all at once, with the following procedure:

1. Add the 2.9 PPA to your repository path list:

```
sudo add-apt-repository ppa:maas/2.9
```

2. Run the release upgrade:

```
sudo do-release-upgrade --allow-third-party
```

3. Reboot your machine (requested by the upgrade script).

4. Check that your upgrade was successful:

```
lsb_release -a
```

If the ugprade was successful, this command should yield output similar to the following:

```
No LSB modules are available.
Distributor ID:	Ubuntu
Description:	Ubuntu 20.04.1 LTS
Release:	20.04
Codename:	focal
```

You have now upgraded to the Ubuntu 20.04 LTS base, and if you check your running MAAS install, you should see that the version has been updated to the latest stable 2.9 release.
[/tab]
[/tabs]
* How to use availability zones
This article will help you learn:

- [How to list availability zones](#heading--list-zones)
- [How to add an availability zone](#heading--add-a-zone)
- [How to edit an existing availability zone](#heading--edit-a-zone)
- [How to delete an existing availability zone](#heading--delete-a-zone)
- [How to assign a machine to an availability zone](#heading--assign-a-node-to-a-zone)
- [How to allocate a machine in a particular zone](#heading--allocate-a-node-in-a-zone)

You can find more theory about availability zones [elsewhere in this documentation set](/t/about-networking/6680#heading--about-availability-zones).

[tabs]
[tab version="v3.4 Snap,v3.4 Packages" view="UI"]
** How to list availability zones

To see a list of availability zones, select *AZs* from the top tab bar.

** How to add an availability zone

To create a zone:

1. Select *AZs*.

2. Select *Add AZ*.

3. Enter a *Name* for the zone.

4. Optionally enter a *Description* for the zone.

5. Select *Add AZ* to register your changes.

** How to edit an existing availability zone

To edit a zone:

1. Select *AZs*.

2. Select an AZ by clicking on its name.

3. Select *Edit* on the far right.

4. Update the *Name* for the zone, if desired.

5. Optionally update the *Description* for the zone, if desired.

6. Select *Update AZ* to register your changes.

** How to delete an existing availability zone

To delete a zone:

1. Select *AZs*.

2. Select an AZ by clicking on its name.

3. Select *Delete AZ* in the top right corner.

4. Update the *Name* for the zone, if desired.

5. Confirm by selecting the red *Delete AZ* button which appears.  Once you make this selection, the AZ will be deleted and no undo is possible.

** How to assign a machine to an availability zone

To assign a machine to a zone:

1. Select *Machines*.

2. Select one or more machines by clicking their checkboxes.

3. Select *Categorise >> Set zone**.

4. In the popup dialogue, choose the *Zone* from the dropdown.

5. Select *Set zone for machine* to register your changes.


** How to allocate a machine in a particular zone

Allocating a machine in a particular zone can only be done via the MAAS CLI.
[/tab]
[tab version="v3.3 Snap,v3.3 Packages,v3.2 Snap,v3.2 Packages,v3.1 Snap,v3.1 Packages,v3.0 Snap,v3.0 Packages,v2.9 Snap,v2.9 Packages" view="UI"]
** How to list availability zones

To see a list of availability zones, select *AZs* from the top tab bar.

** How to add an availability zone

To create a zone:

1. Select *AZs*.

2. Select *Add AZ*.

3. Enter a *Name* for the zone.

4. Optionally enter a *Description* for the zone.

5. Select *Add AZ* to register your changes.

** How to edit an existing availability zone

To edit a zone:

1. Select *AZs*.

2. Select an AZ by clicking on its name.

3. Select *Edit* on the far right.

4. Update the *Name* for the zone, if desired.

5. Optionally update the *Description* for the zone, if desired.

6. Select *Update AZ* to register your changes.

** How to delete an existing availability zone

To delete a zone:

1. Select *AZs*.

2. Select an AZ by clicking on its name.

3. Select *Delete AZ* in the top right corner.

4. Update the *Name* for the zone, if desired.

5. Confirm by selecting the red *Delete AZ* button which appears.  Once you make this selection, the AZ will be deleted and no undo is possible.

** How to assign a machine to an availability zone

To assign a machine to a zone:

1. Select *Machines*.

2. Select one or more machines by clicking their checkboxes.

3. Select *Take action >> Set zone**.

4. In the popup dialogue, choose the *Zone* from the dropdown.

5. Select *Set zone for machine* to register your changes.

You can also change the zone for a machine under *Machines >> "machine-name" >> Configuration >> Edit*.

** How to allocate a machine in a particular zone

Allocating a machine in a particular zone can only be done via the MAAS CLI.
[/tab]
[tab version="v3.4 Snap,v3.4 Packages,v3.3 Snap,v3.3 Packages,v3.2 Snap,v3.2 Packages,v3.1 Snap,v3.1 Packages,v3.0 Snap,v3.0 Packages,v2.9 Snap,v2.9 Packages" view="CLI"]

** How to list availability zones

To see a list of availability zones, enter the following command:

```
maas $PROFILE zones read \
| jq -r '(["ZONE","NAME","DESCRIPTION"]
| (., map(length*"-"))), (.[] | [.id, .name, .description])
| @tsv' | column -t
```

which produces output similar to:

```
ZONE  NAME         DESCRIPTION
----  ----         -----------
5     BizOffice
1     default
4     Inventory
2     Medications
3     Payroll
6     ProServ
```

** How to add an availability zone

To create a zone, enter the following command:

```
maas $PROFILE zones create name=$ZONE_NAME description=$ZONE_DESCRIPTION
```

** How to edit an existing availability zone

To edit a zone, enter a command similar to the following:

```
maas $PROFILE zone update $OLD_ZONE_NAME name=$NEW_ZONE_NAME \
description=$ZONE_DESCRIPTION
```

** How to delete an existing availability zone

To delete a zone, enter a command like this:

```
maas $PROFILE zone delete $ZONE_NAME
```

** How to assign a machine to an availability zone

To assign a machine to a zone, first retrieve the machine's system ID like this:

```
maas PROFILE machines read | jq '.[] | .hostname, .system_id'
```

Then enter the following command, using the system ID you just retrieved:

```
maas admin machine update $SYSTEM_ID zone=$ZONE_NAME
```

** How to deploy a machine in a particular zone

To deploy in a particular zone:

1. First acquire the machine, assigning it to the particular zone:

```nohighlight
maas $PROFILE machines allocate zone=$ZONE_NAME system_id=$SYSTEM_ID 
```

2. Then deploy the machine as normal:

```nohighlight
maas $PROFILE machine deploy system_id=$SYSTEM_ID
```
[/tab]
[/tabs]

* How to use controller tags
This article will show you:

[tabs]
[tab version="v3.4 Snap,v3.4 Packages,v3.3 Snap,v3.3 Packages,v3.2 Snap,v3.2 Packages,v3.1 Snap,v3.1 Packages,v3.0 Snap,v3.0 Packages,v2.9 Snap,v2.9 Packages" view="CLI"]

- [How to discover the ID of your region controller(s)](#heading--discover-the-id-of-your-region-controllers)
- [How to assign tags to a region controller](#heading--assign-tags-to-a-region-controller)
- [How to remove tags from a region controller](#heading--remove-tags-from-a-region-controller)
- [How to list tags for all region controllers](#heading--list-tags-for-all-region-controllers)
- [How to view tags for one region controller](#heading--view-tags-for-one-region-controller)
- [How to discover the ID of your rack controller(s)](#heading--discover-the-id-of-your-rack-controllers)
- [How to assign tags to a rack controller](#heading--assign-tags-to-a-rack-controller)
- [How to remove tags from a rack controller](#heading--remove-tags-from-a-rack-controller)
- [How to list tags for all rack controllers](#heading--list-tags-for-all-rack-controllers)
- [How to view tags for one rack controller](#heading--view-tags-for-one-rack-controller)

** How to discover the ID of your region controller(s)

You can discover the ID of your region controller(s) with the following command:

```nohighlight
maas $PROFILE region-controllers read \
| jq -r '(["name","id"]
|(.,map(length*"-"))),(.[]|[.hostname,.system_id])
| @tsv' | column -t
```

For example:

```nohighlight
maas admin region-controllers read \
| jq -r '(["name","id"]
|(.,map(length*"-"))),(.[]|[.hostname,.system_id])
| @tsv' | column -t
```

Typical output would look something like this:

```nohighlight
name                         id
----                         --
bill-Lenovo-Yoga-C740-15IML  86xya8
```

** How to assign tags to a region controller

To add tags to a region controller, you can use a command of this form:

```nohighlight
maas $PROFILE tag update-nodes $TAG_NAME add=$SYSTEM_ID
```

If you need to find the ID of your region controller(s), you can [look it up](#heading--discover-the-id-of-your-region-controllers).

For example:

```nohighlight
maas admin tag update-nodes virtual add=86xya8
```

This command produces output similar to the following:

```nohighlight
Success.
Machine-readable output follows:
{
    "added": 1,
    "removed": 0
}
```

You can check your work by [listing all tags for your region controllers](#heading--list-tags-for-all-region-controllers).

** How to remove tags from a region controller

To remove tags from a region controller, you can use a command like this:

```nohighlight
maas $PROFILE tag update-nodes $TAG_NAME remove=$SYSTEM_ID
```

If you need to find the ID of your region controller(s), you can [look it up](#heading--discover-the-id-of-your-region-controllers).

For example:

```nohighlight
maas admin tag update-nodes virtual remove=86xya8
```

This command produces output similar to the following:

```nohighlight
Success.
Machine-readable output follows:
{
    "added": 0,
    "removed": 1
}
```

You can check your work by [listing all tags for your region controllers](#heading--list-tags-for-all-region-controllers).

** How to list tags for all region controllers

To list tags for all region controllers, you can use a command similar to this:

```nohighlight
maas $PROFILE region-controllers read | jq -r '(["hostname","sysid","tags"]|(.,map(length*"-"))),(.[]|[.hostname,.system_id,.tag_names[]]) | @tsv' | column -t
```

For example:

```nohighlight
maas admin region-controllers read | jq -r '(["hostname","sysid","tags"]|(.,map(length*"-"))),(.[]|[.hostname,.system_id,.tag_names[]]) | @tsv' | column -t
```

This will produce output something like this:

```nohighlight
hostname                     sysid   tags
--------                     -----   ----
bill-Lenovo-Yoga-C740-15IML  86xya8  virtual  lxd-vm-host
```

** How to view tags for one region controller

To view tags for a specific region controller, you can try a command like this:

```nohighlight
maas $PROFILE region-controller read $SYSTEM_ID | jq -r '(["hostname","sysid","tags"]|(.,map(length*"-"))),([.hostname,.system_id,.tag_names[]]) | @tsv' | column -t
```

If you need to find the ID of your region controller(s), you can [look it up](#heading--discover-the-id-of-your-region-controllers).

For example:

```nohighlight
maas admin region-controller read 86xya8 \
| jq -r '(["hostname","sysid","tags"]
|(.,map(length*"-"))),([.hostname,.system_id,.tag_names[]])
| @tsv' | column -t
```

This should produce output similar to the following:

```nohighlight
hostname                     sysid   tags
--------                     -----   ----
bill-Lenovo-Yoga-C740-15IML  86xya8  virtual  lxd-vm-host
```



** How to discover the ID of your rack controller(s)

You can discover the ID of your rack controller(s) with the following command:

```nohighlight
maas $PROFILE rack-controllers read \
| jq -r '(["name","id"]
|(.,map(length*"-"))),(.[]|[.hostname,.system_id])
| @tsv' | column -t
```

For example:

```nohighlight
maas admin rack-controllers read \
| jq -r '(["name","id"]
|(.,map(length*"-"))),(.[]|[.hostname,.system_id])
| @tsv' | column -t
```

Typical output would look something like this:

```nohighlight
name                         id
----                         --
bill-Lenovo-Yoga-C740-15IML  86xya8
```

** How to assign tags to a rack controller

To add tags to a rack controller, you can use a command of this form:

```nohighlight
maas $PROFILE tag update-nodes $TAG_NAME add=$SYSTEM_ID
```

If you need to find the ID of your rack controller(s), you can [look it up](#heading--discover-the-id-of-your-rack-controllers).

For example:

```nohighlight
maas admin tag update-nodes virtual add=86xya8
```

This command produces output similar to the following:

```nohighlight
Success.
Machine-readable output follows:
{
    "added": 1,
    "removed": 0
}
```

You can check your work by [listing all tags for your rack controllers](#heading--list-tags-for-all-rack-controllers).

** How to remove tags from a rack controller

To remove tags from a rack controller, you can use a command like this:

```nohighlight
maas $PROFILE tag update-nodes $TAG_NAME remove=$SYSTEM_ID
```

If you need to find the ID of your rack controller(s), you can [look it up](#heading--discover-the-id-of-your-rack-controllers).

For example:

```nohighlight
maas admin tag update-nodes virtual remove=86xya8
```

This command produces output similar to the following:

```nohighlight
Success.
Machine-readable output follows:
{
    "added": 0,
    "removed": 1
}
```

You can check your work by [listing all tags for your rack controllers](#heading--list-tags-for-all-rack-controllers).

** How to list tags for all rack controllers

To list tags for all rack controllers, you can use a command similar to this:

```nohighlight
maas $PROFILE rack-controllers read | jq -r '(["hostname","sysid","tags"]|(.,map(length*"-"))),(.[]|[.hostname,.system_id,.tag_names[]]) | @tsv' | column -t
```

For example:

```nohighlight
maas admin rack-controllers read | jq -r '(["hostname","sysid","tags"]|(.,map(length*"-"))),(.[]|[.hostname,.system_id,.tag_names[]]) | @tsv' | column -t
```

This will produce output something like this:

```nohighlight
hostname                     sysid   tags
--------                     -----   ----
bill-Lenovo-Yoga-C740-15IML  86xya8  virtual  lxd-vm-host
```

** How to view tags for one rack controller

To view tags for a specific rack controller, you can try a command like this:

```nohighlight
maas $PROFILE rack-controller read $SYSTEM_ID | jq -r '(["hostname","sysid","tags"]|(.,map(length*"-"))),([.hostname,.system_id,.tag_names[]]) | @tsv' | column -t
'''

If you need to find the ID of your rack controller(s), you can [look it up](#heading--discover-the-id-of-your-rack-controllers).

For example:

```nohighlight
maas admin rack-controller read 86xya8 \
| jq -r '(["hostname","sysid","tags"]
|(.,map(length*"-"))),([.hostname,.system_id,.tag_names[]])
| @tsv' | column -t
```

This should produce output similar to the following:

```nohighlight
hostname                     sysid   tags
--------                     -----   ----
bill-Lenovo-Yoga-C740-15IML  86xya8  virtual  lxd-vm-host
```
[/tab]
[tab version="v3.4 Snap,v3.4 Packages,v3.3 Snap,v3.3 Packages,v3.2 Snap,v3.2 Packages,v3.1 Snap,v3.1 Packages,v3.0 Snap,v3.0 Packages,v2.9 Snap,v2.9 Packages" view="UI"]
- [How to create and assign controller tags ](#heading--create-and-assign-controller-tags-)
- [How to remove and delete controller tags](#heading--remove-and-delete-controller-tags)
- [How to view controller tags](#heading--view-controller-tags)

** How to create and assign controller tags 

To create and assign a controller tag:

1. Select *Controllers*.

2. Select the controller you wish to update.

3. Select *Configuration*.

4. Select *Controller configuration >> Edit*.

5. Select the *Tags* field.

6. Enter a new tag(s) to assign it to the controller.

7. Select *Save changes* to register your new tag(s).

** How to remove and delete controller tags

To remove (and possibly delete) a controller tag:

1. Select *Controllers*.

2. Select the controller you wish to update.

3. Select *Configuration*.

4. Select *Controller configuration >> Edit*.

5. Select the *Tags* field.

6. Select the *X* on the tag name to delete it.

7. Select *Save changes* to register your changes.

** How to view controller tags

1. Select *Controllers*.

2. Select the controller you wish to update.

3. Select *Configuration*.

4. View the tags for this controller under the *Tags* row.

4. Select *Cancel* to dismiss this screen.
[/tab]
[/tabs]

* How to use HashiCorp Vault with MAAS
[tabs]
[tab version="v3.4 Snap,v3.4 Packages,v3.3 Snap,v3.3 Packages"]
For MAAS to be able to integrate with Vault, a few steps are required.  Specifically, you must get a role_id and wrapped_token via Vault CLI (follow the instructions from [Hashicorp Vault](https://learn.hashicorp.com/tutorials/vault/approle-best-practices?in=vault/auth-methods#approle-response-wrapping)`↗`).

As an example only, MAAS can be configured by a Vault admin using the `vault` CLI.

1) The `approle` engine must be enabled. This can be checked with:

```nohighlight
$ vault auth list
```

You should verify that it's mounted under `approle/`.  If not, it can be enabled via:

```nohighlight
$ vault auth enable approle
```

2) The KV v2 engine is mounted under the desired path (`secret/` by default, but can be configured as desired). A new KV engine can be mounted with:

```nohighlight
$ vault secrets enable -path $SECRETS_MOUNT kv-v2
```

3) An appropriate policy that can be assigned to approles configured in MAAS should be configured, providing read/write access to the secrets paths that MAAS will use.  As an example, here is a minimal policy:

```nohighlight
path "$SECRETS_MOUNT/metadata/$SECRETS_PATH/" {
	capabilities = ["list"]
}

path "$SECRETS_MOUNT/metadata/$SECRETS_PATH/*" {
	capabilities = ["read", "update", "delete", "list"]
}

path "$SECRETS_MOUNT/data/${SECRETS_PATH}/*" {
	capabilities = ["read", "create", "update", "delete"]
}
```
Here, `$SECRETS_PATH` is the desired path prefix under which MAAS will store secrets. This, together with `$SECRETS_MOUNT`, will be used when configuring MAAS later. Such a policy can be created in Vault as follows (assuming it was written to `$POLICY_FILE`):

```nohighlight
$ vault policy write $MAAS_POLICY $POLICY_FILE
```

4) For each MAAS region controller, create a role using the policy created above:

```nohighlight
$ vault write auth/approle/role/$ROLE_NAME \
policies=$MAAS_POLICY token_ttl=5m
```

The TTL for tokens can be tweaked as desired.

While it's technically possible to use the same approle for all controllers, it's suggested to use different ones for each.  This increases security and reduces exposure in case credentials are leaked from one controller.

Fetch the role ID for the created role with the following command:

```nohighlight
$ vault read auth/approle/role/$ROLE_NAME/role-id
```

5) For each created role, create a secret ID, returned with a wrapping token. This, together with the role ID, will be provided to the MAAS controller:

```nohighlight
$ vault write -wrap-ttl=5m auth/approle/role/$ROLE_NAME/secret-id
```

** Integrating Vault with MAAS

Once MAAS is installed and configured, it's possible to integrate it with Vault with a few steps, using the CLI:

```nohighlight
sudo maas config-vault configure $URL $APPROLE_ID $WRAPPED_TOKEN $SECRETS_PATH --mount $SECRET_MOUNT
```
where the `$APPROLE_ID` and `$WRAPPED_TOKEN` are the ones obtained by Vault in the previous steps.

[note]
This operation must be performed on each region controller before the integration process can continue.
[/note]

Once this operation has been performed on all region controllers, it's possible to migrate secrets to Vault and complete the integration process, with the following command:

```nohighlight
$ sudo maas config-vault migrate
```
During migration, MAAS might be offline for a few seconds, but it will refresh quickly.  After this command, MAAS will be fully integrated and functional with Vault. You can confirm success in the UI by checking *Settings --> Configuration --> Security --> Secret Storage*.

[note] 
If you try to migrate secrets before all region controllers are configured with Vault, the migrate command will fail with an error message.
[/note]

If you've configured all region controllers with Vault, but haven't yet migrated the secrets, the integration process will simply remain incomplete.  The UI will remind you:

<a href="https://discourse.maas.io/uploads/default/original/2X/5/558b495841536f38600bbe67c4d4293a3e94bd0b.png" target = "_blank"><img src="https://discourse.maas.io/uploads/default/original/2X/5/558b495841536f38600bbe67c4d4293a3e94bd0b.png"></a>

** How to unseal Vault

There are two conditions that may cause Vault-protected secrets to become unavailable: when the Vault is sealed (using `vault operator seal` -- see the [Vault documentation](https://www.hashicorp.com/products/vault)`↗` and when the Vault is unreachable through misconfiguration or other failure.

When the Vault is sealed, all queries involving secrets will fail with a user error mentioning that the Vault has been sealed.  Unsealing the Vault requires operator intervention, via the `vault operator unseal` command (again, see the [Vault documentation](https://www.hashicorp.com/products/vault)`↗`.  MAAS will indicate when this is needed.

Vault may become unreachable due to a network failure, due to incorrect configuration of a region controller, or other unintentional situations.  When the Vault is unreachable, MAAS will inform the users that interactions with Vault will fail.

MAAS will make every attempt to present a meaningful error if Vault is not functional.  This includes related authentication errors when attempting to login to MAAS.
[/tab]
[tab version="v3.2 Snap,v3.2 Packages,v3.1 Snap,v3.1 Packages,v3.0 Snap,v3.0 Packages,v2.9 Snap,v2.9 Packages"]
[Hashicorp Vault](https://www.vaultproject.io/)`↗` is integrated with MAAS version 3.3.  To enable Vault for use with your MAAS, please upgrade to MAAS 3.3.
[/tab]
[/tabs]

* How to use images
Every deployed machine runs a MAAS-supplied image, whether standard or custom, downloaded from a stream or picked up from a local mirror.

** [Standard images](/t/how-to-use-standard-images/5124)

MAAS keeps a broad inventory of Ubuntu versions available for different architectures.

** [Custom images](/t/how-to-build-custom-images/5104)

You can build and deploy many other types of images, including CentOS, RHEL, Rocky, SLES, and even Windows.

** [Local image mirroring](/t/how-to-mirror-images-locally/5927)

Speed up image downloads by using a local image mirror.

** [VMWare images](/t/how-to-employ-vmware-images/5144)

MAAS also supports deploying VMWare images to appropriate virtual machine configurations.

** [RT kernel](/t/how-to-deploy-a-rt-kernel/6658)

Need a real-time kernel?  It's simple to deploy one with cloud-init.
* How to use machine tags
This article will help you learn:

[tabs]
[tab version="v3.4 Snap,v3.4 Packages" view="UI"]
- [How to create and assign machine tags to machines](#heading--create-and-assign-machine-tags-to-machines)
- [How to remove and delete machine tags from machines](#heading--remove-and-delete-machine-tags-from-machines)
- [How to list machine tags for multiple machines](#heading--list-machine-tags-for-multiple-machines)
- [How to view machine tags for one machine](#heading--view-machine-tags-for-one-machine)
- [How to view machine tags for a VM host](#heading--view-machine-tags-for-a-vm-host)

** How to create and assign machine tags to machines

If you want to create a new tag, and simultaneously assign it to one or more machines, use the following steps:

1. Select *Machines*.

2. Select the checkbox next to the machine(s) you want to tag.

3. Select *Categorise >> Tag*.

4. Create and/or assign the desired tag, as described in the [general tagging procedure](#heading--create-and-assign-tags) above.

5. Select *Save* to register your changes.

** How to remove and delete machine tags from machines

To remove machine tags from a machine:

1. Select *Machines*.

2. Select the machine in question by clicking on its name.

3. Check the *Tags* pane under *Machine summary* to confirm the tags applied to the machine.

4. Select *Edit*.

5. Follow the [general tag removal procedure](#heading--delete-and-remove-tags).

** How to list machine tags for multiple machines

In the MAAS UI, you don't explicitly list all machine tags; instead, you filter by them using the "Filters" drop-down:

1. Select *Machines*.

2. Select the *Filters* dropdown.

3. Select *Tags* in the dropdown.

4. Select one or more tags by clicking on the tag name.

The machine list will automatically filter by (be limited to) the machines matching the selected tag(s).

Remove a tag from the search filter by either by deselecting it in the *Tags* section.

** How to view machine tags for one machine

To view the tags assigned to a specific machine, use the following procedure:

1. Select *Machines*.

2. Select the machine of interest by clicking on its name.

3. Select *Configuration*.

4. Scroll down to the *Tags* section.

** How to view machine tags for a VM host

To view the machine tags assigned to a VM host, here's the procedure you'll follow:

1. Under *KVM*, select the type of VM host you're investigating.

2. Select a VM host by clicking on its name.

3. Select *KVM host settings*.

4. You can view, edit, add, or delete tags in the *Tags* box.

Note that you can only see the tags for a VM host in the same place that you change it.  For a more comprehensive list of VM host tags, use the MAAS CLI.

[/tab]
[tab version="v3.3 Snap,v3.3 Packages,v3.2 Snap,v3.2 Packages,v3.1 Snap,v3.1 Packages,v3.0 Snap,v3.0 Packages,v2.9 Snap,v2.9 Packages" view="UI"]
- [How to create and assign machine tags to machines](#heading--create-and-assign-machine-tags-to-machines)
- [How to remove and delete machine tags from machines](#heading--remove-and-delete-machine-tags-from-machines)
- [How to list machine tags for multiple machines](#heading--list-machine-tags-for-multiple-machines)
- [How to view machine tags for one machine](#heading--view-machine-tags-for-one-machine)
- [How to view machine tags for a VM host](#heading--view-machine-tags-for-a-vm-host)

** How to create and assign machine tags to machines

If you want to create a new tag, and simultaneously assign it to one or more machines, use the following steps:

1. Select *Machines*.

2. Select the checkbox next to the machine(s) you want to tag.

3. Select *Take action >> Tag*.

4. Create and/or assign the desired tag, as described in the [general tagging procedure](#heading--create-and-assign-tags) above.

5. Select *Tag machine* to register your changes.

** How to remove and delete machine tags from machines

To remove machine tags from a machine:

1. Select *Machines*.

2. Select the machine in question by clicking on its name.

3. Check the *Tags* pane under *Machine summary* to confirm the tags applied to the machine.

4. Select *Configuration >> Edit*.

5. Follow the [general tag removal procedure](#heading--delete-and-remove-tags).

** How to list machine tags for multiple machines

In the MAAS UI, you don't explicitly list all machine tags; instead, you filter by them using the "Filter by" drop-down.  This filtered list does not distinguish between virtual machines (VMs) and physical machines, unless you've assigned tags to help with that distinction.

Here's how you can filter the machine list by machine tags, using the MAAS UI:

- To list all tags, visit the 'Machines' tab and expand the 'Tags' subsection in the left pane. In this view, you can use tags as machine search filters.

- Select one or several tags. The machines that satisfy all selected tags will display on the right pane. Notice there is a search field at the top of the right pane. You can type a search expression into this field.

Below, tag 'virtual' has been selected (with the mouse), and the search field automatically reflects this. Five machines satisfy this search filter.

<a href="https://assets.ubuntu.com/v1/69aa9997-nodes-tags__2.6-tags-filter.png" target = "_blank"><img src="https://assets.ubuntu.com/v1/69aa9997-nodes-tags__2.6-tags-filter.png"></a>

Remove a tag from the search filter by either hitting the 'x' character alongside a tag or editing the search expression.

** How to view machine tags for one machine

To view the tags assigned to a specific machine, use the following procedure:

- On the machine list, select the machine of interest by clicking on its name.

- On the machine detail screen that comes up, look for the tags on one of the cards presented there: the tags for that machine should be listed there.

** How to view machine tags for a VM host

To view the machine tags assigned to a VM host, here's the procedure you'll follow:

1. Select *KVM*. 

2. Select a VM host by clicking on its name.

3. Select *KVM host settings*.

4. You can view, edit, add, or delete tags in the *Tags* box.

Note that you can only see the tags for a VM host in the same place that you change it.  For a more comprehensive list of VM host tags, use the MAAS CLI.

[/tab]
[tab version="v3.4 Snap,v3.4 Packages,v3.3 Snap,v3.3 Packages,v3.2 Snap,v3.2 Packages,v3.1 Snap,v3.1 Packages,v3.0 Snap,v3.0 Packages,v2.9 Snap,v2.9 Packages" view="CLI"]
- [How to assign machine tags to a machine](#heading--assign-machine-tags-to-a-machine)
- [How to remove machine tags from a machine](#heading--remove-machine-tags-from-a-machine)
- [How to list machine tags for all machines](#heading--list-machine-tags-for-all-machines)
- [How to view machine tags for one machine](#heading--view-machine-tags-for-one-machine)
- [How to discover your virtual machine host ID](#heading--discover-your-vm-host-id)
- [How to assign tags to a virtual machine host](#heading--assign-tags-to-a-vm-host)
- [How to remove tags from a virtual machine host](#heading--remove-tags-from-a-vm-host)
- [How to list tags for all virtual machine hosts](#heading--list-tags-for-all-vm-hosts)
- [How to view tags for one virtual machine host](#heading--view-tags-for-one-vm-host)

** How to assign machine tags to a machine

You can assign tags to a physical or virtual machine with the following command:

```nohighlight
maas $PROFILE tag update-nodes $TAG_NAME add=$SYSTEM_ID
```

For example:

```nohighlight
maas admin tag update-nodes new_tag add=g6arwg
```

This returns something like the following:

```nohighlight
Success.
Machine-readable output follows:
{
   "added": 1,
   "removed": 0
}
```

You can check your work by listing machine tags, like this:

```nohighlight
maas admin machines read | jq -r \
'(["hostname","sysid","machine_tags"]
|(.,map(length*"-"))),(.[]|[.hostname,.system_id,.tag_names[]])
| @tsv' | column -t
```

This should yield output similar to the following:

```nohighlight
hostname       sysid   machine_tags
--------       -----   ------------
divine-stork   8b3ypp  pod-console-logging  virtual
casual-prawn   4end6r  pod-console-logging  virtual
driven-teal    tgaat6  pod-console-logging  virtual
immune-beetle  43xand  pod-console-logging  virtual
good-osprey    napfxk  pod-console-logging  virtual
smart-hen      c4rwq7  pod-console-logging  virtual
boss-satyr     xn8taa  pod-console-logging  androko
golden-martin  8fxery  pod-console-logging  virtual
crack-guinea   qk4b3g  pod-console-logging  virtual
finer-leech    cy3dtr  pod-console-logging  virtual
free-mouse     gxtbq4  pod-console-logging  virtual
humble-bunny   srqnnb  pod-console-logging  virtual
wanted-muskox  ekw7fh  pod-console-logging  virtual
one-boa        by477d  pod-console-logging  virtual
great-urchin   srnx4g  pod-console-logging  virtual
ace-frog       g6arwg  pod-console-logging  virtual  barbar  farquar  new_tag
alive-marlin   gbwnfb  pod-console-logging  virtual
picked-parrot  am77wn  pod-console-logging  virtual
tough-kit      ke3wc7  pod-console-logging  virtual
legal-whale    8nq3mt  pod-console-logging  virtual
game-sponge    76pdc6  pod-console-logging  virtual
fun-ghoul      qxfm7k  pod-console-logging  virtual
aware-earwig   8m8hs7  pod-console-logging  virtual
chief-crane    7fapx7  pod-console-logging  virtual
select-tapir   4ascbr  pod-console-logging  virtual
on-slug        snfs8d  pod-console-logging  virtual
polite-llama   dbqd4m  pod-console-logging  virtual
frank-coyote   wcmk48  pod-console-logging  virtual
usable-condor  ed8hmy  pod-console-logging  virtual
still-imp      h6ra6d  pod-console-logging  virtual
```

** How to remove machine tags from a machine

You can remove a tag from a physical or virtual machine with this command:

```nohighlight
maas $PROFILE tag update-nodes $TAG_NAME remove=$SYSTEM_ID
```

For example:

```nohighlight
maas admin tag update-nodes new_tag remove=g6arwg
```

This would produce output similar to the following:

```nohighlight
Success.
Machine-readable output follows:
{
    "added": 0,
    "removed": 1
}
```

A quick check to verify results should yield something like this:

```nohighlight
hostname       sysid   machine_tags
--------       -----   ------------
ace-frog       g6arwg  pod-console-logging  virtual  barbar  farquar
```

*** Adding and removing machine tags simultaneously from multiple machines

You can simultaneously add and remove tags from multiple machines, as long as you are only modifying one tag, with a command like this one:

```nohighlight
maas $PROFILE tag update-nodes $TAG_NAME add=$SYSTEM_ID1 add=$SYSTEM_ID2 remove=$SYSTEM_ID3
```

For example, to remove the tag "barbar" from machine "g6arwg," but add it to machines "8fxery" and "by477d," you could use a command like this:

```nohighlight
maas admin tag update-nodes barbar add=8fxery add=by477d remove=g6arwg
```

This compound operation would yield a response similar to this:

```nohighlight
Success.
Machine-readable output follows:
{
    "added": 2,
    "removed": 1
}
```

Again, verifying by checking the list of machine tags, we enter a command like this:

```nohighlight
maas admin machines read | jq -r \
'(["hostname","sysid","machine_tags"]
|(.,map(length*"-"))),(.[]|[.hostname,.system_id,.tag_names[]])
| @tsv' | column -t
```

The resulting response looks something like this:

```nohighlight
hostname       sysid   machine_tags
--------       -----   ------------
divine-stork   8b3ypp  pod-console-logging  virtual
casual-prawn   4end6r  pod-console-logging  virtual
driven-teal    tgaat6  pod-console-logging  virtual
immune-beetle  43xand  pod-console-logging  virtual
good-osprey    napfxk  pod-console-logging  virtual
smart-hen      c4rwq7  pod-console-logging  virtual
boss-satyr     xn8taa  pod-console-logging  androko
golden-martin  8fxery  pod-console-logging  virtual  barbar
crack-guinea   qk4b3g  pod-console-logging  virtual
finer-leech    cy3dtr  pod-console-logging  virtual
free-mouse     gxtbq4  pod-console-logging  virtual
humble-bunny   srqnnb  pod-console-logging  virtual
wanted-muskox  ekw7fh  pod-console-logging  virtual
one-boa        by477d  pod-console-logging  virtual  barbar
great-urchin   srnx4g  pod-console-logging  virtual
ace-frog       g6arwg  pod-console-logging  virtual  farquar
alive-marlin   gbwnfb  pod-console-logging  virtual
picked-parrot  am77wn  pod-console-logging  virtual
tough-kit      ke3wc7  pod-console-logging  virtual
legal-whale    8nq3mt  pod-console-logging  virtual
game-sponge    76pdc6  pod-console-logging  virtual
fun-ghoul      qxfm7k  pod-console-logging  virtual
aware-earwig   8m8hs7  pod-console-logging  virtual
chief-crane    7fapx7  pod-console-logging  virtual
select-tapir   4ascbr  pod-console-logging  virtual
on-slug        snfs8d  pod-console-logging  virtual
polite-llama   dbqd4m  pod-console-logging  virtual
frank-coyote   wcmk48  pod-console-logging  virtual
usable-condor  ed8hmy  pod-console-logging  virtual
still-imp      h6ra6d  pod-console-logging  virtual
```

** How to list machine tags for all machines

To list machine tags for all physical and virtual machines, just enter a command similar to this one:

```nohighlight
maas $PROFILE machines read | jq -r '(["hostname","sysid","machine_tags"]|(.,map(length*"-"))),(.[]|[.hostname,.system_id,.tag_names[]]) | @tsv' | column -t
```

For example:

```nohighlight
maas admin machines read | jq -r \
'(["hostname","sysid","machine_tags"]
|(.,map(length*"-"))),(.[]|[.hostname,.system_id,.tag_names[]])
| @tsv' | column -t
```

This gives us a listing similar to this:

```nohighlight
hostname       sysid   machine_tags
--------       -----   ------------
divine-stork   8b3ypp  pod-console-logging  virtual
casual-prawn   4end6r  pod-console-logging  virtual
driven-teal    tgaat6  pod-console-logging  virtual
immune-beetle  43xand  pod-console-logging  virtual
good-osprey    napfxk  pod-console-logging  virtual
smart-hen      c4rwq7  pod-console-logging  virtual
boss-satyr     xn8taa  pod-console-logging  androko
golden-martin  8fxery  pod-console-logging  virtual  barbar
crack-guinea   qk4b3g  pod-console-logging  virtual
finer-leech    cy3dtr  pod-console-logging  virtual
free-mouse     gxtbq4  pod-console-logging  virtual
humble-bunny   srqnnb  pod-console-logging  virtual
wanted-muskox  ekw7fh  pod-console-logging  virtual
one-boa        by477d  pod-console-logging  virtual  barbar
great-urchin   srnx4g  pod-console-logging  virtual
ace-frog       g6arwg  pod-console-logging  virtual  farquar
alive-marlin   gbwnfb  pod-console-logging  virtual
picked-parrot  am77wn  pod-console-logging  virtual
tough-kit      ke3wc7  pod-console-logging  virtual
legal-whale    8nq3mt  pod-console-logging  virtual
game-sponge    76pdc6  pod-console-logging  virtual
fun-ghoul      qxfm7k  pod-console-logging  virtual
aware-earwig   8m8hs7  pod-console-logging  virtual
chief-crane    7fapx7  pod-console-logging  virtual
select-tapir   4ascbr  pod-console-logging  virtual
on-slug        snfs8d  pod-console-logging  virtual
polite-llama   dbqd4m  pod-console-logging  virtual
frank-coyote   wcmk48  pod-console-logging  virtual
usable-condor  ed8hmy  pod-console-logging  virtual
still-imp      h6ra6d  pod-console-logging  virtual
```

** How to view machine tags for one machine

To view tags for one physical or machine, you can enter a command like this:

```nohighlight
maas $PROFILE machine read $SYSTEM_ID | jq -r '(["hostname","sysid","machine_tags"]|(.,map(length*"-"))),([.hostname,.system_id,.tag_names[]]) | @tsv' | column -t
```

For example:

```nohighlight
maas admin machine read 8fxery | jq -r \
'(["hostname","sysid","machine_tags"]
|(.,map(length*"-"))),([.hostname,.system_id,.tag_names[]])
| @tsv' | column -t
```

Typical output from this command might look like this:

```nohighlight
hostname       sysid   machine_tags
--------       -----   ------------
golden-martin  8fxery  pod-console-logging  virtual  barbar
```

** How to discover your virtual machine host ID

If you don't know your VM host ID, you can discover it with this command:

```nohighlight
maas $PROFILE vmhosts read \
| jq -r '(["vm_host_name","id"]
|(.,map(length*"-"))),(.[]|[.name,.id])
| @tsv' | column -t
```

For example:

```nohighlight
maas admin vmhosts read \
| jq -r '(["vm_host_name","id"]
|(.,map(length*"-"))),(.[]|[.name,.id])
| @tsv' | column -t
```

This should produce output similar to the following:

```nohighlight
vm_host_name      id
------------      --
my-lxd-vm-host-1  1
```

** How to assign tags to a virtual machine host

To assign a tag to a virtual machine host, enter the following command:

```nohighlight
maas $PROFILE vmhost add-tag $VMHOST_ID	tag=$TAG_NAME
```

If you don't know the ID of your VM host, you can [look it up beforehand](#heading--discover-your-vm-host-id).

As an example of assigning a tag to a VM host:

```nohighlight
maas admin vmhost add-tag 1 tag=virtual
```

If it worked, this should return `Success`, followed by the JSON that describes the VM host. You can check your work by [listing all VM host tags](#heading--list-tags-for-all-vm-hosts).

** How to remove tags from a virtual machine host

To remove a tag from a virtual machine host, enter the following command:

```nohighlight
maas $PROFILE vmhost remove-tag $VMHOST_ID tag=$TAG_NAME
```

If you don't know the ID of your VM host, you can [look it up beforehand](#heading--discover-your-vm-host-id).

As an example of removing a tag from a VM host:

```nohighlight
maas admin vmhost remove-tag 1 tag=virtual
```

If it worked, this should return `Success`, followed by the JSON that describes the VM host. You can check your work by [listing all VM host tags](#heading--list-tags-for-all-vm-hosts).


** How to list tags for all virtual machine hosts

You can list tags for all VM hosts with the following command:

```nohighlight
maas $PROFILE vmhosts read | jq -r '(["vm_host_name","id","tags"]|(.,map(length*"-"))),(.[]|[.name,.id,.tags[]]) | @tsv' | column -t
```

For example:

```nohighlight
maas admin vmhosts read | jq -r '(["vm_host_name","id","tags"]|(.,map(length*"-"))),(.[]|[.name,.id,.tags[]]) | @tsv' | column -t
```

This should yield output similar to the following:

```nohighlight
vm_host_name      id  tags
------------      --  ----
my-lxd-vm-host-1  1   morkopongo  pod-console-logging  virtual
```

** How to view tags for one virtual machine host

If you want to list the tags for just one VM host, you can use a command like this one:

```nohighlight
maas $PROFILE vmhost read $VMHOST_ID \
| jq -r '(["name","id","tags"]
|(.,map(length*"-"))),([.name,.id,.tags[]])
| @tsv' | column -t
```
If you don't know the ID of your VM host, you can [look it up beforehand](#heading--discover-your-vm-host-id).

As an example of viewing tags for one VM host:

```nohighlight
maas admin vmhost read 1 | jq -r '("name","id","tags"]|(.,map(length*"-"))),([.name,.id,.tags[]]) | @tsv' @ column -t
```

Typical output might look something like this:

```nohighlight
name              id  tags
----              --  ----
my-lxd-vm-host-1  1   morkopongo  pod-console-logging
```
[/tab]
[/tabs]

* How to use network tags
This article will show you:

[tabs]
[tab version="v3.4 Snap,v3.4 Packages,v3.3 Snap,v3.3 Packages,v3.2 Snap,v3.2 Packages,v3.1 Snap,v3.1 Packages,v3.0 Snap,v3.0 Packages,v2.9 Snap,v2.9 Packages" view="UI"]
- [How to create and assign network interface tags](#heading--create-and-assign-network-interface-tags)
- [How to remove and delete network interface tags](#heading--remove-and-delete-network-interface-tags)
- [How to view network interface tags](#heading--view-network-interface-tags)

** How to create and assign network interface tags

To assign a tag to a network interface:

1. Select *Machines*.

2. Select the machine where the desired interface is connected.

3. Select *Network*.

4. Select *Edit physical* in the dropdown at the end of the row for the desired interface.

5. Select the *Tags* field.

6. Enter a tag name to add it.

7. Select *Save interface* to register your changes.

Note that different machines may have the same physical interface name, but different MAC addresses, so it's not typical that interface tags carry over from one machine to the next -- so auto complete menus will be sparse or non-existent most of the time for these tag types.

** How to remove and delete network interface tags

To remove a tag from a network interface:

1. Select *Machines*.

2. Select the machine where the desired interface is connected.

3. Select *Network*.

4. Select *Edit physical* in the dropdown at the end of the row for the desired interface.

5. Select the *Tags* field.

6. Select the *X* on a tag to remove it.

7. Select *Save interface* to register your changes.

Note that different machines may have the same physical interface name, but different MAC addresses, so it's not typical that interface tags carry over from one machine to the next -- so auto complete menus will be sparse or non-existent most of the time for these tag types.

** How to view network interface tags

To view the tags associated with a network interface:

1. Select *Machines*.

2. Select the machine where the desired interface is connected.

3. Select *Network*.

4. Select *Edit physical* in the dropdown at the end of the row for the desired interface.

5. View the assigned tags in the *Tags* field.

6. Select *Cancel* to dismiss this screen when you're done.

Note that different machines may have the same physical interface name, but different MAC addresses, so it's not typical that interface tags carry over from one machine to the next.
[/tab]
[tab version="v3.4 Snap,v3.4 Packages,v3.3 Snap,v3.3 Packages,v3.2 Snap,v3.2 Packages,v3.1 Snap,v3.1 Packages,v3.0 Snap,v3.0 Packages,v2.9 Snap,v2.9 Packages" view="CLI"]
- [How to discover the ID of your network interface](#heading--discover-the-id-of-your-network-interface)
- [How to assign tags to a network interface](#heading--assign-tags-to-a-network-interface)
- [How to remove tags from a network interface](#heading--remove-tags-from-a-network-interface)
- [How to list tags for all network interfaces](#heading--list-tags-for-all-network-interfaces)
- [How to view tags for one network interface](#heading--view-tags-for-one-network-interface)

** How to discover the ID of your network interface

You can use a command of the following form to identify the interfaces associated with a particular device on your MAAS:

```nohighlight
maas $PROFILE interfaces read $SYSTEM_ID \
| jq -r '(["mac_address","type","id","tags"]
|(.,map(length*"-"))),(.[]|[.mac_address,.type,.id,.tags[]])
|@tsv'| column -t
```

For example:

```nohighlight
maas admin interfaces read xn8taa \
| jq -r '(["mac_address","type","id","tags"]
|(.,map(length*"-"))),(.[]|[.mac_address,.type,.id,.tags[]])
|@tsv'| column -t
```

This would produce output similar to the following:

```nohighlight
mac_address        type      id  tags
-----------        ----      --  ----
00:16:3e:18:7f:ee  physical  9   andrpko  plinko  cochise
```

** How to assign tags to a network interface

To assign a tag to a network interface, using both the device system ID and the interface ID, use a command of the following form:

```nohighlight
maas $PROFILE interface add-tag $SYSTEM_ID $INTERFACE_ID tag=$TAG_NAME
```

For example:

```nohighlight
maas admin interface add-tag xn8taa 9 tag=farquar
```

This command, if successful, will produce a long sequence of JSON describing the interface, including the changes introduced by the command above.  You can also check your work by [listing the tags](#heading--list-tags-for-all-network-interfaces) associated with the device.

** How to remove tags from a network interface

To remove a tag from a network interface, use both the device system ID and the interface ID in a command similar to this one:

```nohighlight
maas $PROFILE interface remove-tag $SYSTEM_ID $INTERFACE_ID tag=$TAG_NAME
```

For example:

```nohighlight
maas admin interface remove-tag xn8taa 9 tag=farquar
```

This command, if successful, will produce a long sequence of JSON describing the interface, including the changes introduced by the command above.  You can also check your work by [listing the tags](#heading--list-tags-for-all-network-interfaces) associated with the device.

** How to list tags for all network interfaces

To list all the tags for a given network interface on a given device, use a command like this one:

```nohighlight
maas $PROFILE interfaces read $SYSTEM_ID \
| jq -r '(["mac_address","type","id","tags"]
|(.,map(length*"-"))),(.[]|[.mac_address,.type,.id,.tags[]])
|@tsv'| column -t
```

For example:

```nohighlight
maas admin interfaces read xn8taa \
| jq -r '(["mac_address","type","id","tags"]
|(.,map(length*"-"))),(.[]|[.mac_address,.type,.id,.tags[]])
|@tsv'| column -t
```

This would produce output similar to the following:

```nohighlight
mac_address        type      id  tags
-----------        ----      --  ----
00:16:3e:18:7f:ee  physical  9   andrpko  plinko  cochise  farquar
```

** How to view tags for one network interface

To view tags for one particular network interface on a specific device, try a command formulated like this:

```nohighlight
maas $PROFILE interface read $SYSTEM_ID $INTERFACE_ID \
| jq -r '(["mac_address","type","id","tags"]
|(.,map(length*"-"))),([.mac_address,.type,.id,.tags[]])
|@tsv'| column -t
```

For example:

```nohighlight
maas admin interface read xn8taa 9 \
| jq -r '(["mac_address","type","id","tags"]
|(.,map(length*"-"))),([.mac_address,.type,.id,.tags[]])
|@tsv'| column -t
```

Typical output might look like this:

```nohighlight
mac_address        type      id  tags
-----------        ----      --  ----
00:16:3e:18:7f:ee  physical  9   andrpko  plinko  cochise
```
[/tab]
[/tabs]

* How to use standard images
MAAS is only useful once it has images available to provision its nodes. Therefore, one key post-install task is to select and import images from the boot source. Once MAAS imports images, it will update them on an hourly basis, via a default sync mechanism.  This page explains how to select and import the images that MAAS requires to provision its nodes.

[tabs]
[tab version="v3.4 Snap,v3.4 Packages,v3.3 Snap,v3.3 Packages,v3.2 Snap,v3.2 Packages,v3.1 Snap,v3.1 Packages,v3.0 Snap,v3.0 Packages,v2.9 Snap,v2.9 Packages" view="UI"]

This article will help you learn:

- [How to use image streams](#heading--maas-image-streams)
- [How to import standard images from maas.io](#heading--import-maasio-image-ui)
- [How to use other image mirrors to download images](#heading--image-mirrors)
- [How to import and provision non-Ubuntu images](#heading--other-images)

You can also [mirror images locally](/t/how-to-mirror-images-locally/5927), build your own [custom images](/t/how-to-build-custom-images/5104), or even work with [VMWare images](/t/how-to-employ-vmware-images/5144).  And there's some [background info on images](/t/how-to-acquire-images/6192) available if you need it.

** How to use MAAS image streams

Canonical provides two SimpleStreams for MAAS images: candidate and stable. Both streams contain Ubuntu images, CentOS images, bootloaders extracted from the Ubuntu archive, and release notifications. Either stream can be used in any version of MAAS greater than 2.1 -- but not all images are supported in older versions.

*** How to change the stream with the UI

To switch to the candidate stream: 

1. Select *Images*.

2. Select *Change source*.

3. Select *Custom*.

4. Set the *URL* to `http://images.maas.io/ephemeral-v3/candidate`.

5. Select *Connect*.

MAAS uses the stable stream by default. To switch back to it, simply repeat the above procedure, but set the *URL* to `maas.io`.

*** How to change the stream with the CLI

To switch to a stream with the CLI, enter the following commands:

```
BOOT_SOURCE_ID=$(maas $PROFILE boot-sources read | jq '.[] | select(.url | contains("images.maas.io/ephemeral-v3")) | .id')
maas $PROFILE boot-source update $BOOT_SOURCE_ID url=$STREAM_URL
```
** How to import standard images from maas.io

The *Images* page shows what images and architectures have been selected and downloaded. By default, MAAS will automatically grab the most recent Ubuntu LTS releases (and amd64 architecture). 

You can tell MAAS to sync images hourly, at the region level, using a toggle switch in the top-right corner of the screen.  See [Boot image sources](/t/how-to-acquire-images/6192#boot-image-sources)) for more details. We highly recommended syncing images hourly. Syncing at the rack controller level (from regiond) occurs every 5 min and cannot be disabled.

To remove an image, simply un-select it and click *Save selection*.

** How to use other image mirrors to download images

You can also host Ubuntu images on a mirror. To use these mirrors:

1. Select *Images*.

2. Select *Change source*.

3. Select *Custom*.

4. Enter the mirror *URL*. 

5. Select *Connect* to bring the mirror online.

Advanced options, such as using a GPG key or keyring to validate the mirror path (snap installation location: /snap/maas/current/usr/share/keyrings/ubuntu-cloudimage-keyring.gpg), are revealed by Selecting *Show advanced options*.

Optionally, a local mirror can be set up as the boot source. MAAS will then use it instead of the standard internet-based server. Local mirroring significantly reduces the time required import images. See [Local image mirror](/t/how-to-mirror-images-locally/5927) for instructions.

** How to import and provision non-Ubuntu images

It is also possible to import and provision images other than Ubuntu. Images supported and provided by MAAS will appear in *Images >> Other Images*. These images can be imported and used just like the Ubuntu images above.
[/tab]
[tab version="v3.4 Snap,v3.4 Packages,v3.3 Snap,v3.3 Packages,v3.2 Snap,v3.2 Packages,v3.1 Snap,v3.1 Packages,v3.0 Snap,v3.0 Packages,v2.9 Snap,v2.9 Packages" view="CLI"]
You can tell MAAS to sync images hourly, at the region level.  See [Boot image sources](/t/how-to-acquire-images/6192#boot-image-sources)) for more details. We highly recommended syncing images hourly. Syncing at the rack controller level (from regiond) occurs every 5 min and cannot be disabled.

This article will help you learn:

- [How to get started with the MAAS CLI](/t/try-out-the-maas-cli/5236)
- [How to list boot sources](#heading--list-boot-sources)
- [How to select images](#heading--select-image)
- [How to list image selections](#heading--list-image-selections)
- [How to import newly-selected images](#heading--import-newly-selected-images)
- [How to list currently available images](#heading--list-currently-available-images)
- [How to delete a boot source](#heading--delete-a-boot-source)
- [How to edit a boot source](#heading--edit-a-boot-source)

You can also [mirror images locally](/t/how-to-mirror-images-locally/5927), build your own [custom images](/t/how-to-build-custom-images/5104), or even work with [VMWare images](/t/how-to-employ-vmware-images/5144).  And there's some [background info on images](/t/how-to-acquire-images/6192) available if you need it.

** How to list boot sources

If you want to download boot sources, i.e., the locations from which you may download images, try the following:

``` bash
maas $PROFILE boot-sources read
```

[note]
Although multiple boot sources may be listed, MAAS can only practically work with a single boot source.
[/note]

** How to select images

Use the `boot-source-selections` command to select images from a boot source. After selecting new images, you will need to [import](#heading--import-newly-selected-images) them.

``` bash
maas $PROFILE boot-source-selections create $SOURCE_ID \
    os="ubuntu" release="$SERIES" arches="$ARCH" \
    subarches="$KERNEL" labels="*"
```

For example, to select all kernels for 64-bit Trusty from a boot source with an id of '1':

``` bash
maas $PROFILE boot-source-selections create 1 \
    os="ubuntu" release="trusty" arches="amd64" \
    subarches="*" labels="*"
```

*** Hardware enablement (HWE)

For example, to get just the latest amd64 HWE kernel available for Trusty, which, at time of writing, is from Xenial:

``` bash
maas $PROFILE boot-source-selections create 1 \
    os="ubuntu" release="trusty" arches="amd64" \
    subarches="hwe-x" labels="*"
```

For Xenial kernels (and starting with MAAS 2.1), notation has changed. To select the latest amd64 HWE kernel available for Xenial:

``` bash
maas $PROFILE boot-source-selections create 1 \
    os="ubuntu" release="xenial" arches="amd64" \
    subarches="hwe-16.04" labels="*"
```

** How to list image selections

To list image selections for a boot source:

``` bash
maas $PROFILE boot-source-selections read $SOURCE_ID
```

** How to import newly-selected images

To import newly-selected images (boot resources):

``` bash
maas $PROFILE boot-resources import
```

Once newly-selected images are imported, a sync mechanism is enabled (by default) to keep them up to date. The refresh time interval is 60 minutes.

Available images resulting from this action are reflected in the web UI.

** How to list currently available images

To list currently available/imported images (boot resources):

``` bash
maas $PROFILE boot-resources read
```

** How to delete a boot source

To delete a boot source (the location from which you can download images): 

``` bash
maas $PROFILE boot-source delete $SOURCE_ID
```

If you delete the sole boot source, then the fields 'Sync URL' and 'Keyring Path' in the web UI will take on null values.

** How to edit a boot source

You can edit an existing boot source by changing the GPG keyring file ($KEYRING_FILE) and the location ($URL).

Update the boot source:

``` bash
maas $PROFILE boot-source update $SOURCE_ID \
    url=$URL keyring_filename=$KEYRING_FILE
```

At this time MAAS only supports a boot source containing official MAAS images. As a result, you can only edit a boot source if you have set up a mirror of its images. The location can change, but the keyring remains constant:

KEYRING_FILE=/usr/share/keyrings/ubuntu-cloudimage-keyring.gpg

** How to add a boot source

[note]
To avoid unnecessary complexity, you should probably delete any existing boot sources before adding a new one.
[/note]

Presented below are a couple of use cases for adding a boot source:

- Use a local image mirror (official images)
- If you deleted the default image, recreate it

The general syntax is:

``` bash
maas $PROFILE boot-sources create \
    url=$URL keyring_filename=$KEYRING_FILE
```

The output will include a new numeric ID that identifies the boot source ($SOURCE_ID).

Since MAAS can only practically work with a single boot source, so you will need to delete any existing sources. Note that the location (URL) is the only variable. The only supported keyring is:

KEYRING_FILE=/usr/share/keyrings/ubuntu-cloudimage-keyring.gpg

If you added a sole boot source, then the fields 'Sync URL' and 'Keyring Path' in the web UI will reflect its values.

*** How to use a local image mirror

Once the mirror is set up according to [Local image mirror](/t/how-to-mirror-images-locally/5927) it is just a matter of specifying the mirror location (URL). Since the images come from the default source, you should use the default keyring. If you are following the above mirror document, the variable values should be:

- URL=https://$MIRROR/maas/images/ephemeral-v3/stable/
- KEYRING_FILE=/usr/share/keyrings/ubuntu-cloudimage-keyring.gpg

Where $MIRROR is the mirror server's hostname or IP address.

*** How to recreate the default boot source

Recreate the default boot source if it was ever deleted using the following variable values:

- URL=https://images.maas.io/ephemeral-v3/stable/
- KEYRING_FILE=/usr/share/keyrings/ubuntu-cloudimage-keyring.gpg
[/tab]
[/tabs]

* How to use storage tags
This article explains:

[tabs]
[tab version="v3.4 Snap,v3.4 Packages,v3.3 Snap,v3.3 Packages,v3.2 Snap,v3.2 Packages,v3.1 Snap,v3.1 Packages,v3.0 Snap,v3.0 Packages,v2.9 Snap,v2.9 Packages" view="UI"]
- [How to create and assign block device tags](#heading--create-and-assign-block-device-tags)
- [How to remove and delete block device tags](#heading--remove-and-delete-block-device-tags)
- [How to list block device and partition tags](#heading--list-block-device-and-partition-tags)
- [How to view block device tags](#heading--view-block-device-tags)
- [How to view partition tags](#heading--view-partition-tags)

** How to create and assign block device tags

Remember: In order to create and assign tags to a block device, the device has to be in an "available" state, with no active partitions. 

To create and assign tags to block devices:

1. Select *Machines*.

2. Select the machine that has the block device you want to tag.

3. Select *Storage*.

4. Scroll to *Available disks and partitions*.

5. Select the *Edit...* option at the right end of the row for the block device you want to tag (the wording may vary).

6. Add *Tags* as desired.

7. Register your changes by selecting *Save*.

** How to remove and delete block device tags

Remember: In order to create and assign tags to a block device, the device has to be in an "available" state, with no active partitions. 

To create and assign tags to block devices:

1. Select *Machines*.

2. Select the machine that has the block device you want to tag.

3. Select *Storage*.

4. Scroll to *Available disks and partitions*.

5. Select the *Edit...* option at the right end of the row for the block device you want to tag (the wording may vary).

6. Remove *Tags* by selecting the *X* on the tag name.

7. Register your changes by selecting *Save*.


** How to list block device and partition tags

To see block device and partition tags in the UI, you can list all storage links by using the filter tool on the machine list.  Here's how:

1. Select *Machines*.

2. Select *Filters >> Storage tags* from the dropdown.

3. Select the tags you wish to filter against.  The screen will immediately narrow to match your selections.

4. Uncheck tags to return to the previous view.

** How to view block device tags

To view all tags associated with block devices on a given machine:

1. Select *Machines*.

2. Select the machine you want to examine.

3. Select *Storage*.

4. Scroll down to *Available disks and paritions*.

You can view the various storage tags in this table.

** How to view partition tags

To view all tags associated with partitions on a given machine:

1. Select *Machines*.

2. Select the machine you want to examine.

3. Select *Storage*.

4. Scroll down to *Available disks and paritions*.

You can view the various storage tags in this table.
[/tab]
[tab version="v3.4 Snap,v3.4 Packages,v3.3 Snap,v3.3 Packages,v3.2 Snap,v3.2 Packages,v3.1 Snap,v3.1 Packages,v3.0 Snap,v3.0 Packages,v2.9 Snap,v2.9 Packages" view="CLI"]

- [How to discover the ID of your block device](#heading--discover-the-id-of-your-block-device) 
- [How to assign tags to a block device](#heading--assign-tags-to-a-block-device)
- [How to remove tags from a block device](#heading--remove-tags-from-a-block-device)
- [How to list tags for all block devices](#heading--list-tags-for-all-block-devices)
- [How to view tags for one block device](#heading--view-tags-for-one-block-device)
- [How to discover the ID of your partition](#heading--discover-the-id-of-your-partition)
- [How to assign tags to a partition](#heading--assign-tags-to-a-partition)
- [How to remove tags from a partition](#heading--remove-tags-from-a-partition)
- [How to list tags for all partitions](#heading--list-tags-for-all-partitions)
- [How to view tags for one partition](#heading--view-tags-for-one-partition)

** How to discover the ID of your block device

Block devices do not exist apart from the physical or virtual machines to which they are attached.  Finding the ID of the block device that interests you requires starting with a particular machine, in a command form that looks like this:

```nohighlight
maas $PROFILE block-devices read $SYSTEM_ID \
| jq -r '(["system_id","block_device_id","path","avail_size"]
|(.,map(length*"-"))),(.[]|[.system_id,.id,.path,.available_size])
| @tsv' | column -t
```

For example:

```nohighlight
maas admin block-devices read qk4b3g \
| jq -r '(["system_id","block_device_id","path","avail_size"]
|(.,map(length*"-"))),(.[]|[.system_id,.id,.path,.available_size])
| @tsv' | column -t
```

This example would produce output that looks something like this:

```nohighlight
system_id  block_device_id  path                    avail_size
---------  ---------------  ----                    ----------
qk4b3g     10               /dev/disk/by-dname/sda  0
```

The `path` component is printed to help you confirm that you are choosing the right block device, when there are several present.  The `avail-size` column will tell you whether you can operate on that block device at all: If the available size is "0," for example, you can't set a block device tag on any part of that drive.  Instead, you'd want to see something like this:

```nohighlight
system_id  block_device_id  path                    avail_size
---------  ---------------  ----                    ----------
xn8taa     8                /dev/disk/by-dname/sda  1996488704
```

** How to assign tags to a block device

You can only assign tags to a block device that is available. You can find out whether the block device is available at all when you [discover its ID](#heading--discover-the-id-of-your-block-device).

To assign an existing tag to a block device, you would type a command formulated like this:

```nohighlight
maas $PROFILE block-device add-tag $SYSTEM_ID $BLOCK_DEVICE_ID tag=$TAG_NAME
```

If you're not sure about the ID of your block device, you can [look it up](#heading--discover-the-id-of-your-block-device).

For example:

```nohighlight
maas admin block-device add-tag xn8taa 8 tag=farquar
```

If this command succeeds, it will display `Success`, followed by a JSON sequence describing the new state of the block device.

Note that if you try to add a tag to a block device that is not available, that is, to a block device that is in use, you will get a result like this:

```nohighlight
Not Found
```

** How to remove tags from a block device

You can only remove tags from a block device that is available. You can find out whether the block device is available at all when you [discover its ID](#heading--discover-the-id-of-your-block-device).

To remove an assigned tag from a block device, you would type a command formulated like this:

```nohighlight
maas $PROFILE block-device remove-tag $SYSTEM_ID $BLOCK_DEVICE_ID tag=$TAG_NAME
```

If you're not sure about the ID of your block device, you can [look it up](#heading--discover-the-id-of-your-block-device).

For example:

```nohighlight
maas admin block-device remove-tag xn8taa 8 tag=farquar
```

If this command succeeds, it will display `Success`, followed by a JSON sequence describing the new state of the block device.

Note that if you try to remove a tag from a block device that is not available, that is, from a block device that is in use, you will get a result like this:

```nohighlight
Not Found
```

** How to list tags for all block devices

To list tags for all block devices associated with a physical or virtual machine, you can use a command of this form:

```nohighlight
maas $PROFILE block-devices read $SYSTEM_ID | jq -r '(["id","tags"]|(.,map(length*"-"))),(.[]|[.id,.tags[]]) | @tsv' | column -t
```

For example:

```nohighlight
maas admin block-devices read xn8taa | jq -r '(["id","tags"]|(.,map(length*"-"))),(.[]|[.id,.tags[]]) | @tsv' | column -t
```

This command would produce output similar to this:

```nohighlight
id  tags
--  ----
8   hello  ssd  trinkoplinko
```

** How to view tags for one block device

To view tags for one specific block device, you can enter a command like this:

```nohighlight
maas $PROFILE block-device read $SYSTEM_ID $BLOCK_DEVICE_ID | jq -r '(["id","tags"]|(.,map(length*"-"))),([.id,.tags[]]) | @tsv' | column -t
```

If you're not sure about the ID of your block device, you can [look it up](#heading--discover-the-id-of-your-block-device).

For example:

```nohighlight
maas admin block-device read xn8taa 8 | jq -r '(["id","tags"]|(.,map(length*"-"))),([.id,.tags[]]) | @tsv' | column -t
```

This command would produce output similar to this:

```nohighlight
id  tags
--  ----
8   hello  ssd  trinkoplinko
9   20gig  ssd
10  250Gs  ssd
```


** How to discover the ID of your partition

Partitions do not exist apart from the block devices on which they reside.  Finding the ID of the partition that interests you requires starting with a particular machine and block device, similar to this command:

```nohighlight
maas $PROFILE partitions read $SYSTEM_ID $BLOCK_DEVICE_ID \
| jq -r '(["system_id","block_dev_id","part_id","path"]
|(.,map(length*"-"))),(.[]|[.system_id,.device_id,.id,.path])
|@tsv' | column -t
```

For example:

```nohighlight
maas admin partitions read xn8taa 8 \
| jq -r '(["system_id","block_dev_id","part_id","path"]
|(.,map(length*"-"))),(.[]|[.system_id,.device_id,.id,.path])
|@tsv' | column -t
 ```

This example would produce output that looks something like this:

```nohighlight
system_id  block_dev_id  part_id  path
---------  ------------  -------  ----
xn8taa     8             67       /dev/disk/by-dname/sda-part1
```

The `path` component is printed to help you confirm that you are choosing the right partition, when there are several present.  

** How to assign tags to a partition

You can only assign tags to a partition that is available.  To assign an existing tag to a partition, you would type a command formulated like this:

```nohighlight
maas $PROFILE partition add-tag $SYSTEM_ID $BLOCK_DEVICE_ID $PARTITION_ID tag=$TAG_NAME
```

If you're not sure about the ID of your partition, you can [look it up](#heading--discover-the-id-of-your-partition).

For example:

```nohighlight
maas admin partition add-tag xn8taa 8 67 tag=farquar
```

If this command succeeds, it will display `Success`, followed by a JSON sequence describing the new state of the partition.

Note that if you try to add a tag to a partition that is not available, that is, to a partition that is in use, you will get a result like this:

```nohighlight
Not Found
```

** How to remove tags from a partition

You can only remove tags from a partition that is available.  To remove a existing tag from a partition, you would type a command formulated like this:

```nohighlight
maas $PROFILE partition remove-tag $SYSTEM_ID $BLOCK_DEVICE_ID $PARTITION_ID tag=$TAG_NAME
```

If you're not sure about the ID of your partition, you can [look it up](#heading--discover-the-id-of-your-partition).

For example:

```nohighlight
maas admin partition remove-tag xn8taa 8 67 tag=farquar
```

If this command succeeds, it will display `Success`, followed by a JSON sequence describing the new state of the partition.

Note that if you try to remove a tag from a partition that is not available, that is, from a partition that is in use, you will get a result like this:

```nohighlight
Not Found
```

On the other hand, if you try to remove a tag that is not assigned to the partition you've chosen, MAAS will simply return `Success`, followed by a JSON sequence describing the current state of the partition.

** How to list tags for all partitions

To list tags for all partitions of a particular block device, use a command like this one:

```nohighlight
maas $PROFILE partitions read $SYSTEM_ID $BLOCK_DEVICE_ID \
| jq -r '(["id","tags"]
|(.,map(length*"-"))),(.[]|[.id,.tags[]])
| @tsv' | column -t
```

For example:

```nohighlight
maas admin partitions read xn8taa 8 \
| jq -r '(["id","tags"]
|(.,map(length*"-"))),(.[]|[.id,.tags[]])
| @tsv' | column -t
```

A command like this should return output similar to the following:

```nohighlight
id  tags
--  ----
54  farquar swap opendisk
67  foobar  farquar
97  foobar
```

** How to view tags for one partition

To view tags for one partition, enter a command similar to this:

```nohighlight
maas $PROFILE partition read $SYSTEM_ID $BLOCK_DEVICE_ID $PARTITION_ID | jq -r '(["id","tags"]|(.,map(length*"-"))),([.id,.tags[]]) | @tsv' | column -t
```

If you're not sure about the ID of your partition, you can [look it up](#heading--discover-the-id-of-your-partition).

For example:

```nohighlight
maas admin partition read xn8taa 8 67 | jq -r '(["id","tags"]|(.,map(length*"-"))),([.id,.tags[]]) | @tsv' | column -t
```

This command should produce output similar to this:

```nohighlight
id  tags
--  ----
67  foobar  farquar
```

[/tab]
[/tabs]

* How to use the MAAS Discourse forum
Much of the interaction with the MAAS community takes place via [our Discourse forum](https://discourse.maas.io/).  This article will help you better navigate that forum.

** Discourse categories

Our Discourse forum is divided into a number of public categories:

- The [discourse](https://discourse.maas.io/c/discourse/3) category is a default category provided by the forum tool.  While it sometimes captures posts, it is largely unmonitored and may or may not get you a response within a reasonable timeframe.

- The [docs](https://discourse.maas.io/c/docs/5) category is where our documentation lives.  If you want to edit or comment on documentation, this is the place to do that.

- The [news](https://discourse.maas.io/c/news/7) category is generally confined release announcements and deprecation notices.

- The [users](https://discourse.maas.io/c/users/8) category is where all the action takes place.  This is where you can post (or answer) questions.  This category is monitored by the MAAS developer on duty each week.  We try very hard to minimize the number of posts waiting on answers from us, aiming for that value to be less than 5% of total posts.

- The [integration](https://discourse.maas.io/c/integration/11) category isn't routinely used much any more, so it's advisable to post these questions in "Users" instead.  We may get back to it someday, though.

- The [development](https://discourse.maas.io/c/devel/13) category is intended to be for roadmap and architecture discussions, and is also where we publish our Show and Tell articles.

- The [tutorials](https://discourse.maas.io/c/tutorials/16) category is a place to see various MAAS tutorials.  Some of these tutorials are available elsewhere (e.g., from the [MAAS documentation](https://maas.io/docs).  

- The [deprecations](https://discourse.maas.io/c/deprecations/17) category is strictly for notices and discussions about deprecated features.

- The [performance](https://discourse.maas.io/c/maas-performance/26) category is for notices and discussion about MAAS performance issues.

** User permissions and progression

As a first-time user, you may not immediately have permission to post or comment.  You may continue to review posts until you gain these permissions, or you may message one of the [admins](https://discourse.maas.io/about) by clicking on their picture and then choosing "Message" if you feel you need elevated permissions.

Also please do be sure to review our [FAQ](https://discourse.maas.io/faq) and [Terms of Service](https://ubuntu.com/legal).

** How to post and comment

It's easy to create new posts, and comment on existing posts.

**# How to create a new post

To create a new post:

1. Go to [our Discourse forum](https://discourse.maas.io/)

2. Select a category from the *all categories* dropdown at the top.

3. Select *+New topic* at the top right.

4. Enter a title where it says, *Type title,...*.

5. Tab down to *Type here.  Use markdown...*.

6. Enter your post.  You will need to type at least 20 characters, which is not difficult for most posts.

7. Select *Create topic* (or *Cancel* if you change your mind).

**# How to comment on an existing post

To comment on an existing post:

1. Open the post by clicking on its title in the digest page.  You should be brought to the bottom of the post.

2. Select *Reply*. 

3. Enter your comments where it says, *Type here...*.

4. Select *Reply* (or *Cancel* if you changed your mind).



* How to use virtual machines
MAAS supports several different kinds of virtual machines, with LXD as the preferred VM host.

** [Set up LXD](/t/how-to-set-up-lxd/5208)

LXD is our VM host of choice, so we offer detailed set-up instructions there.  You can also use libvirt and VMWare if you desire.

** [Manage VM hosts](/t/how-to-manage-vm-hosts/5140)

To use virtual machines, you must create and properly configure a VM host.

** [Manage virtual machines](/t/how-to-manage-virtual-machines/5148)

You can create and delete virtual machnines with the MAAS UI.  Using the CLI, you can also do more nuanced things with VMs.
* How to work with log files
MAAS has a robust logging capability, which presents several different views, including a number of node-specific log files and several syslogd-style text logs.  Each of these logs provides different information, or at the very least, different views of the same information.  MAAS gathers logging information about the various MAAS states and records both automated and user-driven actions.

This document will help you learn:

- [About the syslog logging path](#heading--path)
- [How to use a remote syslog server](#heading--using-a-remote-syslog-server)

It also links to reference material for:

- [Commissioning logs](/t/commissioning-log-reference/5248)
- [Test logs](/t/test-log-reference/5314)
- [Event logs](/t/event-log-reference/5252)
- [Audit event logs](/t/audit-event-logs-reference/5256)

The discussion of these logs and their contents can be very extensive, so each type of logging has its own documentation section, reachable from the left-hand menu -- or from the list of questions above.


** How to use a remote syslog server

[tabs]
[tab version="v3.4 Snap,v3.4 Packages" view="UI"]
To add a remote syslog server:

1. Select *Settings* in the left navigation panel.

2. Under *Network* in the *Settings* navigation panel, select *Syslog*.

3. Under *Remote syslog server to forward machine logs*, enter the IP or URL for your syslog server.
[/tab]
[tab version="v3.3 Snap,v3.3 Packages,v3.2 Snap,v3.2 Packages,v3.1 Snap,v3.1 Packages,v3.0 Snap,v3.0 Packages,v2.9 Snap,v2.9 Packages" view="UI"] 
To add a remote syslog server, click the Settings tab and then click the Network services tab. Scroll down to the Syslog section, where you can add a syslog URL or IP:

<a href="https://assets.ubuntu.com/v1/e139d4e9-installconfig-syslog__2.6-remote-syslog.png" target = "_blank"><img src="https://assets.ubuntu.com/v1/e139d4e9-installconfig-syslog__2.6-remote-syslog.png"></a>

Click the Save button to save your changes.
[/tab]
[tab version="v3.4 Snap,v3.4 Packages,v3.3 Snap,v3.3 Packages,v3.2 Snap,v3.2 Packages,v3.1 Snap,v3.1 Packages,v3.0 Snap,v3.0 Packages,v2.9 Snap,v2.9 Packages" view="CLI"]
To add or update a remote syslog server in your MAAS environment:

``` bash
maas $PROFILE maas set-config name="remote_syslog" value="$SYSLOG_FQDN"
```

For example, to set your syslog server to `192.168.100.11`:

``` bash
maas $PROFILE maas set-config name="remote_syslog" value=192.168.100.11
```

If you clear the `remote_syslog` value, MAAS will revert to the default behaviour, which is to send all syslog information to all MAAS region controllers.

For example:

``` bash
maas $PROFILE maas set-config name="remote_syslog" value=""
```

[note]
Note that MAAS controllers' syslogs are not forwarded to the external syslog server -- only machine syslog information is forwarded.
[/note]
[/tab]
[/tabs]

** Using the logs directly

By the way, if you're interested in reading the logs, and you're using snaps, you'll find what you need here:

- /var/snap/maas/common/log/maas.log
- /var/snap/maas/common/log/regiond.log
- /var/snap/maas/common/log/rackd.log
- /var/snap/maas/common/log/rsyslog/$MACHINE_NAME/$RELEVANT_DATE/messages

If you’re using packages, you’ll find the log files in these locations:

- /var/log/maas/maas.log
- /var/log/maas/regiond.log
- /var/log/maas/rackd.log
- /var/log/maas/rsyslog/$MACHINE_NAME/$RELEVANT_DATE/messages

These logs can be very large and hard to search, and the web UI does not separate events by type. For instance, commissioning a simple VM produces logging information like this:

```nohighlight
maas.log:2022-09-29T15:00:16.461402-05:00 neuromancer maas.interface: [info] eth0 (physical) on fun-zebra: IP address automatically unlinked: None:type=AUTO
maas.log:2022-09-29T15:00:16.553396-05:00 neuromancer maas.node: [info] fun-zebra: Status transition from NEW to COMMISSIONING
maas.log:2022-09-29T15:00:16.754265-05:00 neuromancer maas.power: [info] Changing power state (on) of node: fun-zebra (bk7mg8)
maas.log:2022-09-29T15:00:16.759676-05:00 neuromancer maas.node: [info] fun-zebra: Commissioning started
maas.log:2022-09-29T15:00:18.039441-05:00 neuromancer maas.power: [info] Changed power state (on) of node: fun-zebra (bk7mg8)
maas.log:2022-09-29T15:03:26.946507-05:00 neuromancer maas.node: [info] fun-zebra: Storage layout was set to flat.
maas.log:2022-09-29T15:04:07.795515-05:00 neuromancer maas.node: [info] fun-zebra: Status transition from COMMISSIONING to TESTING
maas.log:2022-09-29T15:04:17.288763-05:00 neuromancer maas.node: [info] fun-zebra: Status transition from TESTING to READY
regiond.log:2022-09-29 20:00:16 maasserver.models.signals.interfaces: [info] eth0 (physical) on fun-zebra: deleted IP addresses due to VLAN update (5001 -> 5002).
regiond.log:2022-09-29 20:00:32 maasserver.region_controller: [info] Reloaded DNS configuration; ip 10.103.114.192 connected to fun-zebra on eth0
regiond.log:2022-09-29 20:01:09 maasserver.rpc.leases: [info] Lease update: commit for 10.103.114.192 on 0:16:3e:a2:73:5c at 2022-09-29 20:01:09 (lease time: 600s) (hostname: fun-zebra)
regiond.log:2022-09-29 20:01:14 maasserver.region_controller: [info] Reloaded DNS configuration; ip 10.103.114.192 connected to fun-zebra on eth0
regiond.log:     * node fun-zebra renamed interface eth0 to enp5s0
regiond.log:     * ip 10.103.114.192 connected to fun-zebra on eth0
regiond.log:     * ip 10.103.114.192 disconnected from fun-zebra on eth0
```

Not all of this output is relevant, nor does it all trigger a recorded MAAS event.  Interpreting MAAS logs is a matter of practice with known events in a controlled environment.

*** MAAS CLI events query command

In fact, probably the best way to review events is via the CLI sub-command, `events query`. This sub-command can help you filter and summarise events.  Let's take a look at how this tool works.

*** Basic queries

MAAS events can be queried with the simple CLI command:

```nohighlight
maas $PROFILE events query
```

where `$PROFILE` is your login name for your MAAS CLI.  This command produces a very long JSON listing, something like this:

```nohighlight
Success.
Machine-readable output follows:
{
    "count": 100,
    "events": [
        {
            "username": "unknown",
            "node": "ebd7dc",
            "hostname": "new-name",
            "id": 588448,
            "level": "WARNING",
            "created": "Tue, 27 Sep. 2022 20:49:37",
            "type": "Failed to query node's BMC",
            "description": "Failed to login to virsh console."
        },
        {
            "username": "unknown",
            "node": "mm3tc8",
            "hostname": "fair-marten",
            "id": 588447,
            "level": "WARNING",
            "created": "Tue, 27 Sep. 2022 20:49:37",
            "type": "Failed to query node's BMC",
            "description": "Failed to login to virsh console."
        },
		[... goes on for 100 events, by default ...]
        {
            "username": "unknown",
            "node": "ebd7dc",
            "hostname": "new-name",
            "id": 588442,
            "level": "WARNING",
            "created": "Tue, 27 Sep. 2022 20:39:22",
            "type": "Failed to query node's BMC",
            "description": "Failed to login to virsh console."
        }
    ],
    "next_uri": "/MAAS/api/2.0/events/?op=query&limit=5&after=588448",
    "prev_uri": "/MAAS/api/2.0/events/?op=query&limit=5&before=588442"
}
```

These listings can be very long and very hard to read.  You'll also notice that this particular MAAS has over 500,000 events, so parsing these logs by hand is certainly not practical.  There are two things you should do to make events easier to interpret:

- use the `jq` command, with some invocations we'll give you, to make neat tables out of your event lists.

- use the various filters -- supplied as part of the `events query` command -- to limit your output.

Let's explore both of these things in turn.

*** Using jq with events

We offer a [more complete tutorial on jq](/t/using-jq-with-the-maas-cli/6027) in this documentation set, but for now, we can give you some invocations that will make events much easier to read.  Let's take our example command above and add some `jq` to it to make the output more readable:

```nohighlight
maas $PROFILE events query limit=20 \
| jq -r '(["USERNAME","NODE","HOSTNAME","LEVEL","DATE","TYPE","EVENT"] | 
(., map(length*"-"))),
(.events[] | [.username,.node,.hostname,.level,.created,.type,.description]) 
| @tsv' | column -t -s$'\t'
```

This might give us output something like this:

```nohighlight
USERNAME  NODE    HOSTNAME       LEVEL    DATE                        TYPE                        EVENT
--------  ----    --------       -----    ----                        ----                        -----
unknown   ebd7dc  new-name       WARNING  Thu, 10 Mar. 2022 21:59:52  Failed to query node's BMC  Failed to login to virsh console.
unknown   mm3tc8  fair-marten    WARNING  Thu, 10 Mar. 2022 21:59:52  Failed to query node's BMC  Failed to login to virsh console.
unknown   pbpncx  ruling-bobcat  WARNING  Thu, 10 Mar. 2022 21:59:22  Failed to query node's BMC  Pod pbpncx: Failed to connect to the LXD REST API.
unknown   ebd7dc  new-name       WARNING  Thu, 10 Mar. 2022 21:54:34  Failed to query node's BMC  Failed to login to virsh console.
unknown   mm3tc8  fair-marten    WARNING  Thu, 10 Mar. 2022 21:54:34  Failed to query node's BMC  Failed to login to virsh console.
unknown   pbpncx  ruling-bobcat  WARNING  Thu, 10 Mar. 2022 21:54:05  Failed to query node's BMC  Pod pbpncx: Failed to connect to the LXD REST API.
unknown   ebd7dc  new-name       WARNING  Thu, 10 Mar. 2022 21:49:21  Failed to query node's BMC  Failed to login to virsh console.
unknown   mm3tc8  fair-marten    WARNING  Thu, 10 Mar. 2022 21:49:19  Failed to query node's BMC  Failed to login to virsh console.
unknown   pbpncx  ruling-bobcat  WARNING  Thu, 10 Mar. 2022 21:48:49  Failed to query node's BMC  Pod pbpncx: Failed to connect to the LXD REST API.
unknown   mm3tc8  fair-marten    WARNING  Thu, 10 Mar. 2022 21:44:08  Failed to query node's BMC  Failed to login to virsh console.
admin     ebd7dc  new-name       AUDIT    Thu, 09 Jun. 2022 21:39:54  Node                        Tagging 'new-name'.
unknown   pbpncx  contr-105      ERROR    Rack import error           Unable to import boot images: ('Connection broken: IncompleteRead(4096 bytes read)', IncompleteRead(4096 bytes read))
unknown   mm3tc8  fair-marten    WARNING  Thu, 10 Mar. 2022 21:38:50  Failed to query node's BMC  Failed to login to virsh console.
unknown   ebd7dc  contr-105      DEBUG    Thu, 12 May. 2022 21:38:26  Rack import info            Starting rack boot image import
unknown   pbpncx  ruling-bobcat  WARNING  Thu, 10 Mar. 2022 21:38:21  Failed to query node's BMC  Pod pbpncx: Failed to connect to the LXD REST API.
admin     pbpncx  ruling-bobcat  AUDIT    Thu, 16 Jun. 2022 21:35:16  Node                        Started commissioning on 'ruling-bobcat'.
unknown   mm3tc8  fair-marten    WARNING  Thu, 10 Mar. 2022 21:33:44  Failed to query node's BMC  Failed to login to virsh console.
unknown   pbpncx  ruling-bobcat  WARNING  Thu, 10 Mar. 2022 21:33:16  Failed to query node's BMC  Pod pbpncx: Failed to connect to the LXD REST API.
unknown   knpge8  bolla          INFO     Thu, 10 Mar. 2022 20:21:41  Ready                         
unknown   pbpncx  ruling-bobcat  WARNING  Thu, 10 Mar. 2022 18:01:47  Failed to query node's BMC  <LXDAPIException instance at 0x7f0b53e21dc0 with str error:\n Traceback (most recent call last):\n  File "/snap/maas/19076/usr/lib/python3/dist-packages/twisted/python/reflect.py", line 448, in safe_str\n    return str(o)\n  File "/snap/maas/19076/usr/lib/python3/dist-packages/pylxd/exceptions.py", line 18, in __str__\n    if self.response.status_code == 200:  # Operation failure\nAttributeError: 'LXDAPIException' object has no attribute 'status_code'\n>
```

You'll notice, in this listing, we have a mix of event types and responses.  In one case, the log even recorded a code exception.  You can probably see from this listing that events can be very helpful in tracking behaviours and resolving issues with your MAAS instance.  Even limited to 20 records, though, this output is still hard to parse, so let's explore ways to filter this table.

*** Filter parameters

The `events query` command accepts several different filters, all of them optional:

- *hostname*: Only events relating to the node with the matching hostname will be returned. This can be specified multiple times to get events relating to more than one node.

- *mac_address*: Only nodes with matching MAC addresses will be returned. Note that MAC address is not part of the standard output, so you'd need to look it up elsewhere.

- *id*: Only nodes with matching system IDs will be returned.  This corresponds to the `node` parameter in the JSON listing, not the `id` parameter there, which is a serial event number.

- *zone*: Only nodes in the zone will be returned.  Note that zones are not part of the standard output, so you'd need to look these up elsewhere.

- *level*: The event level to capture.  You can choose from AUDIT, CRITICAL, DEBUG, ERROR, INFO, or WARNING.  The default is INFO.

- *limit*: Number of events to return. The default is 100, the maximum in one command is 1000.

- *before*: Defines an event id to start returning older events.  This is the "id" part of the JSON, not the system ID or "node".  Note that `before` and `after` cannot be used together, as the results are unpredictable.

- *after*: Defines an event id to start returning newer events.  This is the "id" part of the JSON, not the system ID or "node".  Note that `before` and `after` cannot be used together, as the results are unpredictable.

This list of filters gives us a few different ways to simplify the output.  Let's try some of these combinations on the sample data, above.

*** Hostname, system ID, and MAC address filters

We can limit the hostname to, say, "new-name" by entering the following:

```nohighlight
maas $PROFILE events query limit=5 hostname=new-name\
| jq -r '(["USERNAME","NODE","HOSTNAME","LEVEL","DATE","TYPE","EVENT"] | 
(., map(length*"-"))),
(.events[] | [.username,.node,.hostname,.level,.created,.type,.description]) 
| @tsv' | column -t -s$'\t'
```

This might give us the following output:

```nohighlight
USERNAME  NODE    HOSTNAME  LEVEL    DATE                        TYPE                        EVENT
--------  ----    --------  -----    ----                        ----                        -----
unknown   ebd7dc  new-name  WARNING  Tue, 27 Sep. 2022 21:41:37  Failed to query node's BMC  Failed to login to virsh console.
unknown   ebd7dc  new-name  WARNING  Tue, 27 Sep. 2022 21:36:22  Failed to query node's BMC  Failed to login to virsh console.
unknown   ebd7dc  new-name  WARNING  Tue, 27 Sep. 2022 21:31:22  Failed to query node's BMC  Failed to login to virsh console.
unknown   ebd7dc  new-name  WARNING  Tue, 27 Sep. 2022 21:26:07  Failed to query node's BMC  Failed to login to virsh console.
unknown   ebd7dc  new-name  WARNING  Tue, 27 Sep. 2022 21:21:07  Failed to query node's BMC  Failed to login to virsh console.
```

We would get similar results with this command, using the "id" filter:

```nohighlight
maas $PROFILE events query limit=5 id=ebd7dc\
| jq -r '(["USERNAME","NODE","HOSTNAME","LEVEL","DATE","TYPE","EVENT"] | 
(., map(length*"-"))),
(.events[] | [.username,.node,.hostname,.level,.created,.type,.description]) 
| @tsv' | column -t -s$'\t'
```

We can also get similar results by looking up this machine's MAC address (52:54:00:32:8b:ea) and filtering by that parameter instead:

```nohighlight
maas $PROFILE events query limit=5 mac_address=52:54:00:32:8b:ea\
| jq -r '(["USERNAME","NODE","HOSTNAME","LEVEL","DATE","TYPE","EVENT"] | 
(., map(length*"-"))),
(.events[] | [.username,.node,.hostname,.level,.created,.type,.description]) 
| @tsv' | column -t -s$'\t'
```

In this particular case, all three would yield identical outputs.

*** Zone filter

We can look up one of the zones (using the Web UI or other CLI commands), and formulate a filter like this:

```nohighlight
maas $PROFILE events query limit=5 zone=twilight\
| jq -r '(["USERNAME","NODE","HOSTNAME","LEVEL","DATE","TYPE","EVENT"] | 
(., map(length*"-"))),
(.events[] | [.username,.node,.hostname,.level,.created,.type,.description]) 
| @tsv' | column -t -s$'\t'
```

Note that this query yields slightly different records:

```nohighlight
USERNAME  NODE    HOSTNAME     LEVEL    DATE                        TYPE                        EVENT
--------  ----    --------     -----    ----                        ----                        -----
unknown   mm3tc8  fair-marten  WARNING  Tue, 27 Sep. 2022 21:52:07  Failed to query node's BMC  Failed to login to virsh console.
unknown   mm3tc8  fair-marten  WARNING  Tue, 27 Sep. 2022 21:46:52  Failed to query node's BMC  Failed to login to virsh console.
unknown   mm3tc8  fair-marten  WARNING  Tue, 27 Sep. 2022 21:41:37  Failed to query node's BMC  Failed to login to virsh console.
unknown   mm3tc8  fair-marten  WARNING  Tue, 27 Sep. 2022 21:36:22  Failed to query node's BMC  Failed to login to virsh console.
unknown   mm3tc8  fair-marten  WARNING  Tue, 27 Sep. 2022 21:31:22  Failed to query node's BMC  Failed to login to virsh console.
```

*** Level filter

We can choose to look at specific events that match a logging level.  For example, we can repeat this command with `level=AUDIT`:

```nohighlight
maas $PROFILE events query limit=5 level=AUDIT\
| jq -r '(["USERNAME","NODE","HOSTNAME","LEVEL","DATE","TYPE","EVENT"] | 
(., map(length*"-"))),
(.events[] | [.username,.node,.hostname,.level,.created,.type,.description]) 
| @tsv' | column -t -s$'\t'
```

This will yield dramatically different results:

```nohighlight
USERNAME  NODE    HOSTNAME       LEVEL  DATE                        TYPE  EVENT
--------  ----    --------       -----  ----                        ----  -----
admin     ebd7dc  new-name       AUDIT  Thu, 22 Sep. 2022 15:25:55  Node  Overrode failed testing on 'new-name'.
admin     87pnsc  solid-tick     AUDIT  Thu, 22 Sep. 2022 15:22:33  Node  Aborted 'commissioning' on 'solid-tick'.
admin     pbpncx  ruling-bobcat  AUDIT  Thu, 22 Sep. 2022 15:19:00  Node  Started commissioning on 'ruling-bobcat'.
admin     87pnsc  solid-tick     AUDIT  Thu, 22 Sep. 2022 15:18:59  Node  Started commissioning on 'solid-tick'.
zorkobob  mm3tc8  fair-marten    AUDIT  Wed, 21 Sep. 2022 14:21:25  Node  Tagging 'fair-marten'.
zorkobob  mm3tc8  fair-marten    AUDIT  Wed, 21 Sep. 2022 14:10:38  Node  Untagging 'fair-marten'.
```

In fact, there are several different levels associated with MAAS events:

- INFO: the default, used if no `level=` is specified; shows `INFO` and `ERROR` events.  A typical `INFO` event is "Ready", indicating that a machine has reached the "Ready" state.
- CRITICAL: critical MAAS failures; shows only `CRITICAL` events.  These events usually represent severe error conditions that should be immediately remedied.
- ERROR: MAAS errors; shows only `ERROR` events. Typical `ERROR` events include such things as power on/off failures, commissioning timeouts, and image import failures.
- WARNING: failures which may or may not affect MAAS performance; shows `WARNING` and `ERROR` events.  A typical warning event, for example, might include the inability to find and boot a machine.
- DEBUG: information which would help debug MAAS behaviour; shows `DEBUG` and `INFO` events.  Typical `DEBUG` events involve routine image import activities, for example.
- AUDIT: information which helps determine settings and user actions in MAAS; shows only `AUDIT` events.  They are [covered in more detail elsewhere](/t/understanding-maas-audit-events/6372).

*** Combining filters

We can combine the `level` parameter with the `zone` parameter:

```nohighlight
maas $PROFILE events query limit=5 level=AUDIT zone=twilight\
| jq -r '(["USERNAME","NODE","HOSTNAME","LEVEL","DATE","TYPE","EVENT"] | 
(., map(length*"-"))),
(.events[] | [.username,.node,.hostname,.level,.created,.type,.description]) 
| @tsv' | column -t -s$'\t'
```

This combination gives us a very different output:

```nohighlight
USERNAME     NODE    HOSTNAME     LEVEL  DATE                        TYPE  EVENT
--------     ----    --------     -----  ----                        ----  -----
bobslidell   7h3cw7  polong       AUDIT  Tue, 27 Sep. 2022 21:53:38  Node  Set the zone to 'twilight' on 'polong'.
theotherbob  8r6pw7  karura       AUDIT  Tue, 27 Sep. 2022 21:53:34  Node  Set the zone to 'twilight' on 'karura'.
miltwaddams  mm3tc8  fair-marten  AUDIT  Wed, 21 Sep. 2022 14:21:25  Node  Tagging 'fair-marten'.
mikebolton   mm3tc8  fair-marten  AUDIT  Wed, 21 Sep. 2022 14:10:38  Node  Untagging 'fair-marten'.
admin        8r6pw7  karura       AUDIT  Wed, 21 Sep. 2022 14:00:47  Node  Untagging 'karura'.
```

These various filters can be combined, and even repeated as necessary:

```nohighlight
$ maas $PROFILE events query level=AUDIT hostname=karura hostname=polong limit=5 \
| jq -r '(["USERNAME","NODE","HOSTNAME","LEVEL","DATE","TYPE","EVENT"] | 
(., map(length*"-"))),
(.events[] | [.username,.node,.hostname,.level,.created,.type,.description]) 
| @tsv' | column -t -s$'\t'
```

Again, this combination gives us a different view of the event data:

```nohighlight
USERNAME    NODE    HOSTNAME  LEVEL  DATE                        TYPE  EVENT
--------    ----    --------  -----  ----                        ----  -----
stormrider  7h3cw7  polong    AUDIT  Tue, 27 Sep. 2022 21:53:38  Node  Set the zone to 'twilight' on 'polong'.
stormrider  8r6pw7  karura    AUDIT  Tue, 27 Sep. 2022 21:53:34  Node  Set the zone to 'twilight' on 'karura'.
admin       8r6pw7  karura    AUDIT  Wed, 21 Sep. 2022 14:00:47  Node  Untagging 'karura'.
admin       8r6pw7  karura    AUDIT  Wed, 21 Sep. 2022 14:00:01  Node  Tagging 'karura'.
admin       8r6pw7  karura    AUDIT  Wed, 21 Sep. 2022 13:58:11  Node  Untagging 'karura'.
```

*** The limit filter

You can use the `limit` filter to restrict the number of records listed, as we have been doing in many of the examples above.  We can expand the last example to `limit=7`, for instance:

```nohighlight
$ maas $PROFILE events query level=AUDIT hostname=karura hostname=polong limit=7 \
| jq -r '(["USERNAME","NODE","HOSTNAME","LEVEL","DATE","TYPE","EVENT"] | 
(., map(length*"-"))),
(.events[] | [.username,.node,.hostname,.level,.created,.type,.description]) 
| @tsv' | column -t -s$'\t'
```

This gives us a slightly longer list:

```nohighlight
USERNAME    NODE    HOSTNAME  LEVEL  DATE                        TYPE  EVENT
--------    ----    --------  -----  ----                        ----  -----
stormrider  7h3cw7  polong    AUDIT  Tue, 27 Sep. 2022 21:53:38  Node  Set the zone to 'twilight' on 'polong'.
stormrider  8r6pw7  karura    AUDIT  Tue, 27 Sep. 2022 21:53:34  Node  Set the zone to 'twilight' on 'karura'.
admin       8r6pw7  karura    AUDIT  Wed, 21 Sep. 2022 14:00:47  Node  Untagging 'karura'.
admin       8r6pw7  karura    AUDIT  Wed, 21 Sep. 2022 14:00:01  Node  Tagging 'karura'.
admin       8r6pw7  karura    AUDIT  Wed, 21 Sep. 2022 13:58:11  Node  Untagging 'karura'.
admin       8r6pw7  karura    AUDIT  Wed, 21 Sep. 2022 13:57:48  Node  Tagging 'karura'.
admin       7h3cw7  polong    AUDIT  Tue, 13 Sep. 2022 14:14:24  Node  Powered on 'polong'.
```

*** The before and after filters

Let's suppose that we want to repeat the query in the last example, but we want to start from the beginning of the event log (whenever that might have been).  We could modify the above command to something like this:

```nohighlight
$ maas $PROFILE events query level=AUDIT hostname=karura hostname=polong after=0 limit=7 \
| jq -r '(["USERNAME","NODE","HOSTNAME","LEVEL","DATE","TYPE","EVENT"] | 
(., map(length*"-"))),
(.events[] | [.username,.node,.hostname,.level,.created,.type,.description]) 
| @tsv' | column -t -s$'\t'
```

This would give us a different view:

```nohighlight
USERNAME  NODE    HOSTNAME  LEVEL  DATE                        TYPE  EVENT
--------  ----    --------  -----  ----                        ----  -----
ed        8r6pw7  karura    AUDIT  Tue, 12 Jul. 2022 15:08:27  Node  Powered on 'karura'.
ed        8r6pw7  karura    AUDIT  Tue, 12 Jul. 2022 15:08:23  Node  Powered on 'karura'.
ed        8r6pw7  karura    AUDIT  Tue, 12 Jul. 2022 15:08:08  Node  Powered off 'karura'.
admin     8r6pw7  karura    AUDIT  Thu, 23 Jun. 2022 23:26:53  Node  Set the zone to 'asd' on 'karura'.
peter     8r6pw7  karura    AUDIT  Fri, 22 Apr. 2022 08:06:59  Node  Powered off 'karura'.
peter     8r6pw7  karura    AUDIT  Fri, 22 Apr. 2022 07:09:55  Node  Powered on 'karura'.
ed        7h3cw7  polong    AUDIT  Thu, 27 Jan. 2022 14:34:34  Node  Powered on 'polong'.
```

We could also retrieve very recent records using "before":

```nohighlight
$ maas $PROFILE events query level=AUDIT before=500000 limit=7 \
| jq -r '(["USERNAME","NODE","HOSTNAME","LEVEL","DATE","TYPE","EVENT"] | 
(., map(length*"-"))),
(.events[] | [.username,.node,.hostname,.level,.created,.type,.description]) 
| @tsv' | column -t -s$'\t'
```

This would return:

```nohighlight
USERNAME  NODE    HOSTNAME     LEVEL  DATE                        TYPE  EVENT
--------  ----    --------     -----  ----                        ----  -----
peter     8r6pw7  karura       AUDIT  Fri, 22 Apr. 2022 08:06:59  Node  Powered off 'karura'.
peter     8r6pw7  karura       AUDIT  Fri, 22 Apr. 2022 07:09:55  Node  Powered on 'karura'.
admin     ebd7dc  new-name     AUDIT  Thu, 14 Apr. 2022 02:24:02  Node  Aborted 'commissioning' on 'new-name'.
admin     ebd7dc  new-name     AUDIT  Thu, 14 Apr. 2022 02:23:44  Node  Started commissioning on 'new-name'.
ed        mm3tc8  fair-marten  AUDIT  Fri, 08 Apr. 2022 11:02:15  Node  Untagging 'fair-marten'.
ed        mm3tc8  fair-marten  AUDIT  Fri, 08 Apr. 2022 11:02:14  Node  Tagging 'fair-marten'.
admin     mm3tc8  fair-marten  AUDIT  Fri, 11 Feb. 2022 11:00:00  Node  Set the zone to 'twilight' on 'fair-marten'.
```

*** Using different event levels

As mentioned earlier, the `AUDIT` events are [discussed elsewhere](/t/understanding-maas-audit-events/6372).  It may be useful, though to take a closer look at the other event levels here.

**** INFO and DEBUG events

We walked the MAAS machine `fun-zebra` through the following states:

- Commissioning
- Allocation
- Deployment
- Releasing
- Testing (with a premature manual abort)
- Rescue mode

The resulting `level=INFO` and `level=DEBUG` event sets are enlightening.

<details><summary>The raw log output, for reference only.</summary>

```nohighlight
maas.log:2022-09-29T15:00:16.461402-05:00 neuromancer maas.interface: [info] eth0 (physical) on fun-zebra: IP address automatically unlinked: None:type=AUTO
maas.log:2022-09-29T15:00:16.553396-05:00 neuromancer maas.node: [info] fun-zebra: Status transition from NEW to COMMISSIONING
maas.log:2022-09-29T15:00:16.754265-05:00 neuromancer maas.power: [info] Changing power state (on) of node: fun-zebra (bk7mg8)
maas.log:2022-09-29T15:00:16.759676-05:00 neuromancer maas.node: [info] fun-zebra: Commissioning started
maas.log:2022-09-29T15:00:18.039441-05:00 neuromancer maas.power: [info] Changed power state (on) of node: fun-zebra (bk7mg8)
maas.log:2022-09-29T15:03:26.946507-05:00 neuromancer maas.node: [info] fun-zebra: Storage layout was set to flat.
maas.log:2022-09-29T15:04:07.795515-05:00 neuromancer maas.node: [info] fun-zebra: Status transition from COMMISSIONING to TESTING
maas.log:2022-09-29T15:04:17.288763-05:00 neuromancer maas.node: [info] fun-zebra: Status transition from TESTING to READY
maas.log:2022-09-29T16:14:21.778320-05:00 neuromancer maas.node: [info] fun-zebra: Status transition from READY to ALLOCATED
maas.log:2022-09-29T16:14:21.793566-05:00 neuromancer maas.node: [info] fun-zebra: allocated to user case
maas.log:2022-09-29T16:14:27.662829-05:00 neuromancer maas.node: [info] fun-zebra: Status transition from ALLOCATED to DEPLOYING
maas.log:2022-09-29T16:14:31.019526-05:00 neuromancer maas.power: [info] Changing power state (on) of node: fun-zebra (bk7mg8)
maas.log:2022-09-29T16:14:32.334589-05:00 neuromancer maas.power: [info] Changed power state (on) of node: fun-zebra (bk7mg8)
maas.log:2022-09-29T16:22:41.935983-05:00 neuromancer maas.node: [info] fun-zebra: Status transition from DEPLOYING to DEPLOYED
maas.log:2022-09-29T16:23:37.084128-05:00 neuromancer maas.node: [info] fun-zebra: Releasing node
maas.log:2022-09-29T16:23:37.085876-05:00 neuromancer maas.node: [info] fun-zebra: Status transition from DEPLOYED to RELEASING
maas.log:2022-09-29T16:23:37.196437-05:00 neuromancer maas.power: [info] Changing power state (off) of node: fun-zebra (bk7mg8)
maas.log:2022-09-29T16:23:38.546649-05:00 neuromancer maas.node: [info] fun-zebra: Status transition from RELEASING to READY
maas.log:2022-09-29T16:23:38.591042-05:00 neuromancer maas.power: [info] Changed power state (off) of node: fun-zebra (bk7mg8)
maas.log:2022-09-29T16:23:51.876495-05:00 neuromancer maas.node: [info] fun-zebra: Status transition from READY to TESTING
maas.log:2022-09-29T16:23:51.997139-05:00 neuromancer maas.power: [info] Changing power state (on) of node: fun-zebra (bk7mg8)
maas.log:2022-09-29T16:23:52.001167-05:00 neuromancer maas.node: [info] fun-zebra: Testing starting
maas.log:2022-09-29T16:23:53.291863-05:00 neuromancer maas.power: [info] Changed power state (on) of node: fun-zebra (bk7mg8)
maas.log:2022-09-29T16:26:14.607386-05:00 neuromancer maas.power: [info] Changing power state (off) of node: fun-zebra (bk7mg8)
maas.log:2022-09-29T16:26:14.622643-05:00 neuromancer maas.node: [info] fun-zebra: Status transition from TESTING to READY
maas.log:2022-09-29T16:26:14.678433-05:00 neuromancer maas.node: [info] fun-zebra: Testing aborted, stopping node
maas.log:2022-09-29T16:26:16.051940-05:00 neuromancer maas.power: [info] Changed power state (off) of node: fun-zebra (bk7mg8)
maas.log:2022-09-29T16:26:23.081533-05:00 neuromancer maas.node: [info] fun-zebra: Status transition from READY to ENTERING_RESCUE_MODE
maas.log:2022-09-29T16:26:23.160687-05:00 neuromancer maas.power: [info] Changing power state (cycle) of node: fun-zebra (bk7mg8)
maas.log:2022-09-29T16:26:23.163274-05:00 neuromancer maas.node: [info] fun-zebra: Rescue mode starting
maas.log:2022-09-29T16:26:24.528007-05:00 neuromancer maas.power: [info] Changed power state (on) of node: fun-zebra (bk7mg8)
maas.log:2022-09-29T16:28:58.268558-05:00 neuromancer maas.node: [info] fun-zebra: Status transition from ENTERING_RESCUE_MODE to RESCUE_MODE
maas.log:2022-09-29T16:29:52.204837-05:00 neuromancer maas.node: [info] fun-zebra: Status transition from RESCUE_MODE to EXITING_RESCUE_MODE
maas.log:2022-09-29T16:29:52.323798-05:00 neuromancer maas.power: [info] Changing power state (off) of node: fun-zebra (bk7mg8)
maas.log:2022-09-29T16:29:53.708975-05:00 neuromancer maas.node: [info] fun-zebra: Status transition from EXITING_RESCUE_MODE to READY
maas.log:2022-09-29T16:29:53.745776-05:00 neuromancer maas.power: [info] Changed power state (off) of node: fun-zebra (bk7mg8)
maas.log:2022-09-29T16:32:20.147958-05:00 neuromancer maas.node: [info] fun-zebra: moved from default zone to new-zone zone.
regiond.log:2022-09-29 20:00:16 maasserver.models.signals.interfaces: [info] eth0 (physical) on fun-zebra: deleted IP addresses due to VLAN update (5001 -> 5002).
regiond.log:2022-09-29 20:00:32 maasserver.region_controller: [info] Reloaded DNS configuration; ip 10.103.114.192 connected to fun-zebra on eth0
regiond.log:2022-09-29 20:01:09 maasserver.rpc.leases: [info] Lease update: commit for 10.103.114.192 on 0:16:3e:a2:73:5c at 2022-09-29 20:01:09 (lease time: 600s) (hostname: fun-zebra)
regiond.log:2022-09-29 20:01:14 maasserver.region_controller: [info] Reloaded DNS configuration; ip 10.103.114.192 connected to fun-zebra on eth0
regiond.log:     * node fun-zebra renamed interface eth0 to enp5s0
regiond.log:     * ip 10.103.114.192 connected to fun-zebra on eth0
regiond.log:     * ip 10.103.114.192 disconnected from fun-zebra on eth0
regiond.log:2022-09-29 21:15:31 maasserver.rpc.leases: [info] Lease update: commit for 10.103.114.2 on 0:16:3e:a2:73:5c at 2022-09-29 21:15:31 (lease time: 600s) (hostname: fun-zebra)
regiond.log:2022-09-29 21:21:48 maasserver.models.node: [info] fun-zebra: Turning off netboot for node
regiond.log:2022-09-29 21:22:41 metadataserver: [info] No user data registered for node named fun-zebra
regiond.log:2022-09-29 21:23:37 maasserver.models.node: [info] fun-zebra: Turning on netboot for node
regiond.log:2022-09-29 21:23:37 maasserver.models.node: [info] fun-zebra: Turning ephemeral deploy off for node
regiond.log:2022-09-29 21:24:06 maasserver.region_controller: [info] Reloaded DNS configuration; ip 10.103.114.192 connected to fun-zebra on enp5s0
regiond.log:2022-09-29 21:24:43 maasserver.rpc.leases: [info] Lease update: commit for 10.103.114.192 on 0:16:3e:a2:73:5c at 2022-09-29 21:24:43 (lease time: 600s) (hostname: fun-zebra)
regiond.log:2022-09-29 21:24:46 maasserver.region_controller: [info] Reloaded DNS configuration; ip 10.103.114.192 connected to fun-zebra on enp5s0
regiond.log:2022-09-29 21:27:18 maasserver.rpc.leases: [info] Lease update: commit for 10.103.114.192 on 0:16:3e:a2:73:5c at 2022-09-29 21:27:18 (lease time: 600s) (hostname: fun-zebra)
```
</details>

First, let's try this command:

```nohighlight
 maas $PROFILE events query level=INFO hostname=fun-zebra limit=1000 | jq -r '(["USERNAME","NODE","HOSTNAME","LEVEL","DATE","TYPE","EVENT"] | (., map(length*"-"))),(.events[] | [.username,.node,.hostname,.level,.created,.type,.description]) | @tsv' | column -t -s$'\t'
 ```
 
 This will yield a surprisingly compact report:
 
 ```nohighlight
 USERNAME  NODE    HOSTNAME   LEVEL  DATE                        TYPE                   EVENT
--------  ----    --------   -----  ----                        ----                   -----
unknown   bk7mg8  fun-zebra  INFO   Thu, 29 Sep. 2022 21:29:53  Exited rescue mode     
unknown   bk7mg8  fun-zebra  INFO   Thu, 29 Sep. 2022 21:29:52  Powering off           
unknown   bk7mg8  fun-zebra  INFO   Thu, 29 Sep. 2022 21:28:58  Rescue mode            
unknown   bk7mg8  fun-zebra  INFO   Thu, 29 Sep. 2022 21:27:18  Loading ephemeral      
unknown   bk7mg8  fun-zebra  INFO   Thu, 29 Sep. 2022 21:26:40  Performing PXE boot    
unknown   bk7mg8  fun-zebra  INFO   Thu, 29 Sep. 2022 21:26:23  Power cycling          
unknown   bk7mg8  fun-zebra  INFO   Thu, 29 Sep. 2022 21:26:23  Entering rescue mode   
unknown   bk7mg8  fun-zebra  INFO   Thu, 29 Sep. 2022 21:26:14  Powering off           
unknown   bk7mg8  fun-zebra  INFO   Thu, 29 Sep. 2022 21:26:14  Aborted testing        
unknown   bk7mg8  fun-zebra  INFO   Thu, 29 Sep. 2022 21:24:08  Performing PXE boot    
unknown   bk7mg8  fun-zebra  INFO   Thu, 29 Sep. 2022 21:23:51  Powering on            
unknown   bk7mg8  fun-zebra  INFO   Thu, 29 Sep. 2022 21:23:51  Testing                
unknown   bk7mg8  fun-zebra  INFO   Thu, 29 Sep. 2022 21:23:38  Released               
unknown   bk7mg8  fun-zebra  INFO   Thu, 29 Sep. 2022 21:23:37  Powering off           
unknown   bk7mg8  fun-zebra  INFO   Thu, 29 Sep. 2022 21:23:37  Releasing              
unknown   bk7mg8  fun-zebra  INFO   Thu, 29 Sep. 2022 21:22:41  Deployed               
unknown   bk7mg8  fun-zebra  INFO   Thu, 29 Sep. 2022 21:21:49  Rebooting              
unknown   bk7mg8  fun-zebra  INFO   Thu, 29 Sep. 2022 21:18:42  Configuring OS         
unknown   bk7mg8  fun-zebra  INFO   Thu, 29 Sep. 2022 21:17:42  Installing OS          
unknown   bk7mg8  fun-zebra  INFO   Thu, 29 Sep. 2022 21:17:30  Configuring storage    
unknown   bk7mg8  fun-zebra  INFO   Thu, 29 Sep. 2022 21:15:31  Loading ephemeral      
unknown   bk7mg8  fun-zebra  INFO   Thu, 29 Sep. 2022 21:14:48  Performing PXE boot    
unknown   bk7mg8  fun-zebra  INFO   Thu, 29 Sep. 2022 21:14:31  Powering on            
unknown   bk7mg8  fun-zebra  INFO   Thu, 29 Sep. 2022 21:14:27  Deploying              
unknown   bk7mg8  fun-zebra  INFO   Thu, 29 Sep. 2022 20:04:17  Ready                  
unknown   bk7mg8  fun-zebra  INFO   Thu, 29 Sep. 2022 20:04:07  Running test           smartctl-validate on sda
unknown   bk7mg8  fun-zebra  INFO   Thu, 29 Sep. 2022 20:01:27  Gathering information  
unknown   bk7mg8  fun-zebra  INFO   Thu, 29 Sep. 2022 20:01:10  Loading ephemeral      
unknown   bk7mg8  fun-zebra  INFO   Thu, 29 Sep. 2022 20:00:35  Performing PXE boot    
unknown   bk7mg8  fun-zebra  INFO   Thu, 29 Sep. 2022 20:00:16  Powering on            
unknown   bk7mg8  fun-zebra  INFO   Thu, 29 Sep. 2022 20:00:16  Commissioning          
 ```
 
Note that most of the `INFO` events are either machine life-cycle events or key operations within those state changes, such as `Loading ephemeral` after a PXE boot.  `DEBUG` events, on the other hand, include `INFO` events for reference, but provide a much more extensive report of individual actions within each state change.  For instance, here is just the snippet of `DEBUG` information for the host's exit from rescue mode:

```nohighlight
USERNAME  NODE    HOSTNAME   LEVEL  DATE                        TYPE                              EVENT
--------  ----    --------   -----  ----                        ----                              -----
unknown   bk7mg8  fun-zebra  DEBUG  Thu, 29 Sep. 2022 21:29:53  Node powered off                  
unknown   bk7mg8  fun-zebra  DEBUG  Thu, 29 Sep. 2022 21:29:53  Node changed status               From 'Exiting rescue mode' to 'Ready'
unknown   bk7mg8  fun-zebra  INFO   Thu, 29 Sep. 2022 21:29:53  Exited rescue mode                
unknown   bk7mg8  fun-zebra  DEBUG  Thu, 29 Sep. 2022 21:28:58  Node status event                 'cloudinit' running config-power-state-change with frequency once-per-instance
unknown   bk7mg8  fun-zebra  DEBUG  Thu, 29 Sep. 2022 21:28:58  Node status event                 'cloudinit' running config-final-message with frequency always
unknown   bk7mg8  fun-zebra  DEBUG  Thu, 29 Sep. 2022 21:28:58  Node status event                 'cloudinit' running config-phone-home with frequency once-per-instance
unknown   bk7mg8  fun-zebra  DEBUG  Thu, 29 Sep. 2022 21:28:58  Node status event                 'cloudinit' running config-install-hotplug with frequency once-per-instance
unknown   bk7mg8  fun-zebra  DEBUG  Thu, 29 Sep. 2022 21:28:58  Node status event                 'cloudinit' running config-keys-to-console with frequency once-per-instance
unknown   bk7mg8  fun-zebra  DEBUG  Thu, 29 Sep. 2022 21:28:58  Node status event                 'cloudinit' running config-ssh-authkey-fingerprints with frequency once-per-instance
unknown   bk7mg8  fun-zebra  DEBUG  Thu, 29 Sep. 2022 21:28:57  Node status event                 'cloudinit' running config-scripts-user with frequency once-per-instance
unknown   bk7mg8  fun-zebra  DEBUG  Thu, 29 Sep. 2022 21:28:57  Node status event                 'cloudinit' running config-scripts-per-instance with frequency once-per-instance
unknown   bk7mg8  fun-zebra  DEBUG  Thu, 29 Sep. 2022 21:28:57  Node status event                 'cloudinit' running config-scripts-per-boot with frequency always
unknown   bk7mg8  fun-zebra  DEBUG  Thu, 29 Sep. 2022 21:28:57  Node status event                 'cloudinit' running config-scripts-per-once with frequency once
unknown   bk7mg8  fun-zebra  DEBUG  Thu, 29 Sep. 2022 21:28:57  Node status event                 'cloudinit' running config-scripts-vendor with frequency once-per-instance
unknown   bk7mg8  fun-zebra  DEBUG  Thu, 29 Sep. 2022 21:28:57  Node status event                 'cloudinit' running config-rightscale_userdata with frequency once-per-instance
unknown   bk7mg8  fun-zebra  DEBUG  Thu, 29 Sep. 2022 21:28:57  Node status event                 'cloudinit' running config-refresh_rmc_and_interface with frequency always
unknown   bk7mg8  fun-zebra  DEBUG  Thu, 29 Sep. 2022 21:28:57  Node status event                 'cloudinit' running config-reset_rmc with frequency once-per-instance
unknown   bk7mg8  fun-zebra  DEBUG  Thu, 29 Sep. 2022 21:28:57  Node status event                 'cloudinit' running config-salt-minion with frequency once-per-instance
unknown   bk7mg8  fun-zebra  DEBUG  Thu, 29 Sep. 2022 21:28:57  Node status event                 'cloudinit' running config-mcollective with frequency once-per-instance
unknown   bk7mg8  fun-zebra  DEBUG  Thu, 29 Sep. 2022 21:28:57  Node status event                 'cloudinit' running config-chef with frequency always
unknown   bk7mg8  fun-zebra  DEBUG  Thu, 29 Sep. 2022 21:28:57  Node status event                 'cloudinit' running config-puppet with frequency once-per-instance
unknown   bk7mg8  fun-zebra  DEBUG  Thu, 29 Sep. 2022 21:28:57  Node status event                 'cloudinit' running config-write-files-deferred with frequency once-per-instance
unknown   bk7mg8  fun-zebra  DEBUG  Thu, 29 Sep. 2022 21:28:57  Node status event                 'cloudinit' running config-ubuntu-drivers with frequency once-per-instance
unknown   bk7mg8  fun-zebra  DEBUG  Thu, 29 Sep. 2022 21:28:57  Node status event                 'cloudinit' running config-lxd with frequency once-per-instance
unknown   bk7mg8  fun-zebra  DEBUG  Thu, 29 Sep. 2022 21:28:57  Node status event                 'cloudinit' running config-landscape with frequency once-per-instance
unknown   bk7mg8  fun-zebra  DEBUG  Thu, 29 Sep. 2022 21:28:57  Node status event                 'cloudinit' running config-fan with frequency once-per-instance
unknown   bk7mg8  fun-zebra  INFO   Thu, 29 Sep. 2022 21:29:52  Powering off                      
unknown   bk7mg8  fun-zebra  DEBUG  Thu, 29 Sep. 2022 21:29:52  Node changed status               From 'Rescue mode' to 'Exiting rescue mode'
case      bk7mg8  fun-zebra  DEBUG  Thu, 29 Sep. 2022 21:29:52  User stopping rescue mode         (case)
```

Notice the detailed `cloudinit` actions necessary to change the machine's state.  The other state changes have similarly detailed outputs in `DEBUG`.

**** ERROR and WARNING events

Here are a few representative `ERROR` event descriptions taken from a live MAAS machine:

```nohighlight
Node has not been heard from for the last 30 minutes
Node operation 'Commissioning' timed out after 30 minutes.
Unable to import boot images: HTTPConnectionPool(host='localhost', port=5240): Read timed out.
Node operation 'Testing' timed out after 30 minutes.
Power on for the node failed: Failed talking to node's BMC: Failed to login to virsh console.
Unable to import boot images: Invalid sha256 Checksum at http://localhost:5240/MAAS/images-stream/ubuntu/amd64/ga-18.04-lowlatency/bionic/20200206/boot-initrd. Found 834c0eacb1a19526f715f9947bd47904b18ad8c733b0762e690edf6143e10561. Expected addfa86d7c054bd0dc085333ad2850e93223d511d04b59ee516d42d801522324. read 38 bytes expected 61715624 bytes. (size 38 expected 61715624)
``` 

Notice that these `ERROR` events flag failures that are probably going to prevent MAAS from operating properly.  Changing the level to `WARNING` picks up all `ERROR` events, but also includes warnings such as this one:

```nohighlight
Finished importing boot images, the region does not have any boot images available.
```

`WARNINGS` tend to be failures, as well, but failures which are more easily fixed (such as having not successfully downloaded any images).

**** CRITICAL errors

`CRITICAL` errors represent major failures, often code failures or trace-backs.  Any `CRITICAL` errors should be immediately examined and resolved, if possible, and [reported as a bug](/t/how-to-report-a-bug/4446) if not resolvable.

** About MAAS audit events

An audit event is a [MAAS event](/t/understanding-maas-events/6373) tagged with `AUDIT`. It captures changes to the MAAS configuration and machine states. These events provide valuable oversight of user actions and automated updates -- and their effects -- especially when multiple users are interacting with multiple machines.  See [Understanding MAAS events](/t/understanding-maas-events/6373) for basic usage of the CLI `events query` command.

*** Viewing events

Audit events are examined using the MAAS CLI with the `level=AUDIT` parameter set:

```nohighlight
$ maas $PROFILE events query level=AUDIT
```

You can use `jq` to prettify the output:

```nohighlight
$ maas $PROFILE events query level=AUDIT after=0 limit=20 \
| jq -r '(["USERNAME","HOSTNAME","DATE","EVENT"] | 
(., map(length*"-"))),
(.events[] | [.username,.hostname,.created,.description]) 
| @tsv' | column -t -s$'\t'
```

This command might produce output similar to this:

```nohighlight
USERNAME  HOSTNAME     DATE                        EVENT
--------  --------     ----                        -----
unknown   valued-moth  Thu, 21 Apr. 2022 19:45:14  pci device 2 was updated on node 8wmfx3
unknown   valued-moth  Thu, 21 Apr. 2022 19:45:14  pci device 1 was updated on node 8wmfx3
unknown   valued-moth  Thu, 21 Apr. 2022 19:45:14  pci device 1 was updated on node 8wmfx3
unknown   valued-moth  Thu, 21 Apr. 2022 19:45:14  pci device 1 was updated on node 8wmfx3
unknown   valued-moth  Thu, 21 Apr. 2022 19:45:14  pci device 1 was updated on node 8wmfx3
unknown   valued-moth  Thu, 21 Apr. 2022 19:45:14  pci device 1 was updated on node 8wmfx3
unknown   valued-moth  Thu, 21 Apr. 2022 19:45:14  pci device 1 was updated on node 8wmfx3
unknown   valued-moth  Thu, 21 Apr. 2022 19:45:14  pci device 1 was updated on node 8wmfx3
unknown   valued-moth  Thu, 21 Apr. 2022 19:45:14  pci device 1 was updated on node 8wmfx3
unknown   valued-moth  Thu, 21 Apr. 2022 19:45:14  pci device 0 was updated on node 8wmfx3
unknown   valued-moth  Thu, 21 Apr. 2022 19:45:14  block device sda was updated on node 8wmfx3
unknown   valued-moth  Thu, 21 Apr. 2022 19:45:14  interface enp5s0 was updated on node 8wmfx3
unknown   valued-moth  Thu, 21 Apr. 2022 19:45:14  0 bytes of memory was removed on node 8wmfx3
admin     valued-moth  Thu, 21 Apr. 2022 19:36:48  Started deploying 'valued-moth'.
admin     valued-moth  Thu, 21 Apr. 2022 19:36:21  Acquired 'valued-moth'.
admin     unknown      Thu, 21 Apr. 2022 19:21:46  Updated configuration setting 'completed_intro' to 'True'.
admin     unknown      Thu, 21 Apr. 2022 19:20:49  Updated configuration setting 'upstream_dns' to '8.8.8.8'.
admin     unknown      Thu, 21 Apr. 2022 19:20:49  Updated configuration setting 'maas_name' to 'neuromancer'.
admin     unknown      Thu, 21 Apr. 2022 19:20:47  Updated configuration setting 'http_proxy' to ''.
admin     unknown      Thu, 21 Apr. 2022 19:20:24  Logged in admin.
```

You can also use the [various event filters](/t/understanding-maas-events/6373#heading--filter-parameters) with `level=AUDIT` to further restrict your output.

*** The meaning of audit events

Let's walk through a sample of, say, eighteen audit events and see how to interpret and use them.  

```nohighlight
maas $PROFILE events query level=AUDIT limit=18 after=0 | jq -r '(["USERNAME","NODE","HOSTNAME","LEVEL","DATE","TYPE","EVENT"] | 
(., map(length*"-"))),
(.events[] | [.username,.node,.hostname,.level,.created,.type,.description]) 
| @tsv' | column -t -s$'\t'
```

Consider the resulting `jq` output:

```nohighlight
USERNAME     NODE    HOSTNAME       LEVEL  DATE                        TYPE  EVENT
--------     ----    --------       -----  ----                        ----  -----
admin        mm3tc8  fair-marten    AUDIT  Tue, 30 Nov. 2021 09:14:02  Node  Set the zone to 'danger' on 'fair-marten'.
admin        ebd7dc  new-name       AUDIT  Tue, 30 Nov. 2021 09:14:02  Node  Set the zone to 'danger' on 'new-name'.
admin        pbpncx  ruling-bobcat  AUDIT  Tue, 30 Nov. 2021 09:13:52  Node  Set the zone to 'default' on 'ruling-bobcat'.
admin        mm3tc8  fair-marten    AUDIT  Tue, 30 Nov. 2021 09:13:52  Node  Set the zone to 'default' on 'fair-marten'.
admin        ebd7dc  new-name       AUDIT  Tue, 30 Nov. 2021 09:13:52  Node  Set the zone to 'default' on 'new-name'.
admin        mm3tc8  fair-marten    AUDIT  Tue, 30 Nov. 2021 09:11:56  Node  Started commissioning on 'fair-marten'.
admin        ebd7dc  new-name       AUDIT  Tue, 30 Nov. 2021 09:11:55  Node  Started commissioning on 'new-name'.
admin        ebd7dc  new-name       AUDIT  Tue, 30 Nov. 2021 09:09:06  Node  Marked 'new-name' broken.
admin        ebd7dc  new-name       AUDIT  Tue, 30 Nov. 2021 07:51:31  Node  Started commissioning on 'new-name'.
admin        mm3tc8  fair-marten    AUDIT  Tue, 30 Nov. 2021 06:07:03  Node  Started commissioning on 'fair-marten'.
admin        ebd7dc  active-amoeba  AUDIT  Tue, 23 Nov. 2021 08:01:10  Node  Started commissioning on 'active-amoeba'.
admin        ebd7dc  active-amoeba  AUDIT  Tue, 23 Nov. 2021 08:00:47  Node  Marked 'active-amoeba' broken.
admin        pbpncx  ruling-bobcat  AUDIT  Wed, 17 Nov. 2021 00:04:51  Node  Started deploying 'ruling-bobcat'.
admin        ebd7dc  active-amoeba  AUDIT  Mon, 15 Nov. 2021 05:39:48  Node  Set the resource pool to 'default' on 'active-amoeba'.
admin        ebd7dc  active-amoeba  AUDIT  Mon, 08 Nov. 2021 04:07:44  Node  Started testing on 'active-amoeba'.
admin        ebd7dc  active-amoeba  AUDIT  Mon, 08 Nov. 2021 04:05:40  Node  Marked 'active-amoeba' broken.
admin        knpge8  bolla          AUDIT  Wed, 16 Jun. 2021 04:35:50  Node  Started importing images on 'bolla'.
admin        knpge8  bolla          AUDIT  Wed, 10 Jun. 2020 21:07:40  Node  Set the zone to 'danger' on 'bolla'.
```

All of these example events are type `Node`, referring to a machine actions.  Node events are probably the most important audit events, because they capture machine life-cycle changes.  When auditing your MAAS, life-cycle events are often the most useful.

Take a moment to consider the MAAS life-cycle, which can be depicted with this state table:

| Machine state | Cm | Aq | Dp | Rl | Ab | Cl | PC | Ts | Rsq | Bk | Lk | Fx | Tg | RP | SZ | Del |
|---------------|----|----|----|----|----|----|----|----|-----|----|----|----|----|----|----|-----|
| New           | Y  |    |    |    |    |    | Y  | Y  | Y   |    |    |    | Y  | Y  | Y  | Y   |
| Failed        | Y  |    |    |    |    |    |    | Y  | Y   | Y  |    |    | Y  | Y  | Y  | Y   |
| Commissioning |    |    |    |    | Y  |    |    |    |     | Y  |    |    | Y  | Y  | Y  |     |
| Ready         | Y  | Y  | Y  |    |    | Y  |    | Y  | Y   | Y  |    |    | Y  | Y  | Y  | Y   |
| Acquired      | Y  |    | Y  | Y  |    |    |    | Y  | Y   | Y  |    |    | Y  | Y  | Y  | Y   |
| Deploying     |    |    |    | Y  | Y  |    | Y  |    |     |    | Y  |    | Y  | Y  | Y  | Y   |
| Deployed      |    |    |    | Y  |    |    | Y  | Y  | Y   | Y  | Y  |    | Y  | Y  | Y  | Y   |
| Broken        | Y  |    |    |    |    |    | Y  | Y  | Y   |    |    | Y  | Y  | Y  | Y  | Y   |
| Rescue mode   |    |    |    |    |    |    |    |    | X   |    |    |    | Y  | Y  | Y  | Y   |

The key for the table columns is as follows:

- *Cm* - can commission.
- *Aq* - can acquire.
- *Dp* - can deploy.
- *Rl* - can release.
- *Ab* - can abort an operation in progress.
- *Cl* - can clone the machine.
- *PC* - can power-cycle the machine (turn in on or off).
- *Ts* - can run tests on the machine.
- *Rsq* - can put the machine in Rescue Mode.
- *Bk* - can mark the machine as broken.
- *Lk* - can lock the machine, preventing others from accessing it.
- *Fx* - can move a broken machine to a fixed state.
- *Tg* - can set tags for a machine.
- *RP* - can set the resource pool for a machine.
- *SZ* - can set the zone for a machine.
- *Del* - can delete the machine.

*** Using audit events to find out what happened

Consider these example events that audit state changes:

```nohighlight
ID      LEVEL  TYPE           USERNAME  DESCRIPTION
=================================================================================================
589317  AUDIT  Node           bruce     Marked 'ruling-bobcat' broken.
583324  AUDIT  Node           clark     Tagging 'fair-marten'.
583313  AUDIT  Node           clark     Untagging 'fair-marten'.
584190  AUDIT  Node           diana     Overrode failed testing on 'new-name'.
529870  AUDIT  Node           kara      Powered on 'karura'.
529868  AUDIT  Node           kara      Powered off 'karura'.
435099  AUDIT  Node           barry     Set the zone to 'twilight' on 'fair-marten'.
435097  AUDIT  Node           hal       Acquired 'fair-marten'.
430453  AUDIT  Node           jonn      Started testing on 'fair-marten'.
430449  AUDIT  Node           jonn      Marked 'fair-marten' broken.
430445  AUDIT  Node           clark     Aborted 'testing' on 'fair-marten'.
427583  AUDIT  Node           diana     Set the resource pool to 'default' on 'fair-marten'.
426354  AUDIT  Node           bruce     Started commissioning on 'fair-marten'.
423257  AUDIT  Node           kara      Aborted 'commissioning' on 'fair-marten'.
421915  AUDIT  Node           joanna    Started releasing 'ruling-bobcat'.
28471   AUDIT  Settings       natasha   Updated DHCP snippet 'foo'.
28470   AUDIT  Settings       tony      Created DHCP snippet 'foo'.
28465   AUDIT  Settings       bruce2    Saved script 'setup.sh'.
28464   AUDIT  Settings       hank      Updated configuration setting 'enable_third_party_drivers' to 'False'.
8518    AUDIT  Node           kitty     Acquired 'sweet-krill'.
7615    AUDIT  Node           barry     Deleted the 'machine' 'new-bedbug'.
6238    AUDIT  Node           jonn      Started rescue mode on 'fleet-calf'.
5920    AUDIT  Node           diana     Started deploying 'comic-muskox'.
5907    AUDIT  Authorisation  admin     Logged out admin.
5906    AUDIT  Authorisation  admin     Logged in admin.
5896    AUDIT  Authorisation  hank      Created user 'zorko'.
3944    AUDIT  Node           clark     Deleted the 'machine' 'sweet-urchin'.
```

This is a long (but varied) listing, so there are many questions you might be able to answer:

1. Who deployed `comic-muskox`? 

2. What happened to `sweet-urchin`?

3. Why is `fleet-calf` in rescue mode?

4. Where did these changes come from in `setup.sh`?

5. What caused `ruling-bobcat` to be marked as broken?

6. Who's responsible for the DHCP snippet called `foo`?

Audit events don't answer all questions, but they help you discover whom to ask.

*** Auditing with finesse

You can use the MAAS CLI, `jq`, and command line text tools to finesse your auditing.  First, you'll have to get a feel for how MAAS describes audit events:

- Set the resource pool to 
- Started commissioning 
- Aborted 'commissioning'
- Started releasing 
- Created DHCP snippet
- Saved script 
- Updated configuration setting 
- Deleted the 'machine' 
- Created user

You can use these snippets as search keys.  Say you walk into the data centre one day and a couple of machines just aren't there any more.  You could run this command:

```nohighlight
$ maas $PROFILE events query limit=1000 after=0 level=AUDIT \
hostname=new-bedbug hostname=sweet-urchin \
| jq -r '(.events[] | [.id,.level,.type,.username,.description]) 
| @tsv' | column -t -s$'\t' \
| grep "Deleted the"
```

Within 30 seconds, you'd know whom to ask:

```nohighlight
7615    AUDIT  Node  barry     Deleted the 'machine' 'new-bedbug'.
3944    AUDIT  Node  clark     Deleted the 'machine' 'sweet-urchin'.
```
Or, you could just check to see what's been deleted:

```nohighlight
33315  AUDIT  Node           Deleted the 'machine' 'keen-lab'.
33314  AUDIT  Node           Deleted the 'machine' 'helloooo'.
31179  AUDIT  Node           Deleted the 'machine' 'firm-ghost'.
31178  AUDIT  Node           Deleted the 'machine' 'proper-troll'.
31177  AUDIT  Node           Deleted the 'machine' 'steady-mammal'.
31176  AUDIT  Node           Deleted the 'machine' 'wired-dove'.
31175  AUDIT  Node           Deleted the 'machine' 'wanted-fox'.
31174  AUDIT  Node           Deleted the 'machine' 'picked-cub'.
31173  AUDIT  Node           Deleted the 'machine' 'claudio'.
31172  AUDIT  Node           Deleted the 'machine' 'next-mullet'.
31171  AUDIT  Node           Deleted the 'machine' 'happy-bengal'.
31170  AUDIT  Node           Deleted the 'machine' 'grown-hawk'.
31169  AUDIT  Node           Deleted the 'machine' 'new-bedbug'.
31168  AUDIT  Node           Deleted the 'machine' 'native-moray'.
31167  AUDIT  Node           Deleted the 'machine' 'fleet-calf'.
31166  AUDIT  Node           Deleted the 'machine' 'daring-ewe'.
31165  AUDIT  Node           Deleted the 'machine' 'sweet-urchin'.
31164  AUDIT  Node           Deleted the 'machine' 'new-chimp'.
31163  AUDIT  Node           Deleted the 'machine' 'humble-bug'.
31162  AUDIT  Node           Deleted the 'machine' 'modern-mutt'.
31161  AUDIT  Node           Deleted the 'machine' 'nice-skink'.
31160  AUDIT  Node           Deleted the 'machine' 'choice-worm'.
31159  AUDIT  Node           Deleted the 'machine' 'wanted-turtle'.
31158  AUDIT  Node           Deleted the 'machine' 'neat-yak'.
31157  AUDIT  Node           Deleted the 'machine' 'superb-piglet'.
31156  AUDIT  Node           Deleted the 'machine' 'rare-ghost'.
31155  AUDIT  Node           Deleted the 'machine' 'unique-weevil'.
31154  AUDIT  Node           Deleted the 'machine' 'finer-akita'.
31153  AUDIT  Node           Deleted the 'machine' 'cool-dog'.
31152  AUDIT  Node           Deleted the 'machine' 'meet-snake'.
31151  AUDIT  Node           Deleted the 'machine' 'native-civet'.
31150  AUDIT  Node           Deleted the 'machine' 'top-burro'.
31149  AUDIT  Node           Deleted the 'machine' 'pro-boa'.
31148  AUDIT  Node           Deleted the 'machine' 'fine-dane'.
31147  AUDIT  Node           Deleted the 'machine' 'clean-ocelot'.
31146  AUDIT  Node           Deleted the 'machine' 'boss-crab'.
31145  AUDIT  Node           Deleted the 'machine' 'crisp-mammal'.
31144  AUDIT  Node           Deleted the 'machine' 'active-panda'.
31143  AUDIT  Node           Deleted the 'machine' 'fit-ram'.
31142  AUDIT  Node           Deleted the 'machine' 'strong-prawn'.
31141  AUDIT  Node           Deleted the 'machine' 'equal-dog'.
31140  AUDIT  Node           Deleted the 'machine' 'sure-kid'.
31139  AUDIT  Node           Deleted the 'machine' 'choice-wren'.
31138  AUDIT  Node           Deleted the 'machine' 'eager-whale'.
31137  AUDIT  Node           Deleted the 'machine' 'fun-boxer'.
31136  AUDIT  Node           Deleted the 'machine' 'clean-filly'.
31135  AUDIT  Node           Deleted the 'machine' 'thingthing'.
31134  AUDIT  Node           Deleted the 'machine' 'prime-walrus'.
28073  AUDIT  Node           Deleted the 'machine' 'ace-boxer'.
28072  AUDIT  Node           Deleted the 'machine' 'active-panda'.
28071  AUDIT  Node           Deleted the 'machine' 'boss-crab'.
24724  AUDIT  Node           Deleted the 'machine' 'ruling-marlin'.
24723  AUDIT  Node           Deleted the 'machine' 'sweet-urchin'.
24722  AUDIT  Node           Deleted the 'machine' 'new-chimp'.
24721  AUDIT  Node           Deleted the 'machine' 'humble-bug'.
24720  AUDIT  Node           Deleted the 'machine' 'next-mullet'.
24719  AUDIT  Node           Deleted the 'machine' 'native-moray'.
24718  AUDIT  Node           Deleted the 'machine' 'grown-hawk'.
24717  AUDIT  Node           Deleted the 'machine' 'happy-bengal'.
24716  AUDIT  Node           Deleted the 'machine' 'picked-cub'.
24715  AUDIT  Node           Deleted the 'machine' 'claudio'.
24714  AUDIT  Node           Deleted the 'machine' 'fleet-calf'.
24713  AUDIT  Node           Deleted the 'machine' 'new-bedbug'.
24712  AUDIT  Node           Deleted the 'machine' 'daring-ewe'.
24711  AUDIT  Node           Deleted the 'machine' 'huge-yeti'.
24502  AUDIT  Node           Deleted the 'machine' 'guided-joey'.
24501  AUDIT  Node           Deleted the 'machine' 'active-adder'.
24500  AUDIT  Node           Deleted the 'machine' 'crisp-chow'.
24499  AUDIT  Node           Deleted the 'machine' 'holy-hippo'.
24498  AUDIT  Node           Deleted the 'machine' 'eager-kid'.
24497  AUDIT  Node           Deleted the 'machine' 'mighty-finch'.
24496  AUDIT  Node           Deleted the 'machine' 'native-koala'.
24415  AUDIT  Node           Deleted the 'machine' 'me'.
24410  AUDIT  Node           Deleted the 'machine' 'you'.
17934  AUDIT  Node           Deleted the 'machine' 'carol'.
17933  AUDIT  Node           Deleted the 'machine' 'bob'.
17932  AUDIT  Node           Deleted the 'machine' 'aaa'.
17931  AUDIT  Node           Deleted the 'machine' 'alice'.
17604  AUDIT  Node           Deleted the 'machine' 'subtle-lark'.
17603  AUDIT  Node           Deleted the 'machine' 'brief-beetle'.
17602  AUDIT  Node           Deleted the 'machine' 'fit-earwig'.
12508  AUDIT  Node           Deleted the 'machine' 'asdf'.
12507  AUDIT  Node           Deleted the 'machine' 'gfd'.
12506  AUDIT  Node           Deleted the 'machine' 'sadasd'.
12505  AUDIT  Node           Deleted the 'machine' 'vocal-krill'.
12504  AUDIT  Node           Deleted the 'machine' 'epic-robin'.
12503  AUDIT  Node           Deleted the 'machine' 'secret-maas'.
12502  AUDIT  Node           Deleted the 'machine' 'thingthing'.
12501  AUDIT  Node           Deleted the 'machine' 'worthy-ray'.
12500  AUDIT  Node           Deleted the 'machine' 'brief-pika'.
12499  AUDIT  Node           Deleted the 'machine' 'sweet-krill'.
12498  AUDIT  Node           Deleted the 'machine' 'awake-dog'.
12497  AUDIT  Node           Deleted the 'machine' 'living-crab'.
12496  AUDIT  Node           Deleted the 'machine' 'quiet-caiman'.
12495  AUDIT  Node           Deleted the 'machine' 'known-kodiak'.
10975  AUDIT  Node           Deleted the 'machine' 'rested-egret'.
10974  AUDIT  Node           Deleted the 'machine' 'good-martin'.
10973  AUDIT  Node           Deleted the 'machine' 'game-elk'.
10972  AUDIT  Node           Deleted the 'machine' 'asda'.
10971  AUDIT  Node           Deleted the 'machine' 'cuddly-eft'.
10970  AUDIT  Node           Deleted the 'machine' 'asdas'.
9423   AUDIT  Node           Deleted the 'machine' 'hostname'.
7615   AUDIT  Node           Deleted the 'machine' 'new-bedbug'.
7614   AUDIT  Node           Deleted the 'machine' 'happy-bengal'.
7613   AUDIT  Node           Deleted the 'machine' 'fleet-calf'.
7612   AUDIT  Node           Deleted the 'machine' 'claudio'.
7611   AUDIT  Node           Deleted the 'machine' 'sweet-urchin'.
7610   AUDIT  Node           Deleted the 'machine' 'picked-cub'.
7609   AUDIT  Node           Deleted the 'machine' 'new-chimp'.
7608   AUDIT  Node           Deleted the 'machine' 'humble-bug'.
7607   AUDIT  Node           Deleted the 'machine' 'grown-hawk'.
7606   AUDIT  Node           Deleted the 'machine' 'native-moray'.
7605   AUDIT  Node           Deleted the 'machine' 'daring-ewe'.
7604   AUDIT  Node           Deleted the 'machine' 'fair-puma'.
7603   AUDIT  Node           Deleted the 'machine' 'funny-panda'.
7602   AUDIT  Node           Deleted the 'machine' 'ace-molly'.
7601   AUDIT  Node           Deleted the 'machine' 'big-locust'.
7600   AUDIT  Node           Deleted the 'machine' 'next-mullet'.
3944   AUDIT  Node           Deleted the 'machine' 'sweet-urchin'.
3943   AUDIT  Node           Deleted the 'machine' 'picked-cub'.
3942   AUDIT  Node           Deleted the 'machine' 'next-mullet'.
3941   AUDIT  Node           Deleted the 'machine' 'new-chimp'.
3940   AUDIT  Node           Deleted the 'machine' 'new-bedbug'.
3939   AUDIT  Node           Deleted the 'machine' 'native-moray'.
3938   AUDIT  Node           Deleted the 'machine' 'humble-bug'.
3937   AUDIT  Node           Deleted the 'machine' 'happy-bengal'.
3936   AUDIT  Node           Deleted the 'machine' 'claudio'.
3935   AUDIT  Node           Deleted the 'machine' 'daring-ewe'.
3934   AUDIT  Node           Deleted the 'machine' 'grown-hawk'.
3933   AUDIT  Node           Deleted the 'machine' 'fleet-calf'.
2685   AUDIT  Node           Deleted the 'machine' 'test-lab'.
2684   AUDIT  Node           Deleted the 'machine' 'test'.
2683   AUDIT  Node           Deleted the 'machine' 'Sootie'.
2682   AUDIT  Node           Deleted the 'machine' 'Tigger'.
```

Of course, that's a complex list, so could simplify, sort, remove any duplicates, and prettify the list a bit with already-available tools:

```nohighlight
$ maas $PROFILE events query limit=1000 after=0 level=AUDIT \
| jq -r '(.events[] | [.description]) | @tsv' \
| column -t -s$'\t' \
| grep "Deleted the" \
| cut -f 4 -d" " \
| sort -u | sed -e"s/'//g" | sed -e"s/\.//g"
```

This would give you a list of machines that have been deleted at least once:

```nohighlight
aaa
ace-boxer
ace-molly
active-adder
active-panda
alice
asda
asdas
asdf
awake-dog
big-locust
bob
boss-crab
brief-beetle
brief-pika
carol
choice-worm
choice-wren
claudio
clean-filly
clean-ocelot
cool-dog
crisp-chow
crisp-mammal
cuddly-eft
daring-ewe
eager-kid
eager-whale
epic-robin
equal-dog
fair-puma
fine-dane
finer-akita
firm-ghost
fit-earwig
fit-ram
fleet-calf
fun-boxer
funny-panda
game-elk
gfd
good-martin
grown-hawk
guided-joey
happy-bengal
helloooo
holy-hippo
hostname
huge-yeti
humble-bug
keen-lab
known-kodiak
living-crab
me
meet-snake
mighty-finch
modern-mutt
native-civet
native-koala
native-moray
neat-yak
new-bedbug
new-chimp
next-mullet
nice-skink
picked-cub
prime-walrus
pro-boa
proper-troll
quiet-caiman
rare-ghost
rested-egret
ruling-marlin
sadasd
secret-maas
Sootie
steady-mammal
strong-prawn
subtle-lark
superb-piglet
sure-kid
sweet-krill
sweet-urchin
test
test-lab
thingthing
Tigger
top-burro
unique-weevil
vocal-krill
wanted-fox
wanted-turtle
wired-dove
worthy-ray
you
```

Still a bit long, but using your imagination and additional command line utilities, you could pare this down even more.

The important points for working with audit data are:

- there are filters available to pinpoint several event attributes, limit the number of records, and focus on a specific set of records.
- the native output of `events query` is JSON; if you have good JSON tools handy, you can use those tools to parse the data further.
- if you don't have JSON tools handy, you can always use `jq` to produce workable text output, which you can then manipulate using standard CLI text tools.

There's probably no limit to what you can figure out if you use audit events properly.

* Image deployment

Here's a conceptual view of the way that images get deployed to create a running MAAS machine:

<a href="https://discourse.maas.io/uploads/default/original/2X/4/4bcb44d49eae1238d6cbd3724f2ec7cab6b8acab.jpeg" target = "_blank"><img src="https://discourse.maas.io/uploads/default/original/2X/4/4bcb44d49eae1238d6cbd3724f2ec7cab6b8acab.jpeg"></a>

This is a good place to explain how images work, including the nuances of `cloud-init` and `curtin` configurations.  Let's take it from the top.

Before a machine can be deployed, it must be added to MAAS, commissioned, and allocated.  Machines can be added to MAAS either by [enlistment](/t/how-to-deploy-physical-machines/6193#heading--about-enlistment) or by direct user action.  This machine must then be [commissioned](/t/how-to-deploy-physical-machines/6193#heading--about-commissioning-machines), which establishes the configuration and resources of the machine.  Finally, that machine must be allocated, that is, assigned to the control of one and only one user, so that conflicting commands to the same machine are not possible.  This aggregate step is represented by the green lines in the above diagram.

When MAAS receives the "deploy" command from the user (blue lines), it must first retrieve (from the MAAS DB) the machine info that was gathered about the machine during commissioning (red lines).  MAAS then boots the machine and waits for the machine's firmware to request a bootable "ephemeral" OS.  This ephemeral OS must be one of the images supplied by [the MAAS simplestreams](https://images.maas.io)`↗` and cannot be a custom OS image.

At the point that MAAS must send the ephemeral OS to the machine, MAAS needs to have this OS downloaded to the rack controller, based on earlier selections made by the user (yellow lines).  Assuming that the a suitable simplestreams image is available, MAAS can send it to the machine.  This ephemeral OS is not deployed, but it is used to download and deploy the image you've chosen for the machine.

When the ephemeral OS boots, it immediately spawns [curtin](https://curtin.readthedocs.io/en/latest/topics/overview.html)`↗`.  Curtin's deployment of the target image can be customised with pre-seeds, shown by the brown lines in the diagram.  These pre-seeds control things which are difficult to change once the target OS has been installed (such as [partitioning](http://caribou.kamikamamak.com/2015/06/26/custom-partitioning-with-maas-and-curtin-2/)`↗`.  Things which can be customized after the target image is running are generally configured with [cloud-init](https://cloudinit.readthedocs.io/en/latest/)`↗`, represented by the pink lines.

To continue with the deployment flow, curtin retrieves the target image, either from the rack controller's cache (if the image is standard) or from the MAAS DB (if the image is custom).  Curtin then installs the target image and reboots the machine.  When the target image boots, it retrieves cloud-init configuration either from the MAAS metadata server (proxied by the region controller), or from cloud-init configuration data packed with the target image -- whichever is "closer".

Once cloud-init has finished, the machine is deployed, that is, up and running, ready to perform whatever functions have been assigned to it.

** Key takeaways from this flowchart

The flowchart above is a bit complicated, but there are few key takeaways you should remember:

1. Machines have to be added, either by enlistment or direct user action, before they can be deployed.

2. Machines must be commissioned before deployment, so that MAAS knows what resources the machine has available.

3. Machines must be allocated before deployment, to lock out all other users (so that no command deadlock can occur).

4. You must have selected and downloaded at least one suitable image from the MAAS simplestreams before you can deploy a machine, because this simplestreams image is used to boot the machine from the network, so that the target OS can then be installed on the machine.

5. If you need to customize things about the machine that can't be changed after the target OS is installed (like partitioning drives), you must use curtin pre-seeds to do this.  You must specify these pre-seeds before you start deployment.

6. If you want to customize things about the machine that can be changed after the target OS is installed (like downloading and installing an application),  you must use cloud-init to do this.  You must specify this cloud-init configuration, at the very least, before deployment begins.

7. If you wish to deploy a custom image, you must pack it and upload it to MAAS before deployment begins.

** About boot sources and why they matter

A region controller downloads its images from a boot source. The main characteristics of a boot source are location (URL) and an associated GPG public keyring.

[note]
A boot resource is another name for an image. So boot resources are found within a boot source.
[/note]

MAAS stores images in the region controller's database, from where the rack controller proxies them to the individual machines.  It's important to note that for ESXi images, network configuration includes only these five parameters:

1.   DHCP
2.   Static/auto IP assignments
3.   Aliases
4.   VLANs
5.   Bonds

Bonds are mapped to NIC teaming in only three ways:

1.   balance-rr -- portid
2.   active-backup -- explicit
3.   802.3ad -- iphash, LACP rate and XMIT hash policy settings ignored

MAAS comes configured with a boot source that should suffice for most users:

[`https://images.maas.io/ephemeral-v3/stable/`](https://images.maas.io/ephemeral-v3/stable/)`↗`

The above URL points to the 'stable' stream (for the v3 format). See [Local image mirror](/t/how-to-mirror-images-locally/5927) for some explanation regarding the availability of other streams.

Although the backend supports multiple boot sources, MAAS itself uses a single source. If multiple sources are detected, the web UI will print a warning and will be unable to manage images.

* Images

MAAS provides supported images for stable Ubuntu releases, and for CentOS 7 and CentOS 8.0 releases.  Other images can be [customised](/t/how-to-build-custom-images/5104) for use with MAAS.

** What is a MAAS image?

MAAS images are more than just the operating system kernel.  In fact, a usable MAAS image consists of at least four things:

- a [bootloader](https://images.maas.io/ephemeral-v3/stable/bootloaders/)`↗`, which boots the computer to the point that an operating system can be loaded.  MAAS currently uses one of three types of bootloaders: open firmware, PXE, and UEFI.
- a bootable kernel.
- an initial ramdisk.
- a squashfs filesystem.

If you were to look at the `squashfs.manifest`, you'd see something like this:

```nohighlight
adduser	3.118ubuntu5
apparmor	3.0.4-2ubuntu2.1
apport	2.20.11-0ubuntu82.1
apport-symptoms	0.24
apt	2.4.5
apt-utils	2.4.5
base-files	12ubuntu4.1
base-passwd	3.5.52build1
bash	5.1-6ubuntu1
bash-completion	1:2.11-5ubuntu1
bc	1.07.1-3build1
bcache-tools	1.0.8-4ubuntu3
bind9-dnsutils	1:9.18.1-1ubuntu1.1
bind9-host	1:9.18.1-1ubuntu1.1
bind9-libs:amd64	1:9.18.1-1ubuntu1.1
binutils	2.38-3ubuntu1
binutils-common:amd64	2.38-3ubuntu1
binutils-x86-64-linux-gnu	2.38-3ubuntu1
bolt	0.9.2-1
bsdextrautils	2.37.2-4ubuntu3
bsdutils	1:2.37.2-4ubuntu3
btrfs-progs	5.16.2-1
busybox-initramfs	1:1.30.1-7ubuntu3
busybox-static	1:1.30.1-7ubuntu3
byobu	5.133-1
ca-certificates	20211016
cloud-guest-utils	0.32-22-g45fe84a5-0ubuntu1
cloud-init	22.2-0ubuntu1~22.04.3
cloud-initramfs-copymods	0.47ubuntu1
cloud-initramfs-dyn-netconf	0.47ubuntu1
command-not-found	22.04.0
console-setup	1.205ubuntu3
console-setup-linux	1.205ubuntu3
coreutils	8.32-4.1ubuntu1
cpio	2.13+dfsg-7
cron	3.0pl1-137ubuntu3
```

This snippet gives you a basic idea of the kinds of things that have to be loaded onto a drive in order for the system to function independently.

*** How images get deployed onto a machine

Here's a conceptual view of the way that images get deployed to create a running MAAS machine:

<a href="https://discourse.maas.io/uploads/default/original/2X/4/4bcb44d49eae1238d6cbd3724f2ec7cab6b8acab.jpeg" target = "_blank"><img src="https://discourse.maas.io/uploads/default/original/2X/4/4bcb44d49eae1238d6cbd3724f2ec7cab6b8acab.jpeg"></a>

This is a good place to explain how images work, including the nuances of `cloud-init` and `curtin` configurations.  Let's take it from the top.

Before a machine can be deployed, it must be added to MAAS, commissioned, and allocated.  Machines can be added to MAAS either by [enlistment](/t/how-to-deploy-physical-machines/6193#heading--about-enlistment) or by direct user action.  This machine must then be [commissioned](/t/how-to-deploy-physical-machines/6193#heading--about-commissioning-machines), which establishes the configuration and resources of the machine.  Finally, that machine must be allocated, that is, assigned to the control of one and only one user, so that conflicting commands to the same machine are not possible.  This aggregate step is represented by the green lines in the above diagram.

When MAAS receives the "deploy" command from the user (blue lines), it must first retrieve (from the MAAS DB) the machine info that was gathered about the machine during commissioning (red lines).  MAAS then boots the machine and waits for the machine's firmware to request a bootable "ephemeral" OS.  This ephemeral OS must be one of the images supplied by [the MAAS simplestreams](https://images.maas.io)`↗` and cannot be a custom OS image.

At the point that MAAS must send the ephemeral OS to the machine, MAAS needs to have this OS downloaded to the rack controller, based on earlier selections made by the user (yellow lines).  Assuming that the a suitable simplestreams image is available, MAAS can send it to the machine.  This ephemeral OS is not deployed, but it is used to download and deploy the image you've chosen for the machine.

When the ephemeral OS boots, it immediately spawns [curtin](https://curtin.readthedocs.io/en/latest/topics/overview.html)`↗`.  Curtin's deployment of the target image can be customised with pre-seeds, shown by the brown lines in the diagram.  These pre-seeds control things which are difficult to change once the target OS has been installed (such as [partitioning](http://caribou.kamikamamak.com/2015/06/26/custom-partitioning-with-maas-and-curtin-2/)`↗`.  Things which can be customized after the target image is running are generally configured with [cloud-init](https://cloudinit.readthedocs.io/en/latest/)`↗`, represented by the pink lines.

To continue with the deployment flow, curtin retrieves the target image, either from the rack controller's cache (if the image is standard) or from the MAAS DB (if the image is custom).  Curtin then installs the target image and reboots the machine.  When the target image boots, it retrieves cloud-init configuration either from the MAAS metadata server (proxied by the region controller), or from cloud-init configuration data packed with the target image -- whichever is "closer".

Once cloud-init has finished, the machine is deployed, that is, up and running, ready to perform whatever functions have been assigned to it.

**** Key takeaways from this flowchart

The flowchart above is a bit complicated, but there are few key takeaways you should remember:

1. Machines have to be added, either by enlistment or direct user action, before they can be deployed.

2. Machines must be commissioned before deployment, so that MAAS knows what resources the machine has available.

3. Machines must be allocated before deployment, to lock out all other users (so that no command deadlock can occur).

4. You must have selected and downloaded at least one suitable image from the MAAS simplestreams before you can deploy a machine, because this simplestreams image is used to boot the machine from the network, so that the target OS can then be installed on the machine.

5. If you need to customize things about the machine that can't be changed after the target OS is installed (like partitioning drives), you must use curtin pre-seeds to do this.  You must specify these pre-seeds before you start deployment.

6. If you want to customize things about the machine that can be changed after the target OS is installed (like downloading and installing an application),  you must use cloud-init to do this.  You must specify this cloud-init configuration, at the very least, before deployment begins.

7. If you wish to deploy a custom image, you must pack it and upload it to MAAS before deployment begins.

*** About boot sources and why they matter

A region controller downloads its images from a boot source. The main characteristics of a boot source are location (URL) and an associated GPG public keyring.

[note]
A boot resource is another name for an image. So boot resources are found within a boot source.
[/note]

MAAS stores images in the region controller's database, from where the rack controller proxies them to the individual machines.  It's important to note that for ESXi images, network configuration includes only these five parameters:

1.   DHCP
2.   Static/auto IP assignments
3.   Aliases
4.   VLANs
5.   Bonds

Bonds are mapped to NIC teaming in only three ways:

1.   balance-rr -- portid
2.   active-backup -- explicit
3.   802.3ad -- iphash, LACP rate and XMIT hash policy settings ignored

MAAS comes configured with a boot source that should suffice for most users:

[`https://images.maas.io/ephemeral-v3/stable/`](https://images.maas.io/ephemeral-v3/stable/)`↗`

The above URL points to the 'stable' stream (for the v3 format). See [Local image mirror](/t/how-to-mirror-images-locally/5927) for some explanation regarding the availability of other streams.

Although the backend supports multiple boot sources, MAAS itself uses a single source. If multiple sources are detected, the web UI will print a warning and will be unable to manage images.

** Custom images

MAAS is much more useful when you can upload images that aren't gathered from [the MAAS image repository](http://images.maas.io/)`↗`, deploy them to MAAS-managed machines, and count on them to work properly. But there's a problem: the typical, off-the-shelf ISO image can't just be uploaded to MAAS and deployed to a machine.  For one thing, the machines couldn't write the image to their disks or boot the images once they're there.  For another, any non-standard configuration items (networking, storage, users, added software) wouldn't be loaded.

We can help guide you in preparing ISO images to run on MAAS machines. Usable MAAS images need both a `curtin` hook script (to write and boot the image), and some `cloud-init` meta-data (to configure the image beyond the out-of-the-box experience).  As long as a prepared image meets these requirements, you can successfully upload it to MAAS, deploy it to a machine, and expect it to run properly on that machine.

This article explains a little more about how MAAS images differ from a standard ISO, and what has to happen to make those off-the-shelf ISOs deployable and usable by MAAS.

*** About transforming an ISO

When it comes to creating images for MAAS machines, you can hand-build images, as long as they meet the `curtin` and `cloud-init` requirements; or you can use a third-party tool called  [packer](https://www.packer.io)`↗` to prepare special versions of these images that will work with MAAS.  There are also static Ubuntu images targeted at older MAAS versions (<3.1).  Beyond providing a bit of technical detail here, we won't shepherd you through hand-building images: you're pretty much on your own there.  We will try to help you understand how to create and customise MAAS-friendly images, mostly focusing on packer templates.

We maintain a [git repo](https://github.com/canonical/packer-maas)`↗` of packer templates for a few popular operating systems.  You can check out this graphic of a real, running lab MAAS instance to get an idea:

<a href="https://discourse.maas.io/uploads/default/original/2X/a/a80ed5eb191a798d049cb82fade4ee117f5128fd.png" target = "_blank"><img src="https://discourse.maas.io/uploads/default/original/2X/a/a80ed5eb191a798d049cb82fade4ee117f5128fd.png"></a>

Packer uses templates (built in HCL2) to run different build, provisioning, and post-processing tools that produce an image MAAS can deploy -- one that you can successfully access and use. These tools might be as simple as a shell command, or as specialised as the RedHat `anaconda` installer.  It really just depends on what's needed to prepare an image so that MAAS can deploy it.

We encourage and document custom images -- and help informally as much as we can -- but we're really not able to offer much support.  After all, other OS images are built from code we don't own, and licensed in ways that may or may not be compatible with a MAAS deployment.  For those reasons, among others, we recommend you customise machines using `cloud-init` user_data and/or `curtin` preseed data, whenever you can, instead of creating a custom image.

[note]
That warning bears repeating: While it may be possible to deploy a certain image with MAAS, the particular use case may not be supported by that image’s vendor due to licensing or technical reasons. Canonical recommends that, whenever possible, you should customise machines using `cloud-init` user_data or `curtin` preseed data, instead of creating a custom image.
[/note]

There are two types of custom images we'll explain here: static Ubuntu images (just below) and [packer images](#heading--about-packer).

*** About static Ubuntu images

MAAS provides the capability for you to build a static Ubuntu OS image to deploy with MAAS, using any image-building method you choose.  You can create the image once, with a fixed configuration, and deploy it to many machines.  This fixed configuration can consist of anything that a normal image would contain: users, packages, etc.  This capability is really targeted at older versions of MAAS, but it should work with MAAS of any vintage.

There are five things that we should explain about static Ubuntu images: 

- [About uploading hand-built Ubuntu images](#heading--about-uploading-hand-built-ubuntu-images)
- [How MAAS handles static Ubuntu images](#heading--about-how-maas-handles-these-images)
- [How MAAS boots static Ubuntu images](#heading--about-how-maas-boots-these-images)
- [About configuring deployed machine networking](#heading--about-configuring-deployed-machine-networking)
- [About configuring deployed machine storage](#heading--about-configuring-deployed-machine-storage)
- [About static image metrics](#heading--about-static-image-metrics)

If you're using newer versions of MAAS (>3.0), we recommend choosing packer, since the packer-maas repository already has a built-in Ubuntu image you can customise -- but the choice is yours.

**** About uploading hand-built Ubuntu images

You can upload hand-built Ubuntu images, containing a kernel, bootloader, and a fixed configuration, for deployment to multiple machines.  The image can be built via a tool, such as [packer](https://www.packer.io)`↗`, or build with scripts. You can upload these images to the boot-resources endpoint, where it will then be available for deployment to machines.

At a minimum, this image must contain a kernel, a bootloader, and a `/curtin/curtin-hooks` script that configures the network. A sample can be found in the [packer-maas repos](https://github.com/canonical/packer-maas/tree/master/ubuntu/scripts)`↗`. The image must be in raw img file format, since that is the format MAAS accepts for upload.  This is the most portable format, and the format most builders support. Upon completing the image build, you will upload this img file to the boot-resources endpoint, specifying the architecture for the image.

**** How MAAS handles static Ubuntu images

MAAS will save the image -- in the same way it would save a `tar.gz` file -- in the database.  MAAS can differentiate between custom Ubuntu images and custom non-Ubuntu images, generating appropriate pre-seed configurations for each image type.

MAAS will also recognise the base Ubuntu version, so it can apply the correct ephemeral OS version for installation.  Custom images are always deployed with the ephemeral operating system. The base_image field is used to select the appropriate version of the ephemeral OS to avoid errors. This ensures a smooth deployment later.

**** How MAAS boots static Ubuntu images

When you decide to deploy a machine with your uploaded, custom image, MAAS ensures that the machine receives the kernel, bootloader and root file system provided in the image. The initial boot loader takes over, and boots an ephemeral OS of the same Ubuntu version as the custom image, to reduce the chances of incompatibilities.  Curtin then writes your entire custom image to disk.  Once the custom image is written to disk, it is not modified by MAAS.

Note that custom non-Ubuntu images still use a standard Ubuntu ephemeral OS to boot, prior to installing the non-Ubuntu OS.

**** About configuring deployed machine networking

If you deploy a machine with a custom Ubuntu image, MAAS allows you to configure the deployed machine's networks just like you would for any other MAAS machine.  If you create an interface and assign it to a subnet or static address, this will be reflected in the deployed machine.

For this reason, MAAS also does some initial diagnostics while installing the custom image.  MAAS will detect when a network configuration is not present and abort the installation with a warning.  Essentially, MAAS checks to be sure that `cloud-init` and `netplan` are present in the images written by `curtin`.  If not, MAAS won't deploy the machine with the image.

**** About configuring deployed machine storage

If you deploy a machine with a custom Ubuntu image, you will also want to be able to configure storage, just like you would do with any other machine.  MAAS facilitates changes to the storage configuration.  You can resize the root partition, as well as attaching and formatting any additional block devices you may desire.

**** About static image metrics

As a user, you want to keep track of how many static images are being used, and how many deployed machines are using static images.  The standard MAAS dashboard reflects both of these metrics.

*** About packer

- [About packer dependencies](#heading--about-packer-dependencies)
- [About packer templates](#heading--about-packer-templates)
- [About the image installation process](#heading--about-the-image-installation-process)
- [About packer-created images](#heading--about-packer-created-images)

The [packer documentation](https://www.packer.io/docs)`↗` has an excellent, in-depth discussion of what packer does, how it works, and where it is limited.  Simply put, packer creates OS images that can be uploaded and deployed using MAAS. We can summarise packer with the following linearised flowchart:

<a href="https://discourse.maas.io/uploads/default/original/2X/4/47cb177f4ee2f52ac00c877449770a23cfa0c9b4.jpeg" target = "_blank"><img src="https://discourse.maas.io/uploads/default/original/2X/4/47cb177f4ee2f52ac00c877449770a23cfa0c9b4.jpeg"></a>

We can walk through packer operation like this:

 - A template is created or obtained which drives the packer build.  The [packer-maas](https://github.com/canonical/packer-maas)`↗` repository uses HCL2 templates.

 - The template specifies packer commands and data sources.

 - The template specifies a builder, which creates the MAAS-consumable images.

 - Multiple builds can run in parallel.  Within the MAAS domain, we typically don't set templates up that way, but it is possible to do so.

 - Provisioners spin up a running version of the image and add things that make it usable, like `curtin` hooks, `cloud-init` meta-data to install custom packages, and so on.

 - Post-processors do things to the built image to make it usable, e.g., compressing the file into a `tar.gz` image.

 - Because packer creates a wide-range of load packages, the results are called "artefacts" in packer terminology.  MAAS simply refers to these as "images".

Note that we said this flow is linearised.  You can see that provisioners might need to run before a post-processor creates an uploadable `tar.gz` image.  The actual flow depends on the template, which depends on the OS being customised into an image.  In the parlance of packer, all of these components -- builders, post-processors, provisioners -- are sometimes referred to collectively as "plugins".
    
*** About packer dependencies

Depending upon which image you are building, packer-maas may require various dependencies.  For example, when customising an Ubuntu image, you'd need to install the following dependencies:

 - qemu-utils
 - qemu-system
 - ovmf
 - cloud-image-utils

These dependencies -- and the functionality they provide -- will be explained in the specific image sections which follow.

*** About packer templates

A [packer template](https://www.packer.io/docs/templates)`↗` could just as easily be called a "packer script".  It contains declarations and commands that sequence and configure plugins.  Templates also have built-in functions to help you customise your artefacts. Our packer-maas templates are implemented in HCL2.

Templates are run by the packer `build` command.  Within packer-maas, packer commands (like `build`) are collected into makefiles that prevent you from having to know a lot about how packer works.  Even so, it's beneficial to take a quick tour of how a typical packer template works.  Let's use the [ubuntu-cloudimg](https://github.com/canonical/packer-maas/blob/master/ubuntu/ubuntu-cloudimg.pkr.hcl)`↗` template as a simple example.

[note]
Building workable templates can be extremely difficult. This section is intended to familiarise you with templates and their components so that you can possibly pinpoint bugs in community-provided templates.  If you want to build your own template, you should rely on the [packer documentation](https://www.packer.io/docs)`↗` as your guide.
[/note]

This template builds a customised Ubuntu image with packer:

```nohighlight
packer {
  required_version = ">= 1.7.0"
  required_plugins {
    qemu = {
      version = "~> 1.0"
      source  = "github.com/hashicorp/qemu"
    }
  }
}

variable "ubuntu_series" {
  type        = string
  default     = "focal"
  description = "The codename of the Ubuntu series to build."
}

variable "filename" {
  type        = string
  default     = "custom-cloudimg.tar.gz"
  description = "The filename of the tarball to produce"
}

variable "kernel" {
  type        = string
  default     = ""
  description = "The package name of the kernel to install. May include version string, e.g linux-image-generic-hwe-22.04=5.15.0.41.43"
}

variable "customize_script" {
  type        = string
  description = "The filename of the script that will run in the VM to customize the image."
}

variable "architecture" {
  type        = string
  default     = "amd64"
  description = "The architecture to build the image for (amd64 or arm64)"
}

variable "headless" {
  type        = bool
  default     = true
  description = "Whether VNC viewer should not be launched."
}

variable "http_directory" {
  type        = string
  default     = "http"
  description = "Directory for files to be accessed over http in the VM."
}

variable "http_proxy" {
  type        = string
  default     = "${env("http_proxy")}"
  description = "HTTP proxy to use when customizing the image inside the VM. The http_proxy enviroment is set, and apt is configured to use the http proxy"
}

variable "https_proxy" {
  type        = string
  default     = "${env("https_proxy")}"
  description = "HTTPS proxy to use when customizing the image inside the VM. The https_proxy enviroment is set, and apt is configured to use the https proxy"
}

variable "no_proxy" {
  type        = string
  default     = "${env("no_proxy")}"
  description = "NO_PROXY environment to use when customizing the image inside the VM."
}

variable "ssh_password" {
  type        = string
  default     = "ubuntu"
  description = "SSH password to use to connect to the VM to customize the image. Needs to match the hashed password in user-data-cloudimg."
}

variable "ssh_username" {
  type        = string
  default     = "root"
  description = "SSH user to use to connect to the VM to customize the image. Needs to match the user in user-data-cloudimg."
}

locals {
  qemu_arch = {
    "amd64" = "x86_64"
    "arm64" = "aarch64"
  }
  uefi_imp = {
    "amd64" = "OVMF"
    "arm64" = "AAVMF"
  }
  qemu_machine = {
    "amd64" = "ubuntu,accel=kvm"
    "arm64" = "virt"
  }
  qemu_cpu = {
    "amd64" = "host"
    "arm64" = "cortex-a57"
  }

  proxy_env = [
    "http_proxy=${var.http_proxy}",
    "https_proxy=${var.https_proxy}",
    "no_proxy=${var.https_proxy}",
  ]
}


source "qemu" "cloudimg" {
  boot_wait      = "2s"
  cpus           = 2
  disk_image     = true
  disk_size      = "4G"
  format         = "qcow2"
  headless       = var.headless
  http_directory = var.http_directory
  iso_checksum   = "file:https://cloud-images.ubuntu.com/${var.ubuntu_series}/current/SHA256SUMS"
  iso_url        = "https://cloud-images.ubuntu.com/${var.ubuntu_series}/current/${var.ubuntu_series}-server-cloudimg-${var.architecture}.img"
  memory         = 2048
  qemu_binary    = "qemu-system-${lookup(local.qemu_arch, var.architecture, "")}"
  qemu_img_args {
    create = ["-F", "qcow2"]
  }
  qemuargs = [
    ["-machine", "${lookup(local.qemu_machine, var.architecture, "")}"],
    ["-cpu", "${lookup(local.qemu_cpu, var.architecture, "")}"],
    ["-device", "virtio-gpu-pci"],
    ["-drive", "if=pflash,format=raw,id=ovmf_code,readonly=on,file=/usr/share/${lookup(local.uefi_imp, var.architecture, "")}/${lookup(local.uefi_imp, var.architecture, "")}_CODE.fd"],
    ["-drive", "if=pflash,format=raw,id=ovmf_vars,readonly=on,file=/usr/share/${lookup(local.uefi_imp, var.architecture, "")}/${lookup(local.uefi_imp, var.architecture, "")}_VARS.fd"],
    ["-drive", "file=output-qemu/packer-qemu,format=qcow2"],
    ["-drive", "file=seeds-cloudimg.iso,format=raw"]
  ]
  shutdown_command       = "sudo -S shutdown -P now"
  ssh_handshake_attempts = 500
  ssh_password           = var.ssh_password
  ssh_timeout            = "45m"
  ssh_username           = var.ssh_username
  ssh_wait_timeout       = "45m"
  use_backing_file       = true
}

build {
  sources = ["source.qemu.cloudimg"]

  provisioner "shell" {
    environment_vars = concat(local.proxy_env, ["DEBIAN_FRONTEND=noninteractive"])
    scripts          = ["${path.root}/scripts/cloudimg/setup-boot.sh"]
  }


  provisioner "shell" {
    environment_vars  = concat(local.proxy_env, ["DEBIAN_FRONTEND=noninteractive"])
    expect_disconnect = true
    scripts           = [var.customize_script]
  }

  provisioner "shell" {
    environment_vars = [
      "CLOUDIMG_CUSTOM_KERNEL=${var.kernel}",
      "DEBIAN_FRONTEND=noninteractive"
    ]
    scripts = ["${path.root}/scripts/cloudimg/install-custom-kernel.sh"]
  }

  provisioner "file" {
    destination = "/tmp/"
    sources     = ["${path.root}/scripts/cloudimg/curtin-hooks"]
  }

  provisioner "shell" {
    environment_vars = ["CLOUDIMG_CUSTOM_KERNEL=${var.kernel}"]
    scripts          = ["${path.root}/scripts/cloudimg/setup-curtin.sh"]
  }

  provisioner "shell" {
    environment_vars = ["DEBIAN_FRONTEND=noninteractive"]
    scripts          = ["${path.root}/scripts/cloudimg/cleanup.sh"]
  }

  post-processor "shell-local" {
    inline = [
      "IMG_FMT=qcow2",
      "source ../scripts/setup-nbd",
      "OUTPUT=$${OUTPUT:-${var.filename}}",
      "source ./scripts/cloudimg/tar-rootfs"
    ]
    inline_shebang = "/bin/bash -e"
  }
}
```

You can see that the sections match the typical structure of a `packer` HCL2 template: declarations (variables); a source declaration; and build tools.  We can deconstruct these briefly to understand what the template is doing.  This will help explain the image creation process.

**** Variables (declaration section)

The variables section of this template looks like this:

```nohighlight
variable "ubuntu_series" {
  type        = string
  default     = "focal"
  description = "The codename of the Ubuntu series to build."
}

variable "filename" {
  type        = string
  default     = "custom-cloudimg.tar.gz"
  description = "The filename of the tarball to produce"
}

variable "kernel" {
  type        = string
  default     = ""
  description = "The package name of the kernel to install. May include version string, e.g linux-image-generic-hwe-22.04=5.15.0.41.43"
}

variable "customize_script" {
  type        = string
  description = "The filename of the script that will run in the VM to customize the image."
}

variable "architecture" {
  type        = string
  default     = "amd64"
  description = "The architecture to build the image for (amd64 or arm64)"
}

variable "headless" {
  type        = bool
  default     = true
  description = "Whether VNC viewer should not be launched."
}

variable "http_directory" {
  type        = string
  default     = "http"
  description = "Directory for files to be accessed over http in the VM."
}

variable "http_proxy" {
  type        = string
  default     = "${env("http_proxy")}"
  description = "HTTP proxy to use when customizing the image inside the VM. The http_proxy enviroment is set, and apt is configured to use the http proxy"
}

variable "https_proxy" {
  type        = string
  default     = "${env("https_proxy")}"
  description = "HTTPS proxy to use when customizing the image inside the VM. The https_proxy enviroment is set, and apt is configured to use the https proxy"
}

variable "no_proxy" {
  type        = string
  default     = "${env("no_proxy")}"
  description = "NO_PROXY environment to use when customizing the image inside the VM."
}

variable "ssh_password" {
  type        = string
  default     = "ubuntu"
  description = "SSH password to use to connect to the VM to customize the image. Needs to match the hashed password in user-data-cloudimg."
}

variable "ssh_username" {
  type        = string
  default     = "root"
  description = "SSH user to use to connect to the VM to customize the image. Needs to match the user in user-data-cloudimg."
}

locals {
  qemu_arch = {
    "amd64" = "x86_64"
    "arm64" = "aarch64"
  }
  uefi_imp = {
    "amd64" = "OVMF"
    "arm64" = "AAVMF"
  }
  qemu_machine = {
    "amd64" = "ubuntu,accel=kvm"
    "arm64" = "virt"
  }
  qemu_cpu = {
    "amd64" = "host"
    "arm64" = "cortex-a57"
  }

  proxy_env = [
    "http_proxy=${var.http_proxy}",
    "https_proxy=${var.https_proxy}",
    "no_proxy=${var.https_proxy}",
  ]
}
```

Most of this is straightforward.  We're going to use a base image of Ubuntu 20.04, keeping the HTTP files in directory `http` and making three possible proxy options available: HTTP, HTTPS, or no proxy.  The produced image will have an SSH username and password of "ubuntu".  It's that simple.

The really complicated "builders" section of the old JSON version is replaced by a "source" section that is much cleaner.  Here's the source section of this HCL2 template, with a few comments added for clarity:

```nohighlight
source "qemu" "cloudimg" {
  boot_wait      = "2s"
# SETS UP THE IMAGE FOR A TWO-CPU VIRTUAL/MACHINE:
  cpus           = 2
  disk_image     = true
# SETS UP THE IMAGE TO EXPECT A 4GB DISK:
  disk_size      = "4G"
  format         = "qcow2"
# WHETHER OR NOT THE IMAGE EXPECTS TO RUN HEADLESS, THAT IS, WITHOUT A CONSOLE:
  headless       = var.headless
# THE HTTP DIRECTORY WILL (HOPEFULLY) BE THE USER'S HTTP DIRECTORY:
  http_directory = var.http_directory
# THE CHECKSUM FOR THE ISO IMAGE WILL BE FOUND HERE:
  iso_checksum   = "file:https://cloud-images.ubuntu.com/${var.ubuntu_series}/current/SHA256SUMS"
# THE ISO IMAGE ITSELF WILL BE FOUND AT THIS URL:
  iso_url        = "https://cloud-images.ubuntu.com/${var.ubuntu_series}/current/${var.ubuntu_series}-server-cloudimg-${var.architecture}.img"
# THE IMAGE SHOULD EXPECT THIS MUCH MEMORY:
  memory         = 2048
  qemu_binary    = "qemu-system-${lookup(local.qemu_arch, var.architecture, "")}"
  qemu_img_args {
    create = ["-F", "qcow2"]
  }
# IF YOU STUDY THE QEMU DOCUMENTATION, IT'S FAIRLY EASY TO SEE WHAT THESE ARGS DO:
  qemuargs = [
    ["-machine", "${lookup(local.qemu_machine, var.architecture, "")}"],
    ["-cpu", "${lookup(local.qemu_cpu, var.architecture, "")}"],
    ["-device", "virtio-gpu-pci"],
    ["-drive", "if=pflash,format=raw,id=ovmf_code,readonly=on,file=/usr/share/${lookup(local.uefi_imp, var.architecture, "")}/${lookup(local.uefi_imp, var.architecture, "")}_CODE.fd"],
    ["-drive", "if=pflash,format=raw,id=ovmf_vars,readonly=on,file=/usr/share/${lookup(local.uefi_imp, var.architecture, "")}/${lookup(local.uefi_imp, var.architecture, "")}_VARS.fd"],
    ["-drive", "file=output-qemu/packer-qemu,format=qcow2"],
    ["-drive", "file=seeds-cloudimg.iso,format=raw"]
  ]
# HERE'S THE SHUTDOWN COMMAND TO USE:
  shutdown_command       = "sudo -S shutdown -P now"
# HERE'S HOW MANY TIMES YOU TRY SSH:
  ssh_handshake_attempts = 500
# HERE'S HOW YOU GATHER THE SSH PASSWORD:
  ssh_password           = var.ssh_password
# USE A REALLY LONG SSH WAIT TIMEOUT:
  ssh_timeout            = "45m"
# HERE'S HOW YOU GATHER THE SSH USERNAME:
  ssh_username           = var.ssh_username
# USE A REALLY LONG SSH TIMEOUT, TOO:
  ssh_wait_timeout       = "45m"
  use_backing_file       = true
}
```

The high number of SSH handshake attempts -- and the really long timeouts -- have to do with trying to catch the system after it has successfully booted.  Because of the way packer works, it has no direct way to be informed that the system has booted.  As a consequence, to finish the build and run provisioners and post-processors, packer has to keep trying for a while until an SSH connection is successful.  In practice, this should only take 2-3 minutes, but this template uses very long values, just to be sure.

**** Build section

The build section of this template lays out the tools that will build the packed image:

```nohighlight
build {
  sources = ["source.qemu.cloudimg"]

  provisioner "shell" {
    environment_vars = concat(local.proxy_env, ["DEBIAN_FRONTEND=noninteractive"])
    scripts          = ["${path.root}/scripts/cloudimg/setup-boot.sh"]
  }


  provisioner "shell" {
    environment_vars  = concat(local.proxy_env, ["DEBIAN_FRONTEND=noninteractive"])
    expect_disconnect = true
    scripts           = [var.customize_script]
  }

  provisioner "shell" {
    environment_vars = [
      "CLOUDIMG_CUSTOM_KERNEL=${var.kernel}",
      "DEBIAN_FRONTEND=noninteractive"
    ]
    scripts = ["${path.root}/scripts/cloudimg/install-custom-kernel.sh"]
  }

  provisioner "file" {
    destination = "/tmp/"
    sources     = ["${path.root}/scripts/cloudimg/curtin-hooks"]
  }

  provisioner "shell" {
    environment_vars = ["CLOUDIMG_CUSTOM_KERNEL=${var.kernel}"]
    scripts          = ["${path.root}/scripts/cloudimg/setup-curtin.sh"]
  }

  provisioner "shell" {
    environment_vars = ["DEBIAN_FRONTEND=noninteractive"]
    scripts          = ["${path.root}/scripts/cloudimg/cleanup.sh"]
  }

  post-processor "shell-local" {
    inline = [
      "IMG_FMT=qcow2",
      "source ../scripts/setup-nbd",
      "OUTPUT=$${OUTPUT:-${var.filename}}",
      "source ./scripts/cloudimg/tar-rootfs"
    ]
    inline_shebang = "/bin/bash -e"
  }
}
```

Rather than walking through each of these lines individually, we can just note that this HCL2 causes packer to:

 - retrieve scripts that set up the bootloader, configure curtin hooks, and install custom packages from a named gzip source.
 - set the homedir and proxy options for the image.
 - set up curtin, networking, and maybe storage for the image.
 - clean up the image prior to post-processing.

The post-processing section of this template prepares the image for use:

```nohighlight
  post-processor "shell-local" {
    inline = [
      "IMG_FMT=qcow2",
      "source ../scripts/setup-nbd",
      "OUTPUT=$${OUTPUT:-${var.filename}}",
      "source ./scripts/cloudimg/tar-rootfs"
    ]
    inline_shebang = "/bin/bash -e"
  }
```

You can see right away that this template has one post-processor (only one `post-processor` entry).  This post-processor is a local shell, invoked with the `-e` option, which causes the shell to terminate if there's an error (rather than continuing with the next command).  In this case, we can see that the shell runs four commands:

 - sets `$IMG_FMT` to "qcow2"
 - runs the script `setup-nbd`
 - sets $OUTPUT to "<name of image>-custom-cloudimg.tar.gz"
 - runs the script `tar-rootfs`

In this case, it's worth a quick look at the two scripts to see what this post-processor does. First, let's glance at `setup-nbd`:

```highlight
#!/bin/bash -e
#
# setup-nbd - Bind Packer qemu output to a free /dev/nbd device.
#
# Author: Lee Trager <lee.trager@canonical.com>
#
# Copyright (C) 2020 Canonical
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as
# published by the Free Software Foundation, either version 3 of the
# License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with this program.  If not, see <http://www.gnu.org/licenses/>.

if [ $UID -ne 0 ]; then
    echo "ERROR: Must be run as root!" >&2
    exit 1
fi

if [ ! -f output-qemu/packer-qemu ]; then
    echo "ERROR: Not in the same path as template!" >&2
    exit
fi

echo 'Loading nbd...'
shopt -s extglob
modprobe nbd
for nbd in /sys/class/block/nbd+([0-9]); do
    if [ "$(cat ${nbd}/size)" -eq 0 ]; then
	nbd="/dev/$(basename $nbd)"
	echo "Using $nbd"
	break
    fi
done

if [ -z "${nbd}" ] || ! echo $nbd | grep -q "/dev"; then
    echo "ERROR: Unable to find nbd device to mount image!" >&2
    exit 1
fi

echo "Binding image to $nbd..."
qemu-nbd -d $nbd
if [ -n "$IMG_FMT" ]; then
    qemu-nbd -c $nbd -f "$IMG_FMT" -n output-qemu/packer-qemu
else
    qemu-nbd -c $nbd -n output-qemu/packer-qemu
fi
echo 'Waiting for partitions to be created...'
tries=0
while [ ! -e "${nbd}p1" -a $tries -lt 60 ]; do
    sleep 1
    tries=$((tries+1))
done
```

As you can see, this is just a well-structured script to export a QEMU image as a Network Block Device, binding it to a `/dev/nbd` directory.  This is first step in creating MAAS-loadable Ubuntu image.  The second step comes in `tar-rootfs`:

```highlight
#!/bin/bash -e
#
# tar-rootfs - Create a tar.gz from a binded /dev/nbd device
#
# Author: Alexsander de Souza <alexsander.souza@canonical.com>
#
# Copyright (C) 2021 Canonical
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as
# published by the Free Software Foundation, either version 3 of the
# License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with this program.  If not, see <http://www.gnu.org/licenses/>.

cleanup() {
    qemu-nbd -d "$nbd"
    [ -d "${TMP_DIR}" ] && rm -rf "${TMP_DIR}"
}
trap cleanup EXIT

if [ ${UID} -ne 0 ]; then
    echo "ERROR: Must be run as root!" >&2
    exit 1
fi

TMP_DIR=$(mktemp -d /tmp/packer-maas-XXXX)

echo 'Mounting root partition...'
mount "${nbd}p2" "${TMP_DIR}"
mount "${nbd}p1" "${TMP_DIR}/boot/efi"

echo "Creating MAAS image $OUTPUT..."
tar -Sczpf "$OUTPUT" --acls --selinux --xattrs -C "${TMP_DIR}" .

echo 'Unmounting image...'
umount "${TMP_DIR}/boot/efi"
umount "${TMP_DIR}"
```

This script just creates a `.tar.gz` from a bound `/dev/nbd` device (where the QEMU image was initially stored by the last script.

As you can see, the process of creating a customised packer image is not overly complex.  Nevertheless, it's a difficult process to get right, hence our community-contributed templates.

*** About the image installation process

Installing a packer-created image is highly dependent on the application.  In the case of MAAS, we use the CLI `boot-resources` command to upload the image to MAAS, something like this:

```nohighlight
$ maas admin boot-resources create \
    name='custom/ubuntu-tgz' \
    title='Ubuntu Custom TGZ' \
    architecture='amd64/generic' \
    filetype='tgz' \
    content@=custom-ubuntu.tar.gz
```

At this point, the image shows up in MAAS, synced to the controller, the same as any other image.

*** About packer-created images

If you're more interested in the anatomy of a packer-created image, for example, an ISO image, you can use `isoinfo` to explore the image file.  The image should be found in the packer git repository, under `<imagename>/packer-cache`.  Ideally, it shouldn't differ too much from any other customised ISO image.  You can explore with a few of the `isoinfo` commands.  For example, you can read the primary volume descriptor like this:

```nohighlight
stormrider@neuromancer:~/mnt/Dropbox/src/git/packer-maas/ubuntu/packer_cache$ isoinfo -d -i ubuntu.iso | more                                                         
CD-ROM is in ISO 9660 format
System id: 
Volume id: Ubuntu-Server 20.04.4 LTS amd64
Volume set id: 
Publisher id: 
Data preparer id: XORRISO-1.2.4 2012.07.20.130001, LIBISOBURN-1.2.4, LIBISOFS-1.2.4, LIBBURN-1.2.4
Application id: 
Copyright File id: 
Abstract File id: 
Bibliographic File id: 
Volume set size is: 1
Volume set sequence number is: 1
Logical block size is: 2048
Volume size is: 650240
El Torito VD version 1 found, boot catalog is in sector 250
Joliet with UCS level 3 found
Rock Ridge signatures version 1 found
Eltorito validation header:
    Hid 1
    Arch 0 (x86)
    ID ''
    Key 55 AA
    Eltorito defaultboot header:
        Bootid 88 (bootable)
        Boot media 0 (No Emulation Boot)
        Load segment 0
        Sys type 0
        Nsect 4
        Bootoff 8EC04 584708
```

You could also generate an exhaustive directory listing with `isoinfo -f -i <isoname>`, and possibly pipe that through `grep` to ensure that your desired packages have been added to the image.  Or, if you prefer to sweep the image directories manually, you can use `isoinfo -l -i <isoname>`.  The larger point, of course, is that a packer-generated image is essentially identical to any prepared ISO image, including, of course, any customisations (e.g., extra software) that the template loads before finalising the image.

** Image streams

Canonical provides two SimpleStreams for MAAS images: candidate and stable. Both streams contain Ubuntu images, CentOS images, bootloaders extracted from the Ubuntu archive, and release notifications. Either stream can be used in any version of MAAS greater than 2.1 -- but not all images are supported in older versions.

*** The candidate stream

The candidate stream contains images and bootloaders which have not been explicitly tested with MAAS. Canonical's automated build process dumps all images and bootloaders here before they are tested with MAAS. This stream is useful when testing a bug fix before an image or bootloader has been promoted to stable. Think of the candidate stream as a preview: it should never be used in a production environment; and users are encouraged to provide feedback on any issues they find with this stream.

This stream is available [here](http://images.maas.io/ephemeral-v3/candidate)`↗`.

*** The stable stream

The stable stream contains images and bootloaders which have been tested with the latest version of MAAS. This is the default stream which should be used in production environments.  This stream is available [here](http://images.maas.io/ephemeral-v3/stable)`↗`.

*** The retired daily stream

Previously there was only one MAAS stream available, daily. This stream has been replaced by the stable stream. Any client using this stream will be automatically redirected to the stable stream.


* Local mirrors

* LXD VM hosts

[tabs]
[tab version="v3.4 Snap,v3.4 Packages,v3.3 Snap,v3.3 Packages,v3.2 Snap,v3.2 Packages,v3.1 Snap,v3.1 Packages,v3.0 Snap,v3.0 Packages" view="UI"]
** About LXD VM host authentication

MAAS 3.1 provides a smoother experience when connecting an existing LXD server to MAAS, guiding the user through manual steps and providing increased connection security with use of certificates. Currently, each MAAS region/rack controller has its own certificate. To add a LXD VM host to MAAS, the user needs to either add the certificate for each controller that can reach the LXD server to the trust list in LXD, or use the trust_password (in which case the controller talking to LXD will automatically add its certificate to the trust).

This doesn’t provide a great user experience, as the former process is cumbersome, and the latter is not suggested for production use for security reasons.  To improve this, MAAS 3.1 manages per-LXD keys/certificates, and provide a way for users to get the content of certificates, to authorise MAAS in LXD.

** About on-the-spot certificate creation

As a MAAS user, you want to register a LXD host into MAAS using certificates for authentication -- to follow LXD production deployment best practices.  The standard way for clients to authenticate with LXD servers is through certificates. The use of trust_password is *only* meant as a way to interact for initial setup.

While prior versions of MAAS support both ways of authentication (and automatically adds the certificate for the rack talking to LXD when registering the VM host), the user experience is lacking, since there's no control over the certificate being used.  In addition, each rack uses a different certificate, making it hard to manage scenarios where multiple racks can connect to a LXD server.

For these reasons, when adding a LXD host, MAAS 3.1 provides a way to generate a secret key and certificate pair to use specifically for that server, and show the certificate to the user, so that they can add it to the LXD server trust list.  The user experience changes to something like the following:

- MAAS generates a secret key and certificate pair for use with a LXD server.
- The user can see the certificate and is guided to add it to the LXD server trust list.
- The user can easily complete the registration of the LXD server once the certificate is trusted in LXD.
- All racks use the same key when talking to the LXD server. 
- If a new rack controller is added, it can communicate with the LXD server out of the box.
- If the trust password is used, it’s not stored in MAAS persistently.
- It’s possible to get the certificate for a LXD server from a URL (e.g. for curl use).

** About bringing your own certificates

As a MAAS user, you may want to register a LXD host into MAAS by providing a private key for a certificate that’s already trusted by the LXD server.  For example, you may already have set up certificates in the server trust for MAAS to use, MAAS should provide a way to import it, instead of generating a new one.

With MAAS 3.1, it’s possible to import an existing key/certificate pair for use with a LXD server when registering it with MAAS.  MAAS stores the key/certificate instead of generating new ones.

[note]
The imported key must not have a passphrase; otherwise, MAAS will not be able to use it.
[/note]

** About LXD VM host project summaries

Each LXD VM host provides a "Project" tab that summarises the current state of the LXD KVM:

<a href="https://discourse.maas.io/uploads/default/original/2X/e/e0cc264a17d67f9530ff8c2ef2bb9522fed0749a.png" target = "_blank"><img src="https://discourse.maas.io/uploads/default/original/2X/e/e0cc264a17d67f9530ff8c2ef2bb9522fed0749a.png"></a>

This tab identifies the project, shows its current resource state, and provides the ability to select existing VM hosts and perform specific actions on them -- as well as being able to compose new VMs on the spot.

** About LXD VM host resource details

This tab presents a summary of the LXD VM host's resource usage:

<a href="https://discourse.maas.io/uploads/default/original/2X/d/d67cf384d6fe903274893eb50a098518d2c1295d.png" target = "_blank"><img src="https://discourse.maas.io/uploads/default/original/2X/d/d67cf384d6fe903274893eb50a098518d2c1295d.png"></a>

The only interactive option on this tab allows you to map or unmap resource usage to NUMA nodes.

** About VM host settings

VM hosts have several settings. Modify these by selecting the 'Settings' tab and editing items directly. Options include a VM host's address, password, network zone, resource pool, and memory and CPU overcommit sliders.

<a href="https://discourse.maas.io/uploads/default/original/2X/2/253afc122d61145be656bb5c3811f9b6c6caa708.png" target = "_blank"><img src="https://discourse.maas.io/uploads/default/original/2X/2/253afc122d61145be656bb5c3811f9b6c6caa708.png"></a>
[/tab]
[tab version="v3.4 Snap,v3.4 Packages,v3.3 Snap,v3.3 Packages,v3.2 Snap,v3.2 Packages,v3.1 Snap,v3.1 Packages,v3.0 Snap,v3.0 Packages" view="CLI"]
Please use the UI interface to interact with LXD VM hosts, by selecting "UI" from the dropdown above.
[/tab]
[tab version="v2.9 Snap,v2.9 Packages view="UI,CLI"]
LXD VM hosts are not available in MAAS version 2.9.  Please upgrade to MAAS version 3.0 or greater to access this functionality.

** About LXD (Beta) vs. libvirt

Libvirt KVMs and LXD VMs are both based on the same underlying virtualisation technology, QEMU. Unlike libvirt KVMs, though, LXD VMs can be managed without requiring SSH access to the VM host. LXD are remotely accessed via secure HTTP transport, which provides better security for LXD-based VMs. In addition, LXD has a better API, and is part of a much larger constellation of enterprise software, offering a wider range of future features and use cases.
[/tab]
[/tabs]

* MAAS

MAAS is an open-source tool that lets you create a data centre from bare-metal servers. You can discover, commission, deploy, and dynamically reconfigure a large network of individual units.  MAAS converts your hardware investment into a cohesive, flexible, distributed data centre, with a minimum of time and effort.

This section will answer a few questions:

- [What is MAAS?](#heading--what-is-maas)
- [What does MAAS offer me?](#heading--what-maas-offers)
- [Can MAAS co-locate key components to conserve resources?](#heading--colocation-of-key-components)

** What is MAAS?

MAAS expands to "Metal As A Service" -- it converts bare-metal servers into cloud instances of virtual machines. There is no need to manage individual units. You can quickly provision or destroy machines, as if they were instances hosted in a public cloud like Amazon AWS, Google GCE, or Microsoft Azure.

MAAS can act as a standalone PXE/preseed service or integrate with other technologies. It works exceptionally well with [Juju](https://juju.is/docs/olm/maas)`↗`, the service and model management tool. MAAS manages the machines and Juju manages the services running on those machines -- a perfect arrangement.  Virtual machines (VMs)`↗` can even act as MAAS machines if they boot from the network via PXE.

<a href="https://discourse.maas.io/uploads/default/original/1X/d19eff9ef45c554d085ee1d657e4ddd810eac6df.jpeg" target="_blank"><img src="https://discourse.maas.io/uploads/default/original/1X/d19eff9ef45c554d085ee1d657e4ddd810eac6df.jpeg"></a>

<details><summary>Tell me about PXE booting</summary>

PXE stands for "Preboot Execution Environment," usually pronounced "pixie."  The term refers to a way of booting an OS image (or other software assembly) downloaded to a client via a NIC.  The NIC must be PXE-capable for this to work.  Many NICs can be configured to support PXE boot with a software switch.

</details>

** What MAAS offers

MAAS can manage a large number of physical machines by merging them into user-defined resource pools. MAAS automatically provisions participating machines and makes them available for use. You can return unused machines to the assigned pool at any time.

[tabs]
[tab version="v3.4 Snap,v3.4 Packages,v3.3 Snap,v3.3 Packages,v3.2 Snap,v3.2 Packages,v3.1 Snap,v3.1 Packages,v3.0 Snap,v3.0 Packages"]
MAAS also discovers all of the USB and PCI devices attached to your physical or virtual machines, and allows you to delete them from the machine's visible configuration, prior to deployment, if you so desire.
[/tab]
[tab version="v2.9 Snap,v2.9 Packages"]
[note]
MAAS 2.9 does not discover USB and PCI devices.  This feature is available from MAAS version 3.0.
[/note]
[/tab]
[/tabs]

[tabs]
[tab version="v3.4 Snap,v3.4 Packages,v3.3 Snap,v3.3 Packages,v3.2 Snap,v3.2 Packages,v3.1 Snap,v3.1 Packages,v3.0 Snap,v3.0 Packages,v2.9 Snap,v2.9 Packages" view="UI"]
** A smooth system-management experience

MAAS integrates all the tools you need into a smooth system-management experience. It includes the following thirteen features:

1. web UI (optimised for mobile devices)
2. Ubuntu, CentOS, Windows, and RHEL installation support
3. open-source IP address management (IPAM)
4. full API/CLI support
5. high availability (optional)
6. IPv6 support
7. inventory of components
8. DHCP and DNS for other devices on the network
9. DHCP relay integration
10. VLAN and fabric support
11. NTP for the entire infrastructure
12. hardware testing
13. composable hardware support

These tools can be controlled from a responsive web UI.  You can easily (re)configure and scale your data centre with MAAS.

<a href="https://discourse.maas.io/uploads/default/original/1X/00968a71b82ce01c45ae3b345ed6b1270d0927bf.jpeg" target = "_blank"><img src="https://discourse.maas.io/uploads/default/original/1X/00968a71b82ce01c45ae3b345ed6b1270d0927bf.jpeg"></a> 

[/tab]
[tab version="v3.4 Snap,v3.4 Packages,v3.3 Snap,v3.3 Packages,v3.2 Snap,v3.2 Packages,v3.1 Snap,v3.1 Packages,v3.0 Snap,v3.0 Packages,v2.9 Snap,v2.9 Packages" view="CLI"]
** A smooth system-management experience

MAAS integrates all the tools you need into a smooth system-management experience. It includes the following eleven features:

1. Ubuntu, CentOS, Windows, and RHEL installation support
2. open-source IP address management (IPAM)
3. high availability (optional)
4. IPv6 support
5. inventory of components
6. DHCP and DNS for other devices on the network
7. DHCP relay integration
8. VLAN and fabric support
9. NTP for the entire infrastructure
10. hardware testing
11. composable hardware support


<a href="https://discourse.maas.io/uploads/default/original/1X/40fdae53957095e5a830458dc5c7a62ea5d78c10.jpeg" target = "_blank"><img src="https://discourse.maas.io/uploads/default/original/1X/40fdae53957095e5a830458dc5c7a62ea5d78c10.jpeg"></a> 

[/tab]
[/tabs]

MAAS works with any system configuration tools. Both the [Chef](https://www.chef.io/chef)`↗` and [Juju](https://jaas.ai/)`↗` teams recommend MAAS as a physical provisioning system.

[note]
Please note that Windows and RHEL images require [Ubuntu Advantage](https://www.ubuntu.com/support)`↗` to work correctly with MAAS.
[/note]

** Colocation of key components

MAAS relies on two key components: the *region controller* and the *rack controller*. The region controller handles operator requests; the rack controller provides high-bandwidth services to multiple racks. In essence, rack controllers manage racks, while the region controller manages the data centre. We generally recommended installing both controllers on the same system.  The default MAAS install delivers this co-located configuration automatically. This all-in-one solution also provides DHCP. 

See [Concepts and terms](/t/maas-glossary/5416#heading--controllers) for a deeper understanding of these components. Note that in special cases, such as [high availability or load balancing](/t/how-to-enable-high-availability/5120), you will want to install multiple region and rack controllers.  You should also review your existing network design to determine whether [MAAS-managed DHCP](/t/how-to-enable-dhcp/5132) will cause problems.

<a href="https://discourse.maas.io/uploads/default/original/1X/3ad2b128fbc034e9f575f21c0415a6e6c55baea3.jpeg" target = "_blank"><img src="https://discourse.maas.io/uploads/default/original/1X/3ad2b128fbc034e9f575f21c0415a6e6c55baea3.jpeg"></a>

* MAAS and Ansible
A user should probably have a solid grasp of the Ansible standard terminology:

- Playbooks / plays
- Hosts and groups
- Inventory

[Ansible](https://www.redhat.com/en/technologies/management/ansible/what-is-ansible)`↗` is a sophisticated IT automation tool that allows users to set up [playbooks](https://docs.ansible.com/ansible/latest/getting_started/get_started_playbook.html)`↗`, which automate complex, repetitive (or error-prone)`↗` setup activities.  While we won't provide a detailed tutorial on Ansible here, there is a bit of terminology you should master before trying to use Ansible with MAAS:

- **[Modules](https://docs.ansible.com/ansible/latest/network/getting_started/basic_concepts.html#modules)`↗`** are binaries (or even pieces of code)`↗` that Ansible can run on a managed node.  These modules can be grouped into named collections.
- **[Tasks](https://docs.ansible.com/ansible/latest/network/getting_started/basic_concepts.html#tasks)`↗`** are individual operations with one or more modules; each task generally accomplishes some otherwise-human-driven function, such as "partition and format /sda".
- **[Plays](https://docs.ansible.com/ansible/latest/network/getting_started/basic_concepts.html#plays)`↗`** are sequences of tasks that Ansible will execute to accomplish larger operations, e.g., "install the OS" ==> "partition and format /sda", "install binary x.7.iso", etc.
- **[Playbooks](https://docs.ansible.com/ansible/latest/getting_started/get_started_playbook.html)`↗`** are YAML files that run plays in a specific order; for example, "install the OS", "install MAAS", "create a region controller", "sync images", etc.
- **[Inventory](https://docs.ansible.com/ansible/latest/network/getting_started/basic_concepts.html#id3)`↗`** is a source(s)`↗` of managed nodes; also called a "hostfile".

These simple descriptions do not fully explain the terms, so it is worthwhile to consult the referenced links, if necessary, before proceeding.  You will also want to understand how Ansible uses the terms "[hosts and groups](https://docs.ansible.com/ansible/latest/user_guide/intro_patterns.html)`↗`", since these are applied somewhat differently than we use them in MAAS.

Playbooks are available to automate the setup for:

- [MAAS region controllers](#heading--MAAS-region-controller): install and configure a MAAS region on a targeted host; running the playbook on hosts with a MAAS region will upgrade it.

- [MAAS rack controllers](#heading--MAAS-rack-controller): install and configure a MAAS rack.

- [MAAS de-installation](#heading--MAAS-de-installation): remove MAAS from a targeted host.

- [MAAS high availability](#heading--MAAS-high-availability): install and configure the HA proxy.

- [PostgreSQL primary role](#heading--PostgreSQL-primary-role): setup the postgres primary role.

- [PostgreSQL secondary role](#heading--PostgreSQL-secondary-role): setup the postgres secondary role.

- [Firewall rules](#heading--Firewall-rules): setup firewall rules.

MAAS Playbooks will eventually be available through Ansible Galaxy.

There is also a set of groups that will automate setting up specific sections of MAAS.  For example, there is a PostgreSQL group that sets up the primary and secondary PostgreSQL roles, bypassing the need to run both playbooks individually.  These groups include:

- [PostgreSQL role bundling scripts](#heading--PostgreSQL-role-bundling-scripts)
- ?? (there must be more, huh?)

After installing ansible, running each of the playbooks on a blank machine will have a fresh install of MAAS ready to go. For example, running the region+rack will setup a region+rack on the host machine.

** Running a MAAS Ansible playbook

In general terms, you can run any of the MAAS Ansible plays with a command of this form:

```nohighlight
ansible-playbook -i hosts \
--extra_vars \
"maas_version=$MAAS_VERSION 
maas_postgres_password=$MAAS_PG_PASSWORD 
maas_postgres_replication_password=$MAAS_PG_REP_PASSWORD 
maas_installation_type=<deb|snap> 
maas_url=$MAAS_URL" \
./site.yaml
```

A command of this form will run all of the plays below (i.e., the entire playbook).  If you want to run the tasks for one particular role (or roles), you can use the form  `--tags <target role(s)>` to limit which parts of the MAAS Ansible playbook run.  Consult the Ansible documentation for more details on additional options and command structure.

** MAAS region controller

As an operator, you want want to install a MAAS region controller onto a given host using Ansible.  To accomplish this, you must:

- [set a maas_region_controller](#heading--Setting-the-maas_region_controller-role) role on a given host, 
- [run the region controller playbook](#heading--Running-the-region-controller-playbook),
- and [find the newly-configured region controller](#heading--Finding-the-new-region-controller) present on that host .

<a href="#heading--Setting-the-maas_region_controller-role">*** id="heading--Setting-the-maas_region_controller-role">Setting the maas_region_controller role

To attach roles to hosts, a user adds each role to their [Inventory file](https://docs.ansible.com/ansible/latest/user_guide/intro_inventory.html#inventory-basics-formats-hosts-and-groups)`↗` in the form of either an INI or a YAML file where each role is followed by the addresses of each host to attach the role to. The example  below attaches the region controller role to a host running on `10.10.0.20` with the user `ubuntu`:*

INI:
```INI
[maas_region_controller]
10.10.0.20 ansible_user=ubuntu
```
YAML:
```YAML
all:
  maas_region_controller:
    hosts:
      10.10.0.20:
        ansible_user: ubuntu
```

*** Running the region controller playbook

When running the playbook for a host with the maas_region_controller role, the playbook installs the MAAS region controller.  The documented ansible variable `maas_installation_type` provides the user with the ability to set whether it’s a deb installation or a snap installation, along with additional variables for MAAS version, snap channel and/or PPA. 

The default installation is a snap.  A successful run of the playbook should give the operator an accessible and ready MAAS instance.  Some notes on installation:

- The installed region controller is used to set a maas_url variable when there is not one already set for later Rack Controller configuration use.
- The operator can optionally enable TLS.
- The playbook sets up the admin user.
- The playbook adds any provided preseeds.
- The playbook only installs the maas-region-api deb if the operator chooses the deb installation.
- Once the region controller is installed, the playbook will run migrations using the configured postgresql primary instance.
- Running on an already configured machine -- but with a new version -- should upgrade the instance.
- The operator can override the postgres DSN variable on any machine (hence not setting `maas_postgres_primary` or `maas_postgres_secondary`) to use an existing PostgreSQL instance not managed by this playbook.
- Optionally, the user can install a grafana agent by setting the variable `install_metrics=true` either in the hosts or on the command line.

`[MAAS_Region_Controller]` variables:
```
maas_version: "latest"          # The version of MAAS to install on the host
maas_installation_type: "snap"  # The installation manager to use
maas_snap_channel: "stable"     # The snap channel, if using snap
maas_url: $Ip_Address           # The url of the database for this MAAS
enable_tls: false               # Whether TLS should be enabled for this MAAS
install_metrics: false          # Whether metrics should be enabled for this MAAS

# Details for the administrative account
admin_username: "admin"
admin_password: "admin"
admin_email: "admin@email.com"
admin_id: "admin"
```
These variables can be defined in the `Hosts` file above, or at the command line using the `--extra_vars` argument.

The playbook uses an ansible variable to determine what version of MAAS to deploy.  The playbook won’t execute (i.e “skipped” in the context of Ansible) if `host_vars` show the Ubuntu version is incompatible with the version and install method.  The Region Controller tasks should be able to execute on multiple hosts in a single execution if the target is an Ansible Group rather than a single host.

*** Finding the new region controller

The newly-installed region controller should be accessible at the specified host ip address, as though the controller had been installed manually.

** MAAS rack controller

As an operator, you want to install a MAAS rack controller to a given host, using Ansible. To accomplish this, you must: 

- [set a maas_rack_controller role](#heading--Setting-the-maas_rack_controller-role) on a given host, 
- [run the playbook](#heading--Running-the-rack-controller-playbook), 
- and [find the newly-configured rack controller](#heading--Finding-the-new-rack-controller), now present on the host.

<a href="#heading--Setting-the-maas_rack_controller-role">*** id="heading--Setting-the-maas_rack_controller-role">Setting the maas_rack_controller role

Assigning a host the `maas_rack_controller` role is straightforward. The role is given a set of hosts in the `hosts` file of the ansible configuration:

INI
```INI
[$role]
$Host_Ip_Address extra_variable=$Variable_Value
$Second_Host_Ip
```
YAML
```
all:
  $role:
    hosts:
      $Host_Ip_Address:
        extra_variable: $Variable_Value
      $Second_Host_Ip
```

*** Running the rack controller playbook

When running the playbook for a host with the `maas_rack_controller` role, the playbook installs the MAAS Rack Controller. The `maas_url` variable is used to connect the Region Controller(s), either previously configured from a Region Controller install task, or provided by the user. If the `maas_url` variable is not set, the Rack Controller tasks are “skipped”.  Some notes about the installation:

- The operator can optionally enable TLS.
- The rack controller tasks should be able to execute on multiple hosts in a single execution if an Ansible Group is targeted rather than a single host.
- Only install the maas-rack-controller deb if using the deb installation.
- Running on an already configured machine but with a new version should upgrade the instance.
- You can optionally install a grafana agent by setting the `install_metrics` variable to `true` either in the hosts file or at the command line.

`[MAAS_Rack_Controller]` Variables
```bash
maas_version: "latest"          # The version of MAAS to install on the host
maas_installation_type: "snap"  # The installation manager to use
maas_snap_channel: "stable"     # The snap channel, if using snap
maas_url: $Ip_Address           # The url of the database for this MAAS
maas_rack_secret:               # The secret used to enroll a MAAS rack
enable_tls: false               # Whether TLS should be enabled for this MAAS
install_metrics: false          # Whether metrics should be enabled for this MAAS
```

*** Finding the new rack controller

The rack controller should be accessible at the specified host IP address, just as if you had installed it there manually.

*** MAAS de-installation

As an operator, you want to be able to revert the MAAS setup installed by this playbook, such that the machine is clean of all MAAS packages or snaps.  In order to teardown a MAAS deployment, you can run a separate entry-point within the playbook to teardown the installed MAAS packages or snaps.  This entry-point is provided in the playbook to remove the installation that the default entry-point provides: Running this playbook with the default configuration with perfectly undo the default installation.

You must [back up](/t/how-to-back-up-maas/5096) the database and MAAS configuration, if desired.  The target machine is restored state prior to installation, with no MAAS, directories, or files present on the system.

*** MAAS high availability

As an operator, you want to install a reverse proxy and configure high-availability region controllers for a given host using Ansible.  Note that HA region controllers require an HAProxy configuration. You can accomplish this with the following steps:

- [set the maas_cluster_proxy](#heading--Set-the-maas_cluster_proxy),
- [have Ansible install and configure HAProxy](#heading--Use-Ansible-to-configure-HAProxy) on the designated instances, 
- and [verify that the HAProxy is forwarding traffic](#heading--Verify-HAProxy-forwarding) to the region controllers.
 
Ansible configures the HAProxy instance for optimal use, such that OS images can be uploaded, for example. An unresponsive Region Controller is taken out of the upstream pool quickly.
The HAProxy instance does not interfere with Nginx/MAAS TLS configuration

<a href="#heading--Set-the-maas_proxy">*** id="heading--Set-the-maas_proxy">Set the maas_proxy role

Set the following in the `hosts` file to set the `maas_proxy` role:

```nohighlight
maas_proxy
my.host ansible_user=ssh_user
```

For example, on a host called "neuromancer" with an SSH-capable user called "stormrider", this YAML would be:

```nohighlight
maas_proxy
neuromancer ansible_user=stormrider
```

*** Use Ansible to configure HAProxy

Run the full playbook, or add `--tags <target role(s)>` to run only the tasks for a given role.

*** Verify HAProxy forwarding

You can verify the HAProxy forwarding by running `curl -L http://<haproxy host>:5240/MAAS` if HAProxy is on a separate host from the region controller; otherwise, change the port number to 5050 like this:

```nohighlight
curl -L http://<haproxy hostd>:5050/MAAS
```

** PostgreSQL primary role

As an operator, you want to install a Postgresql database as a primary to a given host using Ansible. You can accomplish this with the following steps:

- [set the maas_postgresql_primary role](#heading--Set-the-maas_postgresql_primary-role) on a host, 
- [have Ansible install a readable and writable PostgreSQL instance](#heading--Use-Ansible-to-install-a-postgres-instance), with optimal configuration for MAAS,
- [verify that the PostgreSQL instance is working properly](#heading--Verify-the-PostgreSQL-instance).
 
Ansible installs the latest supported version of PostgreSQL supported for the given MAAS version. **Do they select the MAAS version?  How does it know?**  If the playbook runs with other roles set on targeted hosts / groups, the tasks associated with the maas_postgresql_primary role runs first. If the operator sets a variable for importing a backup, the backup is loaded into PostgreSQL. ** How do they set this variable?  What is the variable?**

<a href="#heading--Set-the-maas_postgres_primary-role">*** id="heading--Set-the-maas_postgres_primary-role">Set the maas_postgres_primary role

Set the following in the `hosts` file to set the `maas_postgres_primary` role:

```nohighlight
maas_postgres_primary
my.host ansible_user=ssh_user
```

For example, on a host called "neuromancer" with an SSH-capable user called "stormrider", this YAML would be:

```nohighlight
maas_postgres_primary
neuromancer ansible_user=stormrider
```

*** Use Ansible to install a postgres instance

Run the full playbook, or add `--tags <target role(s)>` to run only the tasks for a given role.

*** Verify the PostgreSQL instance

You can verify the primary by running `sudo -u postgres psql` and making sure you get a prompt.

** PostgreSQL secondary role

As an operator, you want to install a PostgreSQL database as a secondary for a given host using Ansible. You can accomplish this with the following steps:

- set the maas_postregresql_secondary role on a host,
- have Ansible install a readable PostgreSQL instance, with an optimal configuration, set as a failover instance,
- and verify that the failover instance works properly.

Ansible installs the latest supported version of PostgreSQL for the given MAAS version that’s available for the host’s Ubuntu series in the Ubuntu repo. (**need a clearer way to say this**). 

If the playbook runs with other roles set on targeted hosts / groups, the tasks associated with the maas_postgresql_secondary role runs after maas_postgresql_primary but before any other roles.  The configured secondary from this role replicates from a primary PostgreSQL instance created from the maas_postgresql_primary role.

Automated failover is configured manually, external to this playbook. Manual failover can be achieved by a separate set of tasks for the maas_postgresql_secondary role, which once successful, changes the machine’s role to a maas_postgresql_primary and its configuration reflects that.

<a href="#heading--Set-the-maas_postregres_secondary-role-">*** id="heading--Set-the-maas_postregres_secondary-role-">Set the maas_postregres_secondary role 

Set the following in the `hosts` file to set the `maas_postgres_secondary` role:

```nohighlight
maas_postgres_secondary
my.host ansible_user=ssh_user
```

For example, on a host called "neuromancer" with an SSH-capable user called "stormrider", this YAML would be:

```nohighlight
maas_postgres_secondary
neuromancer ansible_user=stormrider
```

*** Use Ansible to install PostgreSQL failover instance

Run the full playbook, or add `--tags <target role(s)>` to run only the tasks for a given role.

*** Verify that the failover instance works properly

You can verify the primary by running `sudo -u postgres psql`; when you get a prompt, enter `select * from pg_stat_replication;`.  This should return a list of all secondaries connected to that primary.

<!--
** Firewall rules

As a operator, you want to be able to setup MAAS in a secure way, following best practices and operational guidance on securing MAAS. In order to make a MAAS setup secure, I would Ansible playbooks to configure firewalls and file permissions based on https://maas.io/docs/how-to-secure-maas.

 
** PostgreSQL role bundling scripts -->

* MAAS and Terraforming
If you wish to use MAAS with [Terraform](https://www.terraform.io/)`↗`, we have made a [provider available](https://github.com/maas/terraform-provider-maas)`↗`.  This article provides reference information about the data sources and resources that can be accessed via this provider.  It does not attempt to explain the mechanics or usage of Terraform or offer any tutorial information related to this MAAS Terraform provider.

<h1 The MAAS Terraform provider

The MAAS provider is a Terraform provider that allows you to manage MAAS resources using the Terraform (CRUD) tool. This provider can be used to manage many aspects of a MAAS environment, including networking, users, machines, and VM hosts.

These aspects can be divided into three categories of Terraform-compliant HCL:

- [API linkages](#heading--terraform-api-linkage)
- [Data sources](#heading--data-sources)
- [Resources](#heading--resources)

We will deal with each of these categories in turn.  For each data source and resource, we will offer a brief definition and description of how that item is employed in MAAS.  If you are new to [Terraform](https://www.terraform.io/)`↗`, or want to explore what terraforming may provide for your MAAS instance, you may wish to consult the [Terraform documentation](https://www.terraform.io/intro)`↗` or one of the many [tutorials available](https://learn.hashicorp.com/collections/terraform/aws-get-started?utm_source=WEBSITE&utm_medium=WEB_IO&utm_offer=ARTICLE_PAGE&utm_content=DOCS)`↗`.

** API linkages

The schema that provides an API linkage to MAAS from Terraform consists of a standard HCL provider block and a provider API block.  As with all Terraform providers, the provider block contains at least two items:

- a source element, which in this case is "maas/maas".
- a version element, which can be sufficiently specified by "~>1.0".

The provider block would look something like this:

```nohighlight
terraform {
  required_providers {
    maas = {
      source  = "maas/maas"
      version = "~>1.0"
    }
  }
}
```

The provider API block contains the necessary credentials to allow Terraform to access your MAAS instance, which include three things:

- an API version.
- an API key.
- an API URL.

A typical provider API block might look like this:

```nohighlight
provider "maas" {
  api_version = "2.0"
  api_key = "<YOUR API KEY>"
  api_url = "http://127.0.0.1:5240/MAAS"
}
```

A completed definition would also include some data sources and resources, like this typical example:

```nohighlight
terraform {
  required_providers {
    maas = {
      source  = "maas/maas"
      version = "~>1.0"
    }
  }
}

provider "maas" {
  api_version = "2.0"
  api_key = "<YOUR API KEY>"
  api_url = "<YOUR API URL>"
}

resource "maas_space" "tf_space" {
  name = "tf-space"
}

resource "maas_fabric" "tf_fabric" {
  name = "tf-fabric"
}

resource "maas_vlan" "tf_vlan" {
  fabric = maas_fabric.tf_fabric.id
  vid = 14
  name = "tf-vlan14"
  space = maas_space.tf_space.name
}
resource "maas_subnet" "tf_subnet" {
  cidr = "10.88.88.0/24"
  fabric = maas_fabric.tf_fabric.id
  vlan = maas_vlan.tf_vlan.vid
  name = "tf_subnet"
  gateway_ip = "10.88.88.1"
  dns_servers = [
    "1.1.1.1",
  ]
  ip_ranges {
    type = "reserved"
    start_ip = "10.88.88.1"
    end_ip = "10.88.88.50"
  }
  ip_ranges {
    type = "dynamic"
    start_ip = "10.88.88.200"
    end_ip = "10.88.88.254"
  }
}
```

See the [Terraform HCL documentation](https://www.terraform.io/language)`↗` for more details about these blocks.

** Data sources

The MAAS Terraform provider offers three data sources, all representing network elements:

- a [fabric](https://discourse.maas.io/t/maas-glossary/5416#heading--fabrics)`↗`, which is essentially a VLAN namespace -- that is, it connects two or more VLANs together.
- a [subnet](https://discourse.maas.io/t/maas-glossary/5416#heading--subnets)`↗`, which is the traditional way of dividing up IP addresses into smaller networks, e.g., 192.168.15.0/24.
- a [VLAN](https://en.wikipedia.org/wiki/VLAN)`↗`, a "virtual LAN", which is a collection of specific addresses or ports that are connected together to form a restricted network.

Each of these data sources has a specific HCL block with elements structured appropriately to manage that MAAS element.

*** Fabric

The [fabric](https://github.com/maas/terraform-provider-maas/blob/master/docs/data_sources/maas_fabric.md)`↗` data source provides minimal details, namely, the fabric ID, of an existing MAAS fabric.  It takes one argument (the fabric name) and exports one attribute (the fabric ID):

```nohighlight
data "maas_fabric" "default" {
  name = "maas"
}
```

Fabrics within MAAS are not widely manipulated in and of themselves, but rather serve as containers for storing VLAN/subnet combinations.

*** Subnet

The [subnet](https://github.com/maas/terraform-provider-maas/blob/master/docs/data_sources/maas_subnet.md)`↗` data source provides a number of details about an existing MAAS network subnet.  The element takes one argument, the subnet CIDR, and exports a number of attributes:

- id - The subnet ID.
- fabric - The subnet fabric.
- vid - The subnet VLAN traffic segregation ID.
- name - The subnet name.
- rdns_mode - How reverse DNS is handled for this subnet. It can have one of the following values:
-- 0 - Disabled, no reverse zone is created.
-- 1 - Enabled, generate reverse zone.
-- 2 - RFC2317, extends 1 to create the necessary parent zone with the appropriate CNAME resource records for the network, if the network is small enough to require the support described in RFC2317.
- allow_dns - Boolean value that indicates if the MAAS DNS resolution is enabled for this subnet.
- allow_proxy - Boolean value that indicates if maas-proxy allows requests from this subnet.
- gateway_ip - Gateway IP address for the subnet.
- dns_servers - List of IP addresses set as DNS servers for the subnet.

Declaring a subnet looks something like this example:

```nohighlight
data "maas_subnet" "vid10" {
  cidr = "10.10.0.0/16"
}
```

Subnets are the network backbone of MAAS, and thus provide a number of attributes that can be manipulated to alter the behaviour of MAAS.

*** VLAN

The [VLAN](https://github.com/maas/terraform-provider-maas/blob/master/docs/data_sources/maas_vlan.md)`↗` data source provides details about an existing MAAS VLAN.  A VLAN takes two arguments:

- fabric - (Required) The fabric identifier (ID or name) for the VLAN.
- vlan - (Required) The VLAN identifier (ID or traffic segregation ID).

A VLAN data source exports a few useful attributes:

- mtu - The MTU used on the VLAN.
- dhcp_on - Boolean value indicating if DHCP is enabled on the VLAN.
- name - The VLAN name.
- space - The VLAN space.

VLAN [spaces](https://discourse.maas.io/t/maas-glossary/5416#heading--spaces)`↗` are used mostly by Juju, but can be employed by other tools, if desired.

The typical definition of a MAAS VLAN in HCL might look like this:

```nohighlight
data "maas_vlan" "vid10" {
  fabric = data.maas_fabric.default.id
  vlan = 10
}
```

VLANs are available as data sources, but generally, subnets are the workhorses of most MAAS instances.

** Resources

The MAAS Terraform provider makes a large number of resources available, currently including the following items.  Because of the large number of items, details of arguments and attributes are not duplicated here, but instead provided from a single source at the given links:

- A [maas_instance](https://github.com/maas/terraform-provider-maas/blob/master/docs/resources/instance.md)`↗` provides a resource to deploy and release machines already configured in MAAS, based on the specified parameters. If no parameters are given, a random machine will be allocated and deployed using the defaults.
- A [maas_vm_host](https://github.com/maas/terraform-provider-maas/blob/master/docs/resources/vm_host.md)`↗` provides a resource to manage MAAS VM hosts.  Note that MAAS VM hosts are not machines, but the host(s) upon which virtual machines are created.
- A [maas_vm_host_machine](https://github.com/maas/terraform-provider-maas/blob/master/docs/resources/vm_host_machine.md)`↗` provides a resource to manage MAAS VM host machines, which represent the individual machines that are spun up on a given VM host.
- A [maas_machine](https://github.com/maas/terraform-provider-maas/blob/master/docs/resources/machine.md)`↗` provides a resource to manage MAAS machines; note that these are typically physical machines (rather than VMs), so they tend to respond differently at times.
- A [maas_network_interface_physical](https://github.com/maas/terraform-provider-maas/blob/master/docs/resources/network_interface_physical.md)`↗` provides a resource to manage a physical network interface from an existing MAAS machine.  Network interfaces can be created and deleted at will via the MAAS CLI/UI, so there may be more than one of these associate with any given machine.
- A [maas_network_interface_link](https://github.com/maas/terraform-provider-maas/blob/master/docs/resources/network_interface_link.md)`↗` provides a resource to manage network configuration on a network interface.  Note that this does not represent the interface itself, but the parameter set that configure that interface.
- A [maas_fabric](https://github.com/maas/terraform-provider-maas/blob/master/docs/resources/fabric.md)`↗` provides a resource to manage MAAS network fabrics, which are [described above](#heading--fabric)`↗`. 
- A [maas_vlan](https://github.com/maas/terraform-provider-maas/blob/master/docs/resources/vlan.md)`↗` provides a resource to manage MAAS network VLANs, also [described above](#heading--vlan)`↗`.
- A [maas_subnet](https://github.com/maas/terraform-provider-maas/blob/master/docs/resources/subnet.md)`↗` provides a resource to manage MAAS network subnets, also [described above](#heading--subnet)`↗`
- A [maas_subnet_ip_range](https://github.com/maas/terraform-provider-maas/blob/master/docs/resources/subnet_ip_range.md)`↗` provides a resource to manage MAAS network subnets IP ranges.  IP ranges carry particular importance when managing DHCP with multiple DHCP servers, for example.
- A [maas_dns_domain](https://github.com/maas/terraform-provider-maas/blob/master/docs/resources/dns_domain.md)`↗` provides a resource to manage MAAS DNS domains.
- A [maas_dns_record](https://github.com/maas/terraform-provider-maas/blob/master/docs/resources/dns_record.md)`↗` provides a resource to manage MAAS DNS domain records.
- A [maas_space](https://github.com/maas/terraform-provider-maas/blob/master/docs/resources/space.md)`↗` provides a resource to manage MAAS network [spaces](https://juju.is/docs/olm/network-spaces)`↗`.
- A [maas_block_device](https://github.com/maas/terraform-provider-maas/blob/master/docs/resources/block_device.md)`↗` provides a resource to manage block devices on MAAS machines.
- A [maas_tag](https://github.com/maas/terraform-provider-maas/blob/master/docs/resources/tag.md)`↗` provides a resource to manage a MAAS tag.  MAAS tags have multiple roles in controlling how machines are configured, booted, and monitored.
- A [maas_user](https://github.com/maas/terraform-provider-maas/blob/master/docs/resources/user.md)`↗` provides a resource to manage MAAS users.  This resource does not provide any control over any Candid or RBAC restrictions that may be in place.

Please visit the links to get details on these resources, since the documentation at those links will always be the most current information available.

* MAAS API reference
MAAS provides a robust API that conforms to the OpenAPI standards.

- [API authentication reference](/t/-/5060): The MAAS API uses [OAuth](http://en.wikipedia.org/wiki/OAuth)`↗` as its 0-legged authentication mechanism.

- [Python API client reference](/t/-/5404): The python-libmaas client library allows developers, integrators and administrators to better interact with MAAS. 

- [API documentation](https://maas.io/docs/api): MAAS API documentation now conforms to the OpenAPI standards.

* MAAS audit events

An audit event is a [MAAS event](/t/understanding-maas-events/6373) tagged with `AUDIT`. It captures changes to the MAAS configuration and machine states. These events provide valuable oversight of user actions and automated updates -- and their effects -- especially when multiple users are interacting with multiple machines.  See [Understanding MAAS events](/t/understanding-maas-events/6373) for basic usage of the CLI `events query` command.

** Viewing events

Audit events are examined using the MAAS CLI with the `level=AUDIT` parameter set:

```nohighlight
$ maas $PROFILE events query level=AUDIT
```

You can use `jq` to prettify the output:

```nohighlight
$ maas $PROFILE events query level=AUDIT after=0 limit=20 \
| jq -r '(["USERNAME","HOSTNAME","DATE","EVENT"] | 
(., map(length*"-"))),
(.events[] | [.username,.hostname,.created,.description]) 
| @tsv' | column -t -s$'\t'
```

This command might produce output similar to this:

```nohighlight
USERNAME  HOSTNAME     DATE                        EVENT
--------  --------     ----                        -----
unknown   valued-moth  Thu, 21 Apr. 2022 19:45:14  pci device 2 was updated on node 8wmfx3
unknown   valued-moth  Thu, 21 Apr. 2022 19:45:14  pci device 1 was updated on node 8wmfx3
unknown   valued-moth  Thu, 21 Apr. 2022 19:45:14  pci device 1 was updated on node 8wmfx3
unknown   valued-moth  Thu, 21 Apr. 2022 19:45:14  pci device 1 was updated on node 8wmfx3
unknown   valued-moth  Thu, 21 Apr. 2022 19:45:14  pci device 1 was updated on node 8wmfx3
unknown   valued-moth  Thu, 21 Apr. 2022 19:45:14  pci device 1 was updated on node 8wmfx3
unknown   valued-moth  Thu, 21 Apr. 2022 19:45:14  pci device 1 was updated on node 8wmfx3
unknown   valued-moth  Thu, 21 Apr. 2022 19:45:14  pci device 1 was updated on node 8wmfx3
unknown   valued-moth  Thu, 21 Apr. 2022 19:45:14  pci device 1 was updated on node 8wmfx3
unknown   valued-moth  Thu, 21 Apr. 2022 19:45:14  pci device 0 was updated on node 8wmfx3
unknown   valued-moth  Thu, 21 Apr. 2022 19:45:14  block device sda was updated on node 8wmfx3
unknown   valued-moth  Thu, 21 Apr. 2022 19:45:14  interface enp5s0 was updated on node 8wmfx3
unknown   valued-moth  Thu, 21 Apr. 2022 19:45:14  0 bytes of memory was removed on node 8wmfx3
admin     valued-moth  Thu, 21 Apr. 2022 19:36:48  Started deploying 'valued-moth'.
admin     valued-moth  Thu, 21 Apr. 2022 19:36:21  Acquired 'valued-moth'.
admin     unknown      Thu, 21 Apr. 2022 19:21:46  Updated configuration setting 'completed_intro' to 'True'.
admin     unknown      Thu, 21 Apr. 2022 19:20:49  Updated configuration setting 'upstream_dns' to '8.8.8.8'.
admin     unknown      Thu, 21 Apr. 2022 19:20:49  Updated configuration setting 'maas_name' to 'neuromancer'.
admin     unknown      Thu, 21 Apr. 2022 19:20:47  Updated configuration setting 'http_proxy' to ''.
admin     unknown      Thu, 21 Apr. 2022 19:20:24  Logged in admin.
```

You can also use the [various event filters](/t/understanding-maas-events/6373#heading--filter-parameters) with `level=AUDIT` to further restrict your output.

*** The meaning of audit events

Let's walk through a sample of, say, eighteen audit events and see how to interpret and use them.  

```nohighlight
maas $PROFILE events query level=AUDIT limit=18 after=0 | jq -r '(["USERNAME","NODE","HOSTNAME","LEVEL","DATE","TYPE","EVENT"] | 
(., map(length*"-"))),
(.events[] | [.username,.node,.hostname,.level,.created,.type,.description]) 
| @tsv' | column -t -s$'\t'
```

Consider the resulting `jq` output:

```nohighlight
USERNAME     NODE    HOSTNAME       LEVEL  DATE                        TYPE  EVENT
--------     ----    --------       -----  ----                        ----  -----
admin        mm3tc8  fair-marten    AUDIT  Tue, 30 Nov. 2021 09:14:02  Node  Set the zone to 'danger' on 'fair-marten'.
admin        ebd7dc  new-name       AUDIT  Tue, 30 Nov. 2021 09:14:02  Node  Set the zone to 'danger' on 'new-name'.
admin        pbpncx  ruling-bobcat  AUDIT  Tue, 30 Nov. 2021 09:13:52  Node  Set the zone to 'default' on 'ruling-bobcat'.
admin        mm3tc8  fair-marten    AUDIT  Tue, 30 Nov. 2021 09:13:52  Node  Set the zone to 'default' on 'fair-marten'.
admin        ebd7dc  new-name       AUDIT  Tue, 30 Nov. 2021 09:13:52  Node  Set the zone to 'default' on 'new-name'.
admin        mm3tc8  fair-marten    AUDIT  Tue, 30 Nov. 2021 09:11:56  Node  Started commissioning on 'fair-marten'.
admin        ebd7dc  new-name       AUDIT  Tue, 30 Nov. 2021 09:11:55  Node  Started commissioning on 'new-name'.
admin        ebd7dc  new-name       AUDIT  Tue, 30 Nov. 2021 09:09:06  Node  Marked 'new-name' broken.
admin        ebd7dc  new-name       AUDIT  Tue, 30 Nov. 2021 07:51:31  Node  Started commissioning on 'new-name'.
admin        mm3tc8  fair-marten    AUDIT  Tue, 30 Nov. 2021 06:07:03  Node  Started commissioning on 'fair-marten'.
admin        ebd7dc  active-amoeba  AUDIT  Tue, 23 Nov. 2021 08:01:10  Node  Started commissioning on 'active-amoeba'.
admin        ebd7dc  active-amoeba  AUDIT  Tue, 23 Nov. 2021 08:00:47  Node  Marked 'active-amoeba' broken.
admin        pbpncx  ruling-bobcat  AUDIT  Wed, 17 Nov. 2021 00:04:51  Node  Started deploying 'ruling-bobcat'.
admin        ebd7dc  active-amoeba  AUDIT  Mon, 15 Nov. 2021 05:39:48  Node  Set the resource pool to 'default' on 'active-amoeba'.
admin        ebd7dc  active-amoeba  AUDIT  Mon, 08 Nov. 2021 04:07:44  Node  Started testing on 'active-amoeba'.
admin        ebd7dc  active-amoeba  AUDIT  Mon, 08 Nov. 2021 04:05:40  Node  Marked 'active-amoeba' broken.
admin        knpge8  bolla          AUDIT  Wed, 16 Jun. 2021 04:35:50  Node  Started importing images on 'bolla'.
admin        knpge8  bolla          AUDIT  Wed, 10 Jun. 2020 21:07:40  Node  Set the zone to 'danger' on 'bolla'.
```

All of these example events are type `Node`, referring to a machine actions.  Node events are probably the most important audit events, because they capture machine life-cycle changes.  When auditing your MAAS, life-cycle events are often the most useful.

Take a moment to consider the MAAS life-cycle, which can be depicted with this state table:

| Machine state | Cm | Aq | Dp | Rl | Ab | Cl | PC | Ts | Rsq | Bk | Lk | Fx | Tg | RP | SZ | Del |
|---------------|----|----|----|----|----|----|----|----|-----|----|----|----|----|----|----|-----|
| New           | Y  |    |    |    |    |    | Y  | Y  | Y   |    |    |    | Y  | Y  | Y  | Y   |
| Failed        | Y  |    |    |    |    |    |    | Y  | Y   | Y  |    |    | Y  | Y  | Y  | Y   |
| Commissioning |    |    |    |    | Y  |    |    |    |     | Y  |    |    | Y  | Y  | Y  |     |
| Ready         | Y  | Y  | Y  |    |    | Y  |    | Y  | Y   | Y  |    |    | Y  | Y  | Y  | Y   |
| Acquired      | Y  |    | Y  | Y  |    |    |    | Y  | Y   | Y  |    |    | Y  | Y  | Y  | Y   |
| Deploying     |    |    |    | Y  | Y  |    | Y  |    |     |    | Y  |    | Y  | Y  | Y  | Y   |
| Deployed      |    |    |    | Y  |    |    | Y  | Y  | Y   | Y  | Y  |    | Y  | Y  | Y  | Y   |
| Broken        | Y  |    |    |    |    |    | Y  | Y  | Y   |    |    | Y  | Y  | Y  | Y  | Y   |
| Rescue mode   |    |    |    |    |    |    |    |    | X   |    |    |    | Y  | Y  | Y  | Y   |

The key for the table columns is as follows:

- *Cm* - can commission.
- *Aq* - can acquire.
- *Dp* - can deploy.
- *Rl* - can release.
- *Ab* - can abort an operation in progress.
- *Cl* - can clone the machine.
- *PC* - can power-cycle the machine (turn in on or off).
- *Ts* - can run tests on the machine.
- *Rsq* - can put the machine in Rescue Mode.
- *Bk* - can mark the machine as broken.
- *Lk* - can lock the machine, preventing others from accessing it.
- *Fx* - can move a broken machine to a fixed state.
- *Tg* - can set tags for a machine.
- *RP* - can set the resource pool for a machine.
- *SZ* - can set the zone for a machine.
- *Del* - can delete the machine.

*** Using audit events to find out what happened

Consider these example events that audit state changes:

```nohighlight
ID      LEVEL  TYPE           USERNAME  DESCRIPTION
=================================================================================================
589317  AUDIT  Node           bruce     Marked 'ruling-bobcat' broken.
583324  AUDIT  Node           clark     Tagging 'fair-marten'.
583313  AUDIT  Node           clark     Untagging 'fair-marten'.
584190  AUDIT  Node           diana     Overrode failed testing on 'new-name'.
529870  AUDIT  Node           kara      Powered on 'karura'.
529868  AUDIT  Node           kara      Powered off 'karura'.
435099  AUDIT  Node           barry     Set the zone to 'twilight' on 'fair-marten'.
435097  AUDIT  Node           hal       Acquired 'fair-marten'.
430453  AUDIT  Node           jonn      Started testing on 'fair-marten'.
430449  AUDIT  Node           jonn      Marked 'fair-marten' broken.
430445  AUDIT  Node           clark     Aborted 'testing' on 'fair-marten'.
427583  AUDIT  Node           diana     Set the resource pool to 'default' on 'fair-marten'.
426354  AUDIT  Node           bruce     Started commissioning on 'fair-marten'.
423257  AUDIT  Node           kara      Aborted 'commissioning' on 'fair-marten'.
421915  AUDIT  Node           joanna    Started releasing 'ruling-bobcat'.
28471   AUDIT  Settings       natasha   Updated DHCP snippet 'foo'.
28470   AUDIT  Settings       tony      Created DHCP snippet 'foo'.
28465   AUDIT  Settings       bruce2    Saved script 'setup.sh'.
28464   AUDIT  Settings       hank      Updated configuration setting 'enable_third_party_drivers' to 'False'.
8518    AUDIT  Node           kitty     Acquired 'sweet-krill'.
7615    AUDIT  Node           barry     Deleted the 'machine' 'new-bedbug'.
6238    AUDIT  Node           jonn      Started rescue mode on 'fleet-calf'.
5920    AUDIT  Node           diana     Started deploying 'comic-muskox'.
5907    AUDIT  Authorisation  admin     Logged out admin.
5906    AUDIT  Authorisation  admin     Logged in admin.
5896    AUDIT  Authorisation  hank      Created user 'zorko'.
3944    AUDIT  Node           clark     Deleted the 'machine' 'sweet-urchin'.
```

This is a long (but varied) listing, so there are many questions you might be able to answer:

1. Who deployed `comic-muskox`? 

2. What happened to `sweet-urchin`?

3. Why is `fleet-calf` in rescue mode?

4. Where did these changes come from in `setup.sh`?

5. What caused `ruling-bobcat` to be marked as broken?

6. Who's responsible for the DHCP snippet called `foo`?

Audit events don't answer all questions, but they help you discover whom to ask.

*** Auditing with finesse

You can use the MAAS CLI, `jq`, and command line text tools to finesse your auditing.  First, you'll have to get a feel for how MAAS describes audit events:

- Set the resource pool to 
- Started commissioning 
- Aborted 'commissioning'
- Started releasing 
- Created DHCP snippet
- Saved script 
- Updated configuration setting 
- Deleted the 'machine' 
- Created user

You can use these snippets as search keys.  Say you walk into the data centre one day and a couple of machines just aren't there any more.  You could run this command:

```nohighlight
$ maas $PROFILE events query limit=1000 after=0 level=AUDIT \
hostname=new-bedbug hostname=sweet-urchin \
| jq -r '(.events[] | [.id,.level,.type,.username,.description]) 
| @tsv' | column -t -s$'\t' \
| grep "Deleted the"
```

Within 30 seconds, you'd know whom to ask:

```nohighlight
7615    AUDIT  Node  barry     Deleted the 'machine' 'new-bedbug'.
3944    AUDIT  Node  clark     Deleted the 'machine' 'sweet-urchin'.
```
Or, you could just check to see what's been deleted:

```nohighlight
33315  AUDIT  Node           Deleted the 'machine' 'keen-lab'.
33314  AUDIT  Node           Deleted the 'machine' 'helloooo'.
31179  AUDIT  Node           Deleted the 'machine' 'firm-ghost'.
31178  AUDIT  Node           Deleted the 'machine' 'proper-troll'.
31177  AUDIT  Node           Deleted the 'machine' 'steady-mammal'.
31176  AUDIT  Node           Deleted the 'machine' 'wired-dove'.
31175  AUDIT  Node           Deleted the 'machine' 'wanted-fox'.
31174  AUDIT  Node           Deleted the 'machine' 'picked-cub'.
31173  AUDIT  Node           Deleted the 'machine' 'claudio'.
31172  AUDIT  Node           Deleted the 'machine' 'next-mullet'.
31171  AUDIT  Node           Deleted the 'machine' 'happy-bengal'.
31170  AUDIT  Node           Deleted the 'machine' 'grown-hawk'.
31169  AUDIT  Node           Deleted the 'machine' 'new-bedbug'.
31168  AUDIT  Node           Deleted the 'machine' 'native-moray'.
31167  AUDIT  Node           Deleted the 'machine' 'fleet-calf'.
31166  AUDIT  Node           Deleted the 'machine' 'daring-ewe'.
31165  AUDIT  Node           Deleted the 'machine' 'sweet-urchin'.
31164  AUDIT  Node           Deleted the 'machine' 'new-chimp'.
31163  AUDIT  Node           Deleted the 'machine' 'humble-bug'.
31162  AUDIT  Node           Deleted the 'machine' 'modern-mutt'.
31161  AUDIT  Node           Deleted the 'machine' 'nice-skink'.
31160  AUDIT  Node           Deleted the 'machine' 'choice-worm'.
31159  AUDIT  Node           Deleted the 'machine' 'wanted-turtle'.
31158  AUDIT  Node           Deleted the 'machine' 'neat-yak'.
31157  AUDIT  Node           Deleted the 'machine' 'superb-piglet'.
31156  AUDIT  Node           Deleted the 'machine' 'rare-ghost'.
31155  AUDIT  Node           Deleted the 'machine' 'unique-weevil'.
31154  AUDIT  Node           Deleted the 'machine' 'finer-akita'.
31153  AUDIT  Node           Deleted the 'machine' 'cool-dog'.
31152  AUDIT  Node           Deleted the 'machine' 'meet-snake'.
31151  AUDIT  Node           Deleted the 'machine' 'native-civet'.
31150  AUDIT  Node           Deleted the 'machine' 'top-burro'.
31149  AUDIT  Node           Deleted the 'machine' 'pro-boa'.
31148  AUDIT  Node           Deleted the 'machine' 'fine-dane'.
31147  AUDIT  Node           Deleted the 'machine' 'clean-ocelot'.
31146  AUDIT  Node           Deleted the 'machine' 'boss-crab'.
31145  AUDIT  Node           Deleted the 'machine' 'crisp-mammal'.
31144  AUDIT  Node           Deleted the 'machine' 'active-panda'.
31143  AUDIT  Node           Deleted the 'machine' 'fit-ram'.
31142  AUDIT  Node           Deleted the 'machine' 'strong-prawn'.
31141  AUDIT  Node           Deleted the 'machine' 'equal-dog'.
31140  AUDIT  Node           Deleted the 'machine' 'sure-kid'.
31139  AUDIT  Node           Deleted the 'machine' 'choice-wren'.
31138  AUDIT  Node           Deleted the 'machine' 'eager-whale'.
31137  AUDIT  Node           Deleted the 'machine' 'fun-boxer'.
31136  AUDIT  Node           Deleted the 'machine' 'clean-filly'.
31135  AUDIT  Node           Deleted the 'machine' 'thingthing'.
31134  AUDIT  Node           Deleted the 'machine' 'prime-walrus'.
28073  AUDIT  Node           Deleted the 'machine' 'ace-boxer'.
28072  AUDIT  Node           Deleted the 'machine' 'active-panda'.
28071  AUDIT  Node           Deleted the 'machine' 'boss-crab'.
24724  AUDIT  Node           Deleted the 'machine' 'ruling-marlin'.
24723  AUDIT  Node           Deleted the 'machine' 'sweet-urchin'.
24722  AUDIT  Node           Deleted the 'machine' 'new-chimp'.
24721  AUDIT  Node           Deleted the 'machine' 'humble-bug'.
24720  AUDIT  Node           Deleted the 'machine' 'next-mullet'.
24719  AUDIT  Node           Deleted the 'machine' 'native-moray'.
24718  AUDIT  Node           Deleted the 'machine' 'grown-hawk'.
24717  AUDIT  Node           Deleted the 'machine' 'happy-bengal'.
24716  AUDIT  Node           Deleted the 'machine' 'picked-cub'.
24715  AUDIT  Node           Deleted the 'machine' 'claudio'.
24714  AUDIT  Node           Deleted the 'machine' 'fleet-calf'.
24713  AUDIT  Node           Deleted the 'machine' 'new-bedbug'.
24712  AUDIT  Node           Deleted the 'machine' 'daring-ewe'.
24711  AUDIT  Node           Deleted the 'machine' 'huge-yeti'.
24502  AUDIT  Node           Deleted the 'machine' 'guided-joey'.
24501  AUDIT  Node           Deleted the 'machine' 'active-adder'.
24500  AUDIT  Node           Deleted the 'machine' 'crisp-chow'.
24499  AUDIT  Node           Deleted the 'machine' 'holy-hippo'.
24498  AUDIT  Node           Deleted the 'machine' 'eager-kid'.
24497  AUDIT  Node           Deleted the 'machine' 'mighty-finch'.
24496  AUDIT  Node           Deleted the 'machine' 'native-koala'.
24415  AUDIT  Node           Deleted the 'machine' 'me'.
24410  AUDIT  Node           Deleted the 'machine' 'you'.
17934  AUDIT  Node           Deleted the 'machine' 'carol'.
17933  AUDIT  Node           Deleted the 'machine' 'bob'.
17932  AUDIT  Node           Deleted the 'machine' 'aaa'.
17931  AUDIT  Node           Deleted the 'machine' 'alice'.
17604  AUDIT  Node           Deleted the 'machine' 'subtle-lark'.
17603  AUDIT  Node           Deleted the 'machine' 'brief-beetle'.
17602  AUDIT  Node           Deleted the 'machine' 'fit-earwig'.
12508  AUDIT  Node           Deleted the 'machine' 'asdf'.
12507  AUDIT  Node           Deleted the 'machine' 'gfd'.
12506  AUDIT  Node           Deleted the 'machine' 'sadasd'.
12505  AUDIT  Node           Deleted the 'machine' 'vocal-krill'.
12504  AUDIT  Node           Deleted the 'machine' 'epic-robin'.
12503  AUDIT  Node           Deleted the 'machine' 'secret-maas'.
12502  AUDIT  Node           Deleted the 'machine' 'thingthing'.
12501  AUDIT  Node           Deleted the 'machine' 'worthy-ray'.
12500  AUDIT  Node           Deleted the 'machine' 'brief-pika'.
12499  AUDIT  Node           Deleted the 'machine' 'sweet-krill'.
12498  AUDIT  Node           Deleted the 'machine' 'awake-dog'.
12497  AUDIT  Node           Deleted the 'machine' 'living-crab'.
12496  AUDIT  Node           Deleted the 'machine' 'quiet-caiman'.
12495  AUDIT  Node           Deleted the 'machine' 'known-kodiak'.
10975  AUDIT  Node           Deleted the 'machine' 'rested-egret'.
10974  AUDIT  Node           Deleted the 'machine' 'good-martin'.
10973  AUDIT  Node           Deleted the 'machine' 'game-elk'.
10972  AUDIT  Node           Deleted the 'machine' 'asda'.
10971  AUDIT  Node           Deleted the 'machine' 'cuddly-eft'.
10970  AUDIT  Node           Deleted the 'machine' 'asdas'.
9423   AUDIT  Node           Deleted the 'machine' 'hostname'.
7615   AUDIT  Node           Deleted the 'machine' 'new-bedbug'.
7614   AUDIT  Node           Deleted the 'machine' 'happy-bengal'.
7613   AUDIT  Node           Deleted the 'machine' 'fleet-calf'.
7612   AUDIT  Node           Deleted the 'machine' 'claudio'.
7611   AUDIT  Node           Deleted the 'machine' 'sweet-urchin'.
7610   AUDIT  Node           Deleted the 'machine' 'picked-cub'.
7609   AUDIT  Node           Deleted the 'machine' 'new-chimp'.
7608   AUDIT  Node           Deleted the 'machine' 'humble-bug'.
7607   AUDIT  Node           Deleted the 'machine' 'grown-hawk'.
7606   AUDIT  Node           Deleted the 'machine' 'native-moray'.
7605   AUDIT  Node           Deleted the 'machine' 'daring-ewe'.
7604   AUDIT  Node           Deleted the 'machine' 'fair-puma'.
7603   AUDIT  Node           Deleted the 'machine' 'funny-panda'.
7602   AUDIT  Node           Deleted the 'machine' 'ace-molly'.
7601   AUDIT  Node           Deleted the 'machine' 'big-locust'.
7600   AUDIT  Node           Deleted the 'machine' 'next-mullet'.
3944   AUDIT  Node           Deleted the 'machine' 'sweet-urchin'.
3943   AUDIT  Node           Deleted the 'machine' 'picked-cub'.
3942   AUDIT  Node           Deleted the 'machine' 'next-mullet'.
3941   AUDIT  Node           Deleted the 'machine' 'new-chimp'.
3940   AUDIT  Node           Deleted the 'machine' 'new-bedbug'.
3939   AUDIT  Node           Deleted the 'machine' 'native-moray'.
3938   AUDIT  Node           Deleted the 'machine' 'humble-bug'.
3937   AUDIT  Node           Deleted the 'machine' 'happy-bengal'.
3936   AUDIT  Node           Deleted the 'machine' 'claudio'.
3935   AUDIT  Node           Deleted the 'machine' 'daring-ewe'.
3934   AUDIT  Node           Deleted the 'machine' 'grown-hawk'.
3933   AUDIT  Node           Deleted the 'machine' 'fleet-calf'.
2685   AUDIT  Node           Deleted the 'machine' 'test-lab'.
2684   AUDIT  Node           Deleted the 'machine' 'test'.
2683   AUDIT  Node           Deleted the 'machine' 'Sootie'.
2682   AUDIT  Node           Deleted the 'machine' 'Tigger'.
```

Of course, that's a complex list, so could simplify, sort, remove any duplicates, and prettify the list a bit with already-available tools:

```nohighlight
$ maas $PROFILE events query limit=1000 after=0 level=AUDIT \
| jq -r '(.events[] | [.description]) | @tsv' \
| column -t -s$'\t' \
| grep "Deleted the" \
| cut -f 4 -d" " \
| sort -u | sed -e"s/'//g" | sed -e"s/\.//g"
```

This would give you a list of machines that have been deleted at least once:

```nohighlight
aaa
ace-boxer
ace-molly
active-adder
active-panda
alice
asda
asdas
asdf
awake-dog
big-locust
bob
boss-crab
brief-beetle
brief-pika
carol
choice-worm
choice-wren
claudio
clean-filly
clean-ocelot
cool-dog
crisp-chow
crisp-mammal
cuddly-eft
daring-ewe
eager-kid
eager-whale
epic-robin
equal-dog
fair-puma
fine-dane
finer-akita
firm-ghost
fit-earwig
fit-ram
fleet-calf
fun-boxer
funny-panda
game-elk
gfd
good-martin
grown-hawk
guided-joey
happy-bengal
helloooo
holy-hippo
hostname
huge-yeti
humble-bug
keen-lab
known-kodiak
living-crab
me
meet-snake
mighty-finch
modern-mutt
native-civet
native-koala
native-moray
neat-yak
new-bedbug
new-chimp
next-mullet
nice-skink
picked-cub
prime-walrus
pro-boa
proper-troll
quiet-caiman
rare-ghost
rested-egret
ruling-marlin
sadasd
secret-maas
Sootie
steady-mammal
strong-prawn
subtle-lark
superb-piglet
sure-kid
sweet-krill
sweet-urchin
test
test-lab
thingthing
Tigger
top-burro
unique-weevil
vocal-krill
wanted-fox
wanted-turtle
wired-dove
worthy-ray
you
```

Still a bit long, but using your imagination and additional command line utilities, you could pare this down even more.

The important points for working with audit data are:

- there are filters available to pinpoint several event attributes, limit the number of records, and focus on a specific set of records.
- the native output of `events query` is JSON; if you have good JSON tools handy, you can use those tools to parse the data further.
- if you don't have JSON tools handy, you can always use `jq` to produce workable text output, which you can then manipulate using standard CLI text tools.

There's probably no limit to what you can figure out if you use audit events properly.

* MAAS controllers
Most of the functionality of MAAS is contained in a series of controllers.  There are two basic types: a region controller and one or more rack controllers. The region controller deals with operator requests, while the rack controller(s) provides high-bandwidth services to the individual machines.  In essence, the region controller interacts with the user, while the rack controllers manage the bare metal.

** About region controllers

A region controller consists of the following components:

- REST API server (TCP port 5240)
- PostgreSQL database
- DNS
- caching HTTP proxy
- web UI

Region controllers are responsible for either a data centre or a single region. Multiple fabrics are used by MAAS to accommodate subdivisions within a single region, such as multiple floors in a data centre.

** About rack controllers

A rack controller provides four services:

- DHCP
- TFTP
- HTTP (for images)
- power management

A rack controller is attached to each "fabric". As the name implies, a typical setup is to have a rack controller in each data centre server rack. The rack controller will cache large items for performance, such as operating system install images, but maintains no independent state other than the credentials required to talk to the region controller.

****# Tell me about fabrics

A fabric is simply a way of linking [VLANs](/t/maas-glossary/5416#heading--vlans) (Virtual LANs) together.  If you're familiar with a VLAN, you know that it's designed to limit network traffic to specific ports (e.g., on a [switch](/t/maas-glossary/5416#heading--switch)) or by evaluating labels called "tags" (unrelated to MAAS tags).  By definition, this would mean that two VLANs can't communicate with each other -- it would defeat the purpose of the VLAN -- unless you implement some extraordinary measures.

For example, let's say that your [hospital](/t/how-to-get-started-with-maas/5092) has three key functions: Patient management, Accounting, and Facilities, each on their own VLAN.  Let's say that there are some situations in which you need to share data between all three of these functions.  To accomplish this, you can create a fabric that joins these three VLANS.  Since this fabric just makes it possible for these VLANs to communicate, you can manage the cross-VLAN access with additional software, or permissions, depending on your application software architecture.

You can learn more about fabrics in the [Concepts and terms](/t/maas-glossary/5416#heading--fabrics) section of this documentation.

** About controller communication

MAAS communication happens in a strict hierarchy, flowing from the UI/API through the region controller, to the rack controller, to the machines (and back).  While [high availability](/t/how-to-enable-high-availability/5120) (HA) may add controllers, it does not change the flow of communication through the MAAS system.  Understanding this message flow may help you with the machine topics which follow.

*** How machines communicate with the rack controller

All machine communication with MAAS is proxied through rack controllers, including HTTP metadata, DNS, syslog and APT (cache-and-forward proxies via Squid). 

MAAS creates an internal DNS domain, not manageable by the user, and a unique DNS resource for each subnet that is managed by MAAS. Each subnet includes all rack controllers that have an IP on that subnet. Booting machines use the subnet DNS resource to resolve the rack controller available for communication. If multiple rack controllers belong to the same subnet, MAAS uses a round-robin algorithm to balance the load across numerous rack controllers. This arrangement ensures that machines always have a rack controller.

Machines use this internal domain for HTTP metadata queries, APT (proxying via Squid), and Syslog. DNS queries, PXE booting, and NTP polls use IP addresses.

The rack controller installs and configures `bind` as a forwarder. All machines communicate via the rack controller directly.

[note]
Zone management and maintenance still happen within the region controller.
[/note]

<a href="https://discourse.maas.io/uploads/default/original/1X/02a7ca58b989c67c74421b9d5e0c8b32907a2de1.jpeg" target = "_blank"><img src="https://discourse.maas.io/uploads/default/original/1X/02a7ca58b989c67c74421b9d5e0c8b32907a2de1.jpeg"></a>

*** How region and rack controllers communicate

The MAAS region and rack controllers interact in a number of different ways, depending upon the operation you've requested.  Consider the process of commissioning a machine, that is, taking over the machine and gathering information on its available resources, including CPU, RAM, storage, and MIB information (obtainable via LLDP requests).  Here's a rough idea of what that sequence looks like -- a sequence that is representative of the communication between rack and region controllers:

1. An operator makes a request of MAAS, either via the Web UI or the API.  
2. MAAS translates this to an API request to the region controller.
3. The region controller locates the rack controller that has BMC access to the machine in question, that is, the rack controller that can power on that machine.
4. That same rack controller powers on the machine via IPMI request.
5. The rack controller tasked with providing DHCP handles assigning an IP address to the machine via the [DORA](/t/maas-glossary/5416#heading--dhcp) sequence (Discover, Offer, Request, Acknowledge).  **Note** that this rack controller doesn't have to be the same one that powers on the machine.
6. The DHCP-managing rack controller inserts itself as the DHCP "next-server" and requests a network boot.
7. (Still) the same rack controller RPCs the region controller to get what's needed to boot an ephemeral Ubuntu kernel, namely the kernel, any kernel parameters, an initrd daemon, and a squashfs load.
8. That same rack controller transforms the RPC response from the region controller into a valid PXE config and tells the machine to come get its files.
9. The booting machine loads the kernel and initrd, boots with that initrd, and then loads the squashfs, eventually making its way up to an ephemeral Ubuntu instance.
10. The booted machine pulls cloud-init metadata from the region controller, proxying through the rackd.
11. cloud-init uses this metadata to gather resource information about the machine and pass it back to the region controller, again proxied by the rackd.
12. The region controller (regiond or "region daemon") stores this machine information in a postgres database that is accessible only to the regiond, making MAAS truly stateless with respect to machines.

Again, this list doesn't represent every interaction between the controllers and machines, but it gives you a good idea of how MAAS works.

<details><summary>Tell me about the DHCP "next-server" statement</summary>

The `next-server` directive is used to specify the host address from which an initial boot file is to be loaded, usually a TFTP server.  In the case of MAAS, the rack controller providing DHCP actually inserts itself, since it can proxy (broker) the delivery of boot bits to the machine in question.
</details>
* MAAS documentation
MAAS is **Metal As A Service**, a service that treats physical servers like virtual machines (instances) in the cloud.

No need to manage servers individually: MAAS turns bare metal into an elastic, cloud-like resource. Enlist and deploy standard or customised operating systems to hardware and virtual machines -- remotely.  Monitor, manage, and secure your metal infrastructure easily and efficiently.

<a href="https://discourse.maas.io/uploads/default/original/1X/18456dbd3fbfec14eddd044816fd0719692282da.jpeg" target = "_blank"><img src="https://discourse.maas.io/uploads/default/original/1X/18456dbd3fbfec14eddd044816fd0719692282da.jpeg"></a>

MAAS comprehensively meets the need to rapidly deploy, destroy, and reconfigure constellations of bare metal.  Any application that requires frequently rearranging the server topology will benefit.

MAAS is applicable to nearly any situation.  It is currently deployed in banking, telecom, and industrial environments, as well as niche uses as diverse as national lotteries, supercomputer front-end validation, streaming music services, disaster recovery, and computer security risk analysis.

** In this documentation

|                                                                                                        |                                                                                                           |
|--------------------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------|
| [Tutorials](/t/tutorials/6140)</br>  Get started - a hands-on introduction to MAAS for new users </br> | [How-to guides](/t/how-to-guides/6142) </br> Step-by-step guides covering key operations and common tasks |
| [Reference](/t/reference/6143) </br> Technical information - specifications, APIs, architecture        | [Explanation](/t/explanation/6141)</br> Detailed explanations of the various MAAS components              |

** Project and community

MAAS is a member of the Ubuntu family. It’s an open source project that warmly welcomes community projects, contributions, suggestions, fixes and constructive feedback.

- [Read our code of conduct](https://ubuntu.com/community/code-of-conduct)`↗`
- [Get support](https://maas.io/docs/how-to-contact-us)`↗`
- [Learn about MAAS performance](https://maas.io/docs/maas-performance)`↗`
- [Join our online chat](/t/how-to-use-our-discourse-forum/6802)
- [Contribute code](https://launchpad.net/maas)`↗`
- [Contribute documentation](/t/how-to-contribute-documentation/6949)
- [Request a feature](/t/how-to-request-a-new-feature/4447)
- [Report a bug](/t/how-to-report-a-bug/4446)

**# Our roadmap

Here's a view of our current roadmap:

<a href="https://discourse.maas.io/uploads/default/original/2X/6/6cb3381fd1cfb2f3a871c281e118d2b94ee05bf1.jpeg" target = "_blank"><img src="https://discourse.maas.io/uploads/default/original/2X/6/6cb3381fd1cfb2f3a871c281e118d2b94ee05bf1.jpeg"></a>

Considering MAAS for your next project? [Get in touch](https://maas.io/docs/how-to-contact-us)`↗`

<!-- nohtml begin-nohtml -->
** Navigation

**# [Home](/t/maas-documentation/25)

**# [Tutorials](/t/-/6140) 

- [Bootstrap MAAS](/t/-/5092)
- [Try out the MAAS CLI](/t/-/5236)
- [Create a custom image](/t/-/6102)
- [Get fancy CLI output](/t/-/6027)

**# [How to get started with MAAS](/t/-/6202)

- [Do a fresh install of MAAS](/t/-/5128)
- [Upgrade MAAS](/t/how-to-upgrade-maas/5436)
- [Spin up MAAS with Ansible](/t/-/6367)

**# [How to set up networks](/t/-/6742)

- [Connect MAAS networks](/t/-/5164)
- [Enable DHCP](/t/-/5132)
- [Use availability zones](/t/-/5152)

**# [How to use images](/t/-/6192)

- [Use standard images](/t/-/5124)
- [Mirror images locally](/t/-/5927)
- [Customise images](/t/-/5104)
- [Employ VMWare images](/t/-/5144)
- [Deploy a RT kernel](/t/-/6658)

**# [How to manage controllers](/t/-/6498)

- [Configure controllers](/t/-/5172) 
- [Enable HA](/t/-/5120)

**# [How to manage machines](/t/-/6193)

- [Make machines available](/t/-/5160)
- [Customise machines](/t/-/5108)
- [Put machines to work](/t/-/5112)

**# [How to use virtual machines](/t/-/6500)

- [Set up LXD](/t/-/5208)
- [Manage VM hosts](/t/-/5140)
- [Manage virtual machines](/t/-/5148)

**# [How to label devices](/t/-/6200)

- [Tag machines](/t/-/5928)
- [Annotate machines](/t/-/5929)
- [Use machine tags](/t/-/5224)
- [Use controller tags](/t/-/5216)
- [Use storage tags](/t/-/5232)
- [Use network tags](/t/-/5228)

**# [How to secure MAAS](/t/-/6503)

- [Improve MAAS security](/t/-/5196)
- [Manage user accounts](/t/-/5184)
- [Enable MAAS native TLS](/t/-/5116)
- [Use Vault with MAAS](/t/-/6942)
- [Set up an air-gapped MAAS](/t/-/5212)

**# [How to operate MAAS](/t/-/6799)

- [Find machines](/t/-/5192)
- [Back up MAAS](/t/-/5096)
- [Monitor MAAS](/t/-/5204)
- [Audit MAAS](/t/-/5987)
- [Troubleshoot MAAS](/t/-/5333)

**# [How to give and receive help](/t/-/5428)

- [Use our discourse forum](/t/-/6802)
- [Get support](https://maas.io/docs/how-to-contact-us)`↗`
- [Request new features](/t/-/4447)
- [Review and report bugs](/t/-/4446)
- [Contribute documentation](/t/-/6949)

**# [General reference](/t/-/6950)

- [Release notes](/t/-/5292)
- [Installation requirements](/t/-/6233)
- [MAAS settings](/t/-/6347)
- [MAAS source code](https://launchpad.net/maas)`↗`
- [Doc style guide](/t/-/4186)
- [Glossary](/t/-/5416)
- [Code of conduct](https://ubuntu.com/community/code-of-conduct)`↗`

**# [API reference](/t/-/6665)

- [API authentication](/t/-/5060)
- [Python API client](/t/-/5404)
- [API documentation](https://maas.io/docs/api)`↗`

**# [Scripts reference](/t/-/5375)

- [Commissioning scripts](/t/-/6605)
- [Hardware test scripts](/t/-/5392)
- [Terraform](/t/-/6327)
<!--
- [cloud-init scripts]()
- [curtin reference]()
-->

**# [Log reference](/t/-/6956)

- [Event logs](/t/-/5252)
- [Audit event logs](/t/-/5256)
- [Commissioning logs](/t/-/5248)
- [Testing logs](/t/-/5314)
<!--
- [controller logs]()
- [maas logs]()
- [network logs]()
- [system logs]()
-->

**# [Machine parameters reference](/t/-/6957)

- [Power drivers](/t/-/5246)
- [Storage layouts](/t/-/5973)
- [Device labelling](/t/-/6941)

**# Explanation

- [MAAS](/t/-/6678)
- [High availability](/t/-/6992)
- [Networking](/t/-/6680)
- [Images](/t/-/6685)
- [Controllers](/t/-/6690)
- [Machines](/t/-/6695)
- [Virtual machines](/t/-/6704)
- [Device labels](/t/-/6709)
- [MAAS events](/t/-/6510)
- [Audit events](/t/-/6372)
- [MAAS logging](/t/-/5240)
- [MAAS security](/t/-/6719)
- [TLS and MAAS](t/-/6720)
- [MAAS performance](/t/-/6178)
- [Ansible](/t/-/6888)

** URLs
[details=Mapping table]
| TOPIC                              | PATH                                                          |
|------------------------------------|---------------------------------------------------------------|
| https://discourse.maas.io/t/-/6720 | /docs/about-tls-and-maas                                      |
| https://discourse.maas.io/t/-/6992 | /docs/about-high-availability                                 |
| https://discourse.maas.io/t/-/6957 | /docs/machine-parameters-reference                            |
| https://discourse.maas.io/t/-/6956 | /docs/maas-logs-reference                                     |
| https://discourse.maas.io/t/-/5375 | /docs/maas-scripts-reference                                  |
| https://discourse.maas.io/t/-/6665 | /docs/api-reference                                           |
| https://discourse.maas.io/t/-/6950 | /docs/general-reference                                       |
| https://discourse.maas.io/t/-/6949 | /docs/how-to-contribute-doc                                   |
| https://discourse.maas.io/t/-/6802 | /docs/how-to-use-our-discourse-forum                          |
| https://discourse.maas.io/t/-/6942 | /docs/how-to-use-hashicorp-vault-with-maas                    |
| https://discourse.maas.io/t/-/6719 | /docs/about-maas-security                                     |
| https://discourse.maas.io/t/-/6941 | /docs/device-labelling-reference                              |
| https://discourse.maas.io/t/-/6709 | /docs/about-device-labels                                     |
| https://discourse.maas.io/t/-/6704 | /docs/about-virtual-machines                                  |
| https://discourse.maas.io/t/-/6799 | /docs/how-to-operate-maas                                     |
| https://discourse.maas.io/t/-/6888 | /docs/about-ansible                                           |
| https://discourse.maas.io/t/-/744  | /docs/building-the-docs                                       |
| https://discourse.maas.io/t/-/745  | /docs/language-details-contributing-to-maas-docs              |
| https://discourse.maas.io/t/-/746  | /docs/working-with-git-and-github                             |
| https://discourse.maas.io/t/-/756  | /docs/install-from-iso                                        |
| https://discourse.maas.io/t/-/777  | /docs/upgrade-2-3-to-2-4-from-ubuntu-16-04                    |
| https://discourse.maas.io/t/-/778  | /docs/upgrade-from-1-9-to-2-x                                 |
| https://discourse.maas.io/t/-/782  | /docs/web-ui                                                  |
| https://discourse.maas.io/t/-/785  | /docs/network-tutorial                                        |
| https://discourse.maas.io/t/-/788  | /docs/whats-new-in-2-6                                        |
| https://discourse.maas.io/t/-/793  | /docs/advanced-cli-tasks                                      |
| https://discourse.maas.io/t/-/794  | /docs/common-cli-tasks                                        |
| https://discourse.maas.io/t/-/795  | /docs/composable-hardware                                     |
| https://discourse.maas.io/t/-/797  | /docs/cli-image-management                                    |
| https://discourse.maas.io/t/-/798  | /docs/cli-interface-management                                |
| https://discourse.maas.io/t/-/799  | /docs/cli-kernel-management                                   |
| https://discourse.maas.io/t/-/815  | /docs/add-an-rsd-host                                         |
| https://discourse.maas.io/t/-/817  | /docs/intel-rack-scale-design-rsd-introduction                |
| https://discourse.maas.io/t/-/818  | /docs/rsd-storage                                             |
| https://discourse.maas.io/t/-/832  | /docs/cli-testing-scripts                                     |
| https://discourse.maas.io/t/-/835  | /docs/historical-release-notes                                |
| https://discourse.maas.io/t/-/839  | /docs/documentation-index                                     |
| https://discourse.maas.io/t/-/1112 | /docs/image-builder                                           |
| https://discourse.maas.io/t/-/2218 | /docs/cli-cookbook                                            |
| https://discourse.maas.io/t/-/2262 | /docs/snap/2.7/cli/about-maas                                 |
| https://discourse.maas.io/t/-/2263 | /docs/snap/2.7/ui/about-maas                                  |
| https://discourse.maas.io/t/-/2264 | /docs/snap/2.8/cli/about-maas                                 |
| https://discourse.maas.io/t/-/2265 | /docs/snap/2.8/ui/about-maas                                  |
| https://discourse.maas.io/t/-/2268 | /docs/deb/2.7/cli/about-maas                                  |
| https://discourse.maas.io/t/-/2269 | /docs/deb/2.7/ui/about-maas                                   |
| https://discourse.maas.io/t/-/2270 | /docs/deb/2.8/cli/about-maas                                  |
| https://discourse.maas.io/t/-/2271 | /docs/deb/2.8/ui/about-maas                                   |
| https://discourse.maas.io/t/-/2274 | /docs/snap/2.7/cli/add-machines                               |
| https://discourse.maas.io/t/-/2275 | /docs/snap/2.7/ui/add-machines                                |
| https://discourse.maas.io/t/-/2276 | /docs/snap/2.8/cli/add-machines                               |
| https://discourse.maas.io/t/-/2277 | /docs/snap/2.8/ui/add-machines                                |
| https://discourse.maas.io/t/-/2280 | /docs/deb/2.7/cli/add-machines                                |
| https://discourse.maas.io/t/-/2281 | /docs/deb/2.7/ui/add-machines                                 |
| https://discourse.maas.io/t/-/2282 | /docs/deb/2.8/cli/add-machines                                |
| https://discourse.maas.io/t/-/2283 | /docs/deb/2.8/ui/add-machines                                 |
| https://discourse.maas.io/t/-/2286 | /docs/snap/2.7/cli/adding-a-vm-host                           |
| https://discourse.maas.io/t/-/2287 | /docs/snap/2.7/ui/adding-a-vm-host                            |
| https://discourse.maas.io/t/-/2288 | /docs/snap/2.8/cli/adding-a-vm-host                           |
| https://discourse.maas.io/t/-/2289 | /docs/snap/2.8/ui/adding-a-vm-host                            |
| https://discourse.maas.io/t/-/2292 | /docs/deb/2.7/cli/adding-a-vm-host                            |
| https://discourse.maas.io/t/-/2293 | /docs/deb/2.7/ui/addupgraing-a-vm-host                        |
| https://discourse.maas.io/t/-/2294 | /docs/deb/2.8/cli/adding-a-vm-host                            |
| https://discourse.maas.io/t/-/2295 | /docs/deb/2.8/ui/adding-a-vm-host                             |
| https://discourse.maas.io/t/-/2298 | /docs/snap/2.7/cli/api-authentication                         |
| https://discourse.maas.io/t/-/2299 | /docs/snap/2.7/ui/api-authentication                          |
| https://discourse.maas.io/t/-/2300 | /docs/snap/2.8/cli/api-authentication                         |
| https://discourse.maas.io/t/-/2301 | /docs/snap/2.8/ui/api-authentication                          |
| https://discourse.maas.io/t/-/2304 | /docs/deb/2.7/cli/api-authentication                          |
| https://discourse.maas.io/t/-/2305 | /docs/deb/2.7/ui/api-authentication                           |
| https://discourse.maas.io/t/-/2306 | /docs/deb/2.8/cli/api-authentication                          |
| https://discourse.maas.io/t/-/2307 | /docs/deb/2.8/ui/api-authentication                           |
| https://discourse.maas.io/t/-/2310 | /docs/snap/2.7/cli/audit-event-logs                           |
| https://discourse.maas.io/t/-/2311 | /docs/snap/2.7/ui/audit-event-logs                            |
| https://discourse.maas.io/t/-/2312 | /docs/snap/2.8/cli/audit-event-logs                           |
| https://discourse.maas.io/t/-/2313 | /docs/snap/2.8/ui/audit-event-logs                            |
| https://discourse.maas.io/t/-/2316 | /docs/deb/2.7/cli/audit-event-logs                            |
| https://discourse.maas.io/t/-/2317 | /docs/deb/2.7/ui/audit-event-logs                             |
| https://discourse.maas.io/t/-/2318 | /docs/deb/2.8/cli/audit-event-logs                            |
| https://discourse.maas.io/t/-/2319 | /docs/deb/2.8/ui/audit-event-logs                             |
| https://discourse.maas.io/t/-/2322 | /docs/snap/2.7/cli/availability-zones                         |
| https://discourse.maas.io/t/-/2323 | /docs/snap/2.7/ui/availability-zones                          |
| https://discourse.maas.io/t/-/2324 | /docs/snap/2.8/cli/availability-zones                         |
| https://discourse.maas.io/t/-/2325 | /docs/snap/2.8/ui/availability-zones                          |
| https://discourse.maas.io/t/-/2328 | /docs/deb/2.7/cli/availability-zones                          |
| https://discourse.maas.io/t/-/2329 | /docs/deb/2.7/ui/availability-zones                           |
| https://discourse.maas.io/t/-/2330 | /docs/deb/2.8/cli/availability-zones                          |
| https://discourse.maas.io/t/-/2331 | /docs/deb/2.8/ui/availability-zones                           |
| https://discourse.maas.io/t/-/2334 | /docs/snap/2.7/cli/backup                                     |
| https://discourse.maas.io/t/-/2335 | /docs/snap/2.7/ui/backup                                      |
| https://discourse.maas.io/t/-/2336 | /docs/snap/2.8/cli/backup                                     |
| https://discourse.maas.io/t/-/2337 | /docs/snap/2.8/ui/backup                                      |
| https://discourse.maas.io/t/-/2340 | /docs/deb/2.7/cli/backup                                      |
| https://discourse.maas.io/t/-/2341 | /docs/deb/2.7/ui/backup                                       |
| https://discourse.maas.io/t/-/2342 | /docs/deb/2.8/cli/backup                                      |
| https://discourse.maas.io/t/-/2343 | /docs/deb/2.8/ui/backup                                       |
| https://discourse.maas.io/t/-/2346 | /docs/snap/2.7/cli/block-devices                              |
| https://discourse.maas.io/t/-/2347 | /docs/snap/2.7/ui/block-devices                               |
| https://discourse.maas.io/t/-/2348 | /docs/snap/2.8/cli/block-devices                              |
| https://discourse.maas.io/t/-/2349 | /docs/snap/2.8/ui/block-devices                               |
| https://discourse.maas.io/t/-/2352 | /docs/deb/2.7/cli/block-devices                               |
| https://discourse.maas.io/t/-/2353 | /docs/deb/2.7/ui/block-devices                                |
| https://discourse.maas.io/t/-/2354 | /docs/deb/2.8/cli/block-devices                               |
| https://discourse.maas.io/t/-/2355 | /docs/deb/2.8/ui/block-devices                                |
| https://discourse.maas.io/t/-/2466 | /docs/snap/2.7/cli/manage-machine-interfaces                  |
| https://discourse.maas.io/t/-/2467 | /docs/snap/2.7/ui/manage-machine-interfaces                   |
| https://discourse.maas.io/t/-/2468 | /docs/snap/2.8/cli/manage-machine-interfaces                  |
| https://discourse.maas.io/t/-/2469 | /docs/snap/2.8/ui/manage-machine-interfaces                   |
| https://discourse.maas.io/t/-/2472 | /docs/deb/2.7/cli/manage-machine-interfaces                   |
| https://discourse.maas.io/t/-/2473 | /docs/deb/2.7/ui/manage-machine-interfaces                    |
| https://discourse.maas.io/t/-/2474 | /docs/deb/2.8/cli/manage-machine-interfaces                   |
| https://discourse.maas.io/t/-/2475 | /docs/deb/2.8/ui/manage-machine-interfaces                    |
| https://discourse.maas.io/t/-/2478 | /docs/snap/2.7/cli/commissioning-and-hardware-testing-scripts |
| https://discourse.maas.io/t/-/2479 | /docs/snap/2.7/ui/commissioning-and-hardware-testing-scripts  |
| https://discourse.maas.io/t/-/2480 | /docs/snap/2.8/cli/commissioning-and-hardware-testing-scripts |
| https://discourse.maas.io/t/-/2481 | /docs/snap/2.8/ui/commissioning-and-hardware-testing-scripts  |
| https://discourse.maas.io/t/-/2484 | /docs/deb/2.7/cli/commissioning-and-hardware-testing-scripts  |
| https://discourse.maas.io/t/-/2485 | /docs/deb/2.7/ui/commissioning-and-hardware-testing-scripts   |
| https://discourse.maas.io/t/-/2486 | /docs/deb/2.8/cli/commissioning-and-hardware-testing-scripts  |
| https://discourse.maas.io/t/-/2487 | /docs/deb/2.8/ui/commissioning-and-hardware-testing-scripts   |
| https://discourse.maas.io/t/-/2490 | /docs/snap/2.7/cli/commissioning-logs                         |
| https://discourse.maas.io/t/-/2491 | /docs/snap/2.7/ui/commissioning-logs                          |
| https://discourse.maas.io/t/-/2492 | /docs/snap/2.8/cli/commissioning-logs                         |
| https://discourse.maas.io/t/-/2493 | /docs/snap/2.8/ui/commissioning-logs                          |
| https://discourse.maas.io/t/-/2496 | /docs/deb/2.7/cli/commissioning-logs                          |
| https://discourse.maas.io/t/-/2497 | /docs/deb/2.7/ui/commissioning-logs                           |
| https://discourse.maas.io/t/-/2498 | /docs/deb/2.8/cli/commissioning-logs                          |
| https://discourse.maas.io/t/-/2499 | /docs/deb/2.8/ui/commissioning-logs                           |
| https://discourse.maas.io/t/-/25   | /docs                                                         |
| https://discourse.maas.io/t/-/25   | /docs/maas-documentation                                      |
| https://discourse.maas.io/t/-/2526 | /docs/snap/2.7/cli/configuration-journey                      |
| https://discourse.maas.io/t/-/2527 | /docs/snap/2.7/ui/configuration-journey                       |
| https://discourse.maas.io/t/-/2528 | /docs/snap/2.8/cli/configuration-journey                      |
| https://discourse.maas.io/t/-/2529 | /docs/snap/2.8/ui/configuration-journey                       |
| https://discourse.maas.io/t/-/2532 | /docs/deb/2.7/cli/configuration-journey                       |
| https://discourse.maas.io/t/-/2533 | /docs/deb/2.7/ui/configuration-journey                        |
| https://discourse.maas.io/t/-/2534 | /docs/deb/2.8/cli/configuration-journey                       |
| https://discourse.maas.io/t/-/2535 | /docs/deb/2.8/ui/configuration-journey                        |
| https://discourse.maas.io/t/-/2538 | /docs/snap/2.7/cli/configuring-tls-encryption                 |
| https://discourse.maas.io/t/-/2539 | /docs/snap/2.7/ui/configuring-tls-encryption                  |
| https://discourse.maas.io/t/-/2540 | /docs/snap/2.8/cli/configuring-tls-encryption                 |
| https://discourse.maas.io/t/-/2541 | /docs/snap/2.8/ui/configuring-tls-encryption                  |
| https://discourse.maas.io/t/-/2544 | /docs/deb/2.7/cli/configuring-tls-encryption                  |
| https://discourse.maas.io/t/-/2545 | /docs/deb/2.7/ui/configuring-tls-encryption                   |
| https://discourse.maas.io/t/-/2546 | /docs/deb/2.8/cli/configuring-tls-encryption                  |
| https://discourse.maas.io/t/-/2547 | /docs/deb/2.8/ui/configuring-tls-encryption                   |
| https://discourse.maas.io/t/-/2563 | /docs/snap/2.7/ui/creating-a-custom-ubuntu-image              |
| https://discourse.maas.io/t/-/2564 | /docs/snap/2.8/cli/creating-a-custom-ubuntu-image             |
| https://discourse.maas.io/t/-/2565 | /docs/snap/2.8/ui/creating-a-custom-ubuntu-image              |
| https://discourse.maas.io/t/-/2568 | /docs/deb/2.7/cli/creating-a-custom-ubuntu-image              |
| https://discourse.maas.io/t/-/2569 | /docs/deb/2.7/ui/creating-a-custom-ubuntu-image               |
| https://discourse.maas.io/t/-/2570 | /docs/deb/2.8/cli/creating-a-custom-ubuntu-image              |
| https://discourse.maas.io/t/-/2571 | /docs/deb/2.8/ui/creating-a-custom-ubuntu-image               |
| https://discourse.maas.io/t/-/2574 | /docs/snap/2.7/cli/creating-and-deleting-vms                  |
| https://discourse.maas.io/t/-/2575 | /docs/snap/2.7/ui/creating-and-deleting-vms                   |
| https://discourse.maas.io/t/-/2576 | /docs/snap/2.8/cli/creating-and-deleting-vms                  |
| https://discourse.maas.io/t/-/2577 | /docs/snap/2.8/ui/creating-and-deleting-vms                   |
| https://discourse.maas.io/t/-/2580 | /docs/deb/2.7/cli/creating-and-deleting-vms                   |
| https://discourse.maas.io/t/-/2581 | /docs/deb/2.7/ui/creating-and-deleting-vms                    |
| https://discourse.maas.io/t/-/2582 | /docs/deb/2.8/cli/creating-and-deleting-vms                   |
| https://discourse.maas.io/t/-/2583 | /docs/deb/2.8/ui/creating-and-deleting-vms                    |
| https://discourse.maas.io/t/-/2586 | /docs/snap/2.7/cli/custom-machine-setup                       |
| https://discourse.maas.io/t/-/2587 | /docs/snap/2.7/ui/custom-machine-setup                        |
| https://discourse.maas.io/t/-/2588 | /docs/snap/2.8/cli/custom-machine-setup                       |
| https://discourse.maas.io/t/-/2589 | /docs/snap/2.8/ui/custom-machine-setup                        |
| https://discourse.maas.io/t/-/2592 | /docs/deb/2.7/cli/custom-machine-setup                        |
| https://discourse.maas.io/t/-/2593 | /docs/deb/2.7/ui/custom-machine-setup                         |
| https://discourse.maas.io/t/-/2594 | /docs/deb/2.8/cli/custom-machine-setup                        |
| https://discourse.maas.io/t/-/2595 | /docs/deb/2.8/ui/custom-machine-setup                         |
| https://discourse.maas.io/t/-/2598 | /docs/snap/2.7/cli/deploy-machines                            |
| https://discourse.maas.io/t/-/2599 | /docs/snap/2.7/ui/deploy-machines                             |
| https://discourse.maas.io/t/-/2600 | /docs/snap/2.8/cli/deploy-machines                            |
| https://discourse.maas.io/t/-/2601 | /docs/snap/2.8/ui/deploy-machines                             |
| https://discourse.maas.io/t/-/2604 | /docs/deb/2.7/cli/deploy-machines                             |
| https://discourse.maas.io/t/-/2605 | /docs/deb/2.7/ui/deploy-machines                              |
| https://discourse.maas.io/t/-/2606 | /docs/deb/2.8/cli/deploy-machines                             |
| https://discourse.maas.io/t/-/2607 | /docs/deb/2.8/ui/deploy-machines                              |
| https://discourse.maas.io/t/-/2610 | /docs/snap/2.7/cli/disk-erasure                               |
| https://discourse.maas.io/t/-/2611 | /docs/snap/2.7/ui/disk-erasure                                |
| https://discourse.maas.io/t/-/2612 | /docs/snap/2.8/cli/disk-erasure                               |
| https://discourse.maas.io/t/-/2613 | /docs/snap/2.8/ui/disk-erasure                                |
| https://discourse.maas.io/t/-/2616 | /docs/deb/2.7/cli/disk-erasure                                |
| https://discourse.maas.io/t/-/2617 | /docs/deb/2.7/ui/disk-erasure                                 |
| https://discourse.maas.io/t/-/2618 | /docs/deb/2.8/cli/disk-erasure                                |
| https://discourse.maas.io/t/-/2619 | /docs/deb/2.8/ui/disk-erasure                                 |
| https://discourse.maas.io/t/-/2634 | /docs/snap/2.7/cli/explore-maas                               |
| https://discourse.maas.io/t/-/2635 | /docs/snap/2.7/ui/explore-maas                                |
| https://discourse.maas.io/t/-/2636 | /docs/snap/2.8/cli/explore-maas                               |
| https://discourse.maas.io/t/-/2637 | /docs/snap/2.8/ui/explore-maas                                |
| https://discourse.maas.io/t/-/2640 | /docs/deb/2.7/cli/explore-maas                                |
| https://discourse.maas.io/t/-/2641 | /docs/deb/2.7/ui/explore-maas                                 |
| https://discourse.maas.io/t/-/2642 | /docs/deb/2.8/cli/explore-maas                                |
| https://discourse.maas.io/t/-/2643 | /docs/deb/2.8/ui/explore-maas                                 |
| https://discourse.maas.io/t/-/2646 | /docs/snap/2.7/cli/give-me-an-example-of-maas                 |
| https://discourse.maas.io/t/-/2647 | /docs/snap/2.7/ui/give-me-an-example-of-maas                  |
| https://discourse.maas.io/t/-/2648 | /docs/snap/2.8/cli/give-me-an-example-of-maas                 |
| https://discourse.maas.io/t/-/2649 | /docs/snap/2.8/ui/give-me-an-example-of-maas                  |
| https://discourse.maas.io/t/-/2652 | /docs/deb/2.7/cli/give-me-an-example-of-maas                  |
| https://discourse.maas.io/t/-/2653 | /docs/deb/2.7/ui/give-me-an-example-of-maas                   |
| https://discourse.maas.io/t/-/2654 | /docs/deb/2.8/cli/give-me-an-example-of-maas                  |
| https://discourse.maas.io/t/-/2655 | /docs/deb/2.8/ui/give-me-an-example-of-maas                   |
| https://discourse.maas.io/t/-/2658 | /docs/snap/2.7/cli/hardening-your-maas-installation           |
| https://discourse.maas.io/t/-/2659 | /docs/snap/2.7/ui/hardening-your-maas-installation            |
| https://discourse.maas.io/t/-/2660 | /docs/snap/2.8/cli/hardening-your-maas-installation           |
| https://discourse.maas.io/t/-/2661 | /docs/snap/2.8/ui/hardening-your-maas-installation            |
| https://discourse.maas.io/t/-/2664 | /docs/deb/2.7/cli/hardening-your-maas-installation            |
| https://discourse.maas.io/t/-/2665 | /docs/deb/2.7/ui/hardening-your-maas-installation             |
| https://discourse.maas.io/t/-/2666 | /docs/deb/2.8/cli/hardening-your-maas-installation            |
| https://discourse.maas.io/t/-/2667 | /docs/deb/2.8/ui/hardening-your-maas-installation             |
| https://discourse.maas.io/t/-/2670 | /docs/snap/2.7/cli/hardware-testing                           |
| https://discourse.maas.io/t/-/2671 | /docs/snap/2.7/ui/hardware-testing                            |
| https://discourse.maas.io/t/-/2672 | /docs/snap/2.8/cli/hardware-testing                           |
| https://discourse.maas.io/t/-/2673 | /docs/snap/2.8/ui/hardware-testing                            |
| https://discourse.maas.io/t/-/2676 | /docs/deb/2.7/cli/hardware-testing                            |
| https://discourse.maas.io/t/-/2677 | /docs/deb/2.7/ui/hardware-testing                             |
| https://discourse.maas.io/t/-/2678 | /docs/deb/2.8/cli/hardware-testing                            |
| https://discourse.maas.io/t/-/2679 | /docs/deb/2.8/ui/hardware-testing                             |
| https://discourse.maas.io/t/-/2682 | /docs/snap/2.7/cli/high-availability                          |
| https://discourse.maas.io/t/-/2683 | /docs/snap/2.7/ui/high-availability                           |
| https://discourse.maas.io/t/-/2684 | /docs/snap/2.8/cli/high-availability                          |
| https://discourse.maas.io/t/-/2685 | /docs/snap/2.8/ui/high-availability                           |
| https://discourse.maas.io/t/-/2688 | /docs/deb/2.7/cli/high-availability                           |
| https://discourse.maas.io/t/-/2689 | /docs/deb/2.7/ui/high-availability                            |
| https://discourse.maas.io/t/-/2690 | /docs/deb/2.8/cli/high-availability                           |
| https://discourse.maas.io/t/-/2691 | /docs/deb/2.8/ui/high-availability                            |
| https://discourse.maas.io/t/-/2694 | /docs/snap/2.7/cli/images                                     |
| https://discourse.maas.io/t/-/2695 | /docs/snap/2.7/ui/images                                      |
| https://discourse.maas.io/t/-/2696 | /docs/snap/2.8/cli/images                                     |
| https://discourse.maas.io/t/-/2697 | /docs/snap/2.8/ui/images                                      |
| https://discourse.maas.io/t/-/2700 | /docs/deb/2.7/cli/images                                      |
| https://discourse.maas.io/t/-/2701 | /docs/deb/2.7/ui/images                                       |
| https://discourse.maas.io/t/-/2702 | /docs/deb/2.8/cli/images                                      |
| https://discourse.maas.io/t/-/2703 | /docs/deb/2.8/ui/images                                       |
| https://discourse.maas.io/t/-/2718 | /docs/snap/2.7/cli/controllers                                |
| https://discourse.maas.io/t/-/2719 | /docs/snap/2.7/ui/controllers                                 |
| https://discourse.maas.io/t/-/2720 | /docs/snap/2.8/cli/controllers                                |
| https://discourse.maas.io/t/-/2721 | /docs/snap/2.8/ui/controllers                                 |
| https://discourse.maas.io/t/-/2724 | /docs/deb/2.7/cli/controllers                                 |
| https://discourse.maas.io/t/-/2725 | /docs/deb/2.7/ui/controllers                                  |
| https://discourse.maas.io/t/-/2726 | /docs/deb/2.8/cli/controllers                                 |
| https://discourse.maas.io/t/-/2727 | /docs/deb/2.8/ui/controllers                                  |
| https://discourse.maas.io/t/-/2730 | /docs/snap/2.7/cli/machines                                   |
| https://discourse.maas.io/t/-/2731 | /docs/snap/2.7/ui/machines                                    |
| https://discourse.maas.io/t/-/2732 | /docs/snap/2.8/cli/machines                                   |
| https://discourse.maas.io/t/-/2733 | /docs/snap/2.8/ui/machines                                    |
| https://discourse.maas.io/t/-/2736 | /docs/deb/2.7/cli/machines                                    |
| https://discourse.maas.io/t/-/2737 | /docs/deb/2.7/ui/machines                                     |
| https://discourse.maas.io/t/-/2738 | /docs/deb/2.8/cli/machines                                    |
| https://discourse.maas.io/t/-/2739 | /docs/deb/2.8/ui/machines                                     |
| https://discourse.maas.io/t/-/2742 | /docs/snap/2.7/cli/vm-hosting                                 |
| https://discourse.maas.io/t/-/2743 | /docs/snap/2.7/ui/vm-hosting                                  |
| https://discourse.maas.io/t/-/2744 | /docs/snap/2.8/cli/vm-hosting                                 |
| https://discourse.maas.io/t/-/2745 | /docs/snap/2.8/ui/vm-hosting                                  |
| https://discourse.maas.io/t/-/2748 | /docs/deb/2.7/cli/vm-hosting                                  |
| https://discourse.maas.io/t/-/2749 | /docs/deb/2.7/ui/vm-hosting                                   |
| https://discourse.maas.io/t/-/2750 | /docs/deb/2.8/cli/vm-hosting                                  |
| https://discourse.maas.io/t/-/2751 | /docs/deb/2.8/ui/vm-hosting                                   |
| https://discourse.maas.io/t/-/2754 | /docs/snap/2.7/cli/ip-ranges                                  |
| https://discourse.maas.io/t/-/2755 | /docs/snap/2.7/ui/ip-ranges                                   |
| https://discourse.maas.io/t/-/2756 | /docs/snap/2.8/cli/ip-ranges                                  |
| https://discourse.maas.io/t/-/2757 | /docs/snap/2.8/ui/ip-ranges                                   |
| https://discourse.maas.io/t/-/2760 | /docs/deb/2.7/cli/ip-ranges                                   |
| https://discourse.maas.io/t/-/2761 | /docs/deb/2.7/ui/ip-ranges                                    |
| https://discourse.maas.io/t/-/2762 | /docs/deb/2.8/cli/ip-ranges                                   |
| https://discourse.maas.io/t/-/2763 | /docs/deb/2.8/ui/ip-ranges                                    |
| https://discourse.maas.io/t/-/2766 | /docs/snap/2.7/cli/ipv6-addressing                            |
| https://discourse.maas.io/t/-/2767 | /docs/snap/2.7/ui/ipv6-addressing                             |
| https://discourse.maas.io/t/-/2768 | /docs/snap/2.8/cli/ipv6-addressing                            |
| https://discourse.maas.io/t/-/2769 | /docs/snap/2.8/ui/ipv6-addressing                             |
| https://discourse.maas.io/t/-/2772 | /docs/deb/2.7/cli/ipv6-addressing                             |
| https://discourse.maas.io/t/-/2773 | /docs/deb/2.7/ui/ipv6-addressing                              |
| https://discourse.maas.io/t/-/2774 | /docs/deb/2.8/cli/ipv6-addressing                             |
| https://discourse.maas.io/t/-/2775 | /docs/deb/2.8/ui/ipv6-addressing                              |
| https://discourse.maas.io/t/-/2778 | /docs/snap/2.7/cli/kernel-boot-options                        |
| https://discourse.maas.io/t/-/2779 | /docs/snap/2.7/ui/kernel-boot-options                         |
| https://discourse.maas.io/t/-/2780 | /docs/snap/2.8/cli/kernel-boot-options                        |
| https://discourse.maas.io/t/-/2781 | /docs/snap/2.8/ui/kernel-boot-options                         |
| https://discourse.maas.io/t/-/2784 | /docs/deb/2.7/cli/kernel-boot-options                         |
| https://discourse.maas.io/t/-/2785 | /docs/deb/2.7/ui/kernel-boot-options                          |
| https://discourse.maas.io/t/-/2786 | /docs/deb/2.8/cli/kernel-boot-options                         |
| https://discourse.maas.io/t/-/2802 | /docs/snap/2.7/cli/local-image-mirror                         |
| https://discourse.maas.io/t/-/2803 | /docs/snap/2.7/ui/local-image-mirror                          |
| https://discourse.maas.io/t/-/2804 | /docs/snap/2.8/cli/local-image-mirror                         |
| https://discourse.maas.io/t/-/2805 | /docs/snap/2.8/ui/local-image-mirror                          |
| https://discourse.maas.io/t/-/2808 | /docs/deb/2.7/cli/local-image-mirror                          |
| https://discourse.maas.io/t/-/2809 | /docs/deb/2.7/ui/local-image-mirror                           |
| https://discourse.maas.io/t/-/2810 | /docs/deb/2.8/cli/local-image-mirror                          |
| https://discourse.maas.io/t/-/2811 | /docs/deb/2.8/ui/local-image-mirror                           |
| https://discourse.maas.io/t/-/2814 | /docs/snap/2.7/cli/maas-cli                                   |
| https://discourse.maas.io/t/-/2815 | /docs/snap/2.7/ui/maas-cli                                    |
| https://discourse.maas.io/t/-/2816 | /docs/snap/2.8/cli/maas-cli                                   |
| https://discourse.maas.io/t/-/2817 | /docs/snap/2.8/ui/maas-cli                                    |
| https://discourse.maas.io/t/-/2820 | /docs/deb/2.7/cli/maas-cli                                    |
| https://discourse.maas.io/t/-/2821 | /docs/deb/2.7/ui/maas-cli                                     |
| https://discourse.maas.io/t/-/2822 | /docs/deb/2.8/cli/maas-cli                                    |
| https://discourse.maas.io/t/-/2823 | /docs/deb/2.8/ui/maas-cli                                     |
| https://discourse.maas.io/t/-/2826 | /docs/snap/2.7/cli/maas-communication                         |
| https://discourse.maas.io/t/-/2827 | /docs/snap/2.7/ui/maas-communication                          |
| https://discourse.maas.io/t/-/2828 | /docs/snap/2.8/cli/maas-communication                         |
| https://discourse.maas.io/t/-/2829 | /docs/snap/2.8/ui/maas-communication                          |
| https://discourse.maas.io/t/-/2832 | /docs/deb/2.7/cli/maas-communication                          |
| https://discourse.maas.io/t/-/2833 | /docs/deb/2.7/ui/maas-communication                           |
| https://discourse.maas.io/t/-/2834 | /docs/deb/2.8/cli/maas-communication                          |
| https://discourse.maas.io/t/-/2835 | /docs/deb/2.8/ui/maas-communication                           |
| https://discourse.maas.io/t/-/2838 | /docs/snap/2.7/cli/maas-documentation                         |
| https://discourse.maas.io/t/-/2839 | /docs/snap/2.7/ui/maas-documentation                          |
| https://discourse.maas.io/t/-/2840 | /docs/snap/2.8/cli/maas-documentation                         |
| https://discourse.maas.io/t/-/2841 | /docs/snap/2.8/ui/maas-documentation                          |
| https://discourse.maas.io/t/-/2842 | /docs/snap/2.9/cli/maas-documentation                         |
| https://discourse.maas.io/t/-/2843 | /docs/snap/2.9/ui/maas-documentation                          |
| https://discourse.maas.io/t/-/2844 | /docs/deb/2.7/cli/maas-documentation                          |
| https://discourse.maas.io/t/-/2845 | /docs/deb/2.7/ui/maas-documentation                           |
| https://discourse.maas.io/t/-/2846 | /docs/deb/2.8/cli/maas-documentation                          |
| https://discourse.maas.io/t/-/2847 | /docs/deb/2.8/ui/maas-documentation                           |
| https://discourse.maas.io/t/-/2850 | /docs/snap/2.7/cli/maas-image-builder                         |
| https://discourse.maas.io/t/-/2851 | /docs/snap/2.7/ui/maas-image-builder                          |
| https://discourse.maas.io/t/-/2852 | /docs/snap/2.8/cli/maas-image-builder                         |
| https://discourse.maas.io/t/-/2853 | /docs/snap/2.8/ui/maas-image-builder                          |
| https://discourse.maas.io/t/-/2856 | /docs/deb/2.7/cli/maas-image-builder                          |
| https://discourse.maas.io/t/-/2857 | /docs/deb/2.7/ui/maas-image-builder                           |
| https://discourse.maas.io/t/-/2858 | /docs/deb/2.8/cli/maas-image-builder                          |
| https://discourse.maas.io/t/-/2859 | /docs/deb/2.8/ui/maas-image-builder                           |
| https://discourse.maas.io/t/-/2862 | /docs/snap/2.7/cli/maas-logging                               |
| https://discourse.maas.io/t/-/2863 | /docs/snap/2.7/ui/maas-logging                                |
| https://discourse.maas.io/t/-/2864 | /docs/snap/2.8/cli/maas-logging                               |
| https://discourse.maas.io/t/-/2865 | /docs/snap/2.8/ui/maas-logging                                |
| https://discourse.maas.io/t/-/2868 | /docs/deb/2.7/cli/maas-logging                                |
| https://discourse.maas.io/t/-/2869 | /docs/deb/2.7/ui/maas-logging                                 |
| https://discourse.maas.io/t/-/2870 | /docs/deb/2.8/cli/maas-logging                                |
| https://discourse.maas.io/t/-/2871 | /docs/deb/2.8/ui/maas-logging                                 |
| https://discourse.maas.io/t/-/2874 | /docs/snap/2.7/cli/maas-requirements                          |
| https://discourse.maas.io/t/-/2875 | /docs/snap/2.7/ui/maas-requirements                           |
| https://discourse.maas.io/t/-/2876 | /docs/snap/2.8/cli/maas-requirements                          |
| https://discourse.maas.io/t/-/2877 | /docs/snap/2.8/ui/maas-requirements                           |
| https://discourse.maas.io/t/-/2880 | /docs/deb/2.7/cli/maas-requirements                           |
| https://discourse.maas.io/t/-/2881 | /docs/deb/2.7/ui/maas-requirements                            |
| https://discourse.maas.io/t/-/2882 | /docs/deb/2.8/cli/maas-requirements                           |
| https://discourse.maas.io/t/-/2883 | /docs/deb/2.8/ui/maas-requirements                            |
| https://discourse.maas.io/t/-/2886 | /docs/snap/2.7/cli/tags-and-annotations                       |
| https://discourse.maas.io/t/-/2887 | /docs/snap/2.7/ui/tags-and-annotations                        |
| https://discourse.maas.io/t/-/2888 | /docs/snap/2.8/cli/tags-and-annotations                       |
| https://discourse.maas.io/t/-/2889 | /docs/snap/2.8/ui/tags-and-annotations                        |
| https://discourse.maas.io/t/-/2892 | /docs/deb/2.7/cli/tags-and-annotations                        |
| https://discourse.maas.io/t/-/2893 | /docs/deb/2.7/ui/tags-and-annotations                         |
| https://discourse.maas.io/t/-/2894 | /docs/deb/2.8/cli/tags-and-annotations                        |
| https://discourse.maas.io/t/-/2895 | /docs/deb/2.8/ui/tags-and-annotations                         |
| https://discourse.maas.io/t/-/2898 | /docs/snap/2.7/cli/managing-dhcp                              |
| https://discourse.maas.io/t/-/2899 | /docs/snap/2.7/ui/managing-dhcp                               |
| https://discourse.maas.io/t/-/2900 | /docs/snap/2.8/cli/managing-dhcp                              |
| https://discourse.maas.io/t/-/2901 | /docs/snap/2.8/ui/managing-dhcp                               |
| https://discourse.maas.io/t/-/2904 | /docs/deb/2.7/cli/managing-dhcp                               |
| https://discourse.maas.io/t/-/2905 | /docs/deb/2.7/ui/managing-dhcp                                |
| https://discourse.maas.io/t/-/2906 | /docs/deb/2.8/cli/managing-dhcp                               |
| https://discourse.maas.io/t/-/2907 | /docs/deb/2.8/ui/managing-dhcp                                |
| https://discourse.maas.io/t/-/2910 | /docs/snap/2.7/cli/managing-stp                               |
| https://discourse.maas.io/t/-/2911 | /docs/snap/2.7/ui/managing-stp                                |
| https://discourse.maas.io/t/-/2912 | /docs/snap/2.8/cli/managing-stp                               |
| https://discourse.maas.io/t/-/2913 | /docs/snap/2.8/ui/managing-stp                                |
| https://discourse.maas.io/t/-/2916 | /docs/deb/2.7/cli/managing-stp                                |
| https://discourse.maas.io/t/-/2917 | /docs/deb/2.7/ui/managing-stp                                 |
| https://discourse.maas.io/t/-/2918 | /docs/deb/2.8/cli/managing-stp                                |
| https://discourse.maas.io/t/-/2919 | /docs/deb/2.8/ui/managing-stp                                 |
| https://discourse.maas.io/t/-/2922 | /docs/snap/2.7/cli/network-discovery                          |
| https://discourse.maas.io/t/-/2923 | /docs/snap/2.7/ui/network-discovery                           |
| https://discourse.maas.io/t/-/2924 | /docs/snap/2.8/cli/network-discovery                          |
| https://discourse.maas.io/t/-/2925 | /docs/snap/2.8/ui/network-discovery                           |
| https://discourse.maas.io/t/-/2928 | /docs/deb/2.7/cli/network-discovery                           |
| https://discourse.maas.io/t/-/2929 | /docs/deb/2.7/ui/network-discovery                            |
| https://discourse.maas.io/t/-/2930 | /docs/deb/2.8/cli/network-discovery                           |
| https://discourse.maas.io/t/-/2931 | /docs/deb/2.8/ui/network-discovery                            |
| https://discourse.maas.io/t/-/2934 | /docs/snap/2.7/cli/network-testing                            |
| https://discourse.maas.io/t/-/2935 | /docs/snap/2.7/ui/network-testing                             |
| https://discourse.maas.io/t/-/2936 | /docs/snap/2.8/cli/network-testing                            |
| https://discourse.maas.io/t/-/2937 | /docs/snap/2.8/ui/network-testing                             |
| https://discourse.maas.io/t/-/2940 | /docs/deb/2.7/cli/network-testing                             |
| https://discourse.maas.io/t/-/2941 | /docs/deb/2.7/ui/network-testing                              |
| https://discourse.maas.io/t/-/2942 | /docs/deb/2.8/cli/network-testing                             |
| https://discourse.maas.io/t/-/2943 | /docs/deb/2.8/ui/network-testing                              |
| https://discourse.maas.io/t/-/2946 | /docs/snap/2.7/cli/networking                                 |
| https://discourse.maas.io/t/-/2947 | /docs/snap/2.7/ui/networking                                  |
| https://discourse.maas.io/t/-/2948 | /docs/snap/2.8/cli/networking                                 |
| https://discourse.maas.io/t/-/2949 | /docs/snap/2.8/ui/networking                                  |
| https://discourse.maas.io/t/-/2952 | /docs/deb/2.7/cli/networking                                  |
| https://discourse.maas.io/t/-/2953 | /docs/deb/2.7/ui/networking                                   |
| https://discourse.maas.io/t/-/2954 | /docs/deb/2.8/cli/networking                                  |
| https://discourse.maas.io/t/-/2955 | /docs/deb/2.8/ui/networking                                   |
| https://discourse.maas.io/t/-/2958 | /docs/snap/2.7/cli/ntp-services                               |
| https://discourse.maas.io/t/-/2959 | /docs/snap/2.7/ui/ntp-services                                |
| https://discourse.maas.io/t/-/2960 | /docs/snap/2.8/cli/ntp-services                               |
| https://discourse.maas.io/t/-/2961 | /docs/snap/2.8/ui/ntp-services                                |
| https://discourse.maas.io/t/-/2964 | /docs/deb/2.7/cli/ntp-services                                |
| https://discourse.maas.io/t/-/2965 | /docs/deb/2.7/ui/ntp-services                                 |
| https://discourse.maas.io/t/-/2966 | /docs/deb/2.8/cli/ntp-services                                |
| https://discourse.maas.io/t/-/2967 | /docs/deb/2.8/ui/ntp-services                                 |
| https://discourse.maas.io/t/-/2976 | /docs/deb/2.7/cli/package-repositories                        |
| https://discourse.maas.io/t/-/2977 | /docs/deb/2.7/ui/package-repositories                         |
| https://discourse.maas.io/t/-/2978 | /docs/deb/2.8/cli/package-repositories                        |
| https://discourse.maas.io/t/-/2979 | /docs/deb/2.8/ui/package-repositories                         |
| https://discourse.maas.io/t/-/2980 | /docs/deb/2.9/cli/package-repositories                        |
| https://discourse.maas.io/t/-/2981 | /docs/deb/2.9/ui/package-repositories                         |
| https://discourse.maas.io/t/-/2982 | /docs/snap/2.7/cli/partitions                                 |
| https://discourse.maas.io/t/-/2983 | /docs/snap/2.7/ui/partitions                                  |
| https://discourse.maas.io/t/-/2984 | /docs/snap/2.8/cli/partitions                                 |
| https://discourse.maas.io/t/-/2985 | /docs/snap/2.8/ui/partitions                                  |
| https://discourse.maas.io/t/-/2988 | /docs/deb/2.7/cli/partitions                                  |
| https://discourse.maas.io/t/-/2989 | /docs/deb/2.7/ui/partitions                                   |
| https://discourse.maas.io/t/-/2990 | /docs/deb/2.8/cli/partitions                                  |
| https://discourse.maas.io/t/-/2991 | /docs/deb/2.8/ui/partitions                                   |
| https://discourse.maas.io/t/-/2994 | /docs/snap/2.7/cli/postgresql-ha-hot-standby                  |
| https://discourse.maas.io/t/-/2995 | /docs/snap/2.7/ui/postgresql-ha-hot-standby                   |
| https://discourse.maas.io/t/-/2996 | /docs/snap/2.8/cli/postgresql-ha-hot-standby                  |
| https://discourse.maas.io/t/-/2997 | /docs/snap/2.8/ui/postgresql-ha-hot-standby                   |
| https://discourse.maas.io/t/-/2998 | /docs/snap/2.9/cli/postgresql-ha-hot-standby                  |
| https://discourse.maas.io/t/-/2999 | /docs/snap/2.9/ui/postgresql-ha-hot-standby                   |
| https://discourse.maas.io/t/-/3000 | /docs/deb/2.7/cli/postgresql-ha-hot-standby                   |
| https://discourse.maas.io/t/-/3001 | /docs/deb/2.7/ui/postgresql-ha-hot-standby                    |
| https://discourse.maas.io/t/-/3002 | /docs/deb/2.8/cli/postgresql-ha-hot-standby                   |
| https://discourse.maas.io/t/-/3003 | /docs/deb/2.8/ui/postgresql-ha-hot-standby                    |
| https://discourse.maas.io/t/-/3004 | /docs/deb/2.9/cli/postgresql-ha-hot-standby                   |
| https://discourse.maas.io/t/-/3005 | /docs/deb/2.9/ui/postgresql-ha-hot-standby                    |
| https://discourse.maas.io/t/-/3006 | /docs/snap/2.7/cli/power-management                           |
| https://discourse.maas.io/t/-/3007 | /docs/snap/2.7/ui/power-management                            |
| https://discourse.maas.io/t/-/3008 | /docs/snap/2.8/cli/power-management                           |
| https://discourse.maas.io/t/-/3009 | /docs/snap/2.8/ui/power-management                            |
| https://discourse.maas.io/t/-/3012 | /docs/deb/2.7/cli/power-management                            |
| https://discourse.maas.io/t/-/3013 | /docs/deb/2.7/ui/power-management                             |
| https://discourse.maas.io/t/-/3014 | /docs/deb/2.8/cli/power-management                            |
| https://discourse.maas.io/t/-/3015 | /docs/deb/2.8/ui/power-management                             |
| https://discourse.maas.io/t/-/3018 | /docs/snap/2.7/cli/prometheus-metrics                         |
| https://discourse.maas.io/t/-/3019 | /docs/snap/2.7/ui/prometheus-metrics                          |
| https://discourse.maas.io/t/-/3020 | /docs/snap/2.8/cli/prometheus-metrics                         |
| https://discourse.maas.io/t/-/3021 | /docs/snap/2.8/ui/prometheus-metrics                          |
| https://discourse.maas.io/t/-/3024 | /docs/deb/2.7/cli/prometheus-metrics                          |
| https://discourse.maas.io/t/-/3025 | /docs/deb/2.7/ui/prometheus-metrics                           |
| https://discourse.maas.io/t/-/3026 | /docs/deb/2.8/cli/prometheus-metrics                          |
| https://discourse.maas.io/t/-/3027 | /docs/deb/2.8/ui/prometheus-metrics                           |
| https://discourse.maas.io/t/-/3030 | /docs/snap/2.7/cli/proxy                                      |
| https://discourse.maas.io/t/-/3031 | /docs/snap/2.7/ui/proxy                                       |
| https://discourse.maas.io/t/-/3032 | /docs/snap/2.8/cli/proxy                                      |
| https://discourse.maas.io/t/-/3033 | /docs/snap/2.8/ui/proxy                                       |
| https://discourse.maas.io/t/-/3036 | /docs/deb/2.7/cli/proxy                                       |
| https://discourse.maas.io/t/-/3037 | /docs/deb/2.7/ui/proxy                                        |
| https://discourse.maas.io/t/-/3038 | /docs/deb/2.8/cli/proxy                                       |
| https://discourse.maas.io/t/-/3039 | /docs/deb/2.8/ui/proxy                                        |
| https://discourse.maas.io/t/-/3054 | /docs/snap/2.7/cli/rack-controllers                           |
| https://discourse.maas.io/t/-/3055 | /docs/snap/2.7/ui/rack-controllers                            |
| https://discourse.maas.io/t/-/3056 | /docs/snap/2.8/cli/rack-controllers                           |
| https://discourse.maas.io/t/-/3057 | /docs/snap/2.8/ui/rack-controllers                            |
| https://discourse.maas.io/t/-/3060 | /docs/deb/2.7/cli/rack-controllers                            |
| https://discourse.maas.io/t/-/3061 | /docs/deb/2.7/ui/rack-controllers                             |
| https://discourse.maas.io/t/-/3062 | /docs/deb/2.8/cli/rack-controllers                            |
| https://discourse.maas.io/t/-/3063 | /docs/deb/2.8/ui/rack-controllers                             |
| https://discourse.maas.io/t/-/3066 | /docs/snap/2.7/cli/region-controllers                         |
| https://discourse.maas.io/t/-/3067 | /docs/snap/2.7/ui/region-controllers                          |
| https://discourse.maas.io/t/-/3068 | /docs/snap/2.8/cli/region-controllers                         |
| https://discourse.maas.io/t/-/3069 | /docs/snap/2.8/ui/region-controllers                          |
| https://discourse.maas.io/t/-/3072 | /docs/deb/2.7/cli/region-controllers                          |
| https://discourse.maas.io/t/-/3073 | /docs/deb/2.7/ui/region-controllers                           |
| https://discourse.maas.io/t/-/3074 | /docs/deb/2.8/cli/region-controllers                          |
| https://discourse.maas.io/t/-/3075 | /docs/deb/2.8/ui/region-controllers                           |
| https://discourse.maas.io/t/-/3078 | /docs/snap/2.7/cli/resource-pools                             |
| https://discourse.maas.io/t/-/3079 | /docs/snap/2.7/ui/resource-pools                              |
| https://discourse.maas.io/t/-/3080 | /docs/snap/2.8/cli/resource-pools                             |
| https://discourse.maas.io/t/-/3081 | /docs/snap/2.8/ui/resource-pools                              |
| https://discourse.maas.io/t/-/3082 | /docs/snap/2.9/cli/resource-pools                             |
| https://discourse.maas.io/t/-/3083 | /docs/snap/2.9/ui/resource-pools                              |
| https://discourse.maas.io/t/-/3084 | /docs/deb/2.7/cli/resource-pools                              |
| https://discourse.maas.io/t/-/3085 | /docs/deb/2.7/ui/resource-pools                               |
| https://discourse.maas.io/t/-/3086 | /docs/deb/2.8/cli/resource-pools                              |
| https://discourse.maas.io/t/-/3087 | /docs/deb/2.8/ui/resource-pools                               |
| https://discourse.maas.io/t/-/3088 | /docs/deb/2.9/cli/resource-pools                              |
| https://discourse.maas.io/t/-/3089 | /docs/deb/2.9/ui/resource-pools                               |
| https://discourse.maas.io/t/-/3090 | /docs/snap/2.7/cli/select-and-import-images                   |
| https://discourse.maas.io/t/-/3091 | /docs/snap/2.7/ui/select-and-import-images                    |
| https://discourse.maas.io/t/-/3092 | /docs/snap/2.8/cli/select-and-import-images                   |
| https://discourse.maas.io/t/-/3093 | /docs/snap/2.8/ui/select-and-import-images                    |
| https://discourse.maas.io/t/-/3096 | /docs/deb/2.7/cli/select-and-import-images                    |
| https://discourse.maas.io/t/-/3097 | /docs/deb/2.7/ui/select-and-import-images                     |
| https://discourse.maas.io/t/-/3098 | /docs/deb/2.8/cli/select-and-import-images                    |
| https://discourse.maas.io/t/-/3099 | /docs/deb/2.8/ui/select-and-import-images                     |
| https://discourse.maas.io/t/-/3102 | /docs/snap/2.7/cli/storage                                    |
| https://discourse.maas.io/t/-/3103 | /docs/snap/2.7/ui/storage                                     |
| https://discourse.maas.io/t/-/3104 | /docs/snap/2.8/cli/storage                                    |
| https://discourse.maas.io/t/-/3105 | /docs/snap/2.8/ui/storage                                     |
| https://discourse.maas.io/t/-/3106 | /docs/snap/2.9/cli/storage                                    |
| https://discourse.maas.io/t/-/3107 | /docs/snap/2.9/ui/storage                                     |
| https://discourse.maas.io/t/-/3108 | /docs/deb/2.7/cli/storage                                     |
| https://discourse.maas.io/t/-/3109 | /docs/deb/2.7/ui/storage                                      |
| https://discourse.maas.io/t/-/3110 | /docs/deb/2.8/cli/storage                                     |
| https://discourse.maas.io/t/-/3111 | /docs/deb/2.8/ui/storage                                      |
| https://discourse.maas.io/t/-/3112 | /docs/deb/2.9/cli/storage                                     |
| https://discourse.maas.io/t/-/3113 | /docs/deb/2.9/ui/storage                                      |
| https://discourse.maas.io/t/-/3114 | /docs/snap/2.7/cli/subnet-management                          |
| https://discourse.maas.io/t/-/3115 | /docs/snap/2.7/ui/subnet-management                           |
| https://discourse.maas.io/t/-/3116 | /docs/snap/2.8/cli/subnet-management                          |
| https://discourse.maas.io/t/-/3117 | /docs/snap/2.8/ui/subnet-management                           |
| https://discourse.maas.io/t/-/3120 | /docs/deb/2.7/cli/subnet-management                           |
| https://discourse.maas.io/t/-/3121 | /docs/deb/2.7/ui/subnet-management                            |
| https://discourse.maas.io/t/-/3122 | /docs/deb/2.8/cli/subnet-management                           |
| https://discourse.maas.io/t/-/3123 | /docs/deb/2.8/ui/subnet-management                            |
| https://discourse.maas.io/t/-/3126 | /docs/snap/2.7/cli/test-logs                                  |
| https://discourse.maas.io/t/-/3127 | /docs/snap/2.7/ui/test-logs                                   |
| https://discourse.maas.io/t/-/3128 | /docs/snap/2.8/cli/test-logs                                  |
| https://discourse.maas.io/t/-/3129 | /docs/snap/2.8/ui/test-logs                                   |
| https://discourse.maas.io/t/-/3132 | /docs/deb/2.7/cli/test-logs                                   |
| https://discourse.maas.io/t/-/3133 | /docs/deb/2.7/ui/test-logs                                    |
| https://discourse.maas.io/t/-/3134 | /docs/deb/2.8/cli/test-logs                                   |
| https://discourse.maas.io/t/-/3135 | /docs/deb/2.8/ui/test-logs                                    |
| https://discourse.maas.io/t/-/3150 | /docs/snap/2.7/cli/tips-tricks-and-traps                      |
| https://discourse.maas.io/t/-/3151 | /docs/snap/2.7/ui/tips-tricks-and-traps                       |
| https://discourse.maas.io/t/-/3152 | /docs/snap/2.8/cli/tips-tricks-and-traps                      |
| https://discourse.maas.io/t/-/3153 | /docs/snap/2.8/ui/tips-tricks-and-traps                       |
| https://discourse.maas.io/t/-/3156 | /docs/deb/2.7/cli/tips-tricks-and-traps                       |
| https://discourse.maas.io/t/-/3157 | /docs/deb/2.7/ui/tips-tricks-and-traps                        |
| https://discourse.maas.io/t/-/3158 | /docs/deb/2.8/cli/tips-tricks-and-traps                       |
| https://discourse.maas.io/t/-/3159 | /docs/deb/2.8/ui/tips-tricks-and-traps                        |
| https://discourse.maas.io/t/-/3162 | /docs/snap/2.7/cli/troubleshooting                            |
| https://discourse.maas.io/t/-/3163 | /docs/snap/2.7/ui/troubleshooting                             |
| https://discourse.maas.io/t/-/3164 | /docs/snap/2.8/cli/troubleshooting                            |
| https://discourse.maas.io/t/-/3165 | /docs/snap/2.8/ui/troubleshooting                             |
| https://discourse.maas.io/t/-/3168 | /docs/deb/2.7/cli/troubleshooting                             |
| https://discourse.maas.io/t/-/3169 | /docs/deb/2.7/ui/troubleshooting                              |
| https://discourse.maas.io/t/-/3170 | /docs/deb/2.8/cli/troubleshooting                             |
| https://discourse.maas.io/t/-/3171 | /docs/deb/2.8/ui/troubleshooting                              |
| https://discourse.maas.io/t/-/3174 | /docs/snap/2.7/cli/ubuntu-kernels                             |
| https://discourse.maas.io/t/-/3175 | /docs/snap/2.7/ui/ubuntu-kernels                              |
| https://discourse.maas.io/t/-/3176 | /docs/snap/2.8/cli/ubuntu-kernels                             |
| https://discourse.maas.io/t/-/3177 | /docs/snap/2.8/ui/ubuntu-kernels                              |
| https://discourse.maas.io/t/-/3180 | /docs/deb/2.7/cli/ubuntu-kernels                              |
| https://discourse.maas.io/t/-/3181 | /docs/deb/2.7/ui/ubuntu-kernels                               |
| https://discourse.maas.io/t/-/3182 | /docs/deb/2.8/cli/ubuntu-kernels                              |
| https://discourse.maas.io/t/-/3183 | /docs/deb/2.8/ui/ubuntu-kernels                               |
| https://discourse.maas.io/t/-/3198 | /docs/snap/2.7/cli/user-accounts                              |
| https://discourse.maas.io/t/-/3199 | /docs/snap/2.7/ui/user-accounts                               |
| https://discourse.maas.io/t/-/3200 | /docs/snap/2.8/cli/user-accounts                              |
| https://discourse.maas.io/t/-/3201 | /docs/snap/2.8/ui/user-accounts                               |
| https://discourse.maas.io/t/-/3204 | /docs/deb/2.7/cli/user-accounts                               |
| https://discourse.maas.io/t/-/3205 | /docs/deb/2.7/ui/user-accounts                                |
| https://discourse.maas.io/t/-/3206 | /docs/deb/2.8/cli/user-accounts                               |
| https://discourse.maas.io/t/-/3207 | /docs/deb/2.8/ui/user-accounts                                |
| https://discourse.maas.io/t/-/3210 | /docs/snap/2.7/cli/vm-host-networking                         |
| https://discourse.maas.io/t/-/3211 | /docs/snap/2.7/ui/vm-host-networking                          |
| https://discourse.maas.io/t/-/3212 | /docs/snap/2.8/cli/vm-host-networking                         |
| https://discourse.maas.io/t/-/3213 | /docs/snap/2.8/ui/vm-host-networking                          |
| https://discourse.maas.io/t/-/3216 | /docs/deb/2.7/cli/vm-host-networking                          |
| https://discourse.maas.io/t/-/3217 | /docs/deb/2.7/ui/vm-host-networking                           |
| https://discourse.maas.io/t/-/3218 | /docs/deb/2.8/cli/vm-host-networking                          |
| https://discourse.maas.io/t/-/3219 | /docs/deb/2.8/ui/vm-host-networking                           |
| https://discourse.maas.io/t/-/3222 | /docs/snap/2.7/cli/vm-host-storage-pools                      |
| https://discourse.maas.io/t/-/3223 | /docs/snap/2.7/ui/vm-host-storage-pools                       |
| https://discourse.maas.io/t/-/3224 | /docs/snap/2.8/cli/vm-host-storage-pools                      |
| https://discourse.maas.io/t/-/3225 | /docs/snap/2.8/ui/vm-host-storage-pools                       |
| https://discourse.maas.io/t/-/3228 | /docs/deb/2.7/cli/vm-host-storage-pools                       |
| https://discourse.maas.io/t/-/3229 | /docs/deb/2.7/ui/vm-host-storage-pools                        |
| https://discourse.maas.io/t/-/3230 | /docs/deb/2.8/cli/vm-host-storage-pools                       |
| https://discourse.maas.io/t/-/3231 | /docs/deb/2.8/ui/vm-host-storage-pools                        |
| https://discourse.maas.io/t/-/3234 | /docs/snap/2.7/cli/vmware-images                              |
| https://discourse.maas.io/t/-/3235 | /docs/snap/2.7/ui/vmware-images                               |
| https://discourse.maas.io/t/-/3236 | /docs/snap/2.8/cli/vmware-images                              |
| https://discourse.maas.io/t/-/3237 | /docs/snap/2.8/ui/vmware-images                               |
| https://discourse.maas.io/t/-/3240 | /docs/deb/2.7/cli/vmware-images                               |
| https://discourse.maas.io/t/-/3241 | /docs/deb/2.7/ui/vmware-images                                |
| https://discourse.maas.io/t/-/3242 | /docs/deb/2.8/cli/vmware-images                               |
| https://discourse.maas.io/t/-/3243 | /docs/deb/2.8/ui/vmware-images                                |
| https://discourse.maas.io/t/-/3246 | /docs/snap/2.7/cli/vmware-vmfs-datastores                     |
| https://discourse.maas.io/t/-/3247 | /docs/snap/2.7/ui/vmware-vmfs-datastores                      |
| https://discourse.maas.io/t/-/3248 | /docs/snap/2.8/cli/vmware-vmfs-datastores                     |
| https://discourse.maas.io/t/-/3249 | /docs/snap/2.8/ui/vmware-vmfs-datastores                      |
| https://discourse.maas.io/t/-/3252 | /docs/deb/2.7/cli/vmware-vmfs-datastores                      |
| https://discourse.maas.io/t/-/3253 | /docs/deb/2.7/ui/vmware-vmfs-datastores                       |
| https://discourse.maas.io/t/-/3254 | /docs/deb/2.8/cli/vmware-vmfs-datastores                      |
| https://discourse.maas.io/t/-/3255 | /docs/deb/2.8/ui/vmware-vmfs-datastores                       |
| https://discourse.maas.io/t/-/3258 | /docs/snap/2.7/cli/writing-guide                              |
| https://discourse.maas.io/t/-/3259 | /docs/snap/2.7/ui/writing-guide                               |
| https://discourse.maas.io/t/-/3260 | /docs/snap/2.8/cli/writing-guide                              |
| https://discourse.maas.io/t/-/3261 | /docs/snap/2.8/ui/writing-guide                               |
| https://discourse.maas.io/t/-/3264 | /docs/deb/2.7/cli/writing-guide                               |
| https://discourse.maas.io/t/-/3265 | /docs/deb/2.7/ui/writing-guide                                |
| https://discourse.maas.io/t/-/3266 | /docs/deb/2.8/cli/writing-guide                               |
| https://discourse.maas.io/t/-/3267 | /docs/deb/2.8/ui/writing-guide                                |
| https://discourse.maas.io/t/-/3270 | /docs/snap/2.7/cli/zone-examples                              |
| https://discourse.maas.io/t/-/3271 | /docs/snap/2.7/ui/zone-examples                               |
| https://discourse.maas.io/t/-/3272 | /docs/snap/2.8/cli/zone-examples                              |
| https://discourse.maas.io/t/-/3273 | /docs/snap/2.8/ui/zone-examples                               |
| https://discourse.maas.io/t/-/3276 | /docs/deb/2.7/cli/zone-examples                               |
| https://discourse.maas.io/t/-/3277 | /docs/deb/2.7/ui/zone-examples                                |
| https://discourse.maas.io/t/-/3278 | /docs/deb/2.8/cli/zone-examples                               |
| https://discourse.maas.io/t/-/3279 | /docs/deb/2.8/ui/zone-examples                                |
| https://discourse.maas.io/t/-/3318 | /docs/snap/2.7/cli/installation                               |
| https://discourse.maas.io/t/-/3319 | /docs/snap/2.7/ui/installation                                |
| https://discourse.maas.io/t/-/3320 | /docs/snap/2.8/cli/installation                               |
| https://discourse.maas.io/t/-/3321 | /docs/snap/2.8/ui/installation                                |
| https://discourse.maas.io/t/-/3324 | /docs/deb/2.7/cli/installation                                |
| https://discourse.maas.io/t/-/3325 | /docs/deb/2.7/ui/installation                                 |
| https://discourse.maas.io/t/-/3326 | /docs/deb/2.8/cli/installation                                |
| https://discourse.maas.io/t/-/3327 | /docs/deb/2.8/ui/installation                                 |
| https://discourse.maas.io/t/-/3382 | /docs/snap/2.7/cli/event-logs                                 |
| https://discourse.maas.io/t/-/3383 | /docs/snap/2.7/ui/event-logs                                  |
| https://discourse.maas.io/t/-/3384 | /docs/snap/2.8/cli/event-logs                                 |
| https://discourse.maas.io/t/-/3385 | /docs/snap/2.8/ui/event-logs                                  |
| https://discourse.maas.io/t/-/3388 | /docs/deb/2.7/cli/event-logs                                  |
| https://discourse.maas.io/t/-/3389 | /docs/deb/2.7/ui/event-logs                                   |
| https://discourse.maas.io/t/-/3390 | /docs/deb/2.8/cli/event-logs                                  |
| https://discourse.maas.io/t/-/3391 | /docs/deb/2.8/ui/event-logs                                   |
| https://discourse.maas.io/t/-/3771 | /docs/supported-versions                                      |
| https://discourse.maas.io/t/-/4059 | /docs/deb/3.0/cli/package-repositories                        |
| https://discourse.maas.io/t/-/4060 | /docs/deb/3.0/ui/package-repositories                         |
| https://discourse.maas.io/t/-/4065 | /docs/snap/3.0/cli/postgresql-ha-hot-standby                  |
| https://discourse.maas.io/t/-/4066 | /docs/snap/3.0/ui/postgresql-ha-hot-standby                   |
| https://discourse.maas.io/t/-/4067 | /docs/deb/3.0/cli/postgresql-ha-hot-standby                   |
| https://discourse.maas.io/t/-/4068 | /docs/deb/3.0/ui/postgresql-ha-hot-standby                    |
| https://discourse.maas.io/t/-/4077 | /docs/snap/3.0/cli/proxy-log                                  |
| https://discourse.maas.io/t/-/4078 | /docs/snap/3.0/ui/proxy-log                                   |
| https://discourse.maas.io/t/-/4079 | /docs/deb/3.0/cli/proxy-log                                   |
| https://discourse.maas.io/t/-/4080 | /docs/deb/3.0/ui/proxy-log                                    |
| https://discourse.maas.io/t/-/4097 | /docs/snap/3.0/cli/resource-pools                             |
| https://discourse.maas.io/t/-/4098 | /docs/snap/3.0/ui/resource-pools                              |
| https://discourse.maas.io/t/-/4099 | /docs/deb/3.0/cli/resource-pools                              |
| https://discourse.maas.io/t/-/4100 | /docs/deb/3.0/ui/resource-pools                               |
| https://discourse.maas.io/t/-/4105 | /docs/snap/3.0/cli/storage                                    |
| https://discourse.maas.io/t/-/4106 | /docs/snap/3.0/ui/storage                                     |
| https://discourse.maas.io/t/-/4107 | /docs/deb/3.0/cli/storage                                     |
| https://discourse.maas.io/t/-/4108 | /docs/deb/3.0/ui/storage                                      |
| https://discourse.maas.io/t/-/4157 | /docs/snap/3.0/cli/using-rbac-with-maas                       |
| https://discourse.maas.io/t/-/4158 | /docs/snap/3.0/ui/using-rbac-with-maas                        |
| https://discourse.maas.io/t/-/4159 | /docs/deb/3.0/cli/using-rbac-with-maas                        |
| https://discourse.maas.io/t/-/4160 | /docs/deb/3.0/ui/using-rbac-with-maas                         |
| https://discourse.maas.io/t/-/4186 | /docs/how-to-help-improve-the-doc                             |
| https://discourse.maas.io/t/-/4268 | /docs/deb/2.8/ui/air-gapped-maas                              |
| https://discourse.maas.io/t/-/4269 | /docs/deb/2.8/cli/air-gapped-maas                             |
| https://discourse.maas.io/t/-/4271 | /docs/snap/2.8/ui/air-gapped-maas                             |
| https://discourse.maas.io/t/-/4272 | /docs/snap/2.8/cli/air-gapped-maas                            |
| https://discourse.maas.io/t/-/4285 | /docs/snap/2.7/ui/air-gapped-maas                             |
| https://discourse.maas.io/t/-/4286 | /docs/snap/2.7/cli/air-gapped-maas                            |
| https://discourse.maas.io/t/-/4287 | /docs/deb/2.7/cli/air-gapped-maas                             |
| https://discourse.maas.io/t/-/4288 | /docs/deb/2.7/ui/air-gapped-maas                              |
| https://discourse.maas.io/t/-/4292 | /docs/deb/2.8/cli/whats-new-in-maas                           |
| https://discourse.maas.io/t/-/4293 | /docs/deb/2.8/ui/whats-new-in-maas                            |
| https://discourse.maas.io/t/-/4294 | /docs/deb/2.7/cli/whats-new-in-maas                           |
| https://discourse.maas.io/t/-/4295 | /docs/deb/2.7/ui/whats-new-in-maas                            |
| https://discourse.maas.io/t/-/4298 | /docs/snap/2.8/cli/whats-new-in-maas                          |
| https://discourse.maas.io/t/-/4299 | /docs/snap/2.8/ui/whats-new-in-maas                           |
| https://discourse.maas.io/t/-/4300 | /docs/snap/2.7/cli/whats-new-in-maas                          |
| https://discourse.maas.io/t/-/4301 | /docs/snap/2.7/ui/whats-new-in-maas                           |
| https://discourse.maas.io/t/-/4324 | /docs/snap/2.7/cli/using-image-streams                        |
| https://discourse.maas.io/t/-/4325 | /docs/snap/2.7/ui/using-image-streams                         |
| https://discourse.maas.io/t/-/4326 | /docs/snap/2.8/cli/using-image-streams                        |
| https://discourse.maas.io/t/-/4327 | /docs/snap/2.8/ui/using-image-streams                         |
| https://discourse.maas.io/t/-/4330 | /docs/deb/2.7/cli/using-image-streams                         |
| https://discourse.maas.io/t/-/4331 | /docs/deb/2.7/ui/using-image-streams                          |
| https://discourse.maas.io/t/-/4332 | /docs/deb/2.8/cli/using-image-streams                         |
| https://discourse.maas.io/t/-/4333 | /docs/deb/2.8/ui/using-image-streams                          |
| https://discourse.maas.io/t/-/4446 | /docs/how-to-review-and-report-bugs                           |
| https://discourse.maas.io/t/-/4447 | /docs/how-to-request-new-features                             |
| https://discourse.maas.io/t/-/4460 | /docs/maas-projects                                           |
| https://discourse.maas.io/t/-/4478 | /docs/maas-project-tutorial                                   |
| https://discourse.maas.io/t/-/5060 | /docs/api-authentication-reference                            |
| https://discourse.maas.io/t/-/5068 | /docs/about-vm-hosting                                        |
| https://discourse.maas.io/t/-/5092 | /docs/bootstrap-maas                                          |
| https://discourse.maas.io/t/-/5096 | /docs/how-to-back-up-maas                                     |
| https://discourse.maas.io/t/-/5104 | /docs/how-to-customise-images                                 |
| https://discourse.maas.io/t/-/5108 | /docs/how-to-customise-machines                               |
| https://discourse.maas.io/t/-/5112 | /docs/how-to-put-machines-to-work                             |
| https://discourse.maas.io/t/-/5116 | /docs/how-to-enable-maas-native-tls                           |
| https://discourse.maas.io/t/-/5120 | /docs/how-to-enable-high-availability                         |
| https://discourse.maas.io/t/-/5124 | /docs/how-use-standard-images                                 |
| https://discourse.maas.io/t/-/5128 | /docs/how-to-do-a-fresh-install-of-maas                       |
| https://discourse.maas.io/t/-/5132 | /docs/how-to-enable-dhcp                                      |
| https://discourse.maas.io/t/-/5140 | /docs/how-to-manage-vm-hosts                                  |
| https://discourse.maas.io/t/-/5144 | /docs/how-to-employ-vmware-images                             |
| https://discourse.maas.io/t/-/5148 | /docs/how-to-manage-virtual-machines                          |
| https://discourse.maas.io/t/-/5152 | /docs/how-to-use-availability-zones                           |
| https://discourse.maas.io/t/-/5160 | /docs/how-to-make-machines-available                          |
| https://discourse.maas.io/t/-/5164 | /docs/how-to-connect-maas-networks                            |
| https://discourse.maas.io/t/-/5172 | /docs/how-to-configure-controllers                            |
| https://discourse.maas.io/t/-/5184 | /docs/how-to-manage-user-accounts                             |
| https://discourse.maas.io/t/-/5192 | /docs/how-to-find-machines                                    |
| https://discourse.maas.io/t/-/5196 | /docs/how-to-improve-maas-security                            |
| https://discourse.maas.io/t/-/5204 | /docs/how-to-monitor-maas                                     |
| https://discourse.maas.io/t/-/5208 | /docs/how-to-set-up-lxd                                       |
| https://discourse.maas.io/t/-/5212 | /docs/how-to-set-up-an-air-gapped-maas                        |
| https://discourse.maas.io/t/-/5216 | /docs/how-to-use-controller-tags                              |
| https://discourse.maas.io/t/-/5224 | /docs/how-to-use-machine-tags                                 |
| https://discourse.maas.io/t/-/5228 | /docs/how-to-use-network-tags                                 |
| https://discourse.maas.io/t/-/5232 | /docs/how-to-use-storage-tags                                 |
| https://discourse.maas.io/t/-/5236 | /docs/try-out-the-maas-cli                                    |
| https://discourse.maas.io/t/-/5240 | /docs/about-maas-logging                                      |
| https://discourse.maas.io/t/-/5246 | /docs/power-drivers-reference                                 |
| https://discourse.maas.io/t/-/5248 | /docs/commissioning-logs-reference                            |
| https://discourse.maas.io/t/-/5252 | /docs/event-logs-reference                                    |
| https://discourse.maas.io/t/-/5256 | /docs/audit-event-logs-reference                              |
| https://discourse.maas.io/t/-/5292 | /docs/what-is-new-with-maas                                   |
| https://discourse.maas.io/t/-/5307 | /docs/snap/3.1/cli/postgresql-ha-hot-standby                  |
| https://discourse.maas.io/t/-/5308 | /docs/snap/3.1/ui/postgresql-ha-hot-standby                   |
| https://discourse.maas.io/t/-/5309 | /docs/deb/3.1/ui/postgresql-ha-hot-standby                    |
| https://discourse.maas.io/t/-/5310 | /docs/deb/3.1/cli/postgresql-ha-hot-standby                   |
| https://discourse.maas.io/t/-/5314 | /docs/testing-logs-reference                                  |
| https://discourse.maas.io/t/-/5329 | /docs/tips-and-tricks                                         |
| https://discourse.maas.io/t/-/5333 | /docs/how-to-troubleshoot-maas                                |
| https://discourse.maas.io/t/-/5392 | /docs/hardware-test-scripts-reference                         |
| https://discourse.maas.io/t/-/5404 | /docs/python-api-client-reference                             |
| https://discourse.maas.io/t/-/5416 | /docs/maas-glossary                                           |
| https://discourse.maas.io/t/-/5428 | /docs/how-to-give-and-receive-help                            |
| https://discourse.maas.io/t/-/5436 | /docs/how-to-upgrade-maas                                     |
| https://discourse.maas.io/t/-/5448 | /docs/how-to-contact-us                                       |
| https://discourse.maas.io/t/-/5706 | /docs/how-to-upgrade-older-maas-installs                      |
| https://discourse.maas.io/t/-/5806 | /docs/about-tcp-ip-networks                                   |
| https://discourse.maas.io/t/-/5807 | /docs/about-dhcp                                              |
| https://discourse.maas.io/t/-/5808 | /docs/about-cloud-networks                                    |
| https://discourse.maas.io/t/-/5927 | /docs/how-to-mirror-images-locally                            |
| https://discourse.maas.io/t/-/5928 | /docs/how-to-tag-machines                                     |
| https://discourse.maas.io/t/-/5929 | /docs/how-to-annotate-machines                                |
| https://discourse.maas.io/t/-/5961 | /docs/what-is-new-with-maas-2-9                               |
| https://discourse.maas.io/t/-/5962 | /docs/what-is-new-with-maas-3-2                               |
| https://discourse.maas.io/t/-/5963 | /docs/what-is-new-with-maas-3-0                               |
| https://discourse.maas.io/t/-/5964 | /docs/what-is-new-with-maas-3-1                               |
| https://discourse.maas.io/t/-/5973 | /docs/storage-layouts-reference                               |
| https://discourse.maas.io/t/-/5976 | /docs/about-customising-machines                              |
| https://discourse.maas.io/t/-/5987 | /docs/how-to-audit-maas                                       |
| https://discourse.maas.io/t/-/5993 | /docs/what-is-new-with-maas-2-7                               |
| https://discourse.maas.io/t/-/5994 | /docs/what-is-new-with-maas-2-8                               |
| https://discourse.maas.io/t/-/6027 | /docs/get-fancy-cli-output                                    |
| https://discourse.maas.io/t/-/6099 | /docs/about-creating-custom-images                            |
| https://discourse.maas.io/t/-/6102 | /docs/create-a-custom-image                                   |
| https://discourse.maas.io/t/-/6140 | /docs/tutorials                                               |
| https://discourse.maas.io/t/-/6141 | /docs/explanation                                             |
| https://discourse.maas.io/t/-/6142 | /docs/how-to-guides                                           |
| https://discourse.maas.io/t/-/6143 | /docs/reference                                               |
| https://discourse.maas.io/t/-/6178 | /docs/about-maas-performance                                  |
| https://discourse.maas.io/t/-/6192 | /docs/how-to-use-images                                       |
| https://discourse.maas.io/t/-/6193 | /docs/how-to-manage-machines                                  |
| https://discourse.maas.io/t/-/6200 | /docs/how-to-label-devices                                    |
| https://discourse.maas.io/t/-/6202 | /docs/how-to-get-started-with-maas                            |
| https://discourse.maas.io/t/-/6203 | /docs/maas-technical-reference                                |
| https://discourse.maas.io/t/-/6233 | /docs/maas-installation-requirements                          |
| https://discourse.maas.io/t/-/6327 | /docs/maas-terraform-reference                                |
| https://discourse.maas.io/t/-/6346 | /docs/what-is-new-with-maas-3-3                               |
| https://discourse.maas.io/t/-/6367 | /docs/how-to-spin-up-maas-with-ansible                        |
| https://discourse.maas.io/t/-/6347 | /docs/maas-settings-reference                                 |
| https://discourse.maas.io/t/-/6372 | /docs/about-maas-audit-events                                 |
| https://discourse.maas.io/t/-/6373 | /docs/understanding-maas-events                               |
| https://discourse.maas.io/t/-/6498 | /docs/how-to-manage-controllers                               |
| https://discourse.maas.io/t/-/6500 | /docs/how-to-use-virtual-machines                             |
| https://discourse.maas.io/t/-/6503 | /docs/how-to-secure-maas                                      |
| https://discourse.maas.io/t/-/6510 | /docs/about-maas-events                                       |
| https://discourse.maas.io/t/-/6605 | /docs/commissioning-scripts-reference                         |
| https://discourse.maas.io/t/-/6658 | /docs/how-to-deploy-a-rt-kernel-via-cloud-init                |
| https://discourse.maas.io/t/-/6678 | /docs/about-maas                                              |
| https://discourse.maas.io/t/-/6680 | /docs/about-networking                                        |
| https://discourse.maas.io/t/-/6685 | /docs/about-images                                            |
| https://discourse.maas.io/t/-/6690 | /docs/about-controllers                                       |
| https://discourse.maas.io/t/-/6695 | /docs/about-machines                                          |
| https://discourse.maas.io/t/-/6742 | /docs/how-to-set-up-networks                                  |
|                                    |                                                               |
[/details]

** Redirects

[details=Mapping table]
| PATH                                               | LOCATION                                      |
|----------------------------------------------------|-----------------------------------------------|
| /docs/how-to-use-tags                              | /docs/how-to-tag-machines                     |
| /docs/how-to-create-vm-hosts                       | /docs/how-to-manage-vm-hosts                  |
| /docs/how-to-protect-your-secrets                  | /docs/how-to-secure-maas                      |
| /docs/about-rbac                                   | /docs/about-maas-security                     |
| /docs/how-to-keep-maas-backed-up                   | /docs/how-to-back-up-maas                     |
| /docs/how-to-back-up-maas                          | /docs/how-to-back-up-maas                     |
| /docs/commissioning-scripts-tech-reference         | /docs/commissioning-scripts-reference         |
| /docs/how-to-terraform-with-maas                   | /docs/maas-terraform-reference                |
| /docs/maas-terraform-provider-reference            | /docs/maas-terraform-reference                |
| /docs/understanding-maas-audit-events              | /docs/about-maas-audit-events                 |
| /docs/how-to-gauge-maas-performance                | /docs/about-maas-performance                  |
| /docs/maas-performance                             | /docs/about-maas-performance                  |
| /docs/how-to-enable-vault                          | /docs/how-to-use-hashicorp-vault-with-maas    |
| /docs/how-to-unseal-vault                          | /docs/how-to-use-hashicorp-vault-with-maas    |
| /docs/how-to-manage-availability-zones             | /docs/how-to-use-availability-zones           |
| /docs/test-log-reference                           | /docs/testing-logs-reference                  |
| /docs/how-to-build-custom-images                   | /docs/how-to-customise-images                 |
| /docs/power-management-reference                   | /docs/power-drivers-reference                 |
| /docs/how-to-change-maas-settings                  | /docs/maas-settings-reference                 |
| /docs/how-to-ask-for-help                          | /docs/how-to-give-and-receive-help            |
| /docs/how-to-report-a-bug                          | /docs/how-to-review-and-report-bugs           |
| /docs/request-a-feature                            | /docs/how-to-request-new-features             |
| /docs/how-to-request-a-feature                     | /docs/how-to-request-new-features             |
| /docs/how-to-work-with-log-files                   | /docs/about-maas-logging                      |
| /docs/how-to-work-with-audit-event-logs            | /docs/how-to-audit-maas                       |
| /docs/how-to-diagnose-issues                       | /docs/how-to-troubleshoot-maas                |
| /docs/how-to-observe-a-live-maas                   | /docs/how-to-monitor-maas                     |
| /docs/how-to-label-and-find-machines               | /docs/how-to-label-devices                    |
| /docs/how-to-create-and-manage-vms                 | /docs/how-to-manage-virtual-machines          |
| /docs/how-to-adjust-your-controllers               | /docs/how-to-configure-controllers            |
| /docs/how-to-install-maas                          | /docs/how-to-do-a-fresh-install-of-maas       |
| /docs/how-to-deploy-virtual-machines               | /docs/how-to-use-virtual-machines             |
| /docs/about-tags                                   | /docs/about-device-labels                     |
| /docs/ansible-playbooks-reference                  | /docs/how-to-spin-up-maas-with-ansible        |
| /docs/basic-tutorials                              | /docs/tutorials                               |
| /docs/commissioning-script-reference               | /docs/commissioning-scripts-reference         |
| /docs/configuration-settings-reference             | /docs/maas-settings-reference                 |
| /docs/custom-image-tutorial                        | /docs/create-a-custom-image                   |
| /docs/deb/3.0/cli/how-to-work-with-tags            | /docs/how-to-tag-machines                     |
| /docs/deb/3.0/ui/how-to-work-with-tags             | /docs/how-to-tag-machines                     |
| /docs/deb/3.1/cli/how-to-work-with-tags            | /docs/how-to-tag-machines                     |
| /docs/deb/3.1/ui/how-to-work-with-tags             | /docs/how-to-tag-machines                     |
| /docs/deploy-nodes                                 | /docs/how-to-put-machines-to-work             |
| /docs/how-to-acquire-images                        | /docs/how-to-use-images                       |
| /docs/how-to-build-maas-images                     | /docs/how-to-customise-images                 |
| /docs/how-to-choose-images                         | /docs/how-to-use-images                       |
| /docs/how-to-configure-networking                  | /docs/how-to-set-up-networks                  |
| /docs/how-to-create-a-custom-ubuntu-image          | /docs/how-to-customise-images                 |
| /docs/how-to-create-custom-images                  | /docs/how-to-customise-images                 |
| /docs/how-to-deploy-maas                           | /docs/how-to-manage-machines                  |
| /docs/how-to-deploy-machines                       | /docs/how-to-put-machines-to-work             |
| /docs/how-to-deploy-physical-machines              | /docs/how-to-manage-machines                  |
| /docs/how-to-enable-tls-encryption                 | /docs/how-to-enable-maas-native-tls           |
| /docs/how-to-get-help                              | /docs/how-to-give-and-receive-help            |
| /docs/how-to-get-maas-running                      | /docs/how-to-get-started-with-maas            |
| /docs/how-to-import-images                         | /docs/how-use-standard-images                 |
| /docs/how-to-manage-dhcp                           | /docs/how-to-enable-dhcp                      |
| /docs/how-to-manage-ip-addresses                   | /docs/how-to-enable-dhcp                      |
| /docs/how-to-manage-ip-ranges                      | /docs/how-to-enable-dhcp                      |
| /docs/how-to-manage-machine-interfaces             | /docs/how-to-connect-maas-networks            |
| /docs/how-to-manage-machines                       | /docs/how-to-make-machines-available          |
| /docs/how-to-manage-proxies                        | /docs/how-to-connect-maas-networks            |
| /docs/how-to-manage-racks                          | /docs/how-to-configure-controllers            |
| /docs/how-to-manage-regions                        | /docs/how-to-configure-controllers            |
| /docs/how-to-manage-vms                            | /docs/how-to-manage-virtual-machines          |
| /docs/how-to-manage-vmware-images                  | /docs/how-to-employ-vmware-images             |
| /docs/how-to-search-maas                           | /docs/how-to-find-machines                    |
| /docs/how-to-set-up-maas-metrics                   | /docs/how-to-monitor-maas                     |
| /docs/how-to-set-up-ntp-services                   | /docs/how-to-connect-maas-networks            |
| /docs/how-to-tune-controllers                      | /docs/how-to-manage-controllers               |
| /docs/how-to-use-image-streams                     | /docs/how-use-standard-images                 |
| /docs/how-to-use-lxd                               | /docs/how-to-set-up-lxd                       |
| /docs/how-to-use-maas-in-an-air-gapped-environment | /docs/how-to-set-up-an-air-gapped-maas        |
| /docs/how-to-use-the-maas-cli                      | /docs/try-out-the-maas-cli                    |
| /docs/how-to-work-with-annotations                 | /docs/how-to-annotate-machines                |
| /docs/how-to-work-with-tags                        | /docs/how-to-tag-machines                     |
| /docs/installation-requirements                    | /docs/maas-installation-requirements          |
| /docs/maas-bootstrap-tutorial                      | /docs/bootstrap-maas                          |
| /docs/maas-bootstrap-tutorial                      | /docs/get-started-with-maas                   |
| /docs/maas-concepts-and-terms-reference            | /docs/maas-glossary                           |
| /docs/maas-logging-reference                       | /docs/maas-logs-reference                     |
| /docs/maas-terraform-provider                      | /docs/maas-terraform-provider-reference       |
| /docs/report-a-bug                                 | /docs/how-to-review-and-report-bugs           |
| /docs/request-a-feature                            | /docs/how-to-request-a-feature                |
| /docs/snap/2.7/cli/how-to-upgrade-maas             | /docs/how-to-upgrade-maas                     |
| /docs/snap/2.7/cli/installation-tech-reference     | /docs/snap/2.7/cli/installation               |
| /docs/snap/2.7/cli/interactive-search              | /docs/how-to-find-machines                    |
| /docs/snap/2.7/ui/how-to-upgrade-maas              | /docs/how-to-upgrade-maas                     |
| /docs/snap/2.7/ui/installation-tech-reference      | /docs/snap/ui/ui/installation                 |
| /docs/snap/2.7/ui/interactive-search               | /docs/how-to-find-machines                    |
| /docs/snap/2.8/cli/how-to-upgrade-maas             | /docs/how-to-upgrade-maas                     |
| /docs/snap/2.8/cli/installation-tech-reference     | /docs/snap/2.8/cli/installation               |
| /docs/snap/2.8/cli/interactive-search              | /docs/how-to-find-machines                    |
| /docs/snap/2.8/ui/how-to-upgrade-maas              | /docs/how-to-upgrade-maas                     |
| /docs/snap/2.8/ui/installation-tech-reference      | /docs/snap/2.8/ui/installation                |
| /docs/snap/2.8/ui/interactive-search               | /docs/how-to-find-machines                    |
| /docs/snap/2.9/cli/how-to-work-with-tags           | /docs/how-to-tag-machiness                    |
| /docs/snap/2.9/ui/how-to-work-with-tags            | /docs/how-to-tag-machines                     |
| /docs/snap/3.0/cli/how-to-work-with-tags           | /docs/how-to-tag-machines                     |
| /docs/snap/3.0/ui/how-to-work-with-tags            | /docs/how-to-tag-machines                     |
| /docs/snap/3.1/cli/how-to-work-with-tags           | /docs/how-to-tag-machines                     |
| /docs/snap/3.1/ui/how-to-work-with-tags            | /docs/how-to-tag-machines                     |
| /docs/ssl                                          | /docs/how-to-enable-maas-native-tls           |
| /docs/storage                                      | /docs/about-machines#heading--machine-storage |
| /docs/stp                                          | /docs/how-to-connect-maas-networks            |
| /docs/subnet-management                            | /docs/how-to-connect-maas-newtorks            |
| /docs/technical-reference                          | /docs/maas-technical-reference                |
| /docs/using-jq-with-the-maas-cli                   | /docs/get-fancy-cli-output                    |
| /docs/vmware-images                                | /docs/how-to-employ-vmware-images             |
| /docs/whats-new-in-maas                            | /docs/what-is-new-with-maas                   |
[/details]
<!-- nohtml end-nohtml -->

* MAAS documentation
MAAS is **Metal As A Service**, a service that treats physical servers like virtual machines (instances) in the cloud.

No need to manage servers individually: MAAS turns bare metal into an elastic, cloud-like resource. Enlist and deploy standard or customised operating systems to hardware and virtual machines -- remotely.  Monitor, manage, and secure your metal infrastructure easily and efficiently.

<a href="https://discourse.maas.io/uploads/default/original/1X/18456dbd3fbfec14eddd044816fd0719692282da.jpeg" target = "_blank"><img src="https://discourse.maas.io/uploads/default/original/1X/18456dbd3fbfec14eddd044816fd0719692282da.jpeg"></a>

MAAS comprehensively meets the need to rapidly deploy, destroy, and reconfigure constellations of bare metal.  Any application that requires frequently rearranging the server topology will benefit.

MAAS is applicable to nearly any situation.  It is currently deployed in banking, telecom, and industrial environments, as well as niche uses as diverse as national lotteries, supercomputer front-end validation, streaming music services, disaster recovery, and computer security risk analysis.

** In this documentation

|                                                                                       |                                                                                             |
|---------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------|
| [Tutorials](/t/tutorials/6140)</br>  Hands-on introduction to MAAS features for new users       | [How-to guides](/t/how-to-guides/6142) </br> Step-by-step guides covering key operations and common tasks |
| [Reference](/t/reference/6143) </br> Technical information - specifications, APIs, architecture | [Explanation](/t/explanation/6141) </br> Detailed theory on the inner workings of MAAS                  |


** Project and community

MAAS is a member of the Ubuntu family. It’s an open source project that warmly welcomes community projects, contributions, suggestions, fixes and constructive feedback.

- [Read our code of conduct](https://ubuntu.com/community/code-of-conduct)`↗`
- [Get support](https://maas.io/docs/how-to-contact-us)`↗`
- [Learn about MAAS performance](https://maas.io/docs/maas-performance)`↗`
- [Join our online chat](/t/how-to-use-our-discourse-forum/6802)`↗`
- [Contribute code](https://launchpad.net/maas)`↗`
- [Improve our doc](/t/how-to-contribute-documentation/6949)`↗`
- [Request a feature](/t/how-to-request-a-new-feature/4447)`↗`
- [Report a bug](/t/how-to-report-a-bug/4446)`↗`

**# Our roadmap

Here's a view of our current roadmap:

<a href="https://discourse.maas.io/uploads/default/original/2X/6/6cb3381fd1cfb2f3a871c281e118d2b94ee05bf1.jpeg" target = "_blank"><img src="https://discourse.maas.io/uploads/default/original/2X/6/6cb3381fd1cfb2f3a871c281e118d2b94ee05bf1.jpeg"></a>

Considering MAAS for your next project? [Get in touch](https://maas.io/docs/how-to-contact-us)`↗`

<!-- nohtml begin-nohtml -->
** Navigation

<!--
[details=Documentation versions]
| Path | Version |
|--|--|
|  | [snap-2.9](/t/unlisted-docs-testing/4315) |
| snap-2.8 | [snap-2.8](/t/unlisted-docs-testing-snap-2-8/4668) |
[/details]
-->

[details=Navigation items]
| Level | Path                              | Navlink                                                         |
|-------|-----------------------------------|-----------------------------------------------------------------|
| 0     | /                                 | [MAAS Documentation](/t/-/6662)                                 |
| 0     |                                   |                                                                 |
| 1     | tutorials                         | [Tutorials](/t/-/6140)                                          |
| 2     | bootstrap-maas                    | [Bootstrap MAAS](/t/-/5092)                                     |
| 2     | try-out-the-maas-cli              | [Try out the MAAS CLI](/t/-/5236)                               |
| 2     | create-custom-images              | [Create custom images](/t/-/6102)                               |
| 2     | get-fancy-cli-output              | [Get fancy CLI output](/t/-/6027)                               |
| 0     |                                   |                                                                 |
| 1     | how-to-guides                     | [How-to guides](/t/-/6663)                                      |
| 2     | how-to-get-started-with-maas      | [Get started with MAAS](/t/-/6202)                              |
| 3     | how-to-do-a-fresh-install-of-maas | [Do a fresh install of MAAS](/t/-/5128)                         |
| 3     | how-to-upgrade-maas               | [Upgrade MAAS](/t/-/5436)                                       |
| 3     | how-to-spin-up-maas-with-ansible  | [Spin up MAAS with Ansible](/t/-/6367)                          |
| 2     | how-to-set-up-networks            | [Set up networks](/t/-/6742)                                    |
| 3     | how-to-connect-maas-networks      | [Connect MAAS networks](/t/-/5164)                              |
| 3     | how-to-enable-dhcp                | [Enable DHCP](/t/-/5132)                                        |
| 3     | how-to-use-availability-zones     | [Use availability zones](/t/-/5152)                             |
| 2     | how-to-use-images                 | [Use images](/t/-/6192)                                         |
| 3     | how-to-use-standard-images        | [Use standard images](/t/-/5124)                                |
| 3     | how-to-mirror-images-locally      | [Mirror images locally](/t/-/5927)                              |
| 3     | how-to-customise-images           | [Customise images](/t/-/5104)                                   |
| 3     | how-to-employ-vmware-images       | [Employ VMWare images](/t/-/5144)                               |
| 3     | how-to-deploy-a-rt-kernel         | [Deploy a RT kernel](/t/-/6658)                                 |
| 2     | how-to-manage-controllers         | [Manage controllers](/t/-/6498)                                 |
| 3     | how-to-configure-controllers      | [Configure controllers](/t/-/5172)                              |
| 3     | how-to-enable-high-availability   | [Enable high availability](/t/-/5120)                           |
| 2     | how-to-manage-machines            | [Manage machines](/t/-/6193)                                    |
| 3     | how-to-make-machines-available    | [Make machines available](/t/-/5160)                            |
| 3     | how-to-customise-machines         | [Customise machines](/t/-/5108)                                 |
| 3     | how-to-put-machines-to-work       | [Put machines to work](/t/-/5112)                               |
| 2     | how-to-use-virtual-machines       | [Use virtual machines](/t/-/6500)                               |
| 3     | how-to-set-up-lxd                 | [Set up LXD](/t/-/5208)                                         |
| 3     | how-to-manage-vm-hosts            | [Manage VM hosts](/t/-/5140)                                    |
| 3     | how-to-manage-vms                 | [Manage virtual machines](/t/-/5148)                            |
| 2     | how-to-label-devices              | [Label devices](/t/-/6200)                                      |
| 3     | how-to-tag-machines               | [Tag machines](/t/-/5928)                                       |
| 3     | how-to-annotate-machines          | [Annotate machines](/t/-/5929)                                  |
| 3     | how-to-use-machine-tags           | [Use machine tags](/t/-/5224)                                   |
| 3     | how-to-use-controller-tags        | [Use controller tags](/t/-/5216)                                |
| 3     | how-to-use-storage-tags           | [Use storage tags](/t/-/5232)                                   |
| 3     | how-to-use-network-tags           | [Use network tags](/t/-/5228)                                   |
| 2     | how-to-secure-maas                | [Secure MAAS](/t/-/6503)                                        |
| 3     | how-to-improve-maas-security      | [Improve MAAS security](/t/-/5196)                              |
| 3     | how-to-manage-user-accounts       | [Manage user accounts](/t/-/5184)                               |
| 3     | how-to-enable-maas-native-tls     | [Enable MAAS native TLS](/t/-/5116)                             |
| 3     | how-to-use-vault-with-maas        | [Use Vault with MAAS](/t/-/6942)                                |
| 3     | how-to-set-up-an-air-gapped-maas  | [Set up an air-gapped MAAS](/t/-/5212)                          |
| 2     | how-to-operate-maas               | [Operate MAAS](/t/-/6799)                                       |
| 3     | how-to-find-machines              | [Find machines](/t/-/5192)                                      |
| 3     | how-to-back-up-maas               | [Back up MAAS](/t/-/5096)                                       |
| 3     | how-to-monitor-maas               | [Monitor MAAS](/t/-/5204)                                       |
| 3     | how-to-audit-maas                 | [Audit MAAS](/t/-/5987)                                         |
| 3     | how-to-troubleshoot-maas          | [Troubleshoot MAAS](/t/-/5333)                                  |
| 2     | how-to-give-and-receive-help      | [Give and receive help](/t/-/5428)                              |
| 3     | how-to-use-our-discourse-forum    | [Use our Discourse forum](/t/-/6802)                            |
| 3     | how-to-get-support                | [Get support](https://maas.io/docs/how-to-contact-us)           |
| 3     | how-to-request-new-features       | [Request new features](/t/-/4447)                               |
| 3     | how-to-review-and-report-bugs     | [Review and report bugs](/t/-/4446)                             |
| 3     | how-to-contribute-doc             | [Contribute documentation](/t/-/6949)                           |
| 0     |                                   |                                                                 |
| 1     | reference                         | [Reference](/t/-/6143)                                          |
| 2     | general-reference                 | [General reference ](/t/-/6950)                                 |
| 3     | release-notes                     | [Release notes](/t/-/5292)                                      |
| 3     | installation-requirements         | [Installation requirements](/t/-/6233)                          |
| 3     | maas-settings                     | [MAAS settings](/t/-/6347)                                      |
| 3     | maas-source-code                  | [MAAS source code](/https://launchpad.net/maas)                 |
| 3     | doc-style-guide                   | [Doc style guide](/t/-/4186)                                    |
| 3     | maas-glossary                     | [Glossary](/t/-/5416)                                           |
| 3     | code-of-conduct                   | [Code of conduct](https://ubuntu.com/community/code-of-conduct) |
| 2     | api-reference                     | [API reference](/t/-/6665)                                      |
| 3     | api-authentication-reference      | [API authentication](/t/-/5060)                                 |
| 3     | python-api-client-reference       | [Python API client](/t/-/5404)                                  |
| 3     | maas-api-documentation            | [API documentation](https://maas.io/docs/api)                   |
| 2     | scripts-reference                 | [Scripts reference](/t/-/5375)                                  |
| 3     | commissioning-scripts-reference   | [Commissioning scripts](/t/-/6605)                              |
| 3     | testing-scripts-reference         | [Hardware test scripts](/t/-/5392)                              |
| 3     | maas-terraform-reference          | [Terraform](/t/-/6327)                                          |
| 2     | log-reference                     | [Log reference](/t/-/6956)                                      |
| 3     | event-logs-reference              | [Event logs](/t/-/5252)                                         |
| 3     | audit-event-logs-reference        | [Audit event logs](/t/-/5256)                                   |
| 3     | commissioning-logs-reference      | [Commissioning logs](/t/-/5248)                                 |
| 3     | testing-logs-reference            | [Testing logs](/t/-/5314)                                       |
| 2     | machine-parameters-reference      | [Machine parameters reference](/t/-/6957)                       |
| 3     | power-drivers-reference           | [Power drivers](/t/-/5246)                                      |
| 3     | storage-layouts-reference         | [Storage layouts](/t/-/5973)                                    |
| 3     | device-labelling                  | [Device labelling](/t/-/6941)                                   |
| 0     |                                   |                                                                 |
| 1     | explanation                       | [Explanation](/t/-/6141)                                        |
| 2     | about-maas                        | [MAAS](/t/-/6678)                                               |
| 2     | about-high-availability           | [High availability](/t/-/6992)                                  |
| 2     | about-maas-networks               | [Networking](/t/-/6680)                                |
| 2     | about-images                      | [Images](/t/-/6685)                                       |
| 2     | about-controllers                 | [Controllers](/t/-/6690)                                  |
| 2     | about-machines                    | [Machines](/t/-/6695)                                     |
| 2     | about-virtual-machines            | [Virtual machines](/t/-/6704)                             |
| 2     | about-device-labels               | [Device labels](/t/-/6709)                                |
| 2     | about-maas-events                 | [MAAS events](/t/-/6510)                                  |
| 2     | about-audit-events                | [Audit events](/t/-/6372)                                 |
| 2     | about-maas-logging                | [MAAS logging](/t/-/5240)                                 |
| 2     | about-maas-security               | [MAAS security](/t/-/6719)                                |
| 2     | about-maas-performance            | [MAAS performance](/t/-/6178)                             |
| 2     | about-ansible                     | [Ansible](/t/-/6888)                                      |
[/details]

<!--
| 3     | about-commissioning-logs           | [About commissioning logs](/t/-/6855)           |
| 3     | about-testing-logs                 | [About testing logs](/t/-/6856)                 |
| 3     | about-event-logs                   | [About event logs](/t/-/6700)                   |
| 3     | about-audit-logs                   | [About audit logs](/t/-/6857)                   |
| 3     | about-system-logs                  | [About system logs](/t/-/6858)                  |
| 3     | bout-region-logs                   | [About region logs](/t/-/6714)                  |
| 3     | about-rack-logs                    | [About rack logs](/t/-/6715)                    |
| 3     | about-maas-logs                    | [About MAAS logs](/t/-/6716)                    |
| 3     | about-other-logs-of-interest       | [About other logs of interest](/t/-/6718)       |
| 2     | about-labels                       | [About labels](/t/-/6709)                       |
| 3     | about-tags                         | [About tags](/t/-/6710)                         |
| 3     | about-annotations                  | [About annotations](/t/-/6711)                  |
| 2     | about-maas-security                | [About MAAS security](/t/-/6719)                |
| 3     | about-maas-security-best-practices | [About MAAS security best practices](/t/-/6859) |
| 3     | about-user-accounts                | [About user accounts](/t/-/6722)                |
| 3     | about-rbac-and-candid              | [About RBAC and Candid](/t/-/6724)              |
| 3     | about-tls-and-maas                 | [About TLS and MAAS](/t/-/6720)                 |
| 3     | about-vault-and-maas               | [About Vault and MAAS](/t/-/6723)               |
| 3     | about-air-gapped-maas              | [About air-gapped MAAS](/t/-/6721)              |
| 2     | about-maas-operations              | [About MAAS operations](/t/-/6725)              |
| 3     | about-backups                      | [About backups](/t/-/6728)                      |
| 3     | about-monitoring-maas              | [About monitoring MAAS](/t/-/6726)              |
| 3     | about-maas-performance             | [About MAAS performance](/t/-/6727)             |
| 3     | about-maas-and-ansible             | [About MAAS and Ansible](/t/-/6730)             |
| 3     | about-maas-and-terraforming        | [About MAAS and Terraforming](/t/-/6729)        |
| 3     | about-the-machine-lifecycle            | [About the machine lifecycle](/t/-/6696)            |
| 3     | about-the-machine-list                 | [About the machine list](/t/-/6701)                 |
| 3     | about-commissioning-scripts            | [About commissioning scripts](/t/-/6698)            |
| 3     | about-testing-scripts                  | [About testing scripts](/t/-/6699)                  |
| 3     | about-machine-related-events           | [About machine-related events](/t/-/6853)           |
| 3     | about-machine-storage                  | [About machine storage](/t/-/6702)                  |
| 3     | about-disk-erasure                     | [About disk erasure](/t/-/6703)                     |
| 2     | about-virtual-machines                 | [About virtual machines](/t/-/6704)                 |
| 3     | about-vm-hosting                       | [About VM hosting](/t/-/6705)                       |
| 3     | about-numa                             | [About NUMA](/t/-/6708)                             |
| 3     | about-overcommit                       | [About overcommit](/t/-/6854)                       |
| 3     | about-lxd                              | [About LXD](/t/-/6706)                              |
| 3     | about-rack-controllers                 | [About rack controllers](/t/-/6692)                 |
| 3     | about-region-controllers               | [About region controllers](/t/-/6691)               |
| 3     | about-region-rack-communication        | [About region-rack communication](/t/-/6693)        |
| 3     | about-controller-high-availability     | [About controller high availability](/t/-/6694)     |
| 3     | about-maas-images-and-streams          | [About MAAS images and streams](/t/-/6686)          |
| 3     | about-image-deployment                 | [About image deployment](/t/-/6688)                 |
| 3     | about-boot-sources                     | [About boot sources](/t/-/6850)                     |
| 3     | about-local-mirrors                    | [About local mirrors](/t/-/6687)                    |
| 3     | about-cloud-init                       | [About cloud-init](/t/-/6851)                       |
| 3     | about-kernel-options                   | [About kernel options](/t/-/6852)                   |
| 3     | about-custom-images                    | [About custom images](/t/-/6689)                    |
| 3     | about-pxe-booting                      | [About PXE booting](/t/-/6842)                      |
| 3     | about-power-drivers                    | [About power drivers](/t/-/6843)                    |
| 3     | about-discovery                        | [About discovery](/t/-/6844)                        |
| 3     | about-subnets                          | [About subnets](/t/-/6845)                          |
| 3     | about-vlans                            | [About VLANs](/t/-/6846)                            |
| 3     | about-proxies                          | [About proxies](/t/-/6847)                          |
| 3     | about-rpc                              | [About RPC](/t/-/6848)                              |
| 3     | about-availability-zones               | [About availability zones](/t/-/6849)               |
| 3     | what-maas-offers                       | [What MAAS offers](/t/-/6840)                       |
| 3     | how-maas-works                         | [How MAAS works](/t/-/6679)                         |
| 2     | primers                                | [Primers](/t/-/6841)                                |
| 3     | about-tcp-ip                           | [About TCP/IP](/t/-/6683)                           |
| 3     | about-dhcp                             | [About DHCP](/t/-/6682)                             |
| 3     | about-cloud-networking                 | [About cloud networking](/t/-/6684)                 |
| 3     | flat-storage-layout-reference          | [Flat storage layout reference](/t/-/6834)          |
| 3     | lvm-storage-layout-reference           | [LVM storage layout reference](/t/-/6835)           |
| 3     | bcache-storage-layout-reference        | [Bcache storage layout reference](/t/-/6836)        |
| 3     | vmfs6-storage-layout-reference         | [VMFS6 storage layout reference](/t/-/6837)         |
| 3     | blank-storage-layout-reference         | [Blank storage layout reference](/t/-/6838)         |
| 3     | maas-log-reference                     | [MAAS log reference](/t/-/6813)                     |
| 3     | system-log-reference                   | [System log reference](/t/-/6814)                   |
| 3     | apc-power-driver-reference             | [APC power driver reference](/t/-/6815)             |
| 3     | bmc-power-driver-reference             | [BMC power driver reference](/t/-/6816)             |
| 3     | christmann-power-driver-reference      | [Christmann power driver reference](/t/-/6817)      |
| 3     | cisco-rcs-power-driver-reference       | [Cisco RCS power driver reference](/t/-/6818)       |
| 3     | digital-loggers-power-driver-reference | [Digital Loggers power driver reference](/t/-/6819) |
| 3     | facebook-wedge-power-driver-reference  | [Facebook Wedge power driver reference](/t/-/6820)  |
| 3     | hp-moonshot-power-driver-reference     | [HP Moonshot power driver reference](/t/-/6821)     |
| 3     | ibm-hmc-power-driver-reference         | [IBM HMC power driver reference](/t/-/6822)         |
| 3     | ibm-z-power-driver-reference           | [IBM Z power driver reference](/t/-/6823)           |
| 3     | intel-amt-power-driver-reference       | [Intel AMT power driver reference](/t/-/6824)       |
| 3     | ipmi-power-driver-reference            | [IPMI power driver reference](/t/-/6825)            |
| 3     | lxd-power-driver-reference             | [LXD power driver reference](/t/-/6826)             |
| 3     | microsoft-ocs-power-driver-reference   | [Microsoft OCS power driver reference](/t/-/6827)   |
| 3     | openstack-nova-power-driver-reference  | [OpenStack Nova power driver reference](/t/-/6828)  |
| 3     | proxmox-power-driver-reference         | [Proxmox power driver reference](/t/-/6829)         |
| 3     | redfish-power-driver-reference         | [Redfish power driver reference](/t/-/6830)         |
| 3     | seamicro-15000-power-driver-reference  | [SeaMicro 15000 power driver reference](/t/-/6831)  |
| 3     | virsh-libvirt-power-driver-reference   | [Virsh libvirt power driver reference](/t/-/6832)   |
| 3     | vmware-power-driver-reference          | [VMWare power driver reference](/t/-/6833)          |
-->

** Redirects

[details=Mapping table]
|PATH|LOCATION|
|--------|--------|
|/docs/about-rbac|/docs/how-to-protect-your-secrets|
|/docs/maas-terraform-provider|/docs/maas-terraform-provider-reference|
|/docs/commissioning-script-reference|/docs/commissioning-scripts-tech-reference|
|/docs/technical-reference|/docs/maas-technical-reference|
|/docs/how-to-set-up-maas-metrics|/docs/how-to-observe-a-live-maas|
|/docs/how-to-back-up-maas|/docs/how-to-keep-maas-backed-up|
|/docs/how-to-use-maas-in-an-air-gapped-environment|/docs/how-to-set-up-an-air-gapped-maas|
|/docs/how-to-enable-tls-encryption|/docs/how-to-enable-maas-native-tls|
|/docs/how-to-secure-maas|/docs/how-to-improve-maas-security|
|/docs/maas-logging-reference|/docs/how-to-work-with-log-files|
|/docs/how-to-search-maas|/docs/how-to-find-machines|
|/docs/how-to-work-with-annotations|/docs/how-to-annotate-machines|
|/docs/how-to-work-with-tags|/docs/how-to-tag-machines|
|/docs/how-to-use-tags|/docs/how-to-label-and-find-machines|
|/docs/how-to-manage-vms|/docs/how-to-create-and-manage-vms|
|/docs/how-to-manage-vm-hosts|/docs/how-to-create-vm-hosts|
|/docs/how-to-use-lxd|/docs/how-to-set-up-lxd|
|/docs/how-to-deploy-machines|/docs/how-to-put-machines-to-work|
|/docs/how-to-manage-machines|/docs/how-to-make-machines-available|
|/docs/how-to-deploy-maas|/docs/how-to-deploy-physical-machines|
|/docs/how-to-manage-controllers|/docs/how-to-adjust-your-controllers|
|/docs/how-to-manage-vmware-images|/docs/how-to-employ-vmware-images|
|/docs/how-to-create-custom-images|/docs/how-to-build-custom-images|
|/docs/how-to-import-images|/docs/how-use-standard-images|
|/docs/how-to-choose-images|/docs/how-to-acquire-images|
|/docs/how-to-manage-ip-addresses|/docs/how-to-enable-dhcp|
|/docs/how-to-manage-networks|/docs/how-to-connect-maas-networks|
|/docs/how-to-configure-networking|/docs/how-to-set-up-networks|
|/docs/report-a-bug|/docs/how-to-report-a-bug|
|/docs/request-a-feature|/docs/how-to-request-a-feature|
|/docs/how-to-get-help|/docs/how-to-ask-for-help| 
|/docs/configuration-settings-reference|/docs/how-to-change-maas-settings|
|/docs/ansible-playbooks-reference|/docs/how-to-spin-up-maas-with-ansible|
|/docs/how-to-get-maas-running|/docs/how-to-get-started-with-maas|
|/docs/create-a-custom-image|/docs/custom-image-tutorial|
|/docs/how-to-use-the-maas-cli|/docs/try-out-the-maas-cli|
|/docs/maas-bootstrap-tutorial|/docs/get-started-with-maas|
|/docs/tutorials|/docs/basic-tutorials|
|/docs/maas-concepts-and-terms-reference|/docs/maas-glossary|
|/docs/installation-requirements|/docs/maas-installation-requirements|
|/docs/whats-new-in-maas|/docs/what-is-new-with-maas|
|/docs/vmware-images|/docs/how-to-employ-vmware-images|
|/docs/about-tags|/docs/how-to-label-and-find-machines|
|/docs/ssl|/docs/how-to-enable-maas-native-tls|
|/docs/stp|/docs/how-to-connect-maas-networks|
|/docs/subnet-management|/docs/how-to-connect-maas-newtorks|
|/docs/deploy-nodes|/docs/how-to-put-machines-to-work|
|/docs/storage|/docs/about-machines#heading--machine-storage|
|/docs/how-to-manage-regions|/docs/how-to-adjust-your-controllers|
|/docs/how-to-manage-racks|/docs/how-to-adjust-your-controllers|
|/docs/how-to-create-a-custom-ubuntu-image|/docs/how-to-build-custom-images|
|/docs/how-to-build-maas-images/docs/how-to-build-custom-images|
|/docs/how-to-use-image-streams|/docs/how-use-standard-images|
|/docs/how-to-manage-dhcp|/docs/how-to-enable-dhcp|
|/docs/how-to-manage-ip-ranges|/docs//docs/how-to-enable-dhcp|
|/docs/how-to-set-up-ntp-services|/docs/how-to-connect-maas-networks|
|/docs/how-to-manage-machine-interfaces|/docs/how-to-connect-maas-networks|
|/docs/how-to-manage-proxies|/docs/how-to-connect-maas-networks|
|/docs/deb/3.0/cli/how-to-work-with-tags|/docs/how-to-tag-machines|
|/docs/deb/3.0/ui/how-to-work-with-tags|/docs/how-to-tag-machines|
|/docs/deb/3.1/cli/how-to-work-with-tags|/docs/how-to-tag-machines|
|/docs/deb/3.1/ui/how-to-work-with-tags|/docs/how-to-tag-machines|
|/docs/snap/2.7/cli/how-to-upgrade-maas|/docs/how-to-upgrade-maas|
|/docs/snap/2.7/cli/installation-tech-reference|/docs/how-to-install-maas|
|/docs/snap/2.7/cli/interactive-search|/docs/how-to-find-machines|
|/docs/snap/2.7/ui/how-to-upgrade-maas|/docs/how-to-upgrade-maas|
|/docs/snap/2.7/ui/installation-tech-reference|/docs/how-to-install-maas|
|/docs/snap/2.7/ui/interactive-search|/docs/how-to-find-machines|
|/docs/snap/2.8/cli/how-to-upgrade-maas|/docs/how-to-upgrade-maas|
|/docs/snap/2.8/cli/installation-tech-reference|/docs/how-to-install-maas|
|/docs/snap/2.8/cli/interactive-search|/docs/how-to-find-machines|
|/docs/snap/2.8/ui/how-to-upgrade-maas|/docs/how-to-upgrade-maas|
|/docs/snap/2.8/ui/installation-tech-reference|/docs/how-to-install-maas|
|/docs/snap/2.8/ui/interactive-search|/docs/how-to-find-machines|
|/docs/snap/2.9/cli/how-to-work-with-tags|/docs/how-to-tag-machiness|
|/docs/snap/2.9/ui/how-to-work-with-tags|/docs/how-to-tag-machines|
|/docs/snap/3.0/cli/how-to-work-with-tags|/docs/how-to-tag-machines|
|/docs/snap/3.0/ui/how-to-work-with-tags|/docs/how-to-tag-machines|
|/docs/snap/3.1/cli/how-to-work-with-tags|/docs/how-to-tag-machines|
|/docs/snap/3.1/ui/how-to-work-with-tags|/docs/how-to-tag-machines|
[/details]
<!-- nohtml end-nohtml -->
* How to help improve the doc
This page contains detailed information on how to become a successful MAAS documentation writer. Welcome to the club.

** Contribution overview</h2>

Once you have permission to edit the doc, you can modify any article by choosing the link at the bottom, entitled, "Help improve this document in the forum."  Recognise that what you change will be posted instantly.  Check your work carefully before saving your edits!

Some questions to ask yourself as you are contributing:

- Does your reply improve the conversation in some way?

- Is your contribution kind to fellow community members?

- Is your constructive criticism focused on ideas, and not people?

- Are you certain of the technical accuracy of your contribution?

If you can answer these questions in the affirmative, your edits should be welcome.

** Style and language</h2>

Please follow these guidelines for style and language:

-   Use a spell checker.
-   Resist being overly formal.
-   Verify hyperlinks and examples.
-   Target audience: intermediate system administrator, not a developer.
-   Use British English (en-GB).
-   Never break a hyperlink with a carriage return. 

Note that the document markup is not particularly complicated.  For the most part, if you rely on the formatting bar just above the editor window -- or use standard HTML -- you shouldn't have much difficulty.

** Headers</h2>

Headers are simple to create, using standard HTML:

    ** id="optional-unique-id">Heading Level 2</h2>
    *** id="optional-unique-id">Heading Level 3</h3>
    ... and so on...
    
** id="bold-and-italic">Bold and Italic Text</h2>

Bold and italic text can be indicated in two different ways.  You can use standard HTML markup:

    <strong>Bold Text</strong>
    <em>Italic Text</em>

You can also use the local editor markup:

    **strong**
    _italic_ (note that's just a single underbar)

** Code blocks</h2>

A code block (or pre-formatted text) is inserted by indenting four spaces:

    maas command do something
    maas command do something else
    ```
You can use this style anytime you want to present:

- a command line sequence,
- a pre-formatted block of text,
- a block of text in which you need to escape markup sequences (as above),
- a code listing where indentation and monospace font are important.

** Inline code</h2>

Use a &lt;code&gt; tag to mark <code>inline filenames and other literals</code> as code examples. Alternatively, you can also use the `backtick`, like this:

    ...can also use the `backtick`, like this...

** Admonishments</h2>

An admonishment distinguishes information from the rest of the text. The syntax begins with the markup tag <code>[note]</code> and ends with the corresponding closure <code>[/note]</code>:

    [note]
    Admonishment text.
    [/note]

** Comments</h2>

Occasionally it may be appropriate to include a comment to explain or organise some text. This ends up as an HTML comment -- which can be read online, even if only in the browser inspection window -- so take it seriously:

```
<!--
The below text may be removed soon.
-->
```

** Hyperlinks</h2>

Links to internal files or external URLs use the following format:

```
[visible text](URL)
```

The `visible text` is what will appear on the web page. The `URL` is used to refer to the destination, which is a  fully-qualified URL.  For example:

    Refer to [Google](https://www.google.com)`↗`.

which would show up as "Refer to [Google](https://www.google.com)`↗`."

** Images</h2>

An image should not be overly cropped - allow for context. When ready, place the image file in the `uploads` directory.

In terms of linking, they are managed very similarly to hyperlinks. However, they are placed on their own line; are preceded by an exclamation point; and both the label and destination have a specific naming convention:

````
<a href="https://discourse.maas.io/uploads/default/original/1X/1f2e6cf2879e391e7ae1ad537cc9ce1baa119f86.png" target = "_blank"><img src="https://discourse.maas.io/uploads/default/original/1X/1f2e6cf2879e391e7ae1ad537cc9ce1baa119f86.png"></a>
````
This image would appear in this way:

<a href="https://discourse.maas.io/uploads/default/original/1X/1f2e6cf2879e391e7ae1ad537cc9ce1baa119f86.png" target = "_blank"><img src="https://discourse.maas.io/uploads/default/original/1X/1f2e6cf2879e391e7ae1ad537cc9ce1baa119f86.png"></a>

** Tiered sections</h2>

You can create optional detail sections (something like and "in-line sidebar") by using these constructions:

    <details>
    <summary>this summary appears with an arrow next to it</summary>
    this text toggles when you click on the arrow
    <details>
    <summary>You can do multiple levels of this.</summary>
    this is yet another hidden level
    </details>
    </details>

This sequence would present like this in the finished document:

<details>
<summary>this summary appears with an arrow next to it</summary>
this text toggles when you click on the arrow
<details>
<summary>You can do multiple levels of this.</summary>
this is yet another hidden level
</details>
</details>

** Capitalisation</h2>

Do not use a "Caps Everywhere" style. It is only used in level one headers and the title metadata. References (visible text) to these page titles (including the navigation) should just capitalise the first letter. Obviously, this does not pertain to words that should always be capitalised according to basic grammar rules (e.g. acronyms, proper nouns).

* MAAS events
MAAS events are key to most debugging efforts.  Events are state changes that happen to MAAS elements, such as controllers, networks, or machines.  These state changes can be caused by MAAS itself, some external agent (such as an external DHCP server), or by users (such as when commissioning a machine).  Being able to review events is often essential to debugging or verifying your MAAS system.

Events can be seen in the MAAS logs, in the UI event log, and in output from the CLI `events query` command.  These three sources provide analogous (but somewhat different information). For example, consider the following log listing, obtained by doing a `grep "fun-zebra" *.log | grep "transition from"` in the MAAS log directory:

```nohighlight
maas.log:2022-09-29T15:04:07.795515-05:00 neuromancer maas.node: [info] fun-zebra: Status transition from COMMISSIONING to TESTING
maas.log:2022-09-29T15:04:17.288763-05:00 neuromancer maas.node: [info] fun-zebra: Status transition from TESTING to READY
```

This information appears this way when events are queried from the CLI:

```nohighlight
        {
            "username": "unknown",
            "node": "bk7mg8",
            "hostname": "fun-zebra",
            "id": 170,
            "level": "INFO",
            "created": "Thu, 29 Sep. 2022 20:04:17",
            "type": "Ready",
            "description": ""
        },
        {
            "username": "unknown",
            "node": "bk7mg8",
            "hostname": "fun-zebra",
            "id": 167,
            "level": "INFO",
            "created": "Thu, 29 Sep. 2022 20:04:07",
            "type": "Running test",
            "description": "smartctl-validate on sda"
        },
```

And it appears like this in the UI events log:

| Time	| Event |
|---|---|
|**Thu, 29 Sep. 2022 20:04:17**	|**Node changed status - From 'Testing' to 'Ready'** |
|Thu, 29 Sep. 2022 20:04:17	|Ready |
|Thu, 29 Sep. 2022 20:04:17	|Script result - smartctl-validate on sda changed status from 'Running' to 'Skipped' |
|Thu, 29 Sep. 2022 20:04:16	|Script result - smartctl-validate on sda changed status from 'Installing dependencies' to 'Running' |
|Thu, 29 Sep. 2022 20:04:07	|Running test - smartctl-validate on sda |
|**Thu, 29 Sep. 2022 20:04:07**	|**Node changed status - From 'Commissioning' to 'Testing'** |

You can see that all three outputs are sources of truth, but the messages are somewhat different, include different information, and contain different levels of detail.


* MAAS glossary
Built on a foundation of networking knowledge, MAAS introduces a number of new terms, and adds some nuances to common terms.  Some of these terms may be common networking terms you never looked up; others represent more complex concepts that may be unique to MAAS.  This article presents and explains some of these important terms.

<details><summary>Show me an alphabetical list of terms</summary>


- [Abort](#heading--abort)

- [Allocated](#heading--allocated)

- [Allocate](#heading--allocate)

- [Bond](#heading--bond)

- [Broken](#heading--broken)

- [Cloud-init](#heading--cloud-init)

- [Commission](#heading--commission)

- [Commissioning](#heading--commissioning)

- [Controllers](#heading--controllers)

- [Delete](#heading--delete)

- [Deployed](#heading--deployed)

- [Deploy](#heading--deploy)

- [Deploying](#heading--deploying)

- [Devices](#heading--devices)

- [DHCP](#heading--dhcp)

- [DHCP relay](#heading--dhcp-relay)

- [Edge clouds](#heading--edge-clouds)

- [Entering rescue mode](#heading--entering-rescue-mode)

- [Exiting rescue mode](#heading--exiting-rescue-mode)

- [Exit rescue mode](#heading--exit-rescue-mode)

- [Fabrics](#heading--fabrics)

- [Failed Commissioning](#heading--failed-commissioning)

- [Failed Deployment](#heading--failed-deployment)

- [Hugepages](#heading--hugepages)

- [Images](#heading--images)

- [Interfaces](#heading--interfaces)

- [IP ranges](#heading--ip-ranges)

- [Isolating CPUs](#heading--isolcpus)

- [Locked](#heading--locked)

- [Lock](#heading--lock)

- [Machine actions](#heading--machine-actions)

- [Machines](#heading--machines)

- [Machine statuses](#heading--machine-statuses)

- [Mark broken](#heading--mark-broken)

- [Mark fixed](#heading--mark-fixed)

- [Network infrastructure](#heading--network-infrastructure)

- [Network interface](#heading--network-interface)

- [Brief network tutorial](#heading--network-tutorial)

- [New](#heading--new)

- [Nodes](#heading--nodes)

- [NUMA/vNUMA](#heading--numa)

- [Override failed](#heading--override-failed)

- [Package repositories](#heading--package-repositories)

- [Personal Package Archives (PPA)](#heading--personal-package-archives-ppa)

- [Physical](#heading--physical)

- [Power off](#heading--power-off)

- [Power on](#heading--power-on)

- [Ready](#heading--ready)

- [Regions](#heading--regions)

- [Release](#heading--release)

- [Rescue mode](#heading--rescue-mode)

- [Rescue mode](#heading--rescue-mode)

- [Series](#heading--series)

- [Set Zone](#heading--set-zone)

- [Spaces](#heading--spaces)

- [SR-IOV](#heading--sr-iov)

- [Subnets](#heading--subnets)

- [Tags](#heading--tags)

- [Test hardware](#heading--test-hardware)

- [Ubuntu package repositories](#heading--ubuntu-package-repositories)

- [Unknown](#heading--unknown)

- [Unlock](#heading--unlock)

- [VLAN](#heading--vlan)

- [VM hosts](#heading--vm-hosts)

- [Zones](#heading--zones)

</details>

** Abort

You can abort any action that permits retries. Currently, only commissioning and deployment permit retries.

** Allocated

The node is allocated (reserved) to a MAAS user. See node action 'Allocate'.

** Allocate

Allocates (reserves) a node to the MAAS user performing the action (and currently logged in). Changes a node's status from 'Ready' to 'Allocated'.

With the CLI, it is necessary to perform this action before deploying. With the web UI, it is done automatically for the user. Allocating in the web UI is used for machine reservation.

** Bond

A bond interface is capable of aggregating two or more physical interfaces into a single logical interface.  You can use bonds in conjunction with a managed switch (using Link Aggregation and Control Protocol, or LACP), or independently (software bonds).

** Broken

The node is broken. See node action 'Mark broken'.

** Cloud-init

Cloud-init is the industry-standard method for initialising cloud instances, independent of platform.  It allows you to automatically provision operating system images, bringing them to a fully running state.  It also allows you to customise networking, storage, user space, applications, and various other components of a functioning system.

There are four stages of cloud-init action:

- Local initialisation - this "as-early-as-possible" stage configures system elements that must be set up before the system can be made fully operational, such as networking configuration, including bridges, VLANs, bonds, and so forth.

- Initialisation - this second stage runs as soon as the network connections are up and running, taking care of custom storage configurations, disk volume expansion, block device setup and filesystem allocations.

- Module configuration - stage three configures the necessary meta-tools to allow full system configuration, such as SSH, apt/yum, and NTP.

- Module finalisation - stage four is the very end of the boot process, installing packages and executing any user-supplied configuration scripts.

These four steps combine (at least) four general datasources to bring an instance into being:

- Disk image - the operating system for the instance; this is a bare-bones, uncustomised version of the chosen OS.

- Metadata - this configuration information is supplied by the cloud provider, specifying things like disk images storage, networking, default users, and other basic customisations.

- User data - data provided by end users or cloud administrators to initialise the instance.  This completely optional data can be anything from shell scripts to highly-structured cloud-config data that trigger cloud-init's built-ins.

- Vendor data - data provided by cloud platform vendors; this is identical (in principle) to user data, but derived from a different source.  In practice, vendor data usually handle things that users wouldn't normally specify, such as mirror setup, NTP service management, etc.

** Commission

This action commissions a node, changing a node's status from 'New' to 'Commissioning' to 'Ready'.

Commissioning enables MAAS to build a detailed inventory of RAM, CPU, storage, NICs and accelerators like GPUs. These are itemised and usable as constraints for machine selection.

If commissioning is unsuccessful, the status becomes 'Failed commissioning'.

Any time a node's underlying networking or disk subsystem has changed, it should be re-commissioned. Typically, you would mark the node as 'Broken' (see below), implement maintenance, and then Commission.

** Commissioning

The node is in the process of commissioning. See node action 'Commission'.

** Controllers

There are two types of controllers: a region controller and a rack controller. The region controller deals with operator requests while one or more rack controllers provide the high-bandwidth services to multiple server racks, as typically found in a data centre.

A region controller consists of the following components:

- REST API server (TCP port 5240)
- PostgreSQL database
- DNS
- caching HTTP proxy
- web UI

Think of a region controller can as being responsible for a data centre, or a single region. Multiple fabrics are used by MAAS to accommodate subdivisions within a single region, such as multiple floors in a data centre.

A rack controller provides the following services:

- DHCP
- TFTP
- HTTP (for images)
- power management

A rack controller is attached to each "fabric". As the name implies, a typical setup is to have a rack controller in each data centre server rack. The rack controller will cache large items for performance, such as operating system install images, but maintains no independent state other than the credentials required to talk to the region controller.

Both the region controller and the rack controller can be scaled-out as well as made highly available.

** Delete

This action removes a node from MAAS. The underlying machine remains unaffected. Upon rebooting, it will be enlisted once more (status 'New').

** Deployed

The node is deployed. See node action 'Deploy'.

The visible status will be the name of the chosen OS (e.g. 'Ubuntu 16.04 LTS').

** Deploy

This action, which includes 'Power on,' deploys a node, changing a node's status from 'Ready' (or 'Allocated') to a deployed status. 

During deployment, MAAS turns on the machine and installs a complete server operating system without manual intervention, configuring network interfaces, disk partitions and more automatically.

If the deployment is unsuccessful, the status becomes 'Failed deployment'.

Note that Juju, often used in conjunction with MAAS, also uses the term "deploy" to mean "deploy an application".

** Deploying

The node is in the process of deploying. See node action 'Deploy'.

The visible status will be Deploying to 'OS', where 'OS' is the name of the OS being deployed (e.g. 'Deploying to Ubuntu 16.04 LTS').

** Devices

A device is a non-deployable node. This entity can be used to track routers, for example.

Static or dynamic IP addresses and DNS names can be assigned to any device or parent node.  These addresses will be automatically deleted when the parent node is deleted or released, along with any IP address reservations.  This arrangement is designed to model and manage the virtual machines or containers running inside a MAAS-deployed node.

** DHCP

The Dynamic Host Control Protocol is a network management system in which a server (or group of servers) dynamically assigns IP addresses and other network parameters to a network device.  This network device may or may not have the capability to provide its own IP address, although to take advantage of DHCP, the device must have been configured to seek out a DHCP server and accept an assigned IP address.  Typically, a network administrator defines a range of reserved IP addresses from which the DHCP server can pull when assigning addresses.

DHCP operates using the four-step "DORA" model -- Discovery, Offer, Request, and Acknowledge:

- Potential DHCP clients broadcast a DHCPDISCOVER message on its attached subnet using destination address 255.255.255.255.

- A connected DHCP server receives the DHCPDISCOVER message and sends a DHCPOFFER message, containing an IP address that the client may use.

- The client replies with a DHCPREQUEST message, requesting the offered address.

- The DHCP server responds with a DHCPACK (acknowledgement) which includes various important configuration parameters, such as the lease duration.

Of course, there is [much more to DHCP](https://en.wikipedia.org/wiki/Dynamic_Host_Configuration_Protocol)`↗`, but what's covered here should be sufficient understanding for using MAAS.

** DHCP relay

A DHCP relay, or relay agent, is a network device that forwards requests and replies between a DHCP client and a DHCP server when both are not on the same physical subnet.

** Edge clouds

Edge clouds are designed to minimise latency, so that your cloud computing experience is nearer to real-time.  The use of "edge" doesn't specifically refer to the edges of the cloud, but to the machines that are at the "edge of the problem," or more to the point, "the edge of the cloud that is closest to your application."  Sometimes these are separate clouds in your own data centre, though they can also be parts of a remote cloud that are closer to you in network terms.

There are a number of complex decision lops and optimisation algorithms used by edge clouds, but the primary purpose is low-latency computing where possible.  If the servers closest to you (network-wise) can handle the load, they do; if not, they can call on other servers just a little further away.

Edge clouds can be planned and enhanced by using NUMA and SR-IOV techniques.  NUMA can help you create SMP nodes on VM cores the shortest (network) distance away from your application.  SR-IOV can, in general, reduce network latency even more by eliminating core involvement in network traffic. You can create virtual machines and assign NUMA nodes to minimise network latency, and then ensure that   MAAS gives you NUMA tools to find out whether you're achieving this sort of optimisation, and help you make decisions about how to adjust and improve over time.

** Entering rescue mode

The node is in the process of entering rescue mode. See node action 'Rescue mode'.

** Exiting rescue mode

The node is in the process of exiting rescue mode. See node action 'Exit rescue mode'.

** Exit rescue mode

This action changes a node's status from 'Rescue mode' to the 'Exiting rescue mode' transitory status and then back to its original status when the operation is complete.

** Fabrics

A **fabric** connects VLANs.  If you understand a VLAN, you know that they permit network connections only between specific switch ports or specifically identified ports ("tagged" ports). Consequently, it would be impossible for two VLANs to communicate with each other.  A fabric makes these VLAN-to-VLAN connections possible.

<details><summary>Take me on a quick, deep dive on fabrics</summary>

We can illustrate a network fabric more easily by rewinding the term to one of its earliest uses: the early phone system.  In a telephone switchboard, subscriber lines (customer phone numbers) ran in a grid pattern in the back of the switchboard, but they didn't touch each other until the operator inserted the plugs of a patch cable to join them.  With some "plugboards" (what a switchboard was actually called), an operator could conference multiple lines by adding more patch cords.

These patch cords essentially acted like a VLAN, allowing only the subscribers whose lines were "patched in" to join the conversation. 

But the switchboard only covered one exchange, that is, one three-digit phone number prefix.  If a subscriber wanted to conference someone from another exchange, there had to be patch from one exchange to another.  This was handled by a long-distance operator.  Each exchange had a more robust outgoing line, called a "trunk line," that connected exchanges in some central place.  The long-distance operators could bridge trunks in a specific way, involving a local operator in each of the "bridged" exchanges.

By now, you're probably starting to recognise a lot of network terms, which is completely appropriate.  Almost all modern networking technology originated in the telephone system.

Now imagine that you want to conference in six people, two in each of three distant exchanges.  Each exchange operator had to patch two numbers and a trunk line.  The long-distance operator had to patch three trunks in a specific way that prevented the conversation from going out to all numbers attached to the trunk.  

The details of the method aren't particularly relevant here, but it usually involved a pair of "bridge clips" that connected non-adjacent wire-crossings, with an insulated portion that laid across wires that weren't meant to be connected.  It looked a lot like a little bridge when properly placed.

Think of each of the local exchange conferences as a VLAN; the long-distance operator's patch cables created what was called a "fabric."  Our use of fabric is exactly the same idea: some number of private "conversations" (connections) connected to each other so that specific people in each "group" can all talk to each other. 

</details>

You could describe a fabric as a VLAN namespace. It's a switch or a combination of switches that use trunking to provide access to specific VLANs. MAAS creates a default fabric ('fabric-0') for each detected subnet during installation.

The following conceptual diagram shows two fabrics in the same data centre or region, each using distinct VLAN ranges and their associated subnets:

<a href="https://discourse.maas.io/uploads/default/original/1X/46177305128bf7f3190f8a7bbd037c33e96f6a9e.png" target = "_blank"><img src="https://discourse.maas.io/uploads/default/original/1X/46177305128bf7f3190f8a7bbd037c33e96f6a9e.png"></a>


** Failed Commissioning

The node failed to commission.

** Failed Deployment

The node failed to deploy.

** Hugepages

Computer memory is addressed not as raw RAM, but as virtual memory. Assisted by the CPU's memory management unit (MMU), the kernel maps virtual memory to a physical location. Virtual memory is divided into pages, which can be swapped in and out to disk during normal operation (hence the term "swap space").  When programs access memory, the CPU needs to know which physical page has the data, so it relies on the kernel's "page table" to find the right virtual-to-physical address mapping.

Since this page table is big and slow, the CPU has a special buffer -- the Translation Lookaside Buffer (TLB) -- that caches address mapping.  This means after the first access to a page, subsequent accesses are much faster.  Since this buffer is implemented in hardware, for speed, the size is limited to, say, 4096 bytes.

When the core is accessing lots of pages, the speed advantage of the TLB can be lost.  Hugepages allow one TLB entry to point to 2MB, instead of just 4096 bytes.  With 512 TLB entries, typically, you can map 1GB of memory.  Hugepages come with a catch, though: if you have to swap pages, it (obviously) takes longer.

There's no tried and true formula for when to use them, but the key consideration is that you want to use most of a hugepage when you swap.  Rather than using little sections of a hugepage, which would mean losing the performance advantage from constant swapping, you want to maximise the use of each hugepage.  While there's no simple, empirical way to calculate this answer, you can do it by trial-and-error observation.

MAAS provides the dashboards and tools necessary to monitor and adjust your use of hugepages, so that you can find the right balance.

** Images

An image is used to provision a machine. As soon as you install MAAS, images are imported based on what series you have selected.  MAAS won't work until it has imported the necessary images.

** Interfaces

** IP ranges

You can reserve IP addresses by adding one or more reserved ranges to a subnet configuration. You can define two types of ranges:

-   **Reserved range** Mode operates differently depending on whether the subnet is managed or unmanaged:
   -   **Managed (subnet)**: MAAS will never assign IP addresses inside this range.  You can use this range for anything, such as infrastructure systems, network hardware, external DHCP, or an OpenStack namespace.
   -   **Unmanaged (subnet)**: MAAS will only assign IP addresses inside this range.
-   **Reserved dynamic range** An IP range that MAAS will use for enlisting, commissioning and, if enabled, MAAS-managed DHCP on the node's VLAN during commissioning, deploying. An initial range is created as part of the DHCP enablement process if done with the web UI. MAAS never uses IP addresses from this range for an unmanaged subnet.

** Isolating CPUs

For certain operations, it's useful to shield a CPU from having to execute general system processes and take interrupts.  These are sometimes referred to as "isolcpus," more correctly described as booting a core with the `isolcpus` boot parameter.  This parameter restricts the shielded core to processes assigned directly to it, avoiding sharing bandwidth with the general scheduler and preventing the core from taking non-specific interrupts.

When used with VMs, users can maximise performance by configuring isolcpus in the kernel, to prevent the general scheduler and other tasks from using bandwidth on your VM core(s).

** Locked

It's not strictly a status, but a machine showing a 'padlock' symbol adjacent to its name is in a locked state.

** Lock

This action marks a machine as locked, preventing the user from performing actions on machines that could change their state. For example, a locked machine cannot be mistakenly powered off or released.

A locked machine has a padlock symbol next to its name.

<a href="https://discourse.maas.io/uploads/default/original/1X/7d1f0928fb599d465916e43e731535dfee60e65a.png" target = "_blank"><img src="https://discourse.maas.io/uploads/default/original/1X/7d1f0928fb599d465916e43e731535dfee60e65a.png"></a>

** Machine actions

Machine actions are essentially "things you can do with nodes". You can trigger them via the web UI or the MAAS CLI. In the web UI, you manage them with the 'Take action' button in the top right corner. An action usually changes the status (see next section) of a node. Below is the full list of possible actions and their meaning, arranged alphabetically.

** Machines

A machine is a node that can be deployed by MAAS.

** Machine statuses

Node statuses are labels used to describe the general state of a node as known to MAAS. A node will undergo various manipulations during their time spent in MAAS, and its status will change accordingly. Actions applied to a node are the most common cause of a status change (see section above.)  Below is the full list of status values and their meaning, arranged alphabetically.

Some aspects of a node can only be modified when a node has a certain status. Here are two typical examples:

- you cannot modify a network interfaces unless the node has a status of either 'Ready' or 'Broken'.
- you cannot modify storage unless the node has a status of either 'Ready' or 'Allocated'.

** Mark broken

Marks a node as broken. Changes a node's status to 'Broken'. Includes action 'Power off'.

You can choose this action if any other action has failed (such as Commission and Deploy). If you mark a node broken, MAAS will not use it. This action would usually be followed by an investigation to determine the source of the problem.

By marking a node broken, you can also flag it for hardware maintenance that would affect MAAS, such as network or disk modifications. In this case, the original status would be 'Deployed'.

You can also mark a newly-commissioned node ('Ready') as 'Broken.'

** Mark fixed

This action fixes a broken node, changing its status from 'Broken' to 'Ready'.

** Network infrastructure

Network infrastructure is a catch-all term covering the physical components of a network, include cables, patch panels, switches, routers, hubs, and other associated network gear.

** Network interface

A network interface, often referred to as a "network interface card" or NIC, is either a separate physical card connected to a node, a set of circuits embedded on a node's motherboard, or a radio transceiver attached to a node in some way.  All network connections require a NIC.  The terms "port" and "adaptor" are also used to refer to a network interface.


** Brief network tutorial

The following is a brief network tutorial, provided as a tool to synchronise understanding.  Note that this tutorial covers mostly those terms routinely needed in the course of using MAAS.

** New

This status represents the first stage of a node's life in MAAS. Typically, a node with this status has just been added to MAAS.

** Nodes

A node is a general term that refers to multiple, more specific objects. Nodes are managed by MAAS through a life cycle, from adding and enlistment into MAAS, through commissioning, allocation and deployment. Nodes are then either released back into the pool of nodes or retired.

Nodes include the following classes of objects:

- Controllers
- Machines
- Devices

See [Machine actions](#heading--machine-actions) and [Machine statuses](#heading--machine-statuses) below for an overview of a node's life cycle.

** NUMA/vNUMA

NUMA stands for "Non-Uniform Memory Access."  In this context, "non-uniform" means that any given CPU core can access its dedicated memory faster than the memory dedicated to other cores.  A NUMA configuration groups core(s) and memory as a dedicated node, which reduces memory access times, so the core won't spend a lot of time stalled in wait states -- that is, waiting for access to data in memory, either because the memory is relatively far away (proximity) or because other cores have access to the same memory (shared memory). In other words, NUMA works better when the core has dedicated memory that is relatively close by.

In this context, "far away" could mean physical distance (more wire or a longer bus distance), more interceding processes (as in virtual machines), or both.  The process of optimising thread and process scheduling so that the core running the code and the required data are close together is sometimes known as "creating affinity." This affinity creates NUMA "nodes," which can be treated as opaque nodes from a symmetric multi-processing (SMP) point of view.  Tasks are assigned to nodes to minimise overhead and wait states.

There is more flexibility in creating affinity when using virtual machines, because memory and core are constructs overlaid on existing hardware, rather than hard silicon.  While this seems as if it might make SMP easier, in fact, it creates difficulties because of the nature of virtual machines and the potential number of interceding processes that manage virtual memory.  For optimum performance, VMs should be aligned to a single NUMA node, so that resources are not split across nodes.

In practice, this means that VMs would be "pinned" to specific cores to create stability.  While the user has the choice of how to pin VMs, MAAS provides visual information that helps the user see how VMs are allocated to physical hardware, and make adjustments if that arrangement isn't (or turns out not to be) optimal.

If you want to dig deeper, there is a [more through treatment of NUMA](https://en.wikipedia.org/wiki/Non-uniform_memory_access)`↗` on Wikipedia.

** Override failed

Allows a machine marked as ‘Failed testing’ to be usable.

** Package repositories

Package repositories managed within MAAS can be of two types:

- Ubuntu package repositories
- Personal Package Archives (PPA)

You can configure repositories in the 'Package repositories' tab on the 'Settings' page. Any enabled repository listed on this page will become automatically available to any subsequently deployed nodes.

MAAS further simplifies the addition of third-party repositories by also allowing the administrator to input their respective GPG keys here. This arrangement means that nodes will have instant access to these repositories (i.e. no need to import the keys into APT).

An added repository can be disabled and re-enabled using a toggle switch to the right of it.

** Personal Package Archives (PPA)

A Personal Package Archive (PPA) is a [Launchpad](https://launchpad.net)-based method for any individual (or team)`↗` to build and distribute packages for Ubuntu.

Adding a PPA is equally straightforward. Using the [`sosreport` PPA](https://launchpad.net/~canonical-support/+archive/ubuntu/support-tools)`↗` as an example, first retrieve the PPA's address from its page on Launchpad:

`ppa:canonical-support/support-tools`

Like before, a public GPG key will be needed. Also get this from the PPA's Launchpad page: 'Technical details about this PPA' &gt; '1024R/9360754F' &gt; '9360754F'.

To add this PPA, then, hit the 'Add repository' button and fill in the fields. Before saving, the form should look something like this:

<a href="https://assets.ubuntu.com/v1/a0962e17-manage-repositories__2.4_add-ppa.png" target = "_blank"><img src="https://assets.ubuntu.com/v1/a0962e17-manage-repositories__2.4_add-ppa.png"></a>

Click 'Add repository' to save the configuration.

See [Launchpad PPAs](https://help.launchpad.net/Packaging/PPA)`↗` for more information on PPAs.

** Physical

After a node is commissioned, MAAS discovers its physical interfaces.

MAAS always creates a device with at least one physical interface.

Before deployment, a MAAS administrator can configure additional interfaces on the node, including one or more of the types mentioned below.

** Power off

This action turns off a node's underlying machine.

** Power on

This action turns on a node's underlying machine.

** Ready

A node bearing this status has been commissioned and is ready for use, including the necessary BMC credentials. MAAS can start or stop this machine, and allocate or (re)deploy it with a fresh operating system.

** Regions

A region is an organisational unit one level above a zone. It contains all information about all machines running in any possible zones. In particular, the PostgreSQL database runs at this level and maintains state for all these machines.

** Release

This action, which includes the 'Power off' action, releases a node back into the pool of available nodes, changing a node's status from 'Deployed' (or 'Allocated') to 'Ready'.

The user has the opportunity to erase the node's storage (disks) before confirming the action. You can configure a default erasure setting on the 'Storage' tab of the 'Settings' page.

** Rescue mode

This action allows you to boot a node ephemerally (Ubuntu running in memory on the underlying machine). By doing so, you can SSH to the machine for maintenance purposes. This action works for a Deployed or Broken node, as well as for a node that failed to deploy.

Authentication and access to the node's storage work the same way it would if the node were deployed. The fact that ephemeral Ubuntu is running is entirely transparent to the user.

The node status is changed to the 'Entering rescue mode' transitory status and then to 'Rescue mode' when the operation is complete.

** Rescue mode

The node is in rescue mode and is ready to accept SSH connections. See node action 'Rescue mode'.

** Series

A series is essentially an operating system version. For Ubuntu, a series takes into account HWE kernels. In practical terms, a series manifests itself in the form of install images that are used to provision MAAS machines. The MAAS administrator can select series as desired.

** Set Zone

This action puts the node in a specific zone.

** Spaces

A space is a logical grouping of subnets that can communicate with one another. Spaces can be arranged to group subnets according to various parameters.  One of the most common examples is a DMZ space, which might group subnets presenting a web interface to the public Internet.  Behind this DMZ would be specific applications that aren't allowed to interact directly with the user, but instead must interact with a Web UI in the DMZ space.  MAAS does not create a default space during installation.

Spaces facilitate machine allocation for [Juju](https://jaas.ai/)`↗`. See [Juju network spaces](https://jaas.ai/docs/spaces)`↗` for more details.

** SR-IOV

With traditional ethernet, a packet comes into the NIC and interrupt is fired for the one core assigned to handle NIC interrupts.  That core has to go get the packet, find the destination MAC address or VLAN tag, then go interrupt the destination core -- which has to get the packet and write it to the memory of the VM it's managing. Statistically speaking, that's basically two core interrupts for every incoming packet.

Many smart NICs are able to sort network packets into queues, based on MAC address or VLAN tag of the intended recipient, a technology sometimes known as "VMDq".  In these cases, each queue has its own interrupt, so each core gets interrupted only for its own packets. This arrangement is much faster than having one core assigned to handle all network interrupts.  Even so, the hypervisor still has to copy every packet from the NIC to the VM, physically touching each packet.

With SR-IOV, it's possible to have no core interrupts when packets come in.  SR-IOV creates "virtual functions," with dedicated queues for transmitting and receiving.  Each VM is directly assigned hardware resources via a virtual function driver, which knows how to DMA-copy data directly between the NIC and the memory space of the relevant VM. Essentially, SR-IOV is like a "jumper wire" between the NIC and the VM, bypassing the core.  This prevents interrupting the core when packets arrive for it, and significantly reduces the core workload when sending network packets.

For a deeper dive, try this [SR-IOV presentation](https://www.youtube.com/watch?v=hRHsk8Nycdg)`↗` from Intel.

** Subnets

A subnet is a "layer 3" network, defined by a network address and a network mask length (in bits) and usually written in "CIDR" format. MAAS supports IPv4 and IPv6 subnets. Examples include:

``` no-highlight
- 0.0.0/8
- 16.0.0/12
- 168.0.0/16
2001:db8:4d41:4153::/64
```

** Tags

A tag (not to be confused with VLAN tags) is user-created and associated with nodes based on their physical properties. These can then be used to identify nodes with particular abilities which can be useful during the deployment of services.

** Test hardware

This action allows the user to select and run scripts to test a machine's underlying hardware.

** Ubuntu package repositories

An Ubuntu package repository is a repository that makes available Ubuntu packages to computers able to connect to it over the network, whether that network is private or public (e.g. the Internet).

MAAS comes equipped with the official Ubuntu repository `archive.ubuntu.com` as well as the equivalent for architectures other than i386 and amd64: `ports.ubuntu.com` as is evident in the default configuration below:

<a href="https://assets.ubuntu.com/v1/77b93794-manage-repositories__2.4_default-repo-config.png" target = "_blank"><img src="https://assets.ubuntu.com/v1/77b93794-manage-repositories__2.4_default-repo-config.png"></a>

Adding a third-party repository is elementary. Begin by basing the configuration on a line you would typically place in a system's `/etc/apt/sources.list` file. For instance, for the Google Chrome repository, the line would look like:

`deb http://dl.google.com/linux/chrome/deb stable main`

You will also need the GPG public key that is associated with the private key that signed this particular repository. Typically, the project's website is consulted to obtain this information. For this example, you could download the key like this:

``` bash
wget https://dl.google.com/linux/linux_signing_key.pub
```

The key now resides in the saved file `linux_signing_key.pub` for later use.

To add this repository, then, hit the 'Add repository' button and fill in the fields using the gathered information. Note that the 'Name' is an arbitrary label to give the repository.

Before saving, the form should look very similar to this:

<a href="https://assets.ubuntu.com/v1/1aa1c512-manage-repositories__2.4_add-repo.png" target = "_blank"><img src="https://assets.ubuntu.com/v1/1aa1c512-manage-repositories__2.4_add-repo.png"></a>

Click 'Add repository' to save the configuration.

A private repository can be built to assist with offline operations, based on the official repository. This repository can also contain custom packages.

** Unknown

Unknown interfaces are sometimes discovered by MAAS. For example, a new DHCP lease that is not associated with any known node or device. Such an interface cannot be user-created.

** Unlock

This action releases a machine from a locked state.

** VLAN

A VLAN interface can be used to connect to a tagged VLAN, if the node is connected to an authorised port.

** VM hosts

VM hosts, also called composable hardware, allow for the dynamic composition of machines from a pool of available hardware resources (e.g. disk space, memory, cores).

** Zones

A physical zone, or just zone, is an organisational unit that contains nodes where each node is in one, and only one, zone. Later, while in production, a node can be taken (allocated) from a specific zone (or not from a specific zone). Since zones, by nature, are custom-designed (except for the 'default' zone), they provide more flexibility than a similar feature offered by a public cloud service (ex: availability zones).

Some prime examples of zone usage include fault-tolerance, service performance, and power management. 

A newly installed MAAS comes with a default zone which contains all nodes unless you create a new zone. You can therefore safely ignore the entire concept if you're not interested in leveraging zones.

You cannot remove the 'default' zone or change its name.


* MAAS installation requirements
** How to check system requirements for MAAS

Before installing MAAS for the first time, you should make sure that the target system meets the minimum requirements for the machines that run MAAS, which vary widely depending on local implementation and usage.  Below, you will find resource estimates based on MAAS components and operating system (Ubuntu Server). We consider both a test configuration (for proof of concept) and a production environment.

** Requirements for a test environment

Here is a proof-of-concept scenario, with all MAAS components installed on a single host. This scenario assumes two complete sets of images (latest two Ubuntu LTS releases) for a single architecture (amd64).

| | Memory (MB) | CPU (GHz) | Disk (GB) |
|:---|----:|----:|----:|
| [Region controller](/t/maas-glossary/785#heading--controllers) (minus PostgreSQL) | 512 | 0.5 | 5 |
| PostgreSQL | 512 | 0.5 | 5 |
| [Rack controller](/t/maas-glossary/785#heading--controllers") | 512 | 0.5 | 5 |
| Ubuntu Server (including logs)| 512 | 0.5 | 5 |

Based on this table, the approximate requirements for this scenario are 2 GB memory, 2 GHz CPU, and 20 GB of disk space.

** Requirements for a production environment

Here is a production scenario designed to handle a high number of sustained client connections. This scenario implements both high availability (region and rack) and load balancing (region). MAAS reserves extra space for images (database and rack controller), while some images, such as those for Microsoft Windows, may require a lot more -- so plan accordingly.

| | Memory (MB) | CPU (GHz) | Disk (GB) |
|:---|----:|----:|----:|
| [Region controller](/t/maas-glossary/785#heading--controllers) (minus PostgreSQL) | 2048 | 2.0 | 5 |
| PostgreSQL | 2048 | 2.0 | 20 |
| [Rack controller](/t/maas-glossary/785#heading--controllers") | 2048 | 2.0 | 20 |
| Ubuntu Server (including logs)| 512 | 0.5 | 5 |

So, based on the above, the approximate requirements for this scenario are:

- A region controller (including PostgreSQL) installed on one host, with 4.5 GB memory, 4.5 GHz CPU, and 45 GB of disk space.
- A duplicate region controller (including PostgreSQL) on a second host, also with 4.5 GB memory, 4.5 GHz CPU, and 45 GB of disk space.
- A rack controller installed on a third host, with 2.5 GB memory, 2.5 GHz CPU, and 40 GB of disk space.
- A duplicate rack controller on a fourth host, also with 2.5 GB memory, 2.5 GHz CPU, and 40 GB of disk space.

The tables above refer to MAAS infrastructure only. They do not cover the resources needed by subsequently-added nodes. Note that machines should have IPMI-based BMC controllers for power cycling, see [Power management](/t/power-management-reference/5246) for more details.

Some examples of factors that influence hardware specifications include:

- the number of connecting clients (client activity)
- how you decide to distribute services
- whether or not you use [high availability/load balancing](/t/how-to-enable-high-availability/5120).
- the number of images that you choose to store (disk space affecting PostgreSQL and the rack controller)

Also, this discussion does not take into account a possible local image mirror, which would be a large consumer of disk space.

One rack controller should only service 1000 machines or less, regardless of how you distribute them across subnets. There is no load balancing at the rack level, so you will need additional, independent rack controllers. Each controller must service its own subnet(s).

* MAAS logging
** About the syslog logging path

[tabs]
[tab version="v3.2 Snap"] 
Syslog data is kept in `/var/snap/maas/common/log/rsyslog/<machine-name><yyyy-mm-dd>/messages`. Every machine known to MAAS will have corresponding syslogs.
[/tab]
[tab version="v3.2 Packages"] 
Syslog data is kept in `/var/log/maas/rsyslog/<machine-name><yyyy-mm-dd>/messages`.  Every machine known to MAAS will have corresponding syslogs.
[/tab]
[tab version="v3.1 Snap"] 
Syslog data is kept in `/var/snap/maas/common/log/rsyslog/<machine-name><yyyy-mm-dd>/messages`. Every machine known to MAAS will have corresponding syslogs.
[/tab]
[tab version="v3.1 Packages"] 
Syslog data is kept in `/var/log/maas/rsyslog/<machine-name><yyyy-mm-dd>/messages`.  Every machine known to MAAS will have corresponding syslogs.
[/tab]
[tab version="v3.0 Snap"] 
Syslog data is kept in `/var/snap/maas/common/log/rsyslog/<machine-name><yyyy-mm-dd>/messages`. Every machine known to MAAS will have corresponding syslogs.
[/tab]
[tab version="v3.0 Packages"] 
Syslog data is kept in `/var/log/maas/rsyslog/<machine-name><yyyy-mm-dd>/messages`.  Every machine known to MAAS will have corresponding syslogs.
[/tab]
[tab version="v2.9 Snap"] 
Syslog data is kept in `/var/snap/maas/common/log/rsyslog/<machine-name><yyyy-mm-dd>/messages`. Every machine known to MAAS will have corresponding syslogs.
[/tab]
[tab version="v2.9 Packages"] 
Syslog data is kept in `/var/log/maas/rsyslog/<machine-name><yyyy-mm-dd>/messages`.  Every machine known to MAAS will have corresponding syslogs.
[/tab]
[/tabs]

* MAAS logs reference
MAAS offers an extensive logging capability across events, scripts, and normal operations.

- [Event logs](/t/-/5252): MAAS event logs will help you diagnose a very large percentage of MAAS issues.

- [Audit event logs](/t/-/5256): Audit event logs help you discover who changed something in the MAAS infrastructure.

- [Commissioning logs](/t/-/5248): Commissioning logs provide a detailed listing of everything that happened while trying to commission a machine.

- [Testing logs](/t/-/5314): Hardware testing logs help you find and diagnose hardware issues.

* MAAS networks
Some elements of MAAS networking are unique to the product, while others are standard networking concepts that are uniquely applied to MAAS.  This section will help you learn:

- [About PXE booting](#heading--about-pxe-booting)
- [About power drivers](#heading--about-power-drivers)
- [About proxies](#heading--about-proxies)
- [About RPC](#heading--about-RPC)
- [About network discovery](#heading--about-network-discovery)
- [About subnets](#heading--subnets)
- [About VLANs](#heading--vlans)
- [About subnet management](#heading--about-subnet-management)
- [About IPv6](#heading--about-ipv6)
- [About availability zones](#heading--about-availability-zones)

** About PXE booting

PXE booting, or [Preboot eXecution Environment](https://en.wikipedia.org/wiki/Preboot_Execution_Environment)`↗`, refers to the ability to boot a machine via a Network Interface Card (NIC).  PXE booting requires a Network Interface Card (NIC) which is equipped with a PXE API which can be accessed by the server wishing to boot the device.

This PXE API has a very small hardware footprint, both to keep NIC density smaller (small footprint PXE ROMs) and to simplify the PXE boot process.  Typically a PXE-capable NIC implements the PXE API with a simple universal network device interface, a tiny UDP/IP stack, a special pre-boot DHCP module, and a trivial file transfer protocol (TFTP) module.  TFTP does have low throughput, but it has been amended twice, once with [better block sizes](https://datatracker.ietf.org/doc/html/rfc2348)`↗`, and once with [better window sizes](https://datatracker.ietf.org/doc/html/rfc7440)`↗`.  Both of these help to allow for payload deliveries large enough to accommodate basic bootable images, such as the ephemeral Ubuntu image that MAAS transmits, via TFTP, in order to commission a machine.

PXE absolutely depends on DHCP, which kicks off the process by assigning an IP address, so that the TFTP server can send a network boot program and its peripheral files.  The machine in question is powered on via its baseboard management controller (BMC).  The UEFI PXE firmware broadcasts a DHCPDISCOVER message (customised for PXE) to the UDP port (67), requesting network config and booting parameters.  Because the discover message is PXE-specific, a PXE-enabled DHCP server is normally required.  Otherwise, the DHCPOFFER will have networking information, but not PXE parameters, which include the IP address of the TFTP server and the name of the network booting program; thus the PXE client would not be able to boot.

If the machine receives a PXE-enabled DHCPOFFER, it will be able to not only set its own network parameters, but also locate the booting resources.  The machine can then transfer the network booting program into its own RAM space, via TFTP.  This booting program, in turn, downloads a basic kernel with initrd.  From there, a full ephemeral Ubuntu image is loaded and commissioning (or deployment) can take place.

Here are some related concepts you may want to explore:

- [UEFI](https://en.wikipedia.org/wiki/Unified_Extensible_Firmware_Interface)`↗`
- [BOOTP](https://en.wikipedia.org/wiki/Bootstrap_Protocol)`↗`
- [TFTP](https://en.wikipedia.org/wiki/Trivial_File_Transfer_Protocol)`↗`
- [HTTP booting](https://documentation.suse.com/sles/15-SP2/html/SLES-all/cha-deployment-prep-uefi-httpboot.html)`↗`
- [Bootstrap loading via TFTP](https://datatracker.ietf.org/doc/html/rfc906)`↗`

If you still have questions about PXE booting, please consider posting a question on our [discourse forum](https://discourse.maas.io/c/users/8)`↗`.

** About power drivers

Power drivers are units of software, embedded in MAAS, that interface with the [BMC](https://en.wikipedia.org/wiki/Intelligent_Platform_Management_Interface#Baseboard_management_controller)`↗` to power-cycle a machine remotely.  Different machines use different BMC configurations.  Typically, these vary by manufacturer, although there are some standard IPMI drivers that can be used.  In addition, it is sometimes possible to control a machine with a specialised BMC via a generic IPMI driver.

IPMI is designed as a set of protocols that bypass the system's CPU, BIOS, UEFI, and/or OS.  Essentially, IPMI provides a network connection directly to the hardware, allowing a number of management and monitoring functions.  From the perspective of MAAS, the main use of IPMI is to access the machine's BMC to power-cycle the machine.  In order for [PXE-booting](#heading--about-pxe-booting) to start, the machine itself must send a PXE-enabled DHCPDISCOVER, which requires the machine to be powered on.

Specific machine models have different IPMI parameters that can or must be used to successfully power on a machine, although many models respond reasonably well to standard IPMI or [Redfish](https://en.wikipedia.org/wiki/Redfish_(specification)`↗`) commands. MAAS includes customised power drivers for all of the machines listed in the [power catalogue](/t/power-management-reference/5246#heading--power-catalogue).

IPMI provides many other functions and capabilities besides power-cycling the machine, such as monitoring system state (e.g., temperature) and possibly adjusting some parameters remotely.  MAAS generally does not avail itself of these additional features.

** About proxies

A [proxy server](https://en.wikipedia.org/wiki/Proxy_server)`↗` ("proxy service" or just "proxy") is an intermediary application that serves to broker network transactions between two hosts.  Proxies provide several benefits, including privacy (protecting internal IP addresses from discovery by those on other networks), security (performing some checks against incoming packets), and load-balancing (routing packets to multiple servers, based on actual load or some statistical algorithm).  

MAAS provides an [internal proxy](/t/how-to-connect-maas-networks/5164#heading--internal-proxy-maas-proxy), which is an HTTP caching proxy server that is available to all hosts residing in any subnet managed by MAAS.  In addition, MAAS allows you to define an [external proxy](/t/how-to-connect-maas-networks/5164#heading--configure-proxy) if desired.

** About RPC

A [Remote Procedure Call](https://www.ibm.com/docs/en/aix/7.1?topic=concepts-remote-procedure-call)`↗`, or RPC, is a method by which one computer can execute a subroutine sent by another process or system.  These procedures run as if they were native to the machine executing them, even though they may have been prepared or coded on the requesting machine.  In the case of MAAS, [RPC is used for communication between the region and rack controllers](/t/about-controllers/5072#heading--rackregion), specifically to transfer the PXE configuration from region to rack.  This allows the relevant MAAS rack to answer the machine's DHCPDISCOVER with a DHCPOFFER that contains the correct PXE booting information to bring the machine to an ephemeral Ubuntu instance. 

** About network discovery

MAAS constantly listens to the network and reports any discovered devices. Devices are identified when the rack controller observes them communicating on an attached IPv4 subnet. Discovered devices that do not correspond to machines and devices already known to MAAS can be listed via the CLI. If a device advertises a hostname using `mDNS` (such as with `avahi` or `Bonjour`), MAAS will also present the discovered hostname when listing devices.

[tabs]
[tab version="v3.4 Snap,v3.4 Packages,v3.3 Snap,v3.3 Packages,v3.2 Snap,v3.2 Packages,v3.1 Snap,v3.1 Packages,v3.0 Snap,v3.0 Packages,v2.9 Snap,v2.9 Packages" view="UI"]
Using the Dashboard, an unknown discovered device can be added to MAAS as a device or as a network interface belonging to a machine or device. Clicking the down arrow to the right of a new device allows values such as 'Type', 'Domain', 'IP Assignment' and 'Parent' to be changed prior to the device being added. Selecting a Parent device is optional.
[/tab]
[tab version="v3.4 Snap,v3.4 Packages,v3.3 Snap,v3.3 Packages,v3.2 Snap,v3.2 Packages,v3.1 Snap,v3.1 Packages,v3.0 Snap,v3.0 Packages,v2.9 Snap,v2.9 Packages" view="CLI"]
An unknown discovered device can be added to MAAS as a device, or as a network interface belonging to a machine or device. You can specify values such as 'Type', 'Domain', 'IP Assignment' and 'Parent' to be changed prior to the device being added. Indicating a Parent device is optional.
[/tab]
[/tabs]

** About subnets

A [subnet](https://en.wikipedia.org/wiki/Subnetwork#firstHeading)`↗` is a "layer 3" network, defined by a network address and a network mask length (in bits) and usually written in "CIDR" format. MAAS supports IPv4 and IPv6 subnets. Examples include:

``` no-highlight
- 0.0.0/8
- 16.0.0/12
- 168.0.0/16
2001:db8:4d41:4153::/64
```

** About VLANs

[VLANs](https://en.wikipedia.org/wiki/Virtual_LAN#firstHeading)`↗` (Virtual LANs) are a common way to create logically separate networks using the same physical infrastructure.

Managed switches can assign VLANs to each port in either a "tagged" or an "untagged" manner. A VLAN is said to be "untagged" on a particular port when it is the default VLAN for that port and requires no special configuration to access it.

You can use also use tagged VLANs with MAAS nodes. If a switch port is configured to allow tagged VLAN frames from a MAAS node, that node can automatically access interfaces on that VLAN.

A "Default VLAN" is created for every fabric, to which every new VLAN-aware object in the fabric will be associated with by default (unless specified otherwise).

** About subnet management

Fabrics, VLANs, and spaces do not require much configuration beyond names and descriptions. You can change the MTU for a VLAN, as well as [enable DHCP](/t/how-to-enable-dhcp/5132#heading--enabling-dhcp).  None of these options requires detailed instruction.

This subsection will help you learn:

- [About managed subnets](#heading--about-managed-subnets)
- [About unmanaged subnets](#heading--about-unmanaged-subnets)
- [About IP address tracking](#heading--about-ip-address-tracking)

A [subnet](https://en.wikipedia.org/wiki/Subnetwork)`↗`, on the other hand, provides a number of configuration options relevant to the day-to-day operation of MAAS. By default, MAAS manages subnets in your configuration, but this is easily changed.

When a subnet is managed, MAAS handles all aspects of IP address allocation. This process includes managing DHCP leases and assigned static addresses. Typically MAAS would have one managed subnet, with any additional subnets unmanaged. This arrangement allows for more control over which subnet gets used for DHCP. Additionally, as detailed below, an unmanaged subnet treats reserved IP ranges differently, in a way that some administrators find more intuitive.

**** About managed subnets

When you enable management for a subnet, MAAS will:

- Lease addresses for DHCP from a reserved dynamic IP range
- Assign static addresses not included in a reserved IP range, typically via 'Auto assign' IP allocation mode for a node.

See [Concepts and terms](/t/maas-glossary/5416#heading--ip-ranges) for an explanation of the two kinds of reserved IP ranges MAAS uses.

If needed, you can also define a static route between two subnets. A static route is defined on a per-subnet basis to use a particular gateway, using a configured destination.

**** About unmanaged subnets

When management is disabled for a subnet, the definition of a reserved IP range differs from the managed mode. Here, a reserved IP range tells MAAS to **only allocate addresses from this range** (via 'Auto assign'). Also, DHCP will never lease addresses from an unmanaged subnet.

**** About IP address tracking

When you enable IP address tracking, MAAS will keep track of all assigned addresses, regardless of whether they come from managed or unmanaged subnets.

** About IPv6

Support for IPv6 in MAAS is similar to support for IPv4.  This subsection will help you learn:

- [About enabling IPv6](#heading--about-enabling-ipv6)
- [About IPv6 subnets](#heading--about-ipv6-subnets)
- [About IPV6 routing](#heading--about-ipv6-routing)

A rack controller in an IPv6 context needs to have the region API server URL specified with brackets:

``` nohighlight
http://[::1]:5240/MAAS/
```

You can access the Web UI and the [MAAS CLI](/t/try-out-the-maas-cli/5236) (that is, logging in to the API server) in the same way on both IPv4 and IPv6. To use an IPv6 address in a URL, surround it with square brackets. For example, on the local machine (`::1`, the IPv6 equivalent of `localhost`):

[note]
MAAS can only control most BMCs using IPv4.
[/note]

**** About enabling IPv6

You enable IPv6 networking in the same way that you enable IPv4 networking: configure a separate rack controller interface for your IPv6 subnet. The IPv6 interface must define a static address range. Provided that you already have a functioning IPv6 network, that's all there is to it. The following sections explain requirements, supported operations, and what to do if you don't yet have a functioning IPv6 network.

An IPv6 interface can use the same network interface on the rack controller as an existing IPv4 network interface. It just defines a different subnet, with IPv6 addressing. A machine that's connected to the IPv4 subnet also connected to the IPv6 subnet on the same network segment.

**** About IPv6 subnets

If you define a reserved static IP range, then machines deployed on the subnet will get a static address in this range. Since IPv6 networks usually are 64 bits wide, you can be generous with the range size. You can typically leave the netmask and broadcast address fields blank.

You may want MAAS to manage DHCP and DNS, but it's not required. Machines do not need a DHCP server at all for IPv6; MAAS configures static IPv6 addresses on a machine's network interface while deploying it. A DHCPv6 server can provide addresses for containers or virtual machines running on the machines, as well as devices on the network that are not managed by MAAS. The machines do not need DHCPv6. MAAS will not be aware of any addresses issued by DHCP, and cannot guarantee that they will stay unchanged.

**** About IPV6 routing

In IPv6, clients do not discover routes through DHCP. Routers make themselves known on their networks by sending out route advertisements. These RAs also contain other configuration items:

- Switches that allow stateless configuration of their unique IP addresses, based on MAC addresses. 
- Switches that enable them to request stateless configuration from a DHCP server.
- Switches that In any allow them to request a stateful IP address from a DHCP server. 

Since a network interface can have any number of IPv6 addresses even on a single subnet, several of these address assignment mechanisms can be combined.

However, when MAAS configures IPv6 networking on a machine, it does not rely on RAs. It statically configures a machine's default IPv6 route to use the router that is set on the cluster interface, so that the machine will know their default gateway. They do not need DHCP and will not auto-configure global addresses.

You may be planning to operate DHCPv6 clients as well, for example, on machines not managed by MAAS, or on virtual machines hosted by MAAS machines.  If this is the case, you may want to configure RAs, so that those clients obtain configuration over DHCP.

If you need RAs, but your gateway does not send them, you could install and configure `radvd` somewhere on the network to advertise its route.


** About availability zones

This subsection explains some characteristics and uses of availability zones.  Here you have the opportunity to learn:

- [About fault tolerance](#heading--fault-tolerance)
- [About service performance](#heading--service-performance)
- [About power management](#heading--power-management)

**** About fault tolerance

Fault tolerance is "the property that enables a system to continue operating properly in the event of the failure of (or one or more faults within) some of its components". To help create better fault tolerance, multiple MAAS zones can be employed.

For this, a zone can be defined in different ways. It can be based on power supply for instance, or it can represent a portion of your network or an entire data centre location.

Machines that work in tandem in order to provide an instance of a service should be allocated in the same zone. The entire service should be replicated in another zone.

**** About service performance

Service performance is the ability of your service to operate in the most efficient manner possible where the typical criteria used is speed. Multiple MAAS zones can be used to help.

Nodes should be allocated in the zone closest to the performance-critical resources they need.

For example, for applications that are highly sensitive to network latency, it may make sense to design a network topology consisting of several smaller networks, and have each of those represented as a zone. The zones can then be used to allocate nodes that have the best performance depending on the service offered.

**** About power management

Power management is concerned with power usage density and cooling. This topic can be addressed with the use of several MAAS zones.

Nodes can be distributed in such a way that power-hungry and/or "hot" systems are located in different zones. This can help mitigate power consumption and heat problems.

* MAAS operations

* MAAS performance
The MAAS engineering team actively works to improve the performance of MAAS.

** Recent performance measurements

Recently, we improved the API performance of MAAS, by testing it with simulated loads.  For this testing, we made the following assumptions:

- five rack controllers
- 48 machines per fabric
- five VMs per LXD host
- three different architectures
- six disks per machine, randomly defined as flat, RAID, LVM, and BCACHE disks
- five network interfaces per machine
- machines in a random status, but mostly Ready or Deployed (which best emulates a real-world scenario)

To measure performance, we use continuous performance monitoring, arranged like this:

<a href="https://discourse.maas.io/uploads/default/original/2X/d/d8a0887dd9d6f01311966c10f5d9093feb76806f.png" target = "_blank"><img src="https://discourse.maas.io/uploads/default/original/2X/d/d8a0887dd9d6f01311966c10f5d9093feb76806f.png"></a>

On a daily basis, we generate simulation data based on the assumptions above, for 10, 100, and 1000 machines.  These three datapoints help us get a sense of how our performance improvements scale.  A Jenkins tool exercises both the REST API and the WebSocket API, trapping the results in a database, from which we can build a dashboard.  The dashboard looks something like this:

<a href="https://discourse.maas.io/uploads/default/original/2X/f/f5f831164e70273e81b4120b442469f665e16b47.png" target = "_blank"><img src="https://discourse.maas.io/uploads/default/original/2X/f/f5f831164e70273e81b4120b442469f665e16b47.png"></a>

Note that we always compare the current stable release with the release in development, so that we can spot issues before they become harder to find and fix.  We also capture profiling data that allows us to find bottlenecks, generating a heatmap that shows which parts of the code are causing issues at the moment.

For example, comparing MAAS 3.2 to MAAS 3.1, machine listings load, on average, 32% faster for the datasets we're using.  

*** Performance efforts to date

Here's a short history of our performance efforts to date:

- This [video show-and-tell](https://discourse.maas.io/t/maas-show-and-tell-is-maas-fast-yet/6105)`↗` documents recent efforts to improve MAAS peformance, with quantitative results.
- Here's some [work done by the UI team](https://discourse.maas.io/t/maas-ui-improving-the-performance-of-maas-ui/5820)`↗` to improve the performance of the UI.

Note that this list only captures the bigger, sustained efforts, although there is a constant focus on weeding out slowdowns when we come across them.

** Collecting your own metrics

It's possible to [collect your own MAAS metrics](/t/how-to-observe-a-live-maas/5204) -- and even share them with the MAAS engineering team.  We are keen to know everything we can about machine counts, network sizes, and MAAS performance in all areas.  Please use the [discourse performance forum](https://discourse.maas.io/c/maas-performance/26)`↗` to share your feedback and observations.

<a href="#headings--recent-developments">** Recent developments

As part of the MAAS 3.2 development effort, we have taken steps to improve the performance of machine listings. To date, we have measured the speed of listing a large number (100-1000) of machines via the REST API to be 32% faster, on average.

<a href="#headings--next-steps">** Next steps

Currently, we are actively working to improve MAAS performance for other operations, such as search.

* MAAS scripts reference
MAAS provides a number of different scripting tools to control and drive its operation.

- [Commissioning scripts](/t/commissioning-scripts-reference/6605): MAAS provides an extensive commissioning toolset, embodied in it flexible commissioning scripts mechanism.

- [Hardware test scripts](/t/hardware-test-scripts-reference/5392): MAAS is capable of testing hardware thoroughly, in a customisable way.

- [Terraform](/t/maas-terraform-reference/6327): MAAS offers a fully-functional Terraform provider.

* MAAS security
As a MAAS administrator, you do not want secrets associated with a MAAS instance to be stored in the database, since it often doesn’t comply with security regulations.  Examples of the MAAS settings which contain secrets are the OMAPI key and the RPC secret. 

[tabs]
[tab version="v3.4 Snap,v3.4 Packages,v3.3 Snap,v3.3 Packages"]
Other examples of such secrets are:

- Database connection credentials
- BMC power credentials (password, keys and certificates) for machines
- VMhost credentials
- Macaroon keys

Beginning with version 3.3, MAAS secrets should instead be stored in [HashiCorp Vault](https://www.hashicorp.com/products/vault)`↗`.

** Summary of Hashicorp Vault

Ideally, you should consult the [Vault documentation](https://developer.hashicorp.com/vault/docs)`↗`, but for convenience, we will provide a brief summary here.

Vault uses identity to protect secrets and encryption keys.  The core component is the `kv` secrets engine, which uses key-value pairs to store arbitrary secrets within an encrypted storage extent managed by Vault.  It's similar to the full-disk-encryption tools that you'd use to protect your hard drive, but limited to a specific path assigned to the secrets engine(s).  

If you're interested, you can [read more about secrets engines](https://developer.hashicorp.com/vault/docs/secrets)`↗` at your leisure.

Vault protects the secrets engine with a sort of upscale `chroot jail`, called a [barrier view](https://developer.hashicorp.com/vault/docs/secrets#barrier-view)`↗`.  This barrier view consists of a folder, named to a randomly-generated UUID, which forms the abolute root directory for that engine.  Secrets engines cannot, for example, `cd ..` above their UUID folder, which becomes an effective barrier for what that engine can view.  

Aside from this short overview, we won't repeat all of the Vault documentation.  Instead, we recommend that you [study the Vault documentation carefully](https://developer.hashicorp.com/vault/docs)`↗`, or even consider [getting certified](https://developer.hashicorp.com/vault/tutorials/associate-cert)`↗` on the Vault product.

[/tab]
[tab version="v3.2 Snap,v3.2 Packages,v3.1 Snap,v3.1 Packages,v3.0 Snap,v3.0 Packages,v2.9 Snap,v2.9 Packages"]
Beginning with version 3.3, MAAS secrets can be stored in [HashiCorp Vault](https://www.hashicorp.com/products/vault)`↗`.  To use Vault to protect your secrets, please upgrade to MAAS 3.3.
[/tab]
[/tabs]

** Multi-tenancy in MAAS

Likewise, you want to grant fine-grained access-controls to different users, based on assigned roles.  Working in concert with RBAC and Candid, MAAS can restrict user access and actions based on four roles:

- Administrator: can access all settings and perform any operation on any machine in any resource pool; equivalent to a MAAS administrator.
- Operator: can act as a MAAS administrator, but only within an assigned resource pool.  Machines in other resource pools -- and system settings -- are not accessible.
- User: can access and manipulate machines that are not currently allocated to other users, within the confines of an assigned resource pool; equivalent to a MAAS user, but only over the resource pool.  Users cannot access any settings.
- Auditor: can view information for all machines within an assigned resource pool; cannot make any changes or alter any settings.

MAAS controls access with the help of RBAC (Role-Based Access Control), Candid (Canonical Identity Manager), and an identity service of your choice, such as SSO (Single Sign-On).  From this point on, we'll refer to this solution as MAAS/RBAC, even though it uses more than two tools.  You can design and deploy a carefully-controlled MAAS environment using MAAS/RBAC.

As a MAAS/RBAC administrator or designer, you should understand the concept of multi-tenancy.  Multi-tenancy means that groups of users own a group of resources (machines), but without knowing about other groups of users -- or their machines.  A common multi-tenancy use case provides different sets of machines for different users or groups of users. 

MAAS alone can achieve this, to some degree, by allowing users to allocate machines, but this approach has some drawbacks:

- Other administrative users can still see the allocated machines, as well as release and reallocate someone else's machines for themselves.
- Other administrative users can take administrative actions on someone else's machines (e.g., deployment).

With MAAS/RBAC, an operator can act as administrator for one resource pool, without the ability to manipulate someone else's machines. Users can only operate within the confines of their resource pool, and auditors can review actions without making any changes.

*** How resource pools are integrated into MAAS/RBAC

Resource pools are just a way of grouping machines.  Any given machine can be assigned to exactly one resource pool.  If you control access to resource pools, and you assign roles properly, you can effectively control user access.

Note that just using resource pools to hide machines is a flawed access control approach, known as "security by obscurity."  What users don't know will hurt you when the users figure it out. More often than not, users figure these things out entirely by accident, and hurt you unintentionally while trying to do the right thing.

Consequently, there must also be some means of active authorisation that allows access to a resource pool.  That authorisation must be based on user identity. There must be some way of controlling what the user can do with that resource pool.  In the parlance of standard security model, "what the user can do" would be called "privilege", but for the purposes of MAAS, we simply call them "permissions."  

*** About identity services

MAAS/RBAC will interface with many identity services, using Candid as a mediator.  While the choice of identity service is up to you, we should cover some general principles of identity servers as they relate to MAAS/RBAC.  Let's take a closer look at [Ubuntu Single Sign-On (SSO)](https://help.ubuntu.com/community/SingleSignOn)`↗`

SSO permits users to log in once to access many network services.  SSO centralises authentication (via Kerberos), account management (via LDAP), resource-sharing (via `pam_mount`), and limited authorisation through group memberships in LDAP, coupled with file system permissions.

RBAC (Role-based access control) does not authenticate users or verify their identity; that function is assigned to the identity service you choose.  RBAC does receive an identity token or "macaroon" (via Candid) that RBAC uses to assign user roles.  MAAS uses these roles, in turn, to control user access and actions.

** How Candid fits into the picture

Direct authentication involves a user entering something unique in response to a challenge, in order to gain access.  "Something unique" means "something you know, something you have, or something you are", e.g., a password, a hardware key, or a fingerprint, respectively.  Authentication can be automated with private/public key exchanges, protected with a password on the first exchange.  Adding another access point (another trusted client) usually means providing a public key, setting a password, or registering some biometric data.  Direct authentication works well when there are a limited number of clients and not a lot of user turnover.

Increase the number of users and services that need to authenticate, and direct authentication becomes an IT nightmare: generating access requests; validating requests; setting up authentication; and then managing access as users move around the organisation.  [Candid](https://github.com/canonical/candid)`↗`, the Canonical identity service, was designed to meet this need.  Candid acts as an authentication gateway that connects a service like RBAC to your chosen identity service.

Candid manages authenticated users via special access tokens ([macaroons](https://askubuntu.com/questions/940640/what-is-a-macaroon))`↗` that confirm user identity.  Unlike standard access tokens, macaroons can be verified independently, in a standard way, so they reduce the network traffic and delays of repeatedly querying the identity server.  Traditional access tokens must be short-lived; macaroons are valid for much longer and they can be refreshed easily.  Macaroons can also be bound to TLS certificates.  And macaroons can be used by multiple clients and services with no loss of security.

Candid can do the following things:

- find users by various identity parameters, such as e-mail, full name, last login time, etc.
- show details for a user, based on e-mail or username.
- add or remove users from ACLs (access control lists), or list members of an ACL.
- add or remove users from arbitrary groups.
- clear the multi-factor authentication (MFA) credentials for a specific user.
- manage Candid agents.

Candid can use certificates and agents, if desired.  You specify the identity provider by URL when instantiating the program.

When a user tries to log into a MAAS which is working with RBAC, MAAS redirects that login to the RBAC server.  RBAC, in turn, requests authentication via Candid, which then consults the specified identity server (at the URL provided on startup).  If the user is authenticated, Candid constructs a macaroon, which is then passed to RBAC and on to MAAS.  This macaroon serves as the user's authentication token until it expires.

** About RBAC

RBAC uses a database to associate a given role with a properly-authenticated user identity.  With RBAC, permissions are associated with roles, not individual users. Within a given resource pool, the role assigned to a properly authenticated user controls what they can and cannot do within that pool.

In the parlance of RBAC, MAAS would be a service, while each resource pool would be considered a separate scope. RBAC/MAAS also recognises scopes that are not tied to machines, including:

- DNS
- Availability zones
- Images
- System settings

RBAC can help MAAS also control access to these "non-machine resources".

Note that it is possible for a given user to be an operator for one resource pool, a user for another, and an auditor for still another, but have no ability to change system settings or manipulate images.  Nothing about RBAC prohibits this arrangement.

*** The MAAS/RBAC permissions model

Here is a thumbnail sketch of the permissions model:

- MAAS maintains resource pools, which are a machine attribute.  Every machine can be assigned to one and only one resource pool.  
- RBAC maintains roles, which are associated with user identities.  For a given user identity to carry out a particular operation in MAAS, that user identity must correspond to a role that has permission to carry out that operation in a given resource pool.
- Candid vouches for the user with a macaroon.
- Some identity service (e.g., SSO) authenticates the user to Candid, so that macaroons are not generated for unrecognised users.

Relationships between roles, resource pools, and users is maintained by RBAC as a source of truth.  MAAS mediates access to resource pools, based on user roles, using information obtained from RBAC. 

*** The MAAS/RBAC security architecture

The following diagram will give you a graphical view of how MAAS, RBAC, Candid, and an identity provider work together to control access to MAAS resources:

<a href="https://discourse.maas.io/uploads/default/original/2X/4/4433c6995c342efebe565f4888a46d7107d1525f.png" target = "_blank"><img src="https://discourse.maas.io/uploads/default/original/2X/4/4433c6995c342efebe565f4888a46d7107d1525f.png"></a>

The step-by-step walk-through of the MAAS/RBAC relationship goes like this:

- When MAAS is initiated with RBAC connected, MAAS pushes a list of resource pools and a global resource key to RBAC.  The global resource key covers things that are not added to resource pools, such as devices or settings.
- When a user tries to login, MAAS redirects that login request to RBAC.
- RBAC, in turn, requests an authentication check from Candid.
- Candid attempts to authenticate the user via whatever identity provider was configured when Candid was started (e.g., SSO).
- If Candid successfully authenticates the user, Candid creates a macaroon as a temporary identity token for the user.
- Candid passes the macaroon back to RBAC.
- RBAC passes the macaroon, in turn, to MAAS, along with a dictionary of groups, role(s) and resource pools corresponding to that user.
- As needed, MAAS then mediates access to resource pools, using the macaroon to recognise the user and their group(s), and using the role/resource pool pairs to adjudicate access.

Note that RBAC does not adjudicate individual permissions against resource pools. RBAC only sends MAAS the combination of users, roles, and related resource pools to MAAS when requested.  The MAAS code has a built-in understanding of the four roles (user, administrator, operator, and auditor) and what those roles can and cannot do with a given item.  

*** How the four MAAS RBAC roles protect MAAS resources

The most important thing to understand about MAAS RBAC roles is that restricted users cannot see or interact with machines in resource pools that aren't permitted for them. This is more than just "security by obscurity," because even if a user knows the name or system ID of a machine in a non-permitted resource pool, that user can't access it.  Removing non-permitted machines from view, though, prevents confusion about what the user can and can't do.

Here is a quick breakdown of how the four roles experience MAAS:

- Administrator: an administrator can do anything that a normal MAAS administrator can do in the absence of RBAC.  This means an admin can see all resource pools, take any action against any machine, and change any MAAS settings.

- Operator: an operator can do almost anything that a normal MAAS administrator can do, but only against machines in their permitted resource pools.  An operator cannot see or change system settings.

- User: a user can do just what a normal MAAS user can do.  They can only view and allocate machines that aren't allocated to someone else, even if that someone else is another user in the same resource pool.  Users can't change or access settings at all.

- Auditor: an auditor can view anything about machines in the resource pool(s) for which they are permitted.  Auditors cannot change or access settings.

MAAS makes no assumptions about how these roles might be used in the day-to-day operation of your MAAS instance.  The capabilities listed above form the complete set of what these roles can do.


* MAAS Terraform provider reference
If you wish to use MAAS with [Terraform](https://www.terraform.io/)`↗`, we have made a [provider available](https://github.com/maas/terraform-provider-maas)`↗`.  This article provides reference information about the data sources and resources that can be accessed via this provider.  It does not attempt to explain the mechanics or usage of Terraform or offer any tutorial information related to this MAAS Terraform provider.

<h1 The MAAS Terraform provider

The MAAS provider is a Terraform provider that allows you to manage MAAS resources using the Terraform (CRUD) tool. This provider can be used to manage many aspects of a MAAS environment, including networking, users, machines, and VM hosts.

These aspects can be divided into three categories of Terraform-compliant HCL:

- [API linkages](#heading--terraform-api-linkage)
- [Data sources](#heading--data-sources)
- [Resources](#heading--resources)

We will deal with each of these categories in turn.  For each data source and resource, we will offer a brief definition and description of how that item is employed in MAAS.  If you are new to [Terraform](https://www.terraform.io/)`↗`, or want to explore what terraforming may provide for your MAAS instance, you may wish to consult the [Terraform documentation](https://www.terraform.io/intro)`↗` or one of the many [tutorials available](https://learn.hashicorp.com/collections/terraform/aws-get-started?utm_source=WEBSITE&utm_medium=WEB_IO&utm_offer=ARTICLE_PAGE&utm_content=DOCS)`↗`.

** API linkages

The schema that provides an API linkage to MAAS from Terraform consists of a standard HCL provider block and a provider API block.  As with all Terraform providers, the provider block contains at least two items:

- a source element, which in this case is "maas/maas".
- a version element, which can be sufficiently specified by "~>1.0".

The provider block would look something like this:

```nohighlight
terraform {
  required_providers {
    maas = {
      source  = "maas/maas"
      version = "~>1.0"
    }
  }
}
```

The provider API block contains the necessary credentials to allow Terraform to access your MAAS instance, which include three things:

- an API version.
- an API key.
- an API URL.

A typical provider API block might look like this:

```nohighlight
provider "maas" {
  api_version = "2.0"
  api_key = "<YOUR API KEY>"
  api_url = "http://127.0.0.1:5240/MAAS"
}
```

A completed definition would also include some data sources and resources, like this typical example:

```nohighlight
terraform {
  required_providers {
    maas = {
      source  = "maas/maas"
      version = "~>1.0"
    }
  }
}

provider "maas" {
  api_version = "2.0"
  api_key = "<YOUR API KEY>"
  api_url = "<YOUR API URL>"
}

resource "maas_space" "tf_space" {
  name = "tf-space"
}

resource "maas_fabric" "tf_fabric" {
  name = "tf-fabric"
}

resource "maas_vlan" "tf_vlan" {
  fabric = maas_fabric.tf_fabric.id
  vid = 14
  name = "tf-vlan14"
  space = maas_space.tf_space.name
}
resource "maas_subnet" "tf_subnet" {
  cidr = "10.88.88.0/24"
  fabric = maas_fabric.tf_fabric.id
  vlan = maas_vlan.tf_vlan.vid
  name = "tf_subnet"
  gateway_ip = "10.88.88.1"
  dns_servers = [
    "1.1.1.1",
  ]
  ip_ranges {
    type = "reserved"
    start_ip = "10.88.88.1"
    end_ip = "10.88.88.50"
  }
  ip_ranges {
    type = "dynamic"
    start_ip = "10.88.88.200"
    end_ip = "10.88.88.254"
  }
}
```

See the [Terraform HCL documentation](https://www.terraform.io/language)`↗` for more details about these blocks.

** Data sources

The MAAS Terraform provider offers three data sources, all representing network elements:

- a [fabric](https://discourse.maas.io/t/maas-glossary/5416#heading--fabrics)`↗`, which is essentially a VLAN namespace -- that is, it connects two or more VLANs together.
- a [subnet](https://discourse.maas.io/t/maas-glossary/5416#heading--subnets)`↗`, which is the traditional way of dividing up IP addresses into smaller networks, e.g., 192.168.15.0/24.
- a [VLAN](https://en.wikipedia.org/wiki/VLAN)`↗`, a "virtual LAN", which is a collection of specific addresses or ports that are connected together to form a restricted network.

Each of these data sources has a specific HCL block with elements structured appropriately to manage that MAAS element.

*** Fabric

The [fabric](https://github.com/maas/terraform-provider-maas/blob/master/docs/data_sources/maas_fabric.md)`↗` data source provides minimal details, namely, the fabric ID, of an existing MAAS fabric.  It takes one argument (the fabric name) and exports one attribute (the fabric ID):

```nohighlight
data "maas_fabric" "default" {
  name = "maas"
}
```

Fabrics within MAAS are not widely manipulated in and of themselves, but rather serve as containers for storing VLAN/subnet combinations.

*** Subnet

The [subnet](https://github.com/maas/terraform-provider-maas/blob/master/docs/data_sources/maas_subnet.md)`↗` data source provides a number of details about an existing MAAS network subnet.  The element takes one argument, the subnet CIDR, and exports a number of attributes:

- id - The subnet ID.
- fabric - The subnet fabric.
- vid - The subnet VLAN traffic segregation ID.
- name - The subnet name.
- rdns_mode - How reverse DNS is handled for this subnet. It can have one of the following values:
-- 0 - Disabled, no reverse zone is created.
-- 1 - Enabled, generate reverse zone.
-- 2 - RFC2317, extends 1 to create the necessary parent zone with the appropriate CNAME resource records for the network, if the network is small enough to require the support described in RFC2317.
- allow_dns - Boolean value that indicates if the MAAS DNS resolution is enabled for this subnet.
- allow_proxy - Boolean value that indicates if maas-proxy allows requests from this subnet.
- gateway_ip - Gateway IP address for the subnet.
- dns_servers - List of IP addresses set as DNS servers for the subnet.

Declaring a subnet looks something like this example:

```nohighlight
data "maas_subnet" "vid10" {
  cidr = "10.10.0.0/16"
}
```

Subnets are the network backbone of MAAS, and thus provide a number of attributes that can be manipulated to alter the behaviour of MAAS.

*** VLAN

The [VLAN](https://github.com/maas/terraform-provider-maas/blob/master/docs/data_sources/maas_vlan.md)`↗` data source provides details about an existing MAAS VLAN.  A VLAN takes two arguments:

- fabric - (Required) The fabric identifier (ID or name) for the VLAN.
- vlan - (Required) The VLAN identifier (ID or traffic segregation ID).

A VLAN data source exports a few useful attributes:

- mtu - The MTU used on the VLAN.
- dhcp_on - Boolean value indicating if DHCP is enabled on the VLAN.
- name - The VLAN name.
- space - The VLAN space.

VLAN [spaces](https://discourse.maas.io/t/maas-glossary/5416#heading--spaces)`↗` are used mostly by Juju, but can be employed by other tools, if desired.

The typical definition of a MAAS VLAN in HCL might look like this:

```nohighlight
data "maas_vlan" "vid10" {
  fabric = data.maas_fabric.default.id
  vlan = 10
}
```

VLANs are available as data sources, but generally, subnets are the workhorses of most MAAS instances.

** Resources

The MAAS Terraform provider makes a large number of resources available, currently including the following items.  Because of the large number of items, details of arguments and attributes are not duplicated here, but instead provided from a single source at the given links:

- A [maas_instance](https://github.com/maas/terraform-provider-maas/blob/master/docs/resources/instance.md)`↗` provides a resource to deploy and release machines already configured in MAAS, based on the specified parameters. If no parameters are given, a random machine will be allocated and deployed using the defaults.
- A [maas_vm_host](https://github.com/maas/terraform-provider-maas/blob/master/docs/resources/vm_host.md)`↗` provides a resource to manage MAAS VM hosts.  Note that MAAS VM hosts are not machines, but the host(s) upon which virtual machines are created.
- A [maas_vm_host_machine](https://github.com/maas/terraform-provider-maas/blob/master/docs/resources/vm_host_machine.md)`↗` provides a resource to manage MAAS VM host machines, which represent the individual machines that are spun up on a given VM host.
- A [maas_machine](https://github.com/maas/terraform-provider-maas/blob/master/docs/resources/machine.md)`↗` provides a resource to manage MAAS machines; note that these are typically physical machines (rather than VMs), so they tend to respond differently at times.
- A [maas_network_interface_physical](https://github.com/maas/terraform-provider-maas/blob/master/docs/resources/network_interface_physical.md)`↗` provides a resource to manage a physical network interface from an existing MAAS machine.  Network interfaces can be created and deleted at will via the MAAS CLI/UI, so there may be more than one of these associate with any given machine.
- A [maas_network_interface_link](https://github.com/maas/terraform-provider-maas/blob/master/docs/resources/network_interface_link.md)`↗` provides a resource to manage network configuration on a network interface.  Note that this does not represent the interface itself, but the parameter set that configure that interface.
- A [maas_fabric](https://github.com/maas/terraform-provider-maas/blob/master/docs/resources/fabric.md)`↗` provides a resource to manage MAAS network fabrics, which are [described above](#heading--fabric)`↗`. 
- A [maas_vlan](https://github.com/maas/terraform-provider-maas/blob/master/docs/resources/vlan.md)`↗` provides a resource to manage MAAS network VLANs, also [described above](#heading--vlan)`↗`.
- A [maas_subnet](https://github.com/maas/terraform-provider-maas/blob/master/docs/resources/subnet.md)`↗` provides a resource to manage MAAS network subnets, also [described above](#heading--subnet)`↗`
- A [maas_subnet_ip_range](https://github.com/maas/terraform-provider-maas/blob/master/docs/resources/subnet_ip_range.md)`↗` provides a resource to manage MAAS network subnets IP ranges.  IP ranges carry particular importance when managing DHCP with multiple DHCP servers, for example.
- A [maas_dns_domain](https://github.com/maas/terraform-provider-maas/blob/master/docs/resources/dns_domain.md)`↗` provides a resource to manage MAAS DNS domains.
- A [maas_dns_record](https://github.com/maas/terraform-provider-maas/blob/master/docs/resources/dns_record.md)`↗` provides a resource to manage MAAS DNS domain records.
- A [maas_space](https://github.com/maas/terraform-provider-maas/blob/master/docs/resources/space.md)`↗` provides a resource to manage MAAS network [spaces](https://juju.is/docs/olm/network-spaces)`↗`.
- A [maas_block_device](https://github.com/maas/terraform-provider-maas/blob/master/docs/resources/block_device.md)`↗` provides a resource to manage block devices on MAAS machines.
- A [maas_tag](https://github.com/maas/terraform-provider-maas/blob/master/docs/resources/tag.md)`↗` provides a resource to manage a MAAS tag.  MAAS tags have multiple roles in controlling how machines are configured, booted, and monitored.
- A [maas_user](https://github.com/maas/terraform-provider-maas/blob/master/docs/resources/user.md)`↗` provides a resource to manage MAAS users.  This resource does not provide any control over any Candid or RBAC restrictions that may be in place.

Please visit the links to get details on these resources, since the documentation at those links will always be the most current information available.

* Machine parameters reference
This reference guide provides detailed parameters related to various machine operations.

- [Power drivers](/t/-/5246): MAAS provides a wide range of power drivers, each with their own extensive parameter set.

- [Storage layouts](/t/-/5973): MAAS supports a number of unique storage formats with their own specific parameter sets.

- [Device labelling](/t/-/6941): MAAS parameterises device labelling in a useful way.

* Machine storage

One of the key elements of managing machines is configuring and managing their storage space.  This subsection will help you learn:

- [About block devices](#heading--about-block-devices)
- [About partitions](#heading--about-partitions)
- [About storage restrictions](#heading--about-storage-restrictions)
- [About VMFS datastores](#heading--about-vmfs-datastores)
- [About UEFI booting](#heading--about-uefi-booting)
- [About final storage modifications](#heading--final-storage-modifications)

The Storage tab on the machine list brings up a form that allows you to view/edit the file system, partitioning and storage parameters for the selected machine:

<a href="https://discourse.maas.io/uploads/default/original/2X/6/658f4814716a1347fda62ab799ba0d72506c128e.png" target = "_blank"><img src="https://discourse.maas.io/uploads/default/original/2X/6/658f4814716a1347fda62ab799ba0d72506c128e.png"></a> 

This tab describes the filesystem(s) in use, as well as the available and used partitions for this machine.  


** About block devices

Once the initial storage layout has been configured on a machine, you can perform many operations to view and adjust the entire storage layout for the machine. In MAAS there are two different types of block devices.

**Physical**

A physical block device is a physically attached block device such as a 100GB hard drive connected to a server.

**Virtual**

A virtual block device is a block device that is exposed by the Linux kernel when an operation is performed. Almost all the operations on a physical block device can be performed on a virtual block device, such as a RAID device exposed as md0.


** About partitions

As with block devices (see [Block devices](#heading--about-block-devices)), MAAS and the MAAS API offer a great deal of control over the creation, formatting, mounting and deletion of partitions.


** About storage restrictions

There are three restrictions for the storage configuration:

1.   An EFI partition is required to be on the boot disk for UEFI.
2.   You cannot place partitions on logical volumes.
3.   You cannot use a logical volume as a Bcache backing device.

Violating these restrictions will prevent a successful deployment.


** About VMFS datastores

MAAS can configure custom local VMware VMFS Datastore layouts to maximise the usage of your local disks when deploying VMware ESXi. As VMware ESXi requires specific partitions for operating system usage, you must first apply the VMFS6 storage layout. This layout creates a VMFS Datastore named `datastore1` which uses the disk space left over on the boot disk after MAAS creates the operating system partitions.


** About UEFI booting

Every layout type supports a machine booting with UEFI. In such a case, MAAS automatically creates an EFI boot partition (`/boot/efi`). Other than setting the machine to boot from UEFI, the user does not need to take any additional action.

[note]
UEFI must be enabled or disabled for the lifespan of the machine. For example, do not enlist a machine with UEFI enabled, and then disable it before commissioning. It won't work!
[/note]

The EFI partition, if created, will be the first partition (`sda1`) and will have a FAT32 filesystem with a size of 512 MB.


** About final storage modifications

Once MAAS provisions a machine with block devices, via a layout or administrator customisation, a regular user can modify the resulting storage configuration at the filesystem level.

** About disk erasure

Disk erasure pertains to the erasing of data on each of a machine's disks when the machine has been released (see [Release action](/t/maas-glossary/5416#heading--release)) back into the pool of available machines. The user can choose from among three erasure types before confirming the Release action, and a default erasure configuration can also be set.  This section will help you learn:

- [About disk erasure types](#heading--about-disk-erasure-types)
- [About standard erasure](#heading--about-standard-erase)
- [About secure erasure](#heading--about-secure-erasure)
- [About quick erasure](#heading--about-quick-erasure)
- [About erasure order of preference](#heading--about-erasure-order-of-preference)

*** About disk erasure types

The three disk erasure types are:

1.   Standard erasure
2.   Secure erasure
3.   Quick erasure

Each of these are explained below.


*** About standard erasure

Overwrites all data with zeros.


*** About secure erasure

Although effectively equivalent to Standard erase, Secure erase is much faster because the disk's firmware performs the operation. Because of this, however, some disks may not be able to perform this erasure type (SCSI, SAS, and FC disks in particular).


*** About quick erasure

Same as Standard erase but only targets the first 1 MB and the last 1 MB of each disk. This removes the partition tables and/or superblock from the disk, making data recovery difficult but not impossible.


*** About erasure order of preference

If all three options are checked when the machine is released, the following order of preference is applied:

1.  Use 'secure erase' if the disk supports it
2.  If it does not, then use 'quick erase'

* Machines

This article will help you sort out the theoretical aspects of MAAS machines.

** About the machine life-cycle

One of the most important things to understand about machines is their life-cycle. In this subsection, you will learn:

- [Introduction to the machine life-cycle](#heading--Introduction-to-the-machine-life-cycle)
- [About machine states](#heading--about-machine-states)
- [About enlistment](#heading--about-enlistment)
- [About commissioning machines](#heading--about-commissioning-machines)
- [About allocation and deployment](#heading--about-allocation-and-deployment)

*** Introduction to the machine life-cycle

Everything that happens to a machine under MAAS control conforms to a specific life-cycle.  All MAAS machines are in a named state, or in transition between states.  Most of these transitions are user-controlled.  Only the "failure" state is reached under the direction of MAAS, when a user's request for certain state changes can't be successfully completed.

In general, the various states and transitions can be summarised in a diagram:

<a href="https://discourse.maas.io/uploads/default/original/2X/b/bd9e5e225ffee4b2e88104e5bbd363dd2ef61a88.jpeg" target = "_blank"><img src="https://discourse.maas.io/uploads/default/original/2X/b/bd9e5e225ffee4b2e88104e5bbd363dd2ef61a88.jpeg"></a>

The central state flow at the bottom of the diagram is the "standard" life-cycle.  If all goes well, you won't have to deviate much from this flow:

- Machines start as servers in your environment, attached to a network or subnet that MAAS can reach and manage.  If those machines are configured to netboot, MAAS can discover them and enlist them, assigning a status of "NEW". By definition, NEW machines are: (2) enabled to network boot, and (2) on a subnet accessible to MAAS. 

- Once you've pared the list to machines that you want MAAS to control, you can choose to commission them.  You can select any machine that is marked "NEW" and tell MAAS to commission it, or, if you add machine manually, MAAS will automatically commission it.  Commissioning PXE boots the machine and loads an ephemeral version of the Ubuntu operating system into the machine's RAM.  MAAS then uses that OS to scan the machine to determine its hardware configuration: CPUs, RAM, storage layouts, PCI and USB devices, and so forth.  Commissioning can be customised -- more on that in a later section.  If a machine fails to properly commission, either because of a commissioning error, or because the commissioning process timed out, that machine enters a "FAILED" state.
  
- MAAS next tests the machine to make sure it's working properly. These basic tests just assure that the discovered hardware works as expected.  Testing can also be customised, if you wish.  Machines that don't pass these tests are moved to a "FAILED" state.

- Having tested it, MAAS then places that machine in the "READY" state, meaning that MAAS should be able to deploy it, based on the gathered hardware information.

- Before you deploy a machine, you should allocate it.  This step essentially involves taking ownership of the machine, so that no other users can deploy it.

- Having allocated a machine, you can deploy it.  When deploying, MAAS again loads an ephemeral Ubuntu OS onto the machine, uses `curtin` to configure the hardware in the way you've specified, and then loads and boots the OS image you've requested.  Deployment also runs some `cloud-init` steps to finish machine configuration, before leaving it up and ready for use.  

Once deployed, there are a couple of minor state changes you can effect without releasing the machine:

- You can lock a machine, if desired, to provide a little extra insurance that it won't accidentally be changed by you -- or anyone.

- Depending upon the machine's duty cycle, you can also power it on, power it off, or even power-cycle it (to effect a reboot, for example).

Note that these minor state changes are not shown in the diagram above.  There are also some exceptional states you can command:

- For any machine that is ready, allocated, or deployed, you can cycle it through a battery of tests at any time.  Be aware, of course, that testing causes the machine to be unavailable for normal use for the duration of the text cycle.

- Machines that are ready, allocated, or deployed can also be placed in "rescue mode".  Essentially, rescue mode is the same as walking to a malfunctioning or mis-configured machine, taking it off the network, and fixing whatever may be wrong with it -- except that you're doing so via SSH, or by running tests from MAAS, rather than standing in front of the machine.  Machines in rescue mode can't enter normal life cycle states until you remove them from rescue mode.  You can, of course, delete them, modify their parameters (tags, zone, and so on), power them off, and mark them broken.  Rescue mode is like a remote repair state that you can control from wherever you are.

- Machines that are allocated or deployed can also be marked broken.  A broken machine powers off by default.  You can still power it on, delete it, or enter rescue mode, but you can't log into it via SSH.  This state is intended for machines that experience catastrophic hardware or software failures and need direct repairs.

There is one more state that a machine can get into: "failed".  This state is entered when commissioning, allocation, or deployment are not successful.  Getting out of a failed state means figuring out what went wrong, correcting it, and retrying the failed operation.  For example, when a machine fails, you can try and commission it again, hopefully after you've found the bug in your custom commissioning script that's causing it to fail (for instance).

Now that we have a solid overview of the life-cycle, let's break down some of these states and transitions in greater detail.

*** About enlistment

MAAS is built to manage machines, including the operating systems on those machines. Enlistment and commissioning are features that make it easier to start managing a machine -- as long as that machine has been configured to netboot. Enlistment enables users to simply connect a machine, configure the firmware properly, and power it on so that MAAS can find it and add it.

Enlistment happens when MAAS starts; it reaches out on connected subnets to locate any nodes -- that is, devices and machines -- that reside on those subnets. MAAS finds a machine that's configured to netboot (e.g., via PXE), boots that machine into Ubuntu, and then sends cloud-init user data which runs standard (i.e., built-in) commissioning scripts. The machine actually adds itself over the MAAS API, and then requests permission to send commissioning data.

Since MAAS doesn't know whether you might intend to actually include these discovered machines in your cloud configuration, it won't automatically take them over, but it will read them to get an idea how they're set up. MAAS then presents these machines to you with a MAAS state of "New." This allows you to examine them and decide whether or not you want MAAS to manage them.

When you configure a machine to netboot -- and turn it on while connected to the network -- MAAS will enlist it, giving it a status of "New."  You can also [add a machine manually](/t/how-to-make-machines-available/5160#heading--how-to-add-a-machine-manually). In either case, the next step is *commissioning*, which boots the machine into an ephemeral Ubuntu kernel so that resource information can be gathered.  You can also run custom commissioning scripts to meet your specific needs.

**** About the enlistment process

When MAAS enlists a machine, it first contacts the DHCP server, so that the machine can be assigned an IP address.  An IP address is necessary to download a kernel and initrd via TFTP, since these functions can't accept domain names.  Once the machine has a bootable kernel, MAAS boots it:

<a href="https://discourse.maas.io/uploads/default/original/1X/76f7113545e6950fec60bdeac06cfaf79b14b3ff.jpeg" target = "_blank"><img src="https://discourse.maas.io/uploads/default/original/1X/76f7113545e6950fec60bdeac06cfaf79b14b3ff.jpeg"></a>

Next, initrd mounts a Squashfs image, ephemerally via HTTP, so that cloud-init can execute:

<a href="https://discourse.maas.io/uploads/default/original/1X/500f9bd2d070790a4007085705035366bee88a4a.jpeg" target = "_blank"><img src="https://discourse.maas.io/uploads/default/original/1X/500f9bd2d070790a4007085705035366bee88a4a.jpeg"></a>

Finally, cloud-init runs enlistment and setup scripts:

<a href="https://discourse.maas.io/uploads/default/original/1X/bd87f78c8ee668a22640bf15607c9e3e532d46bb.jpeg" target = "_blank"><img src="https://discourse.maas.io/uploads/default/original/1X/bd87f78c8ee668a22640bf15607c9e3e532d46bb.jpeg"></a>

The enlistment scripts send information about the machine to the region API server, including the architecture, MAC address and other details.  The API server, in turn, stores these details in the database. This information-gathering process is known as [automatic discovery or network discovery](/t/how-to-set-up-networks/6174#heading--about-network-discovery).

Typically, the next step will be to commission the machine. As an alternative to enlistment, an administrator can add a machine manually. Typically this is done when enlistment doesn't work for some reason. Note that when you manually add a machine, MAAS automatically commissions the machine as soon as you've added it.

After the commissioning process, MAAS places the machine in the ‘Ready’ state. ‘Ready’ is a holding state for machines that are commissioned, waiting to be deployed when needed.

[note]
MAAS runs built-in commissioning scripts during the enlistment phase. When you commission a machine, any customised commissioning scripts you add will have access to data collected during enlistment. Follow the link above for more information about commissioning and commission scripts.
[/note]

**** About BMC enlistment

For IPMI machines, you only need to provide IPMI credentials. MAAS automatically discovers the machine and runs enlistment configuration by matching the BMC address.  For non-IPMI machines, you must specify a non-PXE MAC address. MAAS automatically discovers the machine and runs enlistment configuration by matching the non-PXE MAC address.

**** About adding machines

There are two ways to add a machine to MAAS:

1. If you place the machine on a connected network, and the machine is configured to netboot, MAAS will automatically enlist it.

2. If you add a machine manually, MAAS will automatically commission it.  There are also ways to turn off this automatic commissioning, should you desire to do so.

MAAS typically adds a machine via a combination of DHCP, TFTP, and PXE. By now, you should have enabled MAAS to automatically add devices and machines to your environment. This unattended method of adding machines is called enlistment.

Configuring a computer to boot over PXE is done via its BIOS, often referred to as "netboot" or "network boot". Normally, when you add a machine manually, MAAS will immediately attempt to commission the machine. Note that you will need to configure the underlying machine to netboot, or commissioning will fail. MAAS cannot handle this configuration for you.  While the correct method for configuring network boot depends heavily on your server, there are two common elements:

1. The network card on your server must be able to support PXE, i.e., your NIC -- whether independent or integrated on a motherboard -- must have a boot PROM that supports network booting.  You'll need to consult the documentation for the machine in question to determine this. Note that in MAAS versions before 2.5, you are required to provide the MAC address of the PXE interface when adding a new machine manually.

2. You usually have to interrupt the boot process and enter the BIOS/UEFI menu to configure the network card's PXE stack.  Again, you may need to consult your machine's documentation to pin down this step.

Additional steps will vary widely by machine type and architecture.

Regardless of how MAAS adds a machine, there are no special requirements for the underlying machine itself, other than being able to netboot. In particular, there is no need to install an operating system on it.

**** About cloning machines

[tabs]
[tab version="v3.4 Snap,v3.4 Packages,v3.3 Snap,v3.3 Packages,v3.2 Snap,v3.2 Packages,v3.1 Snap,v3.1 Packages"]
MAAS 3.1 provides the ability to quickly clone or copy configuration from one machine to one or more machines, via the MAAS UI, providing convenient access to an existing API feature.. This is a step towards machine profile templating work. 

Creating a machine profile is a repetitive task. Based on the responses to our survey -- and multiple forum posts, we have learned that most users create multiple machines of the same configuration in batches. Some users create a machine profile template and loop them through the API, while some create a script to interface with the CLI. However, there is no easy way to do this in the UI except by going through each machine and configuring them individually.   

MAAS API already has the cloning functionality, but it was never exposed in the UI. Hence, users may not know that this API feature exists, nor is there any current documentation about how to use this feature.  Although the current cloning API feature does not solve all machine profile templating problems, it is a great place for us to start moving in the direction of machine templates.

**** About copying machine configurations

As a MAAS user -- API or UI -- you may want to copy the configuration of a given machine and apply it to multiple existing machines. Assuming that at least one machine is already set to the desired configuration, you should be able to apply these same settings to a list of destination machines.  This means that a user should be able to:

- select the source machine to copy from.
- validate that the source machine exists.
- select at least 1 destination machine.
- validate that the destination machine(s) exist.
- edit the source machine or destination machines, if needed.
- know at all times which machines are affected.
- see the cloned machines when cloning is successful, or
- get clear failure information, if cloning fails. 

**** About choosing configuration items to copy

As a MAAS user, you will likely want to select whether storage, network, or both configurations should be cloned. The cloning API allows users to choose interfaces and storage separately.  Thus, this new feature should allow the user to:

- clone only the interface (network) configuration.
- clone only the storage configuration.
- clone both configurations.

**** About cloning restrictions

In order for cloning to succeed, a few restrictions must be met:

- The destination interface names must be the same source.
- The destination drive must be equal to or larger than the source drive.
- For static IPs, a new IP will be allocated to the interface on the destination machine
[/tab]
[tab version="v3.0 Snap,v3.0 Packages,v2.9 Snap,v2.9 Packages"]
Cloning machines is available starting with MAAS version 3.1.
[/tab]
[/tabs]

**** About enlisting deployed machines

[tabs]
[tab version="v3.4 Snap,v3.4 Packages,v3.3 Snap,v3.3 Packages,v3.2 Snap,v3.2 Packages,v3.1 Snap,v3.1 Packages"]
In general, when adding a machine to MAAS, it network boots the machine into an ephemeral environment to collect hardware information about the machine. While this is not a destructive action, it doesn’t work if you have machines that are already running a workload.

For one, you might not be able to disrupt the workload in order to network boot it. But also, the machine would be marked as Ready, which is incorrect.

When adding a machine, you may specify that the machine is already deployed. In that case, it won’t be going through the normal commissioning process and will be marked as being deployed.

Such machines lack hardware information. In order to update the information, a script is provided to run a subset of the commissioning scripts and send them back to MAAS.

Because already-deployed machines were not deployed by MAAS, most of the standard MAAS commands will not affect the machine and may, at times, return some odd results.  This is not errant behaviour; the goal of enlisting deployed machines is to avoid disturbing their workload. 
[/tab]
[tab version="v3.0 Snap,v3.0 Packages"]
MAAS version 3.0 cannot enlist deployed machines. Please upgrade to MAAS version 3.1 or greater to gain this capability.
[/tab]
[tab version="v2.9 Snap,v2.9 Packages"]
MAAS version 2.9 cannot enlist deployed machines. Please upgrade to MAAS version 3.1 or greater to gain this capability.
[/tab]
[/tabs]

*** About commissioning machines

When MAAS commissions a machine, the following sequence of events takes place:

1.  DHCP server is contacted
2.  kernel and initrd are received over TFTP
3.  machine boots
4.  initrd mounts a Squashfs image ephemerally over HTTP
5.  cloud-init runs built-in and custom commissioning scripts
6.  machine shuts down

The commissioning scripts will talk to the region API server to ensure that everything is in order and that eventual deployment will succeed.

MAAS chooses the latest Ubuntu LTS release as the default image for commissioning.  If desired, you can select a different image in the "Settings" page of the web UI, by selecting the "General" tab and then scrolling down to the Commissioning section.

[note]
Commissioning requires 60 seconds.
[/note]

**** About commissioning NUMA and SR-IOV nodes

If you are using the NUMA architecture, MAAS versions 2.7 and higher guarantee that machines are assigned to a single NUMA node that contains all the machine's resources. Node boundaries are critical, especially in vNUMA situations.  Splitting nodes can create unnecessary latency.  You want the NUMA node boundaries to match VM boundaries if at all possible.

[note]
You must recommission NUMA/SR-IOV machines that were previously commissioned under version 2.6 or earlier.
[/note]

<a href="https://discourse.maas.io/uploads/default/original/1X/7b47235ff57a570ccba6a6ed09186a3d7483f5a4.png" target = "_blank"><img src="https://discourse.maas.io/uploads/default/original/1X/7b47235ff57a570ccba6a6ed09186a3d7483f5a4.png"></a>

When using these nodes, you can specify a node index for interfaces and physical block devices.  MAAS will display the NUMA node index and details, depending upon your configuration, to include the count of NUMA nodes, number of CPU cores, memory, NICs, and node spaces for bonds and block devices.  You can also filter machines by CPU cores, memory, subnet, VLAN, fabric, space, storage, and RAID, among others.

**** About MAAS commissioning scripts

MAAS runs scripts during enlistment, commissioning and testing to collect data about nodes. Both enlistment and commissioning run all builtin commissioning scripts, though enlistment runs only built-ins. Commissioning also runs any user-uploaded commissioning scripts by default, unless the user manually provides a list of scripts to run. MAAS uses these commissioning scripts to configure hardware and perform other tasks during commissioning, such as updating the firmware. Similarly, MAAS employs hardware testing scripts to evaluate system hardware and report its status.

Scripts can be selected to run from web UI during commissioning, by testing hardware,  or from the command line. Note that MAAS only runs built-in commissioning scripts during enlistment. Custom scripts can be run when you explicitly choose to commission a machine.  A typical administrator workflow (with machine states), using customised commissioning scripts, can be represented as:

Add machine -&gt; Enlistment (runs built-in commissioning scripts MAAS) -&gt; New -&gt; Commission (runs built-in and custom commissioning scripts) -&gt; Ready -&gt; Deploy

NOTE: Scripts are run in alphabetical order in an ephemeral environment.  We recommend running your scripts after any MAAS built-in scripts.  This can be done by naming your scripts 99-z*.  It is possible to reboot the system during commissioning using a script, however, as the environment is ephemeral, any changes to the environment will be destroyed upon reboot (barring, of course, firmware type updates).

When a machine boots, MAAS first instructs it to run cloud-init to set up SSH keys (during commissioning only), set up NTP, and execute a script that runs other commissioning scripts.  Currently, the sequence of MAAS-provided commissioning scripts proceeds like this:

- **maas-support-info:** MAAS gathers information that helps to identify and characterise the machine for debugging purposes, such as the kernel, versioning of various components, etc.  **Runs in parallel with other scripts.**

- **maas-lshw:** this script pulls system BIOS and vendor info, and generates user-defined tags for later use.  **Runs in parallel with other scripts.**

- **20-maas-01-install-lldpd:** this script installs the link layer discovery protocol (LLDP) daemon, which will later capture networking information about the machine.  This script provides some extensive logging.

- **maas-list-modaliases:** this script figures out what hardware modules are loaded, providing a way to autorun certain scripts based on which modules are loaded.  **Runs in parallel with other scripts.**

- **20-maas-02-dhcp-unconfigured-ifaces:** MAAS will want to know all the ways the machine is connected to the network.  Only PXE comes online during boot; this script brings all the other networks online so they can be recognised.  This script provides extensive logging.

- **maas-get-fruid-api-data:** this script gathers information for the Facebook wedge power type.  **Runs in parallel with other scripts.**

- **maas-serial-ports:** this script lists what serial ports are available on the machine.  **Runs in parallel with other scripts.**

[tabs]
[tab version="v3.4 Snap,v3.4 Packages,v3.3 Snap,v3.3 Packages,v3.2 Snap,v3.2 Packages,v3.1 Snap,v3.1 Packages,v3.0 Snap,v3.0 Packages"]
[note]
As of MAAS version 3.0, **40-maas-01-network-interfaces** is no longer used by MAAS.
[/note]
[/tab]
[tab version="v2.9 Snap,v2.9 Packages"]
- **40-maas-01-network-interfaces:** this script is just used to get the IP address, which can then be associated with a VLAN/subnet.
[/tab]
[/tabs]

[tabs]
[tab version="v3.4 Snap,v3.4 Packages,v3.3 Snap,v3.3 Packages,v3.2 Snap,v3.2 Packages,v3.1 Snap,v3.1 Packages,v3.0 Snap,v3.0 Packages"]
- **50-maas-01-commissioning:** this script is the main MAAS tool, gathering information on machine resources, such as storage, network devices, CPU, RAM, details about attached USB and PCI devices, etc.  We currently pull this data using lxd: We use a Go binary built from lxd source that just contains the minimum source to gather the resource information we need.  This script also checks whether the machine being commissioning is a virtual machine, which may affect how MAAS interacts with it.
[/tab]
[tab version="v2.9 Snap,v2.9 Packages"]
- **50-maas-01-commissioning:** this script is the main MAAS tool, gathering information on machine resources, such as storage, network devices, CPU, RAM, etc.  We currently pull this data using lxd: We use a Go binary built from lxd source that just contains the minimum source to gather the resource information we need. This script also checks whether the machine being commissioning is a virtual machine, which may affect how MAAS interacts with it.
[/tab]
[/tabs]

- **maas-capture-lldp:** this script gathers LLDP network information to be presented on the logs page; this data is not used by MAAS at all.  **Runs in parallel with other scripts.**

- **maas-kernel-cmdline:** this script is used to update the boot devices; it double-checks that the right boot interface is selected.

Commissioning runs the same dozen or so scripts as enlistment, gathering all the same information, but with these caveats:

- Commissioning also runs user-supplied commissioning scripts, if present.  Be aware that these scripts run as root, so they can execute any system command.

- Commissioning runs test scripts which are not run during enlistment.

- Commissioning scripts can send BMC configuration data, and can be used to configure BMC data.

- The environment variable BMC_CONFIG_PATH is passed to serially run commissioning scripts; these scripts may write BMC power credentials to BMC_CONFIG_PATH in YAML format, where each key is a power parameter.  The first script to write BMC_CONFIG_PATH is the only script allowed to configure the BMC, allowing you to override MAAS' built-in BMC detection.  If the script returns 0, that value will be send to MAAS.

- All built-in commissioning scripts have been migrated into the database.

- `maas-run-remote-scripts` is capable of enlisting machines, so enlistment `user-data` scripts have been removed.

- The metadata endpoints `http://<MAAS>:5240/<latest or 2012-03-01>/` and `http://<MAAS>:5240/<latest or 2012-03-01>/meta-data/` are now available anonymously for use during enlistment.

In both enlistment and commissioning, MAAS uses either the MAC address or the UUID to identify machines.  Currently, because some machine types encountered by MAAS do **not** use unique MAC addresses, we are trending toward using the UUID.

[note]
To commission a node, it must have a status of "New".
[/note]

You have the option of setting some parameters to change how commissioning runs:

- `enable_ssh`: Optional integer. Controls whether to enable SSH for the commissioning environment using the user's SSH key(s). '1' == True, '0' == False. Roughly equivalent to the **Allow SSH access and prevent machine powering off** in the web UI.

- `skip_bmc_config`: Optional integer.  Controls whether to skip re-configuration of the BMC for IPMI based machines. '1' == True, '0' == False.

- `skip_networking`: Optional integer.  Controls whether to skip re-configuring the networking on the machine after the commissioning has completed. '1' == True, '0' == False. Roughly equivalent to **Retain network configuration** in the web UI.

- `skip_storage`: Optional integer.  Controls whether to skip re-configuring the storage on the machine after the commissioning has completed. '1' == True, '0' == False.  Roughly equivalent to **Retain storage configuration** in the web UI.

- `commissioning_scripts`: Optional string.  A comma separated list of commissioning script names and tags to be run. By default all custom commissioning scripts are run. Built-in commissioning scripts always run. Selecting `update_firmware` or `configure_hba` will run firmware updates or configure HBA's on matching machines.

- `testing_scripts`: Optional string.  A comma separated list of testing script names and tags to be run. By default all tests tagged `commissioning` will be run. Set to `none` to disable running tests.

- `parameters`: Optional string.  Scripts selected to run may define their own parameters. These parameters may be passed using the parameter name. Optionally a parameter may have the script name prepended to have that parameter only apply to that specific script.

**** About machine commissioning logs

MAAS keeps extensive logs of the commissioning process for each machine. These logs present an extremely detailed, timestamped record of completion and status items from the commissioning process.

**** About disabling individual boot methods

[tabs]
[tab version="v3.4 Snap,v3.4 Packages,v3.3 Snap,v3.3 Packages,v3.2 Snap,v3.2 Packages,v3.1 Snap,v3.1 Packages,v3.0 Snap,v3.0 Packages"]
It is possible to disable individual boot methods.  This must be done via the CLI. When a boot method is disabled MAAS will configure MAAS controlled `isc-dhcpd` to not respond to the associated [boot architecture code](https://www.iana.org/assignments/dhcpv6-parameters/dhcpv6-parameters.xhtml#processor-architecture)`↗`. External DHCP servers must be configured manually.

To allow different boot methods to be in different states on separate physical networks using the same VLAN ID configuration is done on the subnet in the UI or API. When using the API boot methods to be disabled may be specified using the MAAS internal name or [boot architecture code](https://www.iana.org/assignments/dhcpv6-parameters/dhcpv6-parameters.xhtml#processor-architecture)`↗` in octet or hex form. 

For MAAS 3.0 and above, the following boot method changes have been implemented:

- UEFI AMD64 HTTP(00:10) has been re-enabled.
- UEFI ARM64 HTTP(00:13) has been enabled.
- UEFI ARM64 TFTP(00:0B) and UEFI ARM64 HTTP(00:13) will now provide a shim and GRUB signed with the Microsoft boot loader keys.
- grub.cfg for all UEFI platforms has been updated to replace the deprecated `linuxefi` and `initrdefi` commands with the standard `linux` and `initrd` commands.
- GRUB debug may now be enabled by enabling rackd debug logging.

[/tab]
[tab version="v2.9 Snap,v2.9 Packages"]
Disabling boot methods is available in MAAS version 3.0 or greater.
[/tab]
[/tabs]


**** About automatic script selection by hardware type

When selecting multiple machines, scripts which declare the `for_hardware` field will only run on machines with matching hardware. To automatically run a script when 'Update firmware' or 'Configure HBA' is selected, you must tag the script with 'update_firmware' or 'configure_hba'.

Similarly, scripts selected by tag on the command line which specify the `for_hardware` field will only run on matching hardware.

**** About script results

A script can output its results to a YAML file, and those results will be associated with the hardware type defined within the script. MAAS provides the path for the results file in an environment variable, `RESULT_PATH`. Scripts should write YAML to this file before exiting.

If the hardware type is storage, for example, and the script accepts a storage type parameter, the result will be associated with a specific storage device.

The YAML file must represent a dictionary with these two fields:

1. `result`: The completion status of the script. This status can be `passed`, `failed`, `degraded`, or `skipped`. If no status is defined, an exit code of `0` indicates a pass while a non-zero value indicates a failure.

2. `results`: A dictionary of results. The key may map to a results key defined as embedded YAML within the script. The value of each result must be a string or a list of strings.

Optionally, a script may define what results to return in the YAML file in the metadata fields.. The `results` field should contain a dictionary of dictionaries. The key for each dictionary is a name which is returned by the results YAML. Each dictionary may contain the following two fields:

1. `title` - The title for the result, used in the UI.

2. `description` - The description of the field used as a tool-tip in the UI.

Here is an example of "degrade detection":

``` python
#!/usr/bin/env python3

# --- Start MAAS 1.0 script metadata ---
# name: example
# results:
#   memspeed:
#     title: Memory Speed
#     description: Bandwidth speed of memory while performing random read writes
# --- End MAAS 1.0 script metadata ---

import os
import yaml

memspeed = some_test()

print('Memspeed: %s' % memspeed)
results = {
   'results': {
       'memspeed': memspeed,
   }
}
if memspeed < 100:
   print('WARN: Memory test passed but performance is low!')
   results['status'] = 'degraded'

result_path = os.environ.get("RESULT_PATH")
if result_path is not None:
   with open(result_path, 'w') as results_file:
       yaml.safe_dump(results, results_file)
```


**** About tags and scripts

As with general tag management, tags make scripts easier to manage; grouping scripts together for commissioning and testing, for example:

``` bash
maas $PROFILE node-script add-tag $SCRIPT_NAME tag=$TAG
maas $PROFILE node-script remove-tag $SCRIPT_NAME tag=$TAG
```

MAAS runs all commissioning scripts by default. However, you can select which custom scripts to run during commissioning by name or tag:

``` bash
maas $PROFILE machine commission \
commissioning_scripts=$SCRIPT_NAME,$SCRIPT_TAG
```

You can also select which testing scripts to run by name or tag:

``` bash
maas $PROFILE machine commission \
testing_scripts=$SCRIPT_NAME,$SCRIPT_TAG
```

Any testing scripts tagged with commissioning will also run during commissioning.

**** About debugging script failures

You can individually access the output from both completed and failed scripts.

<a href="https://assets.ubuntu.com/v1/855015e5-nodes-hw-scripts__2.2_fail.png" target = "_blank"><img src="https://assets.ubuntu.com/v1/855015e5-nodes-hw-scripts__2.2_fail.png"></a>

If you need further details, especially when writing and running your own scripts, you can connect to a machine and examine its logs and environment.

<a href="https://assets.ubuntu.com/v1/da793c67-nodes-hw-scripts__2.4_ssh.png" target = "_blank"><img src="https://assets.ubuntu.com/v1/da793c67-nodes-hw-scripts__2.4_ssh.png"></a>

Because scripts operate within an ephemeral version of Ubuntu, enabling this option stops the machine from shutting down, allowing you to connect and probe a script's status.

As long as you've added your [SSH key](/t/how-to-manage-user-accounts/5184#heading--ssh-keys) to MAAS, you can connect with SSH to the machine's IP with a username of `ubuntu`. Type `sudo -i` to get root access.

**** About testing hardware

If you wish, you can tell MAAS to test machine hardware using well-known Linux utilities.  MAAS can test machines that have a status of **Ready**, **Broken**, or **Deployed**.  You can include testing as part of the commissioning process. When you choose the 'Commission' action, MAAS will display the dialog described below.  Be aware, though, that if the hardware tests fail, the machine will become unavailable for Deployment.

[note]
The majority of testing scripts only work with machines that are backed by physical hardware (e.g. they may be incompatible with VM-based machines).
[/note]

With MAAS, you can easily write, upload and execute your hardware testing scripts and see the results.

**** About machine hardware & test logs

MAAS logs test results and allows you to view a summary of tests run against a particular machine.  You can also example details on any particular tests:

<a href="https://discourse.maas.io/uploads/default/original/2X/e/e53a2c01b57df49e56bb4d95552b6a038249aa97.png" target = "_blank"><img src="https://discourse.maas.io/uploads/default/original/2X/e/e53a2c01b57df49e56bb4d95552b6a038249aa97.png"></a>

You can also examine the "raw" log output:

<a href="https://discourse.maas.io/uploads/default/original/2X/d/dc5bb5e6489a382e257dac605f2dbdc6fa1ca630.png" target = "_blank"><img src="https://discourse.maas.io/uploads/default/original/2X/d/dc5bb5e6489a382e257dac605f2dbdc6fa1ca630.png"></a>

Help interpreting these logs can be found under the [Logging](/t/how-to-work-with-log-files/5240) section of this documentation.

**** About testing machine networking

MAAS provides a comprehensive suite of network and link testing capabilities.  MAAS can check whether or not links are connected, detect slow links, and report link and interface speeds via UI or API. In addition, you can test Internet connectivity against a user-provided list of URLs or IP addresses.  Bonded NICS will be separated during this testing, so that each side of a redundant interface is fully evaluated.

Network testing also includes customisable network testing and commissioning scripts. There are no particular restrictions on these scripts, allowing you to test a wide variety of possible conditions and situations.

**** About post-commission configuration

Once commissioned, you can configure the machine's network interface(s). Specifically, when a machine's status is either "Ready" or "Broken", interfaces can be added/removed, attached to a fabric and linked to a subnet, and provided an IP assignment mode. Tags can also be assigned to specific network interfaces.

*** About allocation and deployment

Once a machine has been commissioned, the next logical step is to deploy it. Deploying a machine means, effectively, to [install an operating system on it](/t/how-to-acquire-images/6192#heading--how-images-deploy), along with any other application loads you wish to run on that machine.

 A detailed picture of deployment looks something like this:

<a href="https://discourse.maas.io/uploads/default/original/2X/f/f7e0fb1916bca084de75fc0479bfec3c95adf7b6.png" target = "_blank"><img src="https://discourse.maas.io/uploads/default/original/2X/f/f7e0fb1916bca084de75fc0479bfec3c95adf7b6.png"></a>

Before deploying a machine, MAAS must allocate it (status 'Allocated'). Allocating a machine reserves the machine for the exclusive use of the allocation process. The machine is no longer available to any other process, including another MAAS instance, or a process such as Juju.

The agent that triggers deployment may vary. For instance, if the machines are destined to run complex, inter-related services that scale up or down frequently, like a "cloud" resource, then [Juju](https://jaas.ai/)`↗` is the recommended deployment agent. Juju will also install and configure services on the deployed machines. If you want to use MAAS to install a base operating system and work on the machines manually, then you can deploy a machine directly with MAAS.

Machines deployed with MAAS will also be ready to accept connections via SSH, to the 'ubuntu' user account.  This connection assumes that you have imported an SSH key has to your MAAS account. This is explained in [SSH keys](/t/how-to-manage-user-accounts/5184#heading--ssh-keys).

[note]
Juju adds SSH keys to machines under its control.
[/note]

MAAS also supports machine customisation with a process called "preseeding." For more information about customising machines, see [How to customise machines](/t/how-to-customise-machines/5108).

To deploy, you must configure the underlying machine to netboot.  Such a machine will undergo the following process, outlined in the above diagram:

1. MAAS boots the machine via the machine's BMC, using whatever power driver is necessary to properly communicate with the machine.
2. The booted machine sends a DHCP Discover request.
3. The MAAS-managed DHCP server (ideally) responds with an IP address and the location of a MAAS-managed HTTP or TFTP boot server.
4. The machine uses the HTTP/TFTP location to request a usable Network Boot Program (NBP).
5. The machine recieves the NBP and boots.
6. The machine firmware requests a bootable image.
7. MAAS sends an ephemeral OS image, including an initrd; this ephemeral (RAM-only) image is necessary for ```curtin``` to carry out any hardware-prep instructions (such as disk paritioning) before the deployed OS is booted.
8. The initrd mounts a SquashFS image, also ephemerally, over HTTP.
9. The machine boots the emphemeral image.
10. The ephemeral image runs ```curtin```, with passed pre-seed information, to configure the machine's hardware.
11. The desired deployment (target) image is retrieved by ```curtin```, which installs and boots that deployment image.  Note that the curtin installer uses an image-based method and is now the only installer used by MAAS. Although the older debian-installer method has been removed, curtin continues to support preseed files. For more information about customising machines see [How to customise machines](/t/how-to-customise-machines/5108).
12. The target image runs its embedded ```cloud-init``` script set, including any customisations and pre-seeds.

Once this is done, the target image is up and running on the machine, and the machine can be considered successfully deployed.

Also note that, before deploying, you should take two key actions:

1. Review and possibly set the [Ubuntu kernels](/t/how-to-customise-machines/5108#heading--about-ubuntu-kernels) and the [Kernel boot options](/t/how-to-customise-machines/5108#heading--about-kernel-boot-options) that will get used by deployed machines.

2. Ensure any pertinent SSH keys are imported (see [SSH keys](/t/how-to-manage-user-accounts/5184#heading--ssh-keys)) to MAAS so it can connect to deployed machines.

** About the machine list

The machine list is the basic dashboard for many MAAS operations.  In this subsection, you will learn:

- [About the machine summary](#heading--about-the-machine-summary)
- [Handling attached USB and PCI devices](#heading--usb-pci-devices)
- [About machine network info](#heading--about-machine-interfaces)
- [About machine configuration info](#heading--machine-config)
- [About resource pools](#heading--about-resource-pools)
- [About tags](#heading--about-tags)
- [About annotations](#heading--about-annotations)
- [About storage](#heading--about-storage)
  
In the illustration below, you see the machine list for a typical small hospital data centre, including servers ready and allocated for functions like Pharmacy, Orders, Charts, and so on:

[tabs]
[tab version="v3.4 Snap,v3.4 Packages,v3.3 Snap,v3.3 Packages,v3.2 Snap,v3.2 Packages,v3.1 Snap,v3.1 Packages,v3.0 Snap,v3.0 Packages,v2.9 Snap,v2.9 Packages" view="UI"]
<a href="https://discourse.maas.io/uploads/default/original/1X/30df04b0bcec5fcf6538590ed795cb0514a64675.jpeg" target = "_blank"><img src="https://discourse.maas.io/uploads/default/original/1X/30df04b0bcec5fcf6538590ed795cb0514a64675.jpeg"></a>

Rolling the cursor over status icons often reveals more details. For example, a failed hardware test script will place a warning icon alongside the hardware type tested by the script. Rolling the cursor over this will reveal which test failed.  Likewise, you can find some immediate options by rolling over the column data items in the machines table.

<a href="https://discourse.maas.io/uploads/default/original/1X/8f78a8877a029e7a44bcd4cf3d138499637fe790.jpeg" target = "_blank"><img src="https://discourse.maas.io/uploads/default/original/1X/8f78a8877a029e7a44bcd4cf3d138499637fe790.jpeg"></a>

The 'Add hardware' drop-down menu is used to add either new machines or a new chassis. This menu changes context when one or more machines are selected from the table, using either the individual checkboxes in the first column or the column title checkbox to select all.

<a href="https://discourse.maas.io/uploads/default/original/1X/9a0747649e6aff999d3c04335eb752accedaf3de.jpeg" target = "_blank"><img src="https://discourse.maas.io/uploads/default/original/1X/9a0747649e6aff999d3c04335eb752accedaf3de.jpeg"></a>

With one or more machines selected, the 'Add hardware' drop-down menu moves to the left, and is joined by the 'Take action' menu.  This menu provides access to the various [machine actions](/t/maas-glossary/5416#node-actions) that can be applied to the selected machine(s):

<a href="https://discourse.maas.io/uploads/default/original/1X/e03d5ac8de9ea4f4827ed057bb2dd83e241aac3b.jpeg" target = "_blank"><img src="https://discourse.maas.io/uploads/default/original/1X/e03d5ac8de9ea4f4827ed057bb2dd83e241aac3b.jpeg"></a>

[note]
The 'Filter by' section limits the machines listed in the table to selected keywords and machine attributes.
[/note]

[/tab]
[tab version="v3.4 Snap,v3.4 Packages,v3.3 Snap,v3.3 Packages,v3.2 Snap,v3.2 Packages,v3.1 Snap,v3.1 Packages,v3.0 Snap,v3.0 Packages,v2.9 Snap,v2.9 Packages" view="CLI"]
```nohighlight
FQDN               POWER  STATUS     OWNER  TAGS     POOL       NOTE     ZONE
----               -----  ------     -----  ----     ----       ----     ----
52-54-00-15-36-f2  off    Ready      -      Orders   Prescrbr   @md-all  Medications
52-54-00-17-64-c8  off    Ready      -      HRMgmt   StaffComp  @tmclck  Payroll
52-54-00-1d-47-95  off    Ready      -      MedSupp  SuppServ   @storag  Inventory
52-54-00-1e-06-41  off    Ready      -      PatPrtl  BusOfc     @bzstns  BizOffice
52-54-00-1e-a5-7e  off    Ready      -      Pharm    Prescrbr   @rxonly  Pharmacy
52-54-00-2e-b7-1e  off    Allocated  admin  NursOrd  NurServ    @nstns   Nursing
52-54-00-2e-c4-40  off    Allocated  admin  MedAdmn  NurServ    @rxonly  Nursing
52-54-00-2e-ee-17  off    Deployed   admin  Charts   ProServ    @md-all  Physician
```

You can generate a list similar to this for your machines with the command:

```nohighlight
maas admin machines read | jq -r '(["FQDN","POWER","STATUS",
"OWNER", "TAGS", "POOL", "NOTE", "ZONE"] | (., map(length*"-"))),
(.[] | [.hostname, .power_state, .status_name, .owner // "-", 
.tag_names[0] // "-", .pool.name, .description // "-", .zone.name]) | @tsv' | column -t
```
[/tab]
[/tabs]

These example machines would typically be duplicated in several different geographies, with a quick way to switch to a redundant node, should anything go wrong (e.g., high availability).  We used the word node there because, In the network language of MAAS, machines are one of several different types of nodes.  A node is simply a network-connected object or, more specifically, an object that can independently communicate on a network. MAAS nodes include controllers, network devices, and of course, machines.

Looking back at the example above, you can see that there are several columns in the machine list, depending on your view:

- **FQDN | MAC**: The fully qualified domain name or the MAC address of the machine.
- **Power**: 'On', 'Off' or 'Error' to highlight an error state.
- **Status**: The current status of the machine, such as 'Ready', 'Commissioning' or 'Failed testing'.
- **Owner**: The MAAS account responsible for the machine.
- **Cores**: The number of CPU cores detected on the machine.
- **RAM**: The amount of RAM, in GiB, discovered on the machine.
- **Disks**: The number of drives detected on the machine.
- **Storage**: The amount of storage, in GB, identified on the machine.

*** About the machine summary

Click a machine's FQDN or MAC address to open a detailed view of a machine's status and configuration.

<a href="https://discourse.maas.io/uploads/default/original/2X/a/a8ff4caf6362a3d695682499a74d64cb189dfc37.png" target = "_blank"><img src="https://discourse.maas.io/uploads/default/original/2X/a/a8ff4caf6362a3d695682499a74d64cb189dfc37.png"></a>

The default view is 'Machine summary', presented as a series of cards detailing the CPU, memory, storage and tag characteristics of the machine, as well as an overview of its current status. When relevant, 'Edit' links take you directly to the settings pane for the configuration referenced within the card.  The machine menu bar within the web UI also includes links to logs, events, and configuration options:

<a href="https://discourse.maas.io/uploads/default/original/2X/2/21e9f4dca3a3e0a6657b5b2a570c9fc68a3e4961.png" target = "_blank"><img src="https://discourse.maas.io/uploads/default/original/2X/2/21e9f4dca3a3e0a6657b5b2a570c9fc68a3e4961.png"></a>

The machine status card presents an overview of CPU, memory, storage, tags, and general settings:

<a href="https://discourse.maas.io/uploads/default/original/2X/a/a8ff4caf6362a3d695682499a74d64cb189dfc37.png" target = "_blank"><img src="https://discourse.maas.io/uploads/default/original/2X/a/a8ff4caf6362a3d695682499a74d64cb189dfc37.png"></a>

The first card presents some basics of the machine resources and configuration:

<a href="https://discourse.maas.io/uploads/default/original/1X/3e50fb21f4985db0a85519e2e933e24658770b9e.jpeg" target = "_blank"><img src="https://discourse.maas.io/uploads/default/original/1X/3e50fb21f4985db0a85519e2e933e24658770b9e.jpeg"></a>

Here are some details on what this card presents, with details on in-card links described in following sections:

- **OVERVIEW** the machine status (in this case "Deployed"), and lists OS version information.  

- **CPU** shows the specifics of the CPU(s), including a link to test the processor(s).

- **MEMORY** gives the total available RAM for this machine, along with a test link.

- **STORAGE** presents the total amount of storage available and the number of disks that provide that storage.  There are two links here: one gives the storage layout (with the opportunity to change it for devices that are in 'Ready' or 'Allocated' states.

- **Owner** identifies the owner of the machine.

- **Domain** indicates the domain in which the machine exists.

- **Zone** shows the AZ in which this machine resides, along with a link to edit the machine configuration (to change the AZ, if desired).

- **Resource pool** shows the pool to which this machine has been assigned, and an edit link.

- **Power type** gives the current power type, which links to the relevant edit form.

- **Tags** presents the list of tags associated with this machine, editable via the link.

Note that clicking any of the links in this card will either present a pop-up form or take you to another item in the machine menu -- so using the browser "back" button will take you completely away from this machine's page.  For example, you can choose the "Test CPU" option, which brings up this overlay:

<a href="https://discourse.maas.io/uploads/default/original/2X/6/6d7fe50e5b296a37a03269a1f5be3d25a2a2481a.png" target = "_blank"><img src="https://discourse.maas.io/uploads/default/original/2X/6/6d7fe50e5b296a37a03269a1f5be3d25a2a2481a.png"></a>

From this screen, you can choose test scripts and run the tests (in the background) as the interface returns to the Machine summary.  A linked note in the CPU block lets you know that the tests are in progress:

<a href="https://discourse.maas.io/uploads/default/original/2X/3/3e140872c407e5b9eb06960b5b42353765567192.png" target = "_blank"><img src="https://discourse.maas.io/uploads/default/original/2X/3/3e140872c407e5b9eb06960b5b42353765567192.png"></a> 

And you can watch the results under the "Tests" option in the Machine menu:

<a href="https://discourse.maas.io/uploads/default/original/2X/f/f398c9ed670af8c0886ccc1ed8bf586e3faf1e53.png" target = "_blank"><img src="https://discourse.maas.io/uploads/default/original/2X/f/f398c9ed670af8c0886ccc1ed8bf586e3faf1e53.png"></a> 

The rest of the cards on the Machine summary are either self-explanatory, or they're covered in the sections below.  The main point is this: You can see that nearly everything about machines takes place within the main menu's "Machines" option. 

*** Handling attached USB and PCI devices

[tabs]
[tab version="v3.4 Snap,v3.4 Packages,v3.3 Snap,v3.3 Packages,v3.2 Snap,v3.2 Packages,v3.1 Snap,v3.1 Packages,v3.0 Snap,v3.0 Packages"]
The machines in your MAAS may have devices attached to them via USB or PCI interface, such as keyboards, cameras, network cards, GPUs, etc.  MAAS will recognise these devices and make them visible to you when a machine is commissioned.

For example, the machine details presents USB and PCI devices like this:

<a href="https://discourse.maas.io/uploads/default/original/2X/8/87f42bafe321d45af94d73216f933a9067f01df2.png" target = "_blank"><img src="https://discourse.maas.io/uploads/default/original/2X/8/87f42bafe321d45af94d73216f933a9067f01df2.png"></a>

Note that this page now includes two new tabs: "PCI devices" and "USB."  For each USB/PCI device attached to your machine, these tabs will list:

- device type
- vendor ID
- a product description
- a product ID
- the driver name
- the containing NUMA node (if any)
- the device address

A typical PCI device tab would look something like this:

<a href="https://discourse.maas.io/uploads/default/original/2X/8/82e1e6f8bc511047ac5f773430f7e5812c7a24d4.png" target = "_blank"><img src="https://discourse.maas.io/uploads/default/original/2X/8/82e1e6f8bc511047ac5f773430f7e5812c7a24d4.png"></a>

The USB tab presents similar information in the same format.

[note]
If you are upgrading from a previous version of MAAS, PCI and USB devices aren't modelled, so you will have to recommission the machine to capture these devices.
[/note]


Once you've commissioned the machine, you have the option of deleting
PCI/USB devices from the machine in any machine state, via the CLI
only, using the following command:

```
maas $PROFILE node-device delete $SYSTEM_ID $DEVICE_ID
```

where:

- $PROFILE   = your user profile (e.g., "admin")
- $SYSTEM_ID = the ID of the machine in question (e.g., "ngx7ry")
- $DEVICE_ID = the ID of the device you want to delete 

If the device is still present in the system, it will be recognised again (and thus "recreated")
when the machine is commissioned again.

*** About machine network info

The Network "tab" provides you with a way to view/edit the network and interface configuration for a machine: 

<a href="https://discourse.maas.io/uploads/default/original/2X/c/c5316db130ae05a9cdabcd49ffaa69f0bb405d1d.png" target = "_blank"><img src="https://discourse.maas.io/uploads/default/original/2X/c/c5316db130ae05a9cdabcd49ffaa69f0bb405d1d.png"></a> 

In the case of this deployed machine, there are not many editing options.  If the machine is in a 'Ready' state, though, altering the network configuration is possible, as shown in the screenshot above.

Options on this tab are described in the introduction to [Networking](/t/how-to-set-up-networks/6174) article in this documentation set.

*** About machine configuration info

The final tab from the Machine menu allows you to update machine and power configuration options: 

<a href="https://discourse.maas.io/uploads/default/original/2X/7/7cfd77228a5cf1a6f779897d501f14fbf78fd4b4.png" target = "_blank"><img src="https://discourse.maas.io/uploads/default/original/2X/7/7cfd77228a5cf1a6f779897d501f14fbf78fd4b4.png"></a> 

There are two sections to this tab.  The "Machine configuration" section, shown above, offers some general parameters, mostly related to how this machine is grouped and categorised.  More information on these options are found in the relevant sections of the documentation (e.g., tags, resource pools, and so forth). 

The "Power configuration" supplies the parameters necessary for MAAS to access the machine to PXE-boot it: 

<a href="https://discourse.maas.io/uploads/default/original/2X/1/198898362285e4a1308535a4aa701156a67c9616.png" target = "_blank"><img src="https://discourse.maas.io/uploads/default/original/2X/1/198898362285e4a1308535a4aa701156a67c9616.png"></a> 

More information on Power configuration will be found in the [Power management](/t/power-management-reference/5246) section of this documentation.
[/tab]
[tab version="v2.9 Snap,v2.9 Packages"]
USB and PCI devices are supported in MAAS version 3.0 and greater.
[/tab]
[/tabs]

*** About resource pools

Resource pools allow administrators to logically group resources -- machines and VM hosts -- into pools. Pools can help you budget machines for a particular set of functions.  For example, if you're using MAAS to manage a hospital data centre, you may want to keep a certain number of machines reserved for provider use, whether that be for the charts, documentation, or orders application.  You can use resource pools to reserve those machines, regardless of which of the three applications you end up loading onto a particular machine at any given time. 

*** About tags

Tags are short, descriptive, searchable words that can be applied to various MAAS objects, including:

- machines (physical and virtual)
- VM hosts
- controllers (rack and region)
- storage (virtual and physical; block devices or partitions)
- network interfaces
- devices
- nodes (in the CLI only)

Tags serve to help you identify, group, and find objects easily, especially when you routinely deploy hundreds of machines.


*** About annotations

Annotations are descriptive, searchable phrases that apply only to machines.  There are two types of annotations: static (always present in any machine state), and dynamic (only present in allocated or deployed states).  Annotations help you identify, characterise, and inform others about your machines.


*** About storage

You have significant latitude when choosing the final storage configuration of a deployed machine. MAAS supports traditional disk partitioning, as well as more complex options such as LVM, RAID, and bcache. MAAS also supports UEFI as a boot mechanism.  This article explains boot mechanisms and layouts, and offers some advice on how to configure layouts and manage storage.

[note]
MAAS doesn’t currently support deploying with ZFS for devices other than the root one.  For this reason, ZFS is disrecommended.
[/note]

A machine's storage is dependant upon the underlying system's disks, but its configuration (i.e., disk usage) is the result of a storage template. In MAAS, this template is called a layout, and MAAS applies it to a machine during commissioning.  Once a layout is applied, a regular user can make modifications to a machine at the filesystem level to arrive at the machine's final storage configuration.  When a machine is no longer needed, a user can choose from among several disk erasure types before releasing it.

[note]
MAAS supports storage configuration for CentOS and RHEL deployments. Support includes RAID, LVM, and custom partitioning with different file systems (ZFS and bcache excluded). This support requires a newer version of Curtin, [available as a PPA](https://launchpad.net/ubuntu/+source/curtin)`↗`.
[/note]

** Customising machines

Prior to deployment, MAAS machines can be customised in a number of ways, including:

- machine storage.
- commissioning and deployment configurations (known as "pre-seeding").
- custom Ubuntu kernels.
- kernel boot options. 
- resource pools.

[tabs]
[tab version="v3.4 Snap,v3.4 Packages,v3.3 Snap,v3.3 Packages,v3.2 Snap,v3.2 Packages"]
Machines can also be customised post-deployment, while running, either by changing out the physical hardware, or by altering the VM from the VM host side.  This will allow you to:

- add or remove disks
- add or remove network interfaces
- add or remove PCI devices
- add or remove USB devices

[note]
You cannot update these parameters from within MAAS on a deployed machine.
[/note]

While deploying a machine, you can configure that machine to periodically sync its hardware configuration.  Deployed machines will passively update changes to the BMC and tags for that machine, on-the-fly, as these changes are made.

This article will help you learn:

- [About customising machines prior to deployment](#heading--about-customising-machines-prior-to-deployment)
- [About customising deployed machines](#heading--about-customising-deployed-machines)

In short, this article will explain these possible customisations, and provide detailed instructions on how to customise your own machines as desired.
[/tab]
[tab version="v3.1 Snap,v3.1 Packages,v3.0 Snap,v3.0 Packages,v2.9 Snap,v2.9 Packages"]
This article will help you learn [about customising machines](#heading--about-customising-machines-prior-to-deployment)

In short, this article will explain these possible customisations, and provide detailed instructions on how to customise your own machines as desired.
[/tab]
[/tabs]

** About customising machines prior to deployment

In this section, you'll learn:

- [About customising machine storage](#heading--about-customising-machine-storage)
- [About pre-seeding](#heading--about-pre-seeding) (customising commissioning and deployment)
- [About Ubuntu kernels](#heading--about-ubuntu-kernels)
- [About kernel boot options](#heading--about-kernel-boot-options)
- [About resource pools](#heading--about-resource-pools)

*** About customising machine storage

You have significant latitude when choosing the final storage configuration of a deployed machine. MAAS supports traditional disk partitioning, as well as more complex options such as LVM, RAID, and bcache. MAAS also supports UEFI as a boot mechanism.  This article explains boot mechanisms and layouts, and offers some advice on how to configure layouts and manage storage.

[note]
MAAS doesn’t currently support deploying with ZFS for devices other than the root one.  For this reason, ZFS is disrecommended.
[/note]

A machine's storage is dependant upon the underlying system's disks, but its configuration (i.e., disk usage) is the result of a storage template. In MAAS, this template is called a layout, and MAAS applies it to a machine during commissioning.  Once a layout is applied, a regular user can make modifications to a machine at the filesystem level to arrive at the machine's final storage configuration.  When a machine is no longer needed, a user can choose from among several disk erasure types before releasing it.

MAAS supports storage configuration for CentOS and RHEL deployments. Support includes RAID, LVM, and custom partitioning with different file systems (ZFS and bcache excluded). This support requires a newer version of Curtin, [available as a PPA](https://launchpad.net/ubuntu/+source/curtin)`↗`.

**** About UEFI booting

Every layout type supports a machine booting with UEFI. In such a case, MAAS automatically creates an EFI boot partition (`/boot/efi`). Other than setting the machine to boot from UEFI, the user does not need to take any additional action.

[note]
UEFI must be enabled or disabled for the lifespan of the machine. For example, do not enlist a machine with UEFI enabled, and then disable it before commissioning. It won't work!
[/note]

The EFI partition, if created, will be the first partition (`sda1`) and will have a FAT32 filesystem with a size of 512 MB.

**** About block devices

Once the initial storage layout has been configured on a machine, you can perform many operations to view and adjust the entire storage layout for the machine. In MAAS there are two different types of block devices.

**Physical**

A physical block device is a physically attached block device such as a 100GB hard drive connected to a server.

**Virtual**

A virtual block device is a block device that is exposed by the Linux kernel when an operation is performed. Almost all the operations on a physical block device can be performed on a virtual block device, such as a RAID device exposed as md0.

**** About partitions

As with block devices (see [Block devices](#heading--about-block-devices)), MAAS and the MAAS API offer a great deal of control over the creation, formatting, mounting and deletion of partitions.

**** About storage restrictions

There are three restrictions for the storage configuration:

1.   An EFI partition is required to be on the boot disk for UEFI.
2.   You cannot place partitions on logical volumes.
3.   You cannot use a logical volume as a Bcache backing device.

Violating these restrictions will prevent a successful deployment.

**** About VMFS datastores

MAAS can configure custom local VMware VMFS Datastore layouts to maximise the usage of your local disks when deploying VMware ESXi. As VMware ESXi requires specific partitions for operating system usage, you must first apply the VMFS6 storage layout. This layout creates a VMFS Datastore named `datastore1` which uses the disk space left over on the boot disk after MAAS creates the operating system partitions.

**** About final storage modifications

Once MAAS provisions a machine with block devices, via a layout or administrator customisation, a regular user can modify the resulting storage configuration at the filesystem level.

**** About disk erasure

Disk erasure pertains to the erasing of data on each of a machine's disks when the machine has been released (see [Release action](/t/maas-glossary/5416#heading--release)) back into the pool of available machines. The user can choose from among three erasure types before confirming the Release action. A default erasure configuration can also be set.

**** About disk erasure types

The three disk erasure types are:

1.   Standard erasure - Overwrites all data with zeros.
2.   Secure erasure - Although effectively equivalent to Standard erase, Secure erase is much faster because the disk's firmware performs the operation. Because of this, however, some disks may not be able to perform this erasure type (SCSI, SAS, and FC disks in particular).
3.   Quick erasure - Same as Standard erase but only targets the first 1 MB and the last 1 MB of each disk. This removes the partition tables and/or superblock from the disk, making data recovery difficult but not impossible.

If all three options are checked when the machine is released, the following order of preference is applied:

1.  Use 'secure erase' if the disk supports it
2.  If it does not, then use 'quick erase'

It is very important to pay close attention to your selections when erasing disks.

*** About pre-seeding

During machine [enlistment](/t/how-to-deploy-physical-machines/6193), [deployment](/t/how-to-put-machines-to-work/5112), commissioning and machine installation, MAAS sends [Tempita-derived](https://raw.githubusercontent.com/ravenac95/tempita/master/docs/index.txt)`↗` configuration files to the [cloud-init](https://launchpad.net/cloud-init)`↗` process running on the target machine. MAAS refers to this process as **preseeding**. These preseed files are used to configure a machine's ephemeral and installation environments and can be modified or augmented to a custom machine configuration.

Preseeding in MAAS can be achieved in two ways:

1.  [Curtin](https://launchpad.net/curtin)`↗`, a preseeding system similar to Kickstart or d-i (Debian Installer), applies customisation during operating system (OS) image installation. MAAS performs these changes on deployment, during OS installation, but before the machine reboots into the installed OS. Curtin customisations are perfect for administrators who want their deployments to have identical setups all the time, every time. [This blog post](https://blog.ubuntu.com/2017/06/02/customising-maas-installs)`↗` contains an excellent high-level overview of custom MAAS installs using Curtin.

2.  [Cloud-init](https://launchpad.net/cloud-init)`↗`, a system for setting up machines immediately after instantiation. cloud-init applies customisations after the first boot, when MAAS changes a machine's status to 'Deployed.' Customisations are per-instance, meaning that user-supplied scripts must be re-specified on redeployment. Cloud-init customisations are the best way for MAAS users to customise their deployments, similar to how the various cloud services prepare VMs when launching instances.

*** About templates

[tabs]
[tab version="v3.4 Snap,v3.4 Packages,v3.3 Snap,v3.2 Snap,v3.1 Snap,v3.0 Snap,v2.9 Snap"]
The [Tempita](https://raw.githubusercontent.com/ravenac95/tempita/master/docs/index.txt)`↗` template files are found in the `/var/snap/maas/current/preseeds/` directory on the region controller. Each template uses a filename prefix that corresponds to a particular phase of MAAS machine deployment:
[/tab]
[tab version="v3.3 Packages,v3.2 Packages,v3.1 Packages,v3.0 Packages,v2.9 Packages"]
The [Tempita](https://raw.githubusercontent.com/ravenac95/tempita/master/docs/index.txt)`↗` template files are found in the `/etc/maas/preseeds/` directory on the region controller. Each template uses a filename prefix that corresponds to a particular phase of MAAS machine deployment:
[/tab]
[/tabs]


|       Phase       |                 Filename prefix                 |
|:-----------------|:-----------------------------------------------|
| Enlistment  |                      enlist                     |
| Commissioning |                  commissioning                  |
| Installation | curtin ([Curtin](https://launchpad.net/curtin))`↗` |

Additionally, the template for each phase typically consists of two files. The first is a higher-level file that often contains little more than a URL or a link to further credentials, while a second file contains the executable logic.

The `enlist` template, for example, contains only minimal variables, whereas `enlist_userdata` includes both user variables and initialisation logic.

[note]
Tempita’s inheritance mechanism is the reverse of what you might expect. Inherited files, such as `enlist_userdata`, become the new template which can then reference variables from the higher-level file, such as `enlist`.
[/note]

*** About template naming

MAAS interprets templates in lexical order by their filename.  This order allows for base configuration options and parameters to be overridden based on a combination of operating system, architecture, sub-architecture, release, and machine name.

Some earlier versions of MAAS only support Ubuntu. If the machine operating system is Ubuntu, then filenames without `{os}` will also be tried, to maintain backward compatibility.

Consequently, template files are interpreted in the following order:

1.  `{prefix}_{os}_{node_arch}_{node_subarch}_{release}_{node_name}` or `{prefix}_{node_arch}_{node_subarch}_{release}_{node_name}`

2.  `{prefix}_{os}_{node_arch}_{node_subarch}_{release}` or `{prefix}_{node_arch}_{node_subarch}_{release}`

3.  `{prefix}_{os}_{node_arch}_{node_subarch}` or `{prefix}_{node_arch}_{node_subarch}`

4.  `{prefix}_{os}_{node_arch}` or `{prefix}_{node_arch}`

5.  `{prefix}_{os}`

6.  `{prefix}`

7.  `generic`

The machine needs to be the machine name, as shown in the web UI URL.

The prefix can be either `enlist`, `enlist_userdata`, `commissioning`, `curtin`, `curtin_userdata` or `preseed_master`. Alternatively, you can omit the prefix and the following underscore.

For example, to create a generic configuration template for Ubuntu 16.04 Xenial running on an x64 architecture, the file would need to be called `ubuntu_amd64_generic_xenial_node`.

To create the equivalent template for curtin_userdata, the file would be called `curtin_userdata_ubuntu_amd64_generic_xenial_node`.

[note]
Any file targeting a specific machine will replace the values and configuration held within any generic files. If those values are needed, you will need to copy these generic template values into your new file.
[/note]

*** About Ubuntu kernels

MAAS supports four types of kernels for its Ubuntu machines:

- General availability kernels
- Hardware enablement kernels
- Hardware enablement kernels (pre-release)
- Low latency kernels

*** About general availability kernels

The *general availability* (GA) kernel is based on the *generic* kernel that ships with a new Ubuntu version. Subsequent fixes are applied regularly by the 'stable' *stream* used when setting up the global image source for MAAS.

MAAS denotes a GA kernel like this:

`ga-<version>`: The GA kernel reflects the major kernel version of the shipped Ubuntu release. For example, 'ga-16.04' is based on the 'generic' 4.4 Ubuntu kernel. As per Ubuntu policy, a GA kernel will never have its major version upgraded until the underlying release is upgraded.

*** About hardware enablement kernels

New hardware gets released all the time. If an Ubuntu host runs an older kernel, it's unlikely that MAAS can support the hardware. Canonical does make every effort to back-port more recent kernels enabling more hardware. The acronym HWE stands for "Hardware Enablement."

You also gain kernel improvements and new features when installing an HWE kernel.

[note]
There is the notion of an HWE *stack*, which refers to the window manager and kernel when the Ubuntu host is running a desktop environment. HWE stacks do not apply to MAAS since machines are provisioned strictly as non-graphical servers.
[/note]

Note that these back-ported/HWE kernels are only available for LTS releases (e.g. Trusty, Xenial, etc.). For example, the first available HWE kernel for Ubuntu 16.04 LTS (Xenial) will be the GA kernel from Ubuntu 16.10 (Yakkety).

Before MAAS 2.1 on Xenial, HWE kernels are referred to by the notation `hwe-<release letter>`. So, to install the Yakkety HWE kernel on Xenial, the `hwe-y` kernel is used. By default, when using the web UI, MAAS imports all available HWE kernels along with its generic boot images. So if you are importing Trusty images, then the following HWE kernels are included: `hwe-u`, `hwe-v`, `hwe-w`, `hwe-x` (presuming the Xenial HWE kernel is available).

In MAAS 2.1, starting with Xenial kernels, the notation has changed. The following is used to refer to the latest HWE kernel available for Xenial: `hwe-16.04`.

See [LTS Enablement Stack](https://wiki.ubuntu.com/Kernel/LTSEnablementStack)`↗` (Ubuntu wiki) for the latest information on HWE.

*** About pre-release hardware enablement kernels

The pre-release HWE kernel is known as the *edge* HWE kernel.

MAAS denotes the edge kernel like this: `hwe-<version>-edge`.

So 'hwe-16.04' is considered older than 'hwe-16.04-edge'.

See [Rolling LTS Enablement Stack](https://wiki.ubuntu.com/Kernel/RollingLTSEnablementStack#hwe-16.04-edge) (Ubuntu wiki)`↗` for more information.

*** About low latency kernels

The low-latency kernel is based on the GA kernel, but uses a more aggressive configuration to reduce latency. It is categorised as a soft real-time kernel. For more information, see [Criteria for real-time computing](https://en.wikipedia.org/wiki/Real-time_computing#Criteria_for_real-time_computing)`↗` (Wikipedia).

MAAS denotes a low latency kernel in three ways:

1.   `hwe-x-lowlatency`: the Xenial low latency HWE kernel for Trusty
2.   `ga-16.04-lowlatency`: the low latency GA kernel for Xenial
3.   `hwe-16.04-lowlatency`: the low latency HWE kernel for Xenial

*** About choosing a kernel

The kernel installed on a machine during deployment is, by default, the Ubuntu release's native kernel (GA). However, it is possible to tell MAAS to use a different kernel. Via the Web UI, MAAS can help you choose one of these kernels.  There are three different contexts for your choice:

1.   globally (default minimum enlistment and commissioning kernel)
2.   per machine (minimum deploy kernel)
3.   per machine during deployment (specific deploy kernel)

*** About kernel boot options

MAAS can specify kernel boot options to machines on both a global basis (UI and CLI) and a per-machine basis (CLI-only). A full catalogue of available options can be found in the [Linux kernel parameters list](https://www.kernel.org/doc/html/latest/admin-guide/kernel-parameters.html)`↗` at [kernel.org](https://www.kernel.org)`↗`.

*** About resource pools

Resource pools allow administrators to logically group resources -- machines and VM hosts -- into pools. Pools can help you budget machines for a particular set of functions.  For example, if you're using MAAS to manage a hospital data centre, you may want to keep a certain number of machines reserved for provider use, whether that be for the charts, documentation, or orders application.  You can use resource pools to reserve those machines, regardless of which of the three applications you end up loading onto a particular machine at any given time. 

Administrators can manage resource pools on the Machines page in the web UI, under the Resource pools tab, or with the MAAS CLI.   Also note that all MAAS installations have a resource pool named "default." MAAS automatically adds new machines to the default resource pool.

** About customising deployed machines

[tabs]
[tab version="v3.4 Snap,v3.4 Packages,v3.3 Snap,v3.3 Packages,v3.2 Snap,v3.2 Packages"]
MAAS 3.2 provides the capability to customise deployed machines, in that you can update hardware for a running machine on-the-fly.  Specifically, MAAS will update a deployed machine’s data when you do any of the following things:

- add or remove disks
- add or remove network interfaces
- add or remove PCI devices
- add or remove USB devices

You can find the [procedure here](/t/how-to-customise-machines/5108#heading--how-to-enable-hardware-sync-on-a-machine).

While deploying a machine, you can configure that machine to periodically sync its [hardware configuration](#heading--about-hardware-sync).  Deployed machines will also  passively update changes to the BMC and tags for that machine, as these changes are made.

*** About updating hardware

Updating hardware on a deployed machine works by installing a special binary on the deployed machine.   This binary is configured at a given interval and push hardware info to the MAAS metadata endpoint.  By setting “enable_hw_sync” to true on a machine prior to deployment, MAAS will add configuration to install a systemd service and timer that will download the hardware sync binary.  This binary then authenticates the machine, reads the hardware info from the machine and pushes it to MAAS. The interval is set globally in the MAAS settings.

Any changes in hardware are written to the machine’s configuration.  Physical hardware changes will be preserved upon release, while virtual changes, such as a SR-IOV interface, will be dropped.

When deploying a machine from the UI, there is a new “enable_hw_sync” flag available for each machine. This flag marks a machine to be configured with live hardware updates.

When deploying from the CLI, there is an additional `enable_hw_sync` flag on `maas $PROFILE machine deploy`. This flag also marks a machine to be configured with live hardware updates. 

When using the API, there are two additional fields in the request:

- enable_hw_sync: (Boolean) - indicating whether hardware sync should be enabled on the machine, 
- sync_interval: (Int) - indicating the interval, in seconds, that should be set at time of deployment

With respect to `machine.read`, both the RESTful API and Websocket API add the following fields to a response:

- enable_hw_sync: Bool indicating whether hardware sync is enabled on the machine, 
- last_sync: Timestamp of the last time MAAS received hardware sync data for the machine,
- next_sync: Timestamp of the computed estimation of when the next sync should happen,
- sync_interval:  Int the interval, in seconds, that was set at time of deployment
- is_sync_healthy: Bool indicating the sync is working normally when true, false when a sync is late or missing,

With respect to `config.list`, there is a new WebSocket Response result (new “hardware_sync_interval” option):

```nohighlight
[{
   name: "hardware_sync_interval",
   value: String in systemd time span format  e.g. “15m”
	        (only hours, minutes and seconds are recognised)
},…]

 -  hardware_sync_interval is set to `15m` by default
config.update
WebSocket Request params - new “hardware_sync_interval” param

params: {
  name: "hardware_sync_interval",
  value: String in systemd time span format, e.g. “15m”
 }
```

[note]
The API does not throw errors when an invalid string is provided for these parameters.
[/note]

*** About hardware sync

Hardware sync updates the machine’s blockdevice, interface and device sets.  BMC configuration and tags can also be updated on the machine itself. The timestamps of the last sync and the next scheduled sync can be seen in the machine's data.

[/tab]
[tab version="v3.1 Snap,v3.1 Packages,v3.0 Snap,v3.0 Packages,v2.9 Snap,v2.9 Packages"]
[note]
The capability to customise deployed machines is available starting with MAAS version 3.2.
[/note]
[/tab]
[/tabs]

* Measuring MAAS performance
The MAAS engineering team actively works to improve the performance of MAAS.

** Recent performance measurements

Recently, we improved the API performance of MAAS, by testing it with simulated loads.  For this testing, we made the following assumptions:

- five rack controllers
- 48 machines per fabric
- five VMs per LXD host
- three different architectures
- six disks per machine, randomly defined as flat, RAID, LVM, and BCACHE disks
- five network interfaces per machine
- machines in a random status, but mostly Ready or Deployed (which best emulates a real-world scenario)

To measure performance, we use continuous performance monitoring, arranged like this:

<a href="https://discourse.maas.io/uploads/default/original/2X/d/d8a0887dd9d6f01311966c10f5d9093feb76806f.png" target = "_blank"><img src="https://discourse.maas.io/uploads/default/original/2X/d/d8a0887dd9d6f01311966c10f5d9093feb76806f.png"></a>

On a daily basis, we generate simulation data based on the assumptions above, for 10, 100, and 1000 machines.  These three datapoints help us get a sense of how our performance improvements scale.  A Jenkins tool exercises both the REST API and the WebSocket API, trapping the results in a database, from which we can build a dashboard.  The dashboard looks something like this:

<a href="https://discourse.maas.io/uploads/default/original/2X/f/f5f831164e70273e81b4120b442469f665e16b47.png" target = "_blank"><img src="https://discourse.maas.io/uploads/default/original/2X/f/f5f831164e70273e81b4120b442469f665e16b47.png"></a>

Note that we always compare the current stable release with the release in development, so that we can spot issues before they become harder to find and fix.  We also capture profiling data that allows us to find bottlenecks, generating a heatmap that shows which parts of the code are causing issues at the moment.

For example, comparing MAAS 3.2 to MAAS 3.1, machine listings load, on average, 32% faster for the datasets we're using.  

*** Performance efforts to date

Here's a short history of our performance efforts to date:

- This [video show-and-tell](https://discourse.maas.io/t/maas-show-and-tell-is-maas-fast-yet/6105)`↗` documents recent efforts to improve MAAS peformance, with quantitative results.
- Here's some [work done by the UI team](https://discourse.maas.io/t/maas-ui-improving-the-performance-of-maas-ui/5820)`↗` to improve the performance of the UI.

Note that this list only captures the bigger, sustained efforts, although there is a constant focus on weeding out slowdowns when we come across them.

** Collecting your own metrics

It's possible to [collect your own MAAS metrics](/t/how-to-observe-a-live-maas/5204) -- and even share them with the MAAS engineering team.  We are keen to know everything we can about machine counts, network sizes, and MAAS performance in all areas.  Please use the [discourse performance forum](https://discourse.maas.io/c/maas-performance/26)`↗` to share your feedback and observations.

<a href="#headings--recent-developments">** Recent developments

As part of the MAAS 3.2 development effort, we have taken steps to improve the performance of machine listings. To date, we have measured the speed of listing a large number (100-1000) of machines via the REST API to be 32% faster, on average.

<a href="#headings--next-steps">** Next steps

Currently, we are actively working to improve MAAS performance for other operations, such as search.

* Monitoring MAAS

* Networking
Congratulations!  Now that you've [gotten started with MAAS](/t/how-to-get-started-with-maas/6202), configuring your MAAS networks is an important next step.  This major section will help you learn about the theory and practice of MAAS networking.

If you're eager to get started, you can go ahead and [connect your MAAS networks](/t/how-to-connect-maas-networks/5164) and [set up DHCP](/t/how-to-enable-dhcp/5132). You might even want to create some [availability zones](/t/how-to-manage-availability-zones/5152).

But don't feel rushed -- it depends on your comfort level with [MAAS networking](#heading--How-MAAS-networks) and [DHCP itself](#heading--Its-always-DHCP).  There are detailed explanations below, along with some catch-up summaries of [TCP/IP](#heading--TCP-IP-primer) and [cloud networking](#heading--About-cloud-networks).  Scan these first if you feel the need.

** How MAAS networks-connect

Some elements of MAAS networking are unique to the product, while others are standard networking concepts that are uniquely applied to MAAS.  This section will help you learn:

- [PXE booting is essential](#heading--about-pxe-booting)
- [Power drivers define machines](#heading--about-power-drivers)
- [Proxies](#heading--about-proxies)
- [RPC](#heading--about-RPC)
- [What is network discovery?](#heading--about-network-discovery)
- [Subnets](#heading--subnets)
- [VLANs](#heading--vlans)
- [You manage subnets](#heading--about-subnet-management)
- [MAAS is IPv6-enabled](#heading--about-ipv6)
- [Availability zones](#heading--about-availability-zones)

*** PXE booting is essential

PXE booting, or [Preboot eXecution Environment](https://en.wikipedia.org/wiki/Preboot_Execution_Environment)`↗`, refers to the ability to boot a machine via a Network Interface Card (NIC).  PXE booting requires a Network Interface Card (NIC)`↗` which is equipped with a PXE API which can be accessed by the server wishing to boot the device.

This PXE API has a very small hardware footprint, both to keep NIC density smaller (small footprint PXE ROMs) and to simplify the PXE boot process.  Typically a PXE-capable NIC implements the PXE API with a simple universal network device interface, a tiny UDP/IP stack, a special pre-boot DHCP module, and a trivial file transfer protocol (TFTP) module.  TFTP does have low throughput, but it has been amended twice, once with [better block sizes](https://datatracker.ietf.org/doc/html/rfc2348)`↗`, and once with [better window sizes](https://datatracker.ietf.org/doc/html/rfc7440)`↗`.  Both of these help to allow for payload deliveries large enough to accommodate basic bootable images, such as the ephemeral Ubuntu image that MAAS transmits, via TFTP, in order to commission a machine.

PXE absolutely depends on its bundled, [load-balancing DHCP server](/t/how-to-enable-high-availability/5120#heading--dhcp-ha), which kicks off the process by assigning an IP address.  With this offer, the DHCP server also sets the `next-server` address to the TFTP server.  This allows the booting machine to make a TFTP request, allowing the TFTP server to send a network boot program (NBP) and associated peripheral files.  Considered in detail, the sequence is as follows:

- The machine in question is powered on via its baseboard management controller (BMC).
- The UEFI PXE firmware broadcasts a DHCPDISCOVER message (customised for PXE) to the UDP port (67), requesting network config and booting parameters.
- If the machine receives a PXE-enabled DHCPOFFER, it sets its IP address and requests an NBP from the `next-server` address.
- The machine transfers the network booting program into its own RAM space, via TFTP.
- This NBP downloads an ephemeral kernel with initrd.

From there, a full ephemeral Ubuntu image is loaded and commissioning (or deployment) can take place.

Here are some related concepts you may want to explore:

- [UEFI](https://en.wikipedia.org/wiki/Unified_Extensible_Firmware_Interface)`↗`
- [BOOTP](https://en.wikipedia.org/wiki/Bootstrap_Protocol)`↗`
- [TFTP](https://en.wikipedia.org/wiki/Trivial_File_Transfer_Protocol)`↗`
- [HTTP booting](https://documentation.suse.com/sles/15-SP2/html/SLES-all/cha-deployment-prep-uefi-httpboot.html)`↗`
- [Bootstrap loading via TFTP](https://datatracker.ietf.org/doc/html/rfc906)`↗`

If you still have questions about PXE booting, please consider posting a question on our [discourse forum](https://discourse.maas.io/c/users/8)`↗`.

*** Power drivers define machines

Power drivers are units of software, embedded in MAAS, that interface with the [BMC](https://en.wikipedia.org/wiki/Intelligent_Platform_Management_Interface#Baseboard_management_controller)`↗` to power-cycle a machine remotely.  Different machines use different BMC configurations.  Typically, these vary by manufacturer, although there are some standard IPMI drivers that can be used.  In addition, it is sometimes possible to control a machine with a specialised BMC via a generic IPMI driver.

IPMI is designed as a set of protocols that bypass the system's CPU, BIOS, UEFI, and/or OS.  Essentially, IPMI provides a network connection directly to the hardware, allowing a number of management and monitoring functions.  From the perspective of MAAS, the main use of IPMI is to access the machine's BMC to power-cycle the machine.  In order for [PXE-booting](#heading--about-pxe-booting) to start, the machine itself must send a PXE-enabled DHCPDISCOVER, which requires the machine to be powered on.

Specific machine models have different IPMI parameters that can or must be used to successfully power on a machine, although many models respond reasonably well to standard IPMI or [Redfish](https://en.wikipedia.org/wiki/Redfish_(specification))`↗` commands. MAAS includes customised power drivers for all of the machines listed in the [power catalogue](/t/power-management-reference/5246#heading--power-catalogue)`↗`.

IPMI provides many other functions and capabilities besides power-cycling the machine, such as monitoring system state (e.g., temperature) and possibly adjusting some parameters remotely.  MAAS generally does not avail itself of these additional features.

*** Proxies

A [proxy server](https://en.wikipedia.org/wiki/Proxy_server)`↗` ("proxy service" or just "proxy") is an intermediary application that serves to broker network transactions between two hosts.  Proxies provide several benefits, including privacy (protecting internal IP addresses from discovery by those on other networks), security (performing some checks against incoming packets), and load-balancing (routing packets to multiple servers, based on actual load or some statistical algorithm)`↗`.  

MAAS provides an [internal proxy](/t/how-to-connect-maas-networks/5164#heading--internal-proxy-maas-proxy), which is an HTTP caching proxy server that is available to all hosts residing in any subnet managed by MAAS.  In addition, MAAS allows you to define an [external proxy](/t/how-to-connect-maas-networks/5164#heading--configure-proxy) if desired.

*** About the MAAS internal proxy

MAAS provides an internal proxy server. Although it is set up to work well with APT/package requests, it is effectively an HTTP caching proxy server. If you configure the MAAS region controller as the default gateway for the machines it manages then the proxy will work transparently (on TCP port 3128). Otherwise, machines will need to access it on TCP port 8000.

By default, the proxy is available to all hosts residing in any subnet detected by MAAS, not just MAAS-managed machines. It is therefore recommended to disable access to those subnets that represent untrusted networks.

[tabs]
[tab version="v3.4 Snap,v3.4 Packages,v3.3 Snap"]
MAAS manages its proxy. So although the active configuration, located in file `/var/snap/maas/current/proxy`, can be inspected, it is not to be hand-edited. The proxy is automatically installed with the MAAS snap.
[/tab]
[tab version="v3.3 Packages"]
MAAS manages its proxy. So although the active configuration, located in file `/var/lib/maas/maas-proxy.conf`, can be inspected, it is not to be hand-edited.

You must install the proxy on the same host as the region controller (via the 'maas-proxy' package).
[/tab]
[tab version="v3.2 Snap"]
MAAS manages its proxy. So although the active configuration, located in file `/var/snap/maas/current/proxy`, can be inspected, it is not to be hand-edited. The proxy is automatically installed with the MAAS snap.
[/tab]
[tab version="v3.2 Packages"]
MAAS manages its proxy. So although the active configuration, located in file `/var/lib/maas/maas-proxy.conf`, can be inspected, it is not to be hand-edited.

You must install the proxy on the same host as the region controller (via the 'maas-proxy' package).
[/tab]
[tab version="v3.1 Snap"]
MAAS manages its proxy. So although the active configuration, located in file `/var/snap/maas/current/proxy`, can be inspected, it is not to be hand-edited. The proxy is automatically installed with the MAAS snap.
[/tab]
[tab version="v3.1 Packages"]
MAAS manages its proxy. So although the active configuration, located in file `/var/lib/maas/maas-proxy.conf`, can be inspected, it is not to be hand-edited.

You must install the proxy on the same host as the region controller (via the 'maas-proxy' package).
[/tab]
[tab version="v3.0 Snap"]
MAAS manages its proxy. So although the active configuration, located in file `/var/snap/maas/current/proxy`, can be inspected, it is not to be hand-edited. The proxy is automatically installed with the MAAS snap.
[/tab]
[tab version="v3.0 Packages"]
MAAS manages its proxy. So although the active configuration, located in file `/var/lib/maas/maas-proxy.conf`, can be inspected, it is not to be hand-edited.

You must install the proxy on the same host as the region controller (via the 'maas-proxy' package).
[/tab]
[tab version="v2.9 Snap"]
MAAS manages its proxy. So although the active configuration, located in file `/var/snap/maas/current/proxy`, can be inspected, it is not to be hand-edited. The proxy is automatically installed with the MAAS snap.
[/tab]
[tab version="v2.9 Packages"]
MAAS manages its proxy. So although the active configuration, located in file `/var/lib/maas/maas-proxy.conf`, can be inspected, it is not to be hand-edited.

You must install the proxy on the same host as the region controller (via the 'maas-proxy' package).
[/tab]
[/tabs]

*** RPC

A [Remote Procedure Call](https://www.ibm.com/docs/en/aix/7.1?topic=concepts-remote-procedure-call)`↗`, or RPC, is a method by which one computer can execute a subroutine sent by another process or system.  These procedures run as if they were native to the machine executing them, even though they may have been prepared or coded on the requesting machine.  In the case of MAAS, [RPC is used for communication between the region and rack controllers](/t/how-to-tune-controllers/6498#heading--rackregion)`↗`, specifically to transfer the PXE configuration from region to rack.  This allows the relevant MAAS rack to answer the machine's DHCPDISCOVER with a DHCPOFFER that contains the correct PXE booting information to bring the machine to an ephemeral Ubuntu instance. 

*** What is network discovery?

MAAS constantly listens to the network and reports any discovered devices. Devices are identified when the rack controller observes them communicating on an attached IPv4 subnet. Discovered devices that do not correspond to machines and devices already known to MAAS can be listed via the CLI. If a device advertises a hostname using `mDNS` (such as with `avahi` or `Bonjour`), MAAS will also present the discovered hostname when listing devices.

[tabs]
[tab version="v3.4 Snap,v3.4 Packages,v3.3 Snap,v3.3 Packages,v3.2 Snap,v3.2 Packages,v3.1 Snap,v3.1 Packages,v3.0 Snap,v3.0 Packages,v2.9 Snap,v2.9 Packages" view="UI"]
Using the Dashboard, an unknown discovered device can be added to MAAS as a device or as a network interface belonging to a machine or device. Clicking the down arrow to the right of a new device allows values such as 'Type', 'Domain', 'IP Assignment' and 'Parent' to be changed prior to the device being added. Selecting a Parent device is optional.
[/tab]
[tab version="v3.4 Snap,v3.4 Packages,v3.3 Snap,v3.3 Packages,v3.2 Snap,v3.2 Packages,v3.1 Snap,v3.1 Packages,v3.0 Snap,v3.0 Packages,v2.9 Snap,v2.9 Packages" view="CLI"]
An unknown discovered device can be added to MAAS as a device, or as a network interface belonging to a machine or device. You can specify values such as 'Type', 'Domain', 'IP Assignment' and 'Parent' to be changed prior to the device being added. Indicating a Parent device is optional.
[/tab]
[/tabs]

*** Subnets

A [subnet](https://en.wikipedia.org/wiki/Subnetwork#firstHeading)`↗` is a "layer 3" network, defined by a network address and a network mask length (in bits)`↗` and usually written in "CIDR" format. MAAS supports IPv4 and IPv6 subnets. Examples include:

``` no-highlight
- 0.0.0/8
- 16.0.0/12
- 168.0.0/16
2001:db8:4d41:4153::/64
```
*** About the subnet summary

The **Subnet summary** section is the largest and most complex of the subnet configuration screens.  It presents the following configurable options:

- **Name**: Subnet names can be any valid text string. By default, they are named with the CIDR of the subnet itself.

- **CIDR**: This is the address parameter for the subnet.  In keeping with standard CIDR notation, the number of bits of the prefix are indicated after the slash.

- **Gateway IP**: This is the address of the default gateway for your subnet, which is the IP address that transfers packets to other subnets or networks. Typically, this is simply the first IP address in a block of addresses (the `.1` address).

- **DNS**: This is the address of a DNS (domain name server, or simply "name server") for your subnet.  It's optional, but can be configured if desired.

- **Description**: This field represents free form text that you can enter to describe your subnet, as needed to keep important notes attached to the definition of the subnet.

- **Managed allocation** refers to the ability of MAAS to completely [manage a subnet](#heading--about-managed-subnets).

- **Active mapping** instructs MAAS to scan the subnet every 3 hours to discover hosts that have not been discovered passively.

- **Proxy access** instructs MAAS to allow clients from this subnet to access the MAAS proxy.

- **Allow DNS resolution** allows subnet clients to use MAAS for DNS resolution.

- **Fabric**: This field allows you to set the subnets fabric.

- **VLAN**: This field allows you to set the subnets VLAN.

- **Space** is presented for clarity, though spaces are managed at the VLAN level.

*** Subnet utilisation

- 'Subnet addresses' shows the total number of addresses associated with the subnet. 

- 'Availability' shows how many of those addresses are unused, and therefore "available".

- 'Used' shows the percentage that is used.

*** IP modes

Four modes determine how a subnet address is assigned when MAAS deploys the machine. You can select one of these modes by clicking on the "IP mode" drop-down menu:

-   **Auto assign**: MAAS will assign a random static address (`iface eth0 inet static`). The pool of available addresses depends on whether the subnet is managed or unmanaged (see [Subnet management](/t/how-to-connect-maas-networks/5164#heading--how-to-toggle-subnet-management)).

-   **Static assign**: The administrator will specify a static address using a secondary field.

-   **DHCP**: The machine leases a dynamic IP address, via either MAAS-managed DHCP or an external DHCP server.

-   **Unconfigured**: The interface is not configured.

*** VLANs

[VLANs](https://en.wikipedia.org/wiki/Virtual_LAN#firstHeading) (Virtual LANs)`↗` are a common way to create logically separate networks using the same physical infrastructure.

Managed switches can assign VLANs to each port in either a "tagged" or an "untagged" manner. A VLAN is said to be "untagged" on a particular port when it is the default VLAN for that port and requires no special configuration to access it.

You can use also use tagged VLANs with MAAS nodes. If a switch port is configured to allow tagged VLAN frames from a MAAS node, that node can automatically access interfaces on that VLAN.

A "Default VLAN" is created for every fabric, to which every new VLAN-aware object in the fabric will be associated with by default (unless specified otherwise).

*** You manage subnets

Fabrics, VLANs, and spaces do not require much configuration beyond names and descriptions. You can change the MTU for a VLAN, as well as [enable DHCP](/t/how-to-enable-dhcp/5132#heading--enabling-dhcp).  None of these options requires detailed instruction.

This subsection will help you learn:

- [About managed subnets](#heading--about-managed-subnets)
- [About unmanaged subnets](#heading--about-unmanaged-subnets)
- [About IP address tracking](#heading--about-ip-address-tracking)

A [subnet](https://en.wikipedia.org/wiki/Subnetwork)`↗`, on the other hand, provides a number of configuration options relevant to the day-to-day operation of MAAS. By default, MAAS manages subnets in your configuration, but this is easily changed.

When a subnet is managed, MAAS handles all aspects of IP address allocation. This process includes managing DHCP leases and assigned static addresses. Typically MAAS would have one managed subnet, with any additional subnets unmanaged. This arrangement allows for more control over which subnet gets used for DHCP. Additionally, as detailed below, an unmanaged subnet treats reserved IP ranges differently, in a way that some administrators find more intuitive.

**** About managed subnets

When you enable management for a subnet, MAAS will:

- Lease addresses for DHCP from a reserved dynamic IP range
- Assign static addresses not included in a reserved IP range, typically via 'Auto assign' IP allocation mode for a node.

See [the glossary](/t/maas-glossary/5416#heading--ip-ranges) for an explanation of the two kinds of reserved IP ranges MAAS uses.

If needed, you can also define a static route between two subnets. A static route is defined on a per-subnet basis to use a particular gateway, using a configured destination.

**** About unmanaged subnets

When management is disabled for a subnet, the definition of a reserved IP range differs from the managed mode. Here, a reserved IP range tells MAAS to **only allocate addresses from this range** (via 'Auto assign'). Also, DHCP will never lease addresses from an unmanaged subnet.

**** About IP address tracking

When you enable IP address tracking, MAAS will keep track of all assigned addresses, regardless of whether they come from managed or unmanaged subnets.

*** MAAS is IPv6-enabled

Support for IPv6 in MAAS is similar to support for IPv4.  This subsection will help you learn:

- [About enabling IPv6](#heading--about-enabling-ipv6)
- [About IPv6 subnets](#heading--about-ipv6-subnets)
- [About IPV6 routing](#heading--about-ipv6-routing)

A rack controller in an IPv6 context needs to have the region API server URL specified with brackets:

``` nohighlight
http://[::1]:5240/MAAS/
```

You can access the Web UI and the [MAAS CLI](/t/try-out-the-maas-cli/5236) (that is, logging in to the API server) in the same way on both IPv4 and IPv6. To use an IPv6 address in a URL, surround it with square brackets. For example, on the local machine (`::1`, the IPv6 equivalent of `localhost`):

[note]
MAAS can only control most BMCs using IPv4.
[/note]

**** About enabling IPv6

You enable IPv6 networking in the same way that you enable IPv4 networking: configure a separate rack controller interface for your IPv6 subnet. The IPv6 interface must define a static address range. Provided that you already have a functioning IPv6 network, that's all there is to it. The following sections explain requirements, supported operations, and what to do if you don't yet have a functioning IPv6 network.

An IPv6 interface can use the same network interface on the rack controller as an existing IPv4 network interface. It just defines a different subnet, with IPv6 addressing. A machine that's connected to the IPv4 subnet also connected to the IPv6 subnet on the same network segment.

**** About IPv6 subnets

If you define a reserved static IP range, then machines deployed on the subnet will get a static address in this range. Since IPv6 networks usually are 64 bits wide, you can be generous with the range size. You can typically leave the netmask and broadcast address fields blank.

You may want MAAS to manage DHCP and DNS, but it's not required. Machines do not need a DHCP server at all for IPv6; MAAS configures static IPv6 addresses on a machine's network interface while deploying it. A DHCPv6 server can provide addresses for containers or virtual machines running on the machines, as well as devices on the network that are not managed by MAAS. The machines do not need DHCPv6. MAAS will not be aware of any addresses issued by DHCP, and cannot guarantee that they will stay unchanged.

**** About IPV6 routing

In IPv6, clients do not discover routes through DHCP. Routers make themselves known on their networks by sending out route advertisements. These RAs also contain other configuration items:

- Switches that allow stateless configuration of their unique IP addresses, based on MAC addresses. 
- Switches that enable them to request stateless configuration from a DHCP server.
- Switches that In any allow them to request a stateful IP address from a DHCP server. 

Since a network interface can have any number of IPv6 addresses even on a single subnet, several of these address assignment mechanisms can be combined.

However, when MAAS configures IPv6 networking on a machine, it does not rely on RAs. It statically configures a machine's default IPv6 route to use the router that is set on the cluster interface, so that the machine will know their default gateway. They do not need DHCP and will not auto-configure global addresses.

You may be planning to operate DHCPv6 clients as well, for example, on machines not managed by MAAS, or on virtual machines hosted by MAAS machines.  If this is the case, you may want to configure RAs, so that those clients obtain configuration over DHCP.

If you need RAs, but your gateway does not send them, you could install and configure `radvd` somewhere on the network to advertise its route.

** About MAAS and DHCP

MAAS enlists and commissions machines through the use of its [load-balancing DHCP server](/t/how-to-enable-high-availability/5120#heading--dhcp-ha) running on an untagged VLAN. Although this MAAS-managed DHCP can also be part of the deploy phase, an external DHCP server can optionally be used instead for this purpose. If MAAS detects an external DHCP server, it will display it on the rack controller's page, accessible by selecting 'Controllers' from the top menu in the web UI.

In addition, the machine subnet is usually on the untagged VLAN. If not, you will need to route DHCP packets between the subnet and the MAAS-provided DHCP subnet. It is also possible to forward DHCP traffic from one VLAN to another using an external DHCP relay service.

This documentation presupposes that MAAS-managed DHCP is used to enlist and commission machines.  Using an external DHCP server for enlistment and commissioning may work, but note that this is not supported. MAAS cannot manage an external DHCP server, nor can it keep leases synchronised when you return a machine to the pool.  Also, an external DHCP server may prevent you from commissioning and deploying machines, because the machine receives the address of its TFTP boot server in the DHCP `next-server` statement, which is part of MAAS DHCP.

*** About IP conflicts

In some cases, MAAS manages a subnet that is not empty, which could result in MAAS assigning a duplicate IP address.  MAAS is capable of detecting IPs in use on a subnet.  Be aware that there are two caveats:

1. If a previously-assigned NIC is in a quiescent state or turned off, MAAS may not detect it before duplicating an IP address.

2. At least one rack controller must have access to the IP-assigned machine in order for this feature to work.

MAAS also recognises when the subnet ARP cache is full, so that it can re-check the oldest IPs added to the cache to search for free IP addresses.

*** External DHCP and reserved IP ranges

If an external DHCP server is used to deploy machines, then a reserved IP range should be created to prevent the address namespace from being corrupted. For instance, address conflicts may occur if you set a machine's IP assignment mode to 'Auto assign' in the context of an external DHCP server. Such a range should correspond to the lease range of the external server.

*** About DHCP relays

You should not enable DHCP relays in MAAS without sufficient planning.  In particular, MAAS does not provide the actual relay. It must be set up as an external service by the administrator. What MAAS does provide is the DHCP configuration that MAAS-managed DHCP requires in order to satisfy any client requests relayed from another VLAN.

*** About MAAS DHCP snippets

[tabs]
[tab version="v3.4 Snap,v3.4 Packages,v3.3 Snap" view="UI"]
When MAAS manages DHCP, you customise it through the use of DHCP snippets. These are user-defined configuration options that can be applied either globally, per subnet, or per machine. You apply a global snippet to all VLANs, subnets, and machines. All three types end up in `/var/snap/maas/common/maas/dhcpd.conf` or `/var/snap/maas/common/maas/dhcpd6.conf`. Be aware that if you edit these files directly, you will need to `sudo` to `root`, as there is no `maas` user in the snap (all relevant files are owned by `root`). For information on what options to use, refer to the [`dhcpd.conf` man page](http://manpages.ubuntu.com/cgi-bin/search.py?q=dhcpd.conf)`↗`.
[/tab]
[tab version="v3.3 Packages" view="CLI"]
When MAAS manages DHCP, you customise it through the use of DHCP snippets. These are user-defined configuration options that can be applied either globally, per subnet, or per machine. You apply a global snippet to all VLANs, subnets, and machines. All three types end up in `/var/lib/maas/dhcpd.conf` or `/var/lib/maas/dhcpd6.conf`. For information on what options to use, refer to the [`dhcpd.conf` man page](http://manpages.ubuntu.com/cgi-bin/search.py?q=dhcpd.conf)`↗`.
[/tab]

[tab version="v3.2 Snap" view="UI"]
When MAAS manages DHCP, you customise it through the use of DHCP snippets. These are user-defined configuration options that can be applied either globally, per subnet, or per machine. You apply a global snippet to all VLANs, subnets, and machines. All three types end up in `/var/snap/maas/common/maas/dhcpd.conf` or `/var/snap/maas/common/maas/dhcpd6.conf`. Be aware that if you edit these files directly, you will need to `sudo` to `root`, as there is no `maas` user in the snap (all relevant files are owned by `root`). For information on what options to use, refer to the [`dhcpd.conf` man page](http://manpages.ubuntu.com/cgi-bin/search.py?q=dhcpd.conf)`↗`.
[/tab]
[tab version="v3.2 Packages" view="CLI"]
When MAAS manages DHCP, you customise it through the use of DHCP snippets. These are user-defined configuration options that can be applied either globally, per subnet, or per machine. You apply a global snippet to all VLANs, subnets, and machines. All three types end up in `/var/lib/maas/dhcpd.conf` or `/var/lib/maas/dhcpd6.conf`. For information on what options to use, refer to the [`dhcpd.conf` man page](http://manpages.ubuntu.com/cgi-bin/search.py?q=dhcpd.conf)`↗`.
[/tab]
[tab version="v3.1 Snap" view="UI"]
When MAAS manages DHCP, you customise it through the use of DHCP snippets. These are user-defined configuration options that can be applied either globally, per subnet, or per machine. You apply a global snippet to all VLANs, subnets, and machines. All three types end up in `/var/snap/maas/common/maas/dhcpd.conf` or `/var/snap/maas/common/maas/dhcpd6.conf`. Be aware that if you edit these files directly, you will need to `sudo` to `root`, as there is no `maas` user in the snap (all relevant files are owned by `root`). For information on what options to use, refer to the [`dhcpd.conf` man page](http://manpages.ubuntu.com/cgi-bin/search.py?q=dhcpd.conf)`↗`.
[/tab]
[tab version="v3.1 Packages" view="CLI"]
When MAAS manages DHCP, you customise it through the use of DHCP snippets. These are user-defined configuration options that can be applied either globally, per subnet, or per machine. You apply a global snippet to all VLANs, subnets, and machines. All three types end up in `/var/lib/maas/dhcpd.conf` or `/var/lib/maas/dhcpd6.conf`. For information on what options to use, refer to the [`dhcpd.conf` man page](http://manpages.ubuntu.com/cgi-bin/search.py?q=dhcpd.conf)`↗`.
[/tab]
[tab version="v3.0 Snap" view="UI"]
When MAAS manages DHCP, you customise it through the use of DHCP snippets. These are user-defined configuration options that can be applied either globally, per subnet, or per machine. You apply a global snippet to all VLANs, subnets, and machines. All three types end up in `/var/snap/maas/common/maas/dhcpd.conf` or `/var/snap/maas/common/maas/dhcpd6.conf`. Be aware that if you edit these files directly, you will need to `sudo` to `root`, as there is no `maas` user in the snap (all relevant files are owned by `root`). For information on what options to use, refer to the [`dhcpd.conf` man page](http://manpages.ubuntu.com/cgi-bin/search.py?q=dhcpd.conf)`↗`.
[/tab]
[tab version="v3.0 Packages" view="CLI"]
When MAAS manages DHCP, you customise it through the use of DHCP snippets. These are user-defined configuration options that can be applied either globally, per subnet, or per machine. You apply a global snippet to all VLANs, subnets, and machines. All three types end up in `/var/lib/maas/dhcpd.conf` or `/var/lib/maas/dhcpd6.conf`. For information on what options to use, refer to the [`dhcpd.conf` man page](http://manpages.ubuntu.com/cgi-bin/search.py?q=dhcpd.conf)`↗`.
[/tab]
[tab version="v2.9 Snap" view="UI"]
When MAAS manages DHCP, you customise it through the use of DHCP snippets. These are user-defined configuration options that can be applied either globally, per subnet, or per machine. You apply a global snippet to all VLANs, subnets, and machines. All three types end up in `/var/snap/maas/common/maas/dhcpd.conf` or `/var/snap/maas/common/maas/dhcpd6.conf`. Be aware that if you edit these files directly, you will need to `sudo` to `root`, as there is no `maas` user in the snap (all relevant files are owned by `root`). For information on what options to use, refer to the [`dhcpd.conf` man page](http://manpages.ubuntu.com/cgi-bin/search.py?q=dhcpd.conf)`↗`.
[/tab]
[tab version="v2.9 Packages" view="CLI"]
When MAAS manages DHCP, you customise it through the use of DHCP snippets. These are user-defined configuration options that can be applied either globally, per subnet, or per machine. You apply a global snippet to all VLANs, subnets, and machines. All three types end up in `/var/lib/maas/dhcpd.conf` or `/var/lib/maas/dhcpd6.conf`. For information on what options to use, refer to the [`dhcpd.conf` man page](http://manpages.ubuntu.com/cgi-bin/search.py?q=dhcpd.conf)`↗`.
[/tab]
[/tabs]

When you create a snippet, MAAS enables it by default.

[note]
Modifications made directly to `dhcpd.conf.template` or `dhcpd6.conf.template` are not supported.
[/note]

** MAAS IP ranges

In MAAS-managed networks, you can further manage your subnets with a reserved range of IP addresses.  You can reserve IP addresses by adding one or more reserved ranges to a subnet configuration. You can define two types of ranges: reserved ranges and reserved dynamic ranges.  

A reserved range operates differently depending on whether the subnet is managed or unmanaged.  For a managed (subnet), MAAS will never assign IP addresses inside this range.  You can use this range for anything, such as infrastructure systems, network hardware, external DHCP, or an OpenStack namespace.  For an unmanaged (subnet), MAAS will only assign IP addresses inside this range -- but MAAS can assign any IP within this range.

A reserved dynamic range is used by MAAS for enlisting, commissioning and, if enabled, MAAS-managed DHCP on the machine's VLAN during commissioning and deployment. If created with the Web UI, an initial range is created as part of the DHCP enablement process. MAAS never uses IP addresses from this range for an unmanaged subnet.

** DHCP primer

The Dynamic Host Control Protocol or DHCP is a key part of how MAAS is able to manage bare-metal servers.  Many issues with MAAS revolve around misunderstanding -- or unintentional misuse -- of DHCP, so it's worth it to take an in-depth look.  This section will help you learn:

- [About DORA](#heading--about-dora)
- [About DHCP traffic](#heading--about-dhcp-traffic)
- [About DHCP standard message types](#heading--about-dhcp-standard-message-types)
- [About DHCP address allocation](#heading--about-dhcp-address-allocation)
- [About multiple DHCP servers serving different IP ranges](#heading--about-multiple-dhcp-servers-diff-ips)
- [About multiple DHCP servers serving overlapping IP ranges](#heading--about-multiple-dhcp-servers-overlapping-ranges)
- [About DHCP relays](#heading--about-dhcp-relays)

Consider the analogy of assigning a street address to your house, which we've already used earlier. Usually, this is done by the local 911 dispatch office, or some other central authority. They typically use either a survey map or a latitude/longitude pair to locate you, before they assign your house numbers from a pool of available addresses, compatible with other addresses in the area.

Let’s assume that you’re not sure who to call, so you just broadcast a message to everyone in range — your neighbours, the local authorities, etc. You explain that you’re at a given latitude and longitude, and that you want to REQUEST an address.

“Hello. My LatLong is 80.0, 35.7. I need a street address! Can someone out there help me?”

It’s a long shot, you know, but it’s the only thing you can think of at the moment. And — surprise — someone authoritative answers you with an OFFER.

“Hello, LatLong 80.0, 35.7! I have a unique address of 62 Wimberly Place, here are the details. By the way, I am at 46 Reardon Lane if you need to reach me.”

You’re not going to miss this chance, so you let the authority know that, yes, you do want that address, just as they gave it to you. In fact, just to make sure, you formally REQUEST the address.

"Hello? 46 Reardon Lane? I’d like to formally request that 62 Wimberly Place address. Is it still available?"

Happily, the authority ACKNOWLEDGES that the address is yours, or at least, for a while, anyway.

"LatLong 80.0, 35.7, that address is still available. I’m marking it down to you! Enjoy, but be aware that you'll need to reclaim this address every three years."

We used “LatLong 80.0, 35.7” because this particular negotiation is basically carried out by shouting across the fences. Even so, the two participants have to know for sure they’re talking to the right party, so identifiers have to be used that are guaranteed to be unique. 62 Reardon Lane has their address, so they’re already unique. 80.0, 35.7 has no address, so they have to use something else that uniquely identifies them.

*** About DORA

The DHCP negotiation process is known as DORA: Discover, Offer, Request, Acknowledge. Just like the exchange above, it’s carried out by the network equivalent of shouting, that is, broadcast exchanges. While the payloads for the messages carry unique addressing information (after the first DISCOVER message), the Destination IP (DIP) is always 255.255.255.255 — broadcast mode. Every machine on the network will see the packet, but they will ignore it when they look at the payload.

By the way, there are several other possible responses at various points in the exchange, such as NACK (from the DHCP server, when the client has waited too long to make the request) and DECLINE (e.g., when the IP address configuration offered isn’t usable by the client).

The network version of the shouting match above looks something like this:

<a href="https://discourse.maas.io/uploads/default/original/2X/a/a8c1756c1f0a9309fa01f1f5ccc0573e33e436fa.jpeg" target="_blank"><img src="https://discourse.maas.io/uploads/default/original/2X/a/a8c1756c1f0a9309fa01f1f5ccc0573e33e436fa.jpeg"></a>

Message 1 ("DISCOVER") is the only one that carries no destination info in its payload.  The
entire exchange takes place in broadcasts, rather than addressed packets. Said differently, the DIP (Destination IP address) is always 255.255.255.255, the broadcast address. The exchange becomes semi-private with the first OFFER, via the use of message payloads.

*** About DHCP traffic

In fact, it’s worthwhile to (very briefly) consider a DHCP packet exchange. Typically, it looks something like this:

``` nohighlight
# DHCP Discover
Ethernet Header: DA=FF-FF-FF-FF-FF-FF, SA=<client MAC>
IP Header: SIP=0.0.0.0, DIP=255.255.255.255
DHCP Payload: Client MAC=<client MAC>

# DHCP Offer
Ethernet Header: DA=FF-FF-FF-FF-FF-FF, SA=<DHCP server MAC>
IP Header: SIP=<DHCP server IP address>, DIP=255.255.255.255
DHCP Payload: Offered IP=<offered IP>, Client MAC=<client MAC>,
Subnet Mask=<Subnet mask>, Router IP=<router IP>, 
DNS=<DNS server1 IP, DNS server2 IP>, IP Lease Time=<time>s,
DHCP Server Identifier=<DHCP server IP address>

# DHCP Request
Ethernet Header: DA=FF-FF-FF-FF-FF-FF, SA=<client MAC>
IP Header: SIP=0.0.0.0, DIP=255.255.255.255
DHCP Payload: Client MAC=<client MAC>, 
Requested IP Address=<offered IP>, 
DHCP Server Identifier=<DHCP server IP address>

# DHCP Ack
Ethernet Header: DA=FF-FF-FF-FF-FF-FF, SA=<DHCP server MAC>
IP Header: SIP=<DHCP server IP address>, DIP=255.255.255.255
DHCP Payload: Offered IP=<offered IP>, Client MAC=<client MAC>,
Subnet Mask=<Subnet mask>, Router IP=<router IP>, 
DNS=<DNS server1 IP, DNS server2 IP>, IP Lease Time=<time>s,
DHCP Server Identifier=<DHCP server IP address>
```

A couple of clarifications can come in handy here:

- The DHCP server provides all the needed configuration parameters in both the OFFER and the ACK messages. Nothing is left out: the client receives an IP address, a subnet mask, a router location, the IPs of both DNS servers, the lease time limit, and the identity of the DHCP server. Nothing else is necessary for the requesting client to begin operating independently on the network.

- The requesting client does not start using the offered IP address until it has received the ACK message. Note that in the DHCP Request, above, the SIP (Source IP) is still 0.0.0.0, even though the client knows the offered IP (it sends it back in the request to confirm).

With that very basic view of DHCP, we can now talk about how multiple DHCP servers and multiple requesting clients can keep things straight.

*** About DHCP standard message types

DHCP is, technically, a network management protocol. In other words, it’s part of a collection of hardware and software tools that help to manage network traffic. DHCP is designed to automatically assign IP addresses and other communication parameters, such as default gateway, domain name, name server IPs, or time server IPs to clients. 

There are (at least) two participants in a DHCP transaction: a server and a client, but the client has to meet some requirements to participate. Specifically, the client has to implement an instance of the DHCP protocol stack; without that, it has no idea how to formulate Discovery and Request packets, nor can it recognise Offers or Acknowledgements (or NAKs, for that matter).

For what it’s worth, the “DHCP protocol stack” just means that a device can handle at least the following standard message types:

- **DHCPDiscover**: a broadcast message sent in the hopes of finding a DHCP server.  Note that clients that don’t get a DHCP response may be able to assign themselves an Automatic Private IPv4 address (APIPA), which should always be in the range 169.254.0.0/16. This is good to know, because you want to pretty much always leave that scope (that range of IP addresses) unused by anything else in your system.

- **DHCPOffer**: also a broadcast message, one that offers an IPv4 address lease; the lease is more than just an IP address, as we saw in the last DHCP blog.

- **DHCPRequest**: If you haven’t noticed by now, DHCP exchanges are little like rolling snowballs: they pick up more protocol information as they go and keep it for the duration of the transaction, sending it back and forth. In this case, the client sends back everything the DHCP server sent, along with a request to actually take the offered lease.

- **DHCPAcknowlegement**: If everything matches up when the DHCP server gets the Request, it responds with an Acknowledgement, which basically says, “Okay, you can lease this IP address for a set period of time.”

- **DHCPNak**: If the client waits too long to Request an Offer (generally, if a different server has already claimed the offered IP address), the DHCP server may respond with a Nak. This requires the client to start over again at Discover.

- **DHCPDecline**: If the client determines that, for some reason, the Offer has a configuration that won’t work for it, it can Decline the offer — that this also means it has to start again at Discover.

- **DHCPRelease**: When a client is done with an IP address, it can send a Release message to cancel the rest of the lease and return the IP address to the server’s available pool.

- **DHCPInform**: This is a relatively new message, which allows a client that already has an IP address to easily get other configuration parameters (related to that IP address) from a DHCP server.

Note that, shortly before a lease expires, most DHCP clients will renew the lease, often with a shortened form of the exchange (Request/Acknowledge) which does not require a full DORA exchange. Also, this renewal exchange takes place directly between the client and the DHCP server, rather than being broadcast across the entire network.

*** About DHCP address allocation

There are (at least) three ways that a DHCP server can assign addresses to requesting clients:

- **Manual or static allocation** essentially means that the client receives a specifically-chosen IP address, or, at a minimum, keeps the first one that it’s assigned until the client decides to release it.

- **Dynamic allocation** means that a DHCP server assigns IP addresses from an available pool (scope) of addresses, which can change to another available address in that scope at any time, depending on the network dynamics.

- **Automatic allocation** is sort of a cross between the other two types. The DHCP server assigns an address from its defined scope, but then remembers which client got what address, and re-assigns that address to the same client when a new request is made.

Regardless of the allocation method, the DHCP server’s scope — its range of IP addresses that it controls (and can assign) — is something that must be user-configured.

DHCP is “connectionless,” meaning that basically everything takes place via UDP, usually by broadcast packets — that is, packets not overtly addressed to a specific device. The messages become targeted pretty quickly, using the payload to specify the IP address of the DHCP server and the MAC address of the requesting client, to avoid requiring every other device on the network to completely decode every DHCP message. Note that it is possible to target a UDP packet at a specific server by choosing a unicast message type.

A DHCP client can request its previous IP address, if it had one, but whether it gets that address or not depends on four things: scope, allocation, topology, and authority. Specifically:

 - The larger the DHCP server’s scope of addresses, the more likely it is that the requested address will be available again.

 - The chances of getting the same IP address again also depend on how the server is allocating addresses (see above). Static allocation guarantees the same address; automatic allocation makes it very likely; with dynamic allocation, it’s impossible to predict.

- Topology also plays into this process: if the DHCP server is using one or more DHCP relays to get some or all of its addresses, the chances of re-using the same IP address go down.

- Authority also affects the probability. An authoritative DHCP server will definitely answer any unanswered DHCPDiscover message, but that server is pulling only from its own scope.

*** About multiple DHCP servers serving different IP ranges

It’s possible to have more than one DHCP server on the same network segment and still have everything work right, with no conflicts and no dropped packets or IP requests. There are three possible scopes for IP ranges to consider:

- **Adjacent scopes**: In this configuration, IP addresses are assigned from portions of the same subnet. For example, one server might control scope 192.168.14.2 – 192.168.14.187, and another server might manage scope 192.168.14.200 – 192.168.14.247. This is the most common (and most reliable) setup for multiple DHCP servers.

- **Heterogeneous scopes**: This arrangement basically has DHCP servers on different subnets, such as 192.168.14.2 – .253 for one server, and 10.17.22.3 – .98 for the other. This can be made to work, but it’s extremely difficult to set up and not so easy to manage. 

- **Overlapping scopes**: In this situation, more than one server can offer the same IP address. There is a way to make this work, by setting up the DHCP servers to talk to one another, but for most applications, this configuration can be avoided. 

Adjacent and heterogeneous scopes are really the same thing. The two servers do not work together; they may not ever be aware of one another. The servers and clients operate independently on a first-come, first-served basis, serving from their specific pool of IP addresses.

A client makes a DHCPRequest. One or both of the servers may answer, depending on load and spare IP addresses. It’s also possible that neither will answer, because they’re both out of IP addresses, but with good network planning — and making one of those servers authoritative — those situations will be kept to a minimum or eliminated entirely.

*** About multiple DHCP servers serving overlapping IP ranges

Some DHCP implementations offer a feature called server conflict detection or SCD. In short, DHCP SCD uses ICMP Echo messages (pings) — with an appropriate wait time — to see if an IP address is in use before trying to lease it to a client. If all the DHCP servers on a given subnet have SCD enabled, you don’t have to worry about whether the DHCP server scopes overlap. You can assign whatever set of IP addresses you want to whichever DHCP server -- even identical IP ranges -- and they will work together without causing any IP conflict errors.

Oddly, SCD is assumed by the creators of DHCP.  In RFC 2131, ping checks are recommended on both ends, in all cases, by the DHCP server and the client:

"As a consistency check, the allocating server SHOULD probe the reused address before allocating the address, e.g., with an ICMP echo request, and the client SHOULD probe the newly received address, e.g., with ARP."

The capital letters there came from the spec itself. Essentially, DHCP servers really should check to make sure the addresses they send out aren’t already in use — and clients that get them should make sure they’re actually free before they use them.

From an architectural perspective, it might make more sense for DHCP servers to be enabled to talk to each other and coordinate assignment of IP addresses. It is possible to build and configure such DHCP servers, but that type of coordination usually isn't possible in a MAAS networking environment.  Usually, in MAAS networks, there is an external source of DHCP outside the control of the MAAS administrator, one which can't be isolated from the MAAS network.  

As a protocol, DHCP is designed to be loosely coupled. Specifically, any client that has the DHCP protocol stack can discover any DHCP server or servers; any server can make an offer; and a client can take whichever offer it wants (though it’s typically coded to take the first DHCP offer that it can accept). Keeping that loosely-coupled architecture intact means letting DHCP servers check to see if the address they’re sending is in use before offering it, and letting clients check to see if an IP address is in use before they request to accept the offer.

There’s no exact count, but it’s fair to say that a non-trivial number of MAAS installation and configuration issues revolve around competing DHCP servers, that is, multiple DHCP servers on the same subnet, using the same scope (or overlapping scopes), colliding with each other and preventing machines from getting IP addresses. This collision usually shows up as an ability to power on a machine, but not to commission it, since it can’t manage to complete the process of getting an IP address via DHCP.

MAAS already has some conflict detection built in.  If MAAS manages a subnet that is not empty -- which could result in MAAS assigning a duplicate IP address -- MAAS is capable of detecting IPs in use on a subnet. Be aware that there are two caveats

1. If a previously-assigned NIC is in a quiescent state or turned off, MAAS may not detect it before duplicating an IP address.

2. At least one rack controller must have access to the IP-assigned machine in order for this feature to work.

MAAS also recognises when the subnet ARP cache is full, so that it can re-check the oldest IPs added to the cache to search for free IP addresses.

If you want your configuration to run more smoothly, it’s useful to enable SCD on every DHCP provider on your network, if you can. It doesn’t hurt anything, and it really doesn’t cost that much (besides a little extra delay when assigning addresses). There are plenty of network issues associated with a large, bare-metal network. There’s no reason why DHCP conflicts need to be one of those issues.

*** About DHCP relays

A DHCP relay is really just a specialised router.  Like all routers, it replaces source and destination addresses of packets crossing its domain so that every server gets the intended messages.  

The only substantial difference is that the DHCP relay knows the IP address of the DHCP server.  When a DHCPRequest reaches the relay from a requesting server, for example, the relay absorbs the broadcast packet and creates a routed unicast packet, bound directly for the DHCP server.  On the way back, the relay converts the DHCPOffer back to a broadcast packet.

** Availability zones
An availability zone is an organisational unit containing nodes, where each node is in exactly one zone. While in production, a machine can be allocated from a specific zone.  Availability zones can be used for fault-tolerance, service performance, and power management. See [Zone examples](/t/how-to-set-up-networks/6174#heading--about-availability-zones) for more details.

A newly installed MAAS comes with a default zone which initially contains all nodes. You cannot remove the 'default' zone or change its name, but you can create new zones and assign machine to them. 
This subsection explains some characteristics and uses of availability zones.  

Here you have the opportunity to learn:

- [About fault tolerance](#heading--fault-tolerance)
- [About service performance](#heading--service-performance)
- [About power management](#heading--power-management)


*** About fault tolerance

Fault tolerance is "the property that enables a system to continue operating properly in the event of the failure of (or one or more faults within) some of its components". To help create better fault tolerance, multiple MAAS zones can be employed.

For this, a zone can be defined in different ways. It can be based on power supply for instance, or it can represent a portion of your network or an entire data centre location.

Machines that work in tandem in order to provide an instance of a service should be allocated in the same zone. The entire service should be replicated in another zone.

*** About service performance

Service performance is the ability of your service to operate in the most efficient manner possible where the typical criteria used is speed. Multiple MAAS zones can be used to help.

Nodes should be allocated in the zone closest to the performance-critical resources they need.

For example, for applications that are highly sensitive to network latency, it may make sense to design a network topology consisting of several smaller networks, and have each of those represented as a zone. The zones can then be used to allocate nodes that have the best performance depending on the service offered.

*** About power management

Power management is concerned with power usage density and cooling. This topic can be addressed with the use of several MAAS zones.

Nodes can be distributed in such a way that power-hungry and/or "hot" systems are located in different zones. This can help mitigate power consumption and heat problems.


** About cloud networks

Cloud network architectures deviate significantly from the architecture of the Internet infrastructure.  These deviations are driven mostly by economics, simplicity, and scalability.  This article will help you learn:

- [About old and new network architectures](#heading--clos-architecture)
- [Problems with the AAG architecture](#heading--aag-problems)
- [Disaggregating the cloud](#heading--disaggregating-the-cloud)
- [Routing still rules](#heading--routing-still-rules)

*** About old and new network architectures

Before there were networks, monolithic applications ran on a mainframe with hardwired I/O devices.  As CPUs proliferated in separate enclosures, LANs like [Banyan Vines](https://en.wikipedia.org/wiki/Banyan_VINES)`↗` grew up.  Proprietary mismatch led to the [OSI model](https://maas.io/docs/about-tcp-ip-networks#heading--about-the-osi-model)`↗`.  Next came the Web, which distributed processing to client devices.  Now, we have the idea of generic switches and servers in cloud and bare-metal clusters.  Servers have shifted from dedicated applications to being completely [virtualised](https://en.wikipedia.org/wiki/Virtualization)`↗`.

A traditional AAC architecture looks like this:

<a href="https://discourse.maas.io/uploads/default/original/2X/e/e15a35da43b2788883ec014efb1832b8f641e872.jpeg" target="_blank"><img src="https://discourse.maas.io/uploads/default/original/2X/e/e15a35da43b2788883ec014efb1832b8f641e872.jpeg"></a>

It’s heavy on network hardware; that is, the radio of switches to servers is (too) high.  Switch-driven networks used hardware packet switching ([merchant silicon](https://etherealmind.com/analysis-merchant-custom-silicon)`↗`) to serve proprietary network configurations.  In theory, bridges needed no configuration, although [congestion](https://en.wikipedia.org/wiki/Network_congestion)`↗` and [mistaken identity](https://en.wikipedia.org/wiki/IP_address#Addressing_conflicts)`↗` led to the need for [STP](https://en.wikipedia.org/wiki/Spanning_Tree_Protocol)`↗`, [per-VLAN trees](https://networklessons.com/spanning-tree/per-vlan-spanning-tree-pvst)`↗`, and IP address redundancy management techniques.

A cloud architecture simplifies these networks:

<a href="https://discourse.maas.io/uploads/default/original/2X/f/fd86954e48538ce9ba8fc6e02df23b0a2337ef12.jpeg" target="_blank"><img src="https://discourse.maas.io/uploads/default/original/2X/f/fd86954e48538ce9ba8fc6e02df23b0a2337ef12.jpeg"></a>

Cloud architecture is a simple [spine-and-leaf network](https://networklessons.com/cisco/ccna-200-301/spine-and-leaf-architecture)`↗`, built from cheap, identical switches connecting cheap, identical servers.  Every switch has a path to every other switch, mitigating congestion.  Any switch can route any traffic anywhere, and any server can do any job (at any time)`↗`.  This architecture moves the developer’s focus from metal to actual computing problems.

*** Problems with the AAG architecture

Modern applications generate much more server-to-server traffic than the original Access-Aggregation-Core networks envisioned.  As a result, the AAG architecture presents several problems under load:

- **VLAN shorting**: At cloud scale, 12 bits of VLAN ID don't provide nearly enough VLAN instances.  Even with Multi-instance/STP, there simply aren't enough virtual LAN networks to accommodate the build-out of even a modest cloud configuration.  Tenant machines come and go rapidly, and STP hello messages simply can't keep up.
- **STP switch-count**: STP is only designed for two aggregation switches, which throttles bandwidth and causes massive, constant congestion in an extended cloud network.
- **Million-plus-packet floods**: Broadcast-and-learn bridges can't keep up with a million-plus MAC addresses, and even if they do, when those cached addresses start to time out, the periodic re-learning floods edge hosts, incapacitating them on a regular basis.
- **Unintentional VLAN partitioning**: Large VLANs plus many devices creates a high-probability of a configuration mistake or an ID refresh miss; these issues cause the network to be invisibly partitioned, making troubleshooting all but impossible.
- **ARP storms**: Hundreds of virtual machines bound to a couple of aggregation switches translates to a network-saturating flurry of ARP refreshes.  There's no easy fix for this without changing the network topology, or setting the refresh timer so high that DHCP hosts will fall off the DNS periodically.

There are other issues usually cited, but these are most concrete.

*** Disaggregating the cloud

An effective cloud architecture breaks routers and bridges into separate hardware and software, sometimes called "switch disaggregation".  A merchant-silicon chassis (the hardware) can be combined with someone else's NOS (network operating system, the software) to produce more standard and cost-effective network switch-gear.  In practice, few cloud providers actually create their own NOS, choosing instead to either integrate merchant silicon with a separately purchased NOS, or, more likely, to buy bundled units from switch providers that have broader integration experience.

Either way, the effect is to have switch-gear which is (1) less likely to be proprietary, and (2) easier to upgrade as the hardware advances.  Switch disaggregation effectively commoditizes the switching elements, effectively eliminating them from network throughput calculations.  In other words, the bridges and routers in a modern cloud network are essentially invisible, uninteresting, and inexpensive, which facilitates the cloud-network build-out model.

*** Routing still rules

Regardless of your network vintage, packet routing is still the fundamental algorithm that gates throughput.  If you're familiar with routing, this section can be skipped. If you're trying to level up your cloud networking knowledge, this section should help quite a bit.

Routing is fundamentally simple: you're trying to get a packet from a source to a destination, using the destination IP address embedded in that packet.  In practice, routing is more complicated.  Most of the routers in a network path are not forwarding directly to the destination.  Instead, they're sending the packet to the next hop, that is, another, reachable router that is closer to the intended destination.

In [the TCP/IP tutorial](#heading--borrowed-from-ma-bell), we talked about telephone-line repeaters that refreshed the signal to keep it crystal clear over long runs of wire.  The signals didn't have to make it to the intended recipient, they just had to make it to the next repeater, which boosted the signal, cleaned up the noise, and sent the call on down the line to another repeater.  Eventually the call would make it to the subscriber.

Routing works very much the same way: most of the routers in the loop understand how to find another router which is close to the intended destination.  That "next hop" will take care of repackaging the packets and sending them further down the line.  What matters in high-traffic cloud networks is not routing, per se, but how the packets are routed.  The choice of routing protocols can make all the difference in whether or not your network lags.  For this reason, we're going to take a more extended look at routing.

**** Multicast routing

One helpful tool for modern cloud networks is the concept of multicast routing.  Stated simply, multicast routing allows one packet to be received by many servers, but only if those servers are interested in receiving it.  Multicast receivers -- which have to subscribe in order to receive the packets -- support a much larger (and more flat) network.  This layout more easily scales to the cloud architecture shown above. Multicasting is generally faster and more efficient for certain payloads; unlike broadcast packets, multicast packets are not examined by every NIC in the packet's path.

For example, all IPv6 communications are multicast, so there's no ARP.  Instead, a process called [neighbour discovery](https://en.wikipedia.org/wiki/Neighbor_Discovery_Protocol)`↗` (NDP) is used.  But IPv4 networks can also handle multicast transactions.  Both IPv4 and IPv6 protocols support [multicast address blocks](https://en.wikipedia.org/wiki/Multicast_address)`↗`, which enable multicasting.

A multicasting server sends one packet, and the network handles replication and addressing (multiplexing) of the packets to the subscribed servers.  Multicast is a one-way protocol: any responses have to be sent by other protocols.  For larger server farms, multicast is a good way to handle things like a software update or a database refresh.  Two special protocols handle subscriptions: [IGMP](https://en.wikipedia.org/wiki/Internet_Group_Management_Protocol)`↗`, used by individual IPv4 receivers, and [PIM](https://en.wikipedia.org/wiki/Protocol_Independent_Multicast)`↗`, which is used by L3 devices (like routers) to manage multicast "trees" across a network or subnet.

* Other logs of interest

* Power drivers
To manage a machine, MAAS must be able to power cycle it, usually through the machine's [BMC](https://en.wikipedia.org/wiki/Intelligent_Platform_Management_Interface#Baseboard_management_controller)`↗` card.  Until you configure the power type, a newly-added machine can't be enlisted and used by MAAS.

[tabs]
[tab version="v3.4 Snap,v3.4 Packages,v3.3 Snap,v3.3 Packages,v3.2 Snap,v3.2 Packages"]
** About IPMI cipher suites

We require the user to explicitly set the cipher suite due to the fact that one BMC’s order is different from another, leading to erroneous discovery. You can explicitly select which cipher suite to use when interacting with a BMC. You do this by selecting the cipher suite in power configuration.  By default, the cipher suite is 3. This is the least secure suite. It is up to you to select a more secure suite if supported and desired.
[/tab]
[tab version="v3.1 Snap,v3.1 Packages,v3.0 Snap,v3.0 Packages,v2.9 Snap,v2.9 Packages"]
** About IPMI cipher suites

When using IPMI, MAAS will attempt to automatically detect the correct cipher suite. MAAS tries to find the most secure cipher suite available. Preference order is 17, 3, 8, 12. If auto-detection fails MAAS will fall back to using freeipmi-tool default, 3, which is what previous versions of MAAS use.

** About better Redfish support in MAAS 3.2

MAAS 3.2 enhances support for Redfish as a BMC protocol by preferring Redfish over IPMI, provided that:

- The BMC has a Redfish Host Interface enabled
- That host interface can be accessed by the MAAS host

MAAS has already supported Redfish for some time.  MAAS 3.2 adds the capability to auto-detect Redfish and prefer it as the BMC protocol, if it's present and enabled.

*** About Redfish

Redfish is an [alternative to the IPMI protocol](https://www.dmtf.org/sites/default/files/Redfish_Tech_Note-November_2018.pdf)`↗` for connecting with machine BMCs.  It provides additional features above and beyond those provided by IPMI; eventually, Redfish should supplant IPMI as the default BMC interface.

*** About the MAAS implementation of Redfish

A machine that isn’t registered in MAAS – but connected to the MAAS PXE network – can be powered on manually and made to network boot. The machine boots into an ephemeral environment to gather information about the machine. A script inside that environment registers the machine in MAAS.

Prior to MAAS 3.2, all such BMC connections were made via IPMI.  With the release of 3.2, if the machine uses either IPMI or Redfish for its BMC, the ephemeral environment will automatically detect it, create a separate user for MAAS and configure the machine, so that MAAS may check and control the machine’s power status.

Redfish can be detected and configured only if the BMC has a Redfish Host Interface enabled and exposed to the host. Redfish is preferred if the machine supports both Redfish and IPMI.

[note]
The name of the user that MAAS creates in the BMC is controlled by the maas_auto_ipmi_user config setting both for IPMI and Redfish; nothing has changed in this regard with the addition of Redfish support.
[/note]

*** About enabling Redfish

There are ways to check whether Redfish is properly enabled.  You can check whether or not a machine can communicate via Redfish with the command: 

```nohighlight
dmidecode -t 42
```

If the machine has been enlisted by MAAS, you can also check the output of the `30-maas-01-bmc-config` commissioning script to discover this.
[/tab]
[/tabs]


* Power drivers reference
To manage a machine, MAAS must be able to power cycle it, usually through the machine's [BMC](https://en.wikipedia.org/wiki/Intelligent_Platform_Management_Interface#Baseboard_management_controller)`↗` card.  Until you configure the power type, a newly-added machine can't be enlisted and used by MAAS.

[tabs]
[tab version="v3.4 Snap,v3.4 Packages,v3.3 Snap,v3.3 Packages,v3.2 Snap,v3.2 Packages"]
** About IPMI cipher suites

We require the user to explicitly set the cipher suite due to the fact that one BMC’s order is different from another, leading to erroneous discovery. You can explicitly select which cipher suite to use when interacting with a BMC. You do this by selecting the cipher suite in power configuration.  By default, the cipher suite is 3. This is the least secure suite. It is up to you to select a more secure suite if supported and desired.
[/tab]
[tab version="v3.1 Snap,v3.1 Packages,v3.0 Snap,v3.0 Packages,v2.9 Snap,v2.9 Packages"]
** About IPMI cipher suites

When using IPMI, MAAS will attempt to automatically detect the correct cipher suite. MAAS tries to find the most secure cipher suite available. Preference order is 17, 3, 8, 12. If auto-detection fails MAAS will fall back to using freeipmi-tool default, 3, which is what previous versions of MAAS use.

** About better Redfish support in MAAS 3.2

MAAS 3.2 enhances support for Redfish as a BMC protocol by preferring Redfish over IPMI, provided that:

- The BMC has a Redfish Host Interface enabled
- That host interface can be accessed by the MAAS host

MAAS has already supported Redfish for some time.  MAAS 3.2 adds the capability to auto-detect Redfish and prefer it as the BMC protocol, if it's present and enabled.

*** About Redfish

Redfish is an [alternative to the IPMI protocol](https://www.dmtf.org/sites/default/files/Redfish_Tech_Note-November_2018.pdf)`↗` for connecting with machine BMCs.  It provides additional features above and beyond those provided by IPMI; eventually, Redfish should supplant IPMI as the default BMC interface.

*** About the MAAS implementation of Redfish

A machine that isn’t registered in MAAS – but connected to the MAAS PXE network – can be powered on manually and made to network boot. The machine boots into an ephemeral environment to gather information about the machine. A script inside that environment registers the machine in MAAS.

Prior to MAAS 3.2, all such BMC connections were made via IPMI.  With the release of 3.2, if the machine uses either IPMI or Redfish for its BMC, the ephemeral environment will automatically detect it, create a separate user for MAAS and configure the machine, so that MAAS may check and control the machine’s power status.

Redfish can be detected and configured only if the BMC has a Redfish Host Interface enabled and exposed to the host. Redfish is preferred if the machine supports both Redfish and IPMI.

[note]
The name of the user that MAAS creates in the BMC is controlled by the maas_auto_ipmi_user config setting both for IPMI and Redfish; nothing has changed in this regard with the addition of Redfish support.
[/note]

*** About enabling Redfish

There are ways to check whether Redfish is properly enabled.  You can check whether or not a machine can communicate via Redfish with the command: 

```nohighlight
dmidecode -t 42
```

If the machine has been enlisted by MAAS, you can also check the output of the `30-maas-01-bmc-config` commissioning script to discover this.
[/tab]
[/tabs]

** Power management reference guide

**** This article will help you learn:

[tabs]
[tab version="v3.4 Snap,v3.4 Packages,v3.3 Snap,v3.3 Packages,v3.2 Snap,v3.2 Packages,v3.1 Snap,v3.1 Packages,v3.0 Snap,v3.0 Packages"]
- [How to configure a machine's power type](#heading--config-power-type)
- [How to configure and use IBM Z with MAAS](#heading--configure-use-ibm-z)
- [How to use the virsh power type](#heading--example-virsh-kvm-power-type)
- [Which BMC drivers are supported](#heading--bmc-driver-support)
[/tab]
[tab version="v2.9 Snap,v2.9 Packages"]
- [How to configure a machine's power type](#heading--config-power-type)
- [How to use the virsh power type](#heading--example-virsh-kvm-power-type)
- [Which BMC drivers are supported](#heading--bmc-driver-support)
[/tab]
[/tabs]

In addition, this article provides a [complete catalogue of power parameters, by type](#heading--power-catalogue).

You may also like to try **[maaspower](https://gilesknap.github.io/maaspower/main/index.html)**`↗` which is a community project designed to be used with the MAAS webhook driver. It is a pluggable system that accepts MAAS webhooks and can translate them to other external systems. Note: it is not supported by Canonical.

[tabs]
[tab version="v3.4 Snap,v3.4 Packages,v3.3 Snap,v3.3 Packages,v3.2 Snap,v3.2 Packages,v3.1 Snap,v3.1 Packages,v3.0 Snap,v3.0 Packages,v2.9 Snap,v2.9 Packages" view="UI"]
** How to configure a machine's power type

To configure a machine's power type, click on the machine from the 'Machines' page of the web UI, then select its 'Configuration' tab. Scroll down until you find the Power configuration. If the power type is undefined, the following will be displayed:

<a href="https://assets.ubuntu.com/v1/4fae5977-nodes-power-types__2.4_undefined.png" target = "_blank"><img src="https://assets.ubuntu.com/v1/4fae5977-nodes-power-types__2.4_undefined.png"></a>

Choose a type in the drop-down menu that corresponds to the machine's underlying machine's BMC card.

<a href="https://assets.ubuntu.com/v1/b53c6613-nodes-power-types__2.4_selection.png" target = "_blank"><img src="https://assets.ubuntu.com/v1/b53c6613-nodes-power-types__2.4_selection.png"></a>

Fill in the resulting form; the information required will depends on the power type:

| CLI power_type code | Description |
|:--------------------|:------------|
| [amt](#heading--amt) |Intel AMT |
| [apc](#heading--apc) | American Power Conversion (APC) PDU |
| [dli](#heading--dli) | Digital Loggers, Inc. PDU |
| [hmc](#heading--hmc) | IBM Hardware Management Console (HMC) |
| [lxd](#heading--lxd) | LXD VM |
| [ipmi](#heading--ipmi) | IPMI |
| [manual](#heading--manual) | Manual power configuration |
| [moonshot](#heading--moonshot) | HP Moonshot - iLO4 (IPMI) |
| [mscm](#heading--mscm) | HP Moonshot - iLO Chassis Manager |
| [msftocs](#heading--mscm) | Microsoft OCS - Chassis Manager |
| [nova](#heading--nova) | OpenStack Nova |
| [openbmc](#heading--openbmc) | OpenBMC Power Driver |
| [proxmox](#heading--proxmox) | ProxMox Power Driver |
| [recs_box](#heading--recs_box) | Christmann RECS-Box Power Driver |
| [redfish](#heading--redfish) | Redfish |
| [sm15k](#heading--sm15k) | SeaMicro 15000 |
| [ucsm](#heading--ucsm) | Cisco UCS Manager |
| [virsh](#heading--virsh) | libvirt KVM |
| [vmware](#heading--vmware) | VMware |
| [webhook](#heading--webhook) | Webhook |
| [wedge](#heading--wedge) | Facebook's Wedge |

Click 'Save changes' to finish. Once that's done, MAAS performs a power check on the machine. A successful power check is a good indication that MAAS can properly communicate with the machine, that is, it should quickly result in a power status of "Power off". A failed attempt will show:

<a href="https://assets.ubuntu.com/v1/3bd5e93b-nodes-power-types__2.4_power-error.png" target = "_blank"><img src="https://assets.ubuntu.com/v1/3bd5e93b-nodes-power-types__2.4_power-error.png"></a>

If you see this error, double-check your entered values by editing the power type, or  consider another power type altogether.

Another possible cause for this error may be the networking: traffic may be getting filtered between the rack controller and the BMC card.

** Power catalogue

The following catalogue helps to explain the fields in the "create machine" dialogue above.  Note that most of the multiple-choice fields have drop-down menus to assist with your choice.

*** Intel AMT

| Form field | Description | Required |
|:-----|:-----|:-----|
| Power password | Password to access unit | Optional |
| Power address | IP address of unit | Required |

*** American Power Conversion (APC) PDU

| Form field | Description | Required |
|:-----|:-----|:-----|
| IP for APC PDU | IP address of unit | Required |
| APU PDU node outlet number (1-16) | PDU node outlet number | Required |
| Power ON outlet delay (seconds) | outlet power ON delay | Optional, default=5 |

*** Digital Loggers, Inc. PDU

| Form field | Description | Required |
|:-----|:-----|:-----|
| Outlet ID | outlet ID | Required |
| Power address | IP address of unit | Required |
| Power user | Username to login | Optional |
| Power password | Password to access unit | Optional |

*** IBM Hardware Management Console (HMC)

| Form field | Description | Required |
|:-----|:-----|:-----|
| IP for HMC | IP address of unit | Required |
| HMC username | Username to login | Optional |
| HMC password | Password to access unit | Optional |
| HMC Managed System server name | HMC managed server name | Required |
| HMC logical partition | HMC logical partition of unit | Required |

*** LXD VMs

| Form field | Description | Required |
|:-----|:-----|:-----|
| LXD address | IP address of unit | Required |
| Instance name | LXD container instance name | Required |
| LXD password | Password to access unit | Optional |

*** IPMI

Some of the fields for this power type have fixed choices, indicated in the "Choices" column.

| Form field | Description | Choices | Required |
|:-----------|:------------|:--------|:---------|
| Power driver | Power driver |`LAN [IPMI 1.5]` | Required |
| | | `LAN_2_0 [IPMI 2.0]`| |
| Power boot type | Boot type | `Automatic` | Required |
| | | `Legacy boot` | |
| | | `EFI boot` | |
| IP address | IP address of unit || Required |
| Power user | Username to login || Optional |
| Power password | Password to access unit || Optional |
| Power MAC | MAC address of unit || Optional |
| K_g | K_g BMC key | | Optional |
| Cipher suite | Cipher suite ID | - `17` <small>(17 - HMAC-SHA256::HMAC_SHA256_128::AES-CBC-128)</small> | Optional |
| | |`3` <small>(3 - HMAC-SHA1::HMAC-SHA1-96::AES-CBC-128)</small> | |
| | |` ` (blank) <small>(freeipmi-tools default)</small> | |
| | |`8` <small>(8 - HMAC-MD5::HMAC-MD5-128::AES-CBC-128)</small> | |
| | |`12` <small>(12 - HMAC-MD5::MD5-128::AES-CBC-128)</small> | |
| Privilege level | IPMI privilege level | `User` | Optional  |
| | | `Operator` | |
| | | `Administrator` | |

*** Manual power configuration

Manual power configuration means exactly that -- manually configured at the unit -- hence there are no parameters to set in the "create machine" UI.

*** HP Moonshot - iLO4 (IPMI)

| Form field | Description | Required |
|:-----|:-----|:-----|
| Power address | IP address of unit | Required |
| Power user | Username to login | Optional |
| Power password | Password to access unit | Optional |
| Power hardware address | Hardware address of unit | Required |

*** HP Moonshot - iLO Chassis Manager

| Form field | Description | Required |
|:-----|:-----|:-----|
| IP for MSCM CLI API | IP address of unit | Required |
| MSCM CLI API user | Username to login | Optional |
| MSCM CLI API password | Password to access unit | Optional |
| Node ID | cXnY | Required |
|  - where  | X = cartridge number | |
|           | Y = node number | |

*** Microsoft OCS - Chassis Manager

| Form field | Description | Required |
|:-----|:-----|:-----|
| Power address | IP address of unit | Required |
| Power port | Port where unit is attached | Optional |
| Power user | Username to login | Optional |
| Power password | Password to access unit | Optional |
| Blade ID | Blade ID (usu. 1-24) | Required |

*** OpenStack Nova

| Form field | Description | Required |
|:-----|:-----|:-----|
| Host UUID | Host UUID | Required |
| Tenant name | Tenant name | Required |
| Username | Username to login | Required |
| Password | Password to access unit | Required |
| Auth URL | URL to access unit | Required |

*** Proxmox

| Form field | Description | Required |
|:-----|:-----|:-----|
| Power type | Proxmox | Required |
| Host name or IP | Power address for the Proxmox driver | Required |
| Username, including realm | Power user, along with realm (i.e., Username@Realm | Required |
| Password | Required if a token name and secret aren't given | Provisional |
| API token name | Token name: must include Username without realm (i.e., Username!Token-name | Provisional |
| API token secret | Token secret | Provisional |
| Node ID | VM name or ID | Optional |
| Verify SSL connections... | Boolean, whether or not to verify SSL connections with the system's root CA certificate | Required |

*** OpenBMC Power Driver

| Form field | Description | Required |
|:-----|:-----|:-----|
| OpenBMC address | IP address of unit | Required |
| OpenBMC user | Username to login | Required |
| OpenBMC password | Password to access unit | Required |

<a href="#heading--recs_box">*** id="heading--recs_box">Christmann RECS-Box Power Driver

| Form field | Description | Required |
|:-----|:-----|:-----|
| Node ID | Node ID | Required |
| Power address | IP address of unit | Required |
| Power port | Port where unit is attached | Optional |
| Power user | Username to login | Optional |
| Power password | Password to access unit | Optional |

*** Redfish

| Form field | Description | Required |
|:-----|:-----|:-----|
| Redfish address | IP address of unit | Required |
| Redfish user | Username to login | Required |
| Redfish password | Password to access unit | Required |
| Node ID | Node ID | Optional |

*** SeaMicro 15000

Some of the fields for this power type have fixed choices, indicated in the "Choices" column.

| Form field | Description | Choices | Required |
|:-----|:-----|:-----|:-----|
| System ID | System ID || Required |
| Power address | IP address of unit || Required |
| Power user | Username to login || Optional |
| Power password | Password to access unit || Optional |
| Power control type | Password to access unit| IPMI | Required |
|  |  | REST API v0.9 | |
|  |  | REST API v2.0 | |

*** Cisco UCS Manager

| Form field | Description | Required |
|:-----|:-----|:-----|
| Server UUID | Server UUID | Required |
| URL for XML API | XML API URL | Required |
| API user | API user | Optional |
| API password | API password | Optional |

*** virsh - libvirt KVM

| Form field | Description | Required |
|:-----|:-----|:-----|
| Address | URL of VM | Required |
| Password | API password | Optional |
| Virsh VM ID | libvirt VM UUID | Required |

*** VMware

| Form field | Description | Required |
|:-----|:-----|:-----|
| VM Name | VM name (if UUID unknown) | Optional |
| VM UUID | VM UUID (if known) | Optional |
| VMware IP | IP address of VM | Required |
| VMware username | Username to access VM | Required |
| VMware password | Password to access VM | Required |
| VMware API port | VMware API port number | Optional |
| VMware API protocol | VMware API protocol | Optional |

*** Facebook's Wedge

| Form field | Description | Required |
|:-----|:-----|:-----|
| IP address | IP address of unit | Required |
| Power user | Username to access unit | Optional |
| Power password | Password to access unit | Optional |

** How to use the virsh power type

Consider a machine backed by VM. Below, a 'Power type' of `Virsh` has been selected, and the 'Power address' of `qemu+ssh://ubuntu@192.168.1.2/system` has been entered (replace values as appropriate).  The value of 'Power ID' is the VM domain (guest) name, here `node2`.

<a href="https://assets.ubuntu.com/v1/c75e00a8-nodes-power-types__2.4_example-virsh.png" target = "_blank"><img src="https://assets.ubuntu.com/v1/c75e00a8-nodes-power-types__2.4_example-virsh.png"></a>

[note]
The machine's hostname -- according to MAAS -- is a randomly chosen string (here `dear.ant`). You should change this hostname to something descriptive, that helps you remember why this machine is in your MAAS network.
[/note]

*** Webhook

It's important to understand that the Webhook power driver is more generic than other drivers, so it has some flexibility that the underlying power driver may not support.  For example, Webhook doesn't require a username or password for the power driver, because not all power drivers work that way.  Nevertheless, the power driver you're connecting to Webhook may actually require a username and/or password.  Understanding and implementing these fields correctly for the chosen back-end power driver is the user's responsibility.

To that end, the "Required" column for this driver refers only to whether Webhook requires a value in each field.  Just because a field is optional for Webhook itself does not mean that the underlying power driver will ultimately allow that field to be unspecified.

| Form field | Description | Required (by Webhook) |
|:-----|:-----|:-----|
| Power type | Webhook (from drop-down list) | Required |
| URI to power on the node | URI to access power driver's API for power on | Required |
| URI to power off the node | URI to access power driver's API for power off | Required |
| URI to query the nodes power status | URI to access power driver's API for power status | Required |
| Regex to confirm the node is on | Regex expression that will return a string if the power is on, and no string if the power is off | Required, defaults supplied |
| Regex to confirm the node is off | Regex expression that will return a string if the power is off, and no string if the power is on | Required, defaults supplied |
| Power user | Username to log into the power driver | Optional |
| Power password | Password to access unit | Optional |
| Power token | Power driver API token (used instead of user and password, if set) | Optional |
| Verify SSL connections... | Boolean, whether or not to verify SSL connections with the system's root CA certificate | Required |
[/tab]
[tab version="v3.4 Snap,v3.4 Packages,v3.3 Snap,v3.3 Packages,v3.2 Snap,v3.2 Packages,v3.1 Snap,v3.1 Packages,v3.0 Snap,v3.0 Packages,v2.9 Snap,v2.9 Packages" view="CLI"]
** How to configure a machine's power type

To (re)configure a machine's power type, first find the machine's $SYSTEM_ID with the following recipe:

```
maas admin machines read | jq -r '(["HOSTNAME","SIS'S"] | (., map(length*"-"))),
(.[] | [.hostname, .system_id]) | @tsv' | column -t
```

Next, use the [MAAS CLI](/t/try-out-the-maas-cli/5236) command `maas machines...` to (re)set the machine's power type, like this:

    maas $PROFILE machine update $SYSTEM_ID power_type="$POWER_TYPE"

where $POWER_TYPE can have the following values:

| CLI power_type code | Description |
|:-----|:-----|
| [amt](#heading--amt) |Intel AMT |
| [apc](#heading--apc) | American Power Conversion (APC) PDU |
| [dli](#heading--dli) | Digital Loggers, Inc. PDU |
| [eaton](#heading--eaton) | Eaton PDU |
| [hmc](#heading--hmc) | IBM Hardware Management Console (HMC) |
| [lxd](#heading--lxd) | LXD VM |
| [ipmi](#heading--ipmi) | IPMI |
| [manual](#heading--manual) | Manual power configuration |
| [moonshot](#heading--moonshot) | HP Moonshot - iLO4 (IPMI) |
| [mscm](#heading--mscm) | HP Moonshot - iLO Chassis Manager |
| [msftocs](#heading--mscm) | Microsoft OCS - Chassis Manager |
| [nova](#heading--nova) | OpenStack Nova |
| [openbmc](#heading--openbmc) | OpenBMC Power Driver |
| [recs_box](#heading--recs_box) | Christmann RECS-Box Power Driver |
| [redfish](#heading--redfish) | Redfish |
| [sm15k](#heading--sm15k) | SeaMicro 15000 |
| [ucsm](#heading--ucsm) | Cisco UCS Manager |
| [virsh](#heading--virsh) | libvirt KVM |
| [vmware](#heading--vmware) | VMware |
| [wedge](#heading--wedge) | Facebook's Wedge |

Note the required and optional parameters associated with each power type.

Once you've successfully processed the command (as indicated by a stream of JSON, headed by "Success!"), MAAS performs a power check on the machine. A successful power check is a good indication that MAAS can properly communicate with the machine, that is, it should quickly result in a power status of "Power off". A failed attempt will return errors that should guide you to fix your power_parameters.

** Power catalogue

*** Intel AMT

All parameters are entered as `key=value`, e.g., `power_type=amt`.  The MAAS CLI will refuse the request with informative errors if required parameters are excluded.

| Parameter | Description | Required |
|:-----|:-----|:-----|
| `power_type` | `amt` | Required |
| `power_address` | IP address of unit | Required |
| `power_pass` | Password to access unit | Optional |

*** American Power Conversion (APC) PDU

All parameters are entered as `key=value`, e.g., `power_type=apc`.  The MAAS CLI will refuse the request with informative errors if required parameters are excluded.

| Parameter | Description | Required |
|:-----|:-----|:-----|
| `power_type` | `apc` | Required |
| `power_address` | IP address of unit | Required |
| `node_outlet` | PDU node outlet number | Required |
| `power_on_delay` | outlet power ON delay | Optional, default=5 |

*** Digital Loggers, Inc. PDU

All parameters are entered as `key=value`, e.g., `power_type=dli`.  The MAAS CLI will refuse the request with informative errors if required parameters are excluded.

| Parameter | Description | Required |
|:-----|:-----|:-----|
| `power_type` | `dli` | Required |
| `outlet_id` | outlet ID | Required |
| `power_address` | IP address of unit | Required |
| `power_user` | Username to login | Optional |
| `power_pass` | Password to access unit | Optional |

*** Eaton PDU

All parameters are entered as `key=value`, e.g., `power_type=eaton`.  The MAAS CLI will refuse the request with informative errors if required parameters are excluded.

| Parameter | Description | Required |
|:-----|:-----|:-----|
| `power_type` | `eaton` | Required |
| `power_address` | IP address of unit | Required |
| `node_outlet` | PDU node outlet number | Required |
| `power_on_delay` | outlet power ON delay | Optional, default=5 |

*** IBM Hardware Management Console (HMC)

All parameters are entered as `key=value`, e.g., `power_type=hmc`.  The MAAS CLI will refuse the request with informative errors if required parameters are excluded.

| Parameter | Description | Required |
|:-----|:-----|:-----|
| `power_type` | `hmc` | Required |
| `power_address` | IP address of unit | Required |
| `server_name` | HMC managed server name | Required |
| `lpar` | HMC logical partition of unit | Required |
| `power_user` | Username to login | Optional |
| `power_pass` | Password to access unit | Optional |

*** LXD VMs

All parameters are entered as `key=value`, e.g., `power_type=lxd`.  The MAAS CLI will refuse the request with informative errors if required parameters are excluded.

| Parameter | Description | Required |
|:-----|:-----|:-----|
| `power_type` | `lxd` | Required |
| `power_address` | IP address of unit | Required |
| `instance_name` | LXD container instance name | Required |
| `power_pass` | Password to access unit | Optional |

*** IPMI

All parameters are entered as `key=value`, e.g., `power_type=amt`.  The MAAS CLI will refuse the request with informative errors if required parameters are excluded. Power driver specific parameters should be prefixed with `power_parameters_{key}`.

Some of the fields for this power type have fixed choices, indicated in the "Choices" column.

| Form field | Description | Choices | Required |
|:-----------|:------------|:--------|:---------|
| `power_driver` | Power driver |`LAN [IPMI 1.5]` | Required |
| | | `LAN_2_0 [IPMI 2.0]`| |
| `power_boot_type` | Boot type | `Automatic` | Required |
| | | `Legacy boot` | |
| | | `EFI boot` | |
| `power_address` | IP address of unit || Required |
| `power_user` | Username to login || Optional |
| `power_pass` | Password to access unit || Optional |
| `mac_address` | MAC address of unit || Optional |
| `k_g` | K_g BMC key | | Optional |
| `cipher_suite_id` | Cipher suite ID |`17` <small>(17 - HMAC-SHA256::HMAC_SHA256_128::AES-CBC-128)</small> | Optional |
| | |`3` <small>(3 - HMAC-SHA1::HMAC-SHA1-96::AES-CBC-128)</small> | |
| | |` ` (blank) <small>(freeipmi-tools default)</small> | |
| | |`8` <small>(8 - HMAC-MD5::HMAC-MD5-128::AES-CBC-128)</small> | |
| | |`12` <small>(12 - HMAC-MD5::MD5-128::AES-CBC-128)</small> | |
| `privilege_level` | IPMI privilege level | `User` | Optional  |
| | | `Operator` | |
| | | `Administrator` | |

*** Manual power configuration

Manual power configuration means exactly that -- manually configured at the unit.  The only MAAS CLI parameter is `power_type=amt`. 

*** HP Moonshot - iLO4 (IPMI)

All parameters are entered as `key=value`, e.g., `power_type=moonshot`.  The MAAS CLI will refuse the request with informative errors if required parameters are excluded.

| Parameter | Description | Required |
|:-----|:-----|:-----|
| `power_type | `moonshot` | Required |
| `power_address` | IP address of unit | Required |
| `power_hwaddress` | Hardware address of unit | Required |
| `power_user` | Username to login | Optional |
| `power_pass` | Password to access unit | Optional |

*** HP Moonshot - iLO Chassis Manager

All parameters are entered as `key=value`, e.g., `power_type=mscm`.  The MAAS CLI will refuse the request with informative errors if required parameters are excluded.

| Parameter | Description | Required |
|:-----|:-----|:-----|
| `power_type | `mscm` | Required |
| `power_address` | IP address of unit | Required |
| `node_id` | cXnY | Required |
|  - where  | X = cartridge number | |
|           | Y = node number | |
| `power_user` | Username to login | Optional |
| `power_pass` | Password to access unit | Optional |

*** Microsoft OCS - Chassis Manager

All parameters are entered as `key=value`, e.g., `power_type=msftocs`.  The MAAS CLI will refuse the request with informative errors if required parameters are excluded.

| Parameter | Description | Required |
|:-----|:-----|:-----|
| `power_type | `msftocs` | Required |
| `power_address` | IP address of unit | Required |
| `blade_id` | Blade ID (usu. 1-24) | Required |
| `power_port` | Port where unit is attached | Optional |
| `power_user` | Username to login | Optional |
| `power_pass` | Password to access unit | Optional |

*** OpenStack Nova

All parameters are entered as `key=value`, e.g., `power_type=nova`.  The MAAS CLI will refuse the request with informative errors if required parameters are excluded.

| Parameter | Description | Required |
|:-----|:-----|:-----|
| `power_type | `nova` | Required |
| `nova_id` | Host UUID | Required |
| `os_tenantname` | Tenant name | Required |
| `os_username` | Username to login | Required |
| `os_password` | Password to access unit | Required |
| `os_authurl` | URL to access unit | Required |

*** OpenBMC Power Driver

All parameters are entered as `key=value`, e.g., `power_type=openbmc`.  The MAAS CLI will refuse the request with informative errors if required parameters are excluded.

| Parameter | Description | Required |
|:-----|:-----|:-----|
| `power_type` | `openbmc` | Required |
| `power_address` | IP address of unit | Required |
| `power_user` | Username to login | Required |
| `power_pass` | Password to access unit | Required |

<a href="#heading--recs_box">*** id="heading--recs_box">Christmann RECS-Box Power Driver

All parameters are entered as `key=value`, e.g., `power_type=recs_box`.  The MAAS CLI will refuse the request with informative errors if required parameters are excluded.

| Parameter | Description | Required |
|:-----|:-----|:-----|
| `power_type | `recs_box` | Required |
| `node_id` | Node ID | Required |
| `power_address` | IP address of unit | Required |
| `power_port` | Port where unit is attached | Optional |
| `power_user` | Username to login | Optional |
| `power_pass` | Password to access unit | Optional |

*** Redfish

All parameters are entered as `key=value`, e.g., `power_type=redfish`.  The MAAS CLI will refuse the request with informative errors if required parameters are excluded.

| Parameter | Description | Required |
|:-----|:-----|:-----|
| `power_type | `redfish` | Required |
| `power_address` | IP address of unit | Required |
| `power_user` | Username to login | Required |
| `power_pass` | Password to access unit | Required |
| `node_id` | Node ID | Optional |

*** SeaMicro 15000

All parameters are entered as `key=value`, e.g., `power_type=sm15k`.  The MAAS CLI will refuse the request with informative errors if required parameters are excluded.

Some of the fields for this power type have fixed choices, indicated in the "Choices" column.

| Parameter | Description | Choices | Required |
|:-----|:-----|:-----|:-----|
| `power_type` | `sm15k` | | Required |
| `system_id` | System ID || Required |
| `power_address` | IP address of unit || Required |
| `power_control` | Password to access unit| ipmi | Required |
|  |  | restapi | |
|  |  | restapi2 | |
| `power_user` | Username to login || Optional |
| `power_pass` | Password to access unit || Optional |

*** Cisco UCS Manager

All parameters are entered as `key=value`, e.g., `power_type=ucsm`.  The MAAS CLI will refuse the request with informative errors if required parameters are excluded.

| Parameter | Description | Required |
|:-----|:-----|:-----|
| `power_type` | `ucsm` | Required |
| `uuid` | Server UUID | Required |
| `power_address` | URL for XML API | Required |
| `power_user` | API user | Optional |
| `power_pass` | API password | Optional |

*** virsh - libvirt KVM

All parameters are entered as `key=value`, e.g., `power_type=virsh`.  The MAAS CLI will refuse the request with informative errors if required parameters are excluded.

| Parameter | Description | Required |
|:-----|:-----|:-----|
| `power_type` | `virsh` | Required |
| `power_id` | libvirt VM UUID | Required |
| `power_address` | URL of VM | Required |
| `power_pass` | API password | Optional |

*** VMware

All parameters are entered as `key=value`, e.g., `power_type=vmware`.  The MAAS CLI will refuse the request with informative errors if required parameters are excluded.

| Parameter | Description | Required |
|:-----|:-----|:-----|
| `power_type` | `vmware` | Required |
| `power_vm_name` | VM name (if UUID unknown) | Optional |
| `power_uuid` | VM UUID (if known) | Optional |
| `power_address` | IP address of VM | Required |
| `power_user` | Username to access VM | Required |
| `power_pass` | Password to access VM | Required |
| `power_port` | VMware API port number | Optional |
| `power_protocol` | VMware API protocol | Optional |

*** Facebook's Wedge

All parameters are entered as `key=value`, e.g., `power_type=amt`.  The MAAS CLI will refuse the request with informative errors if required parameters are excluded.

| Parameter | Description | Required |
|:-----|:-----|:-----|
| `power_type` | `wedge` | Required |
| `power_address` | IP address of unit | Required |
| `power_user` | Username to access unit | Optional |
| `power_pass` | Password to access unit | Optional |



** How to use the virsh power type

Consider a machine backed by a KVM, accessed via `virsh`.  You can create a corresponding MAAS machine and set its power parameters with a command like this one:

    maas admin machines create \
    architecture=amd64 \
    mac_addresses=52:54:00:15:36:f2 \
    power_type=virsh \
    power_parameters_power_id=f677a842-571c-4e65-adc9-11e2cf92d363 \
    power_parameters_power_address=qemu+ssh://stormrider@192.168.123.1/system \
    power_parameters_power_pass=xxxxxxxx

If successful, this will return: 

    Success.

Machine-readable output follows this announcement.  The JSON generated by this command is shown in the detail block.

<details><summary>MAAS command JSON response</summary>
```
{
    "storage": 0.0,
    "tag_names": [],
    "special_filesystems": [],
    "memory": 0,
    "boot_disk": null,
    "virtualblockdevice_set": [],
    "hardware_info": {
        "system_vendor": "Unknown",
        "system_product": "Unknown",
        "system_family": "Unknown",
        "system_version": "Unknown",
        "system_sku": "Unknown",
        "system_serial": "Unknown",
        "cpu_model": "Unknown",
        "mainboard_vendor": "Unknown",
        "mainboard_product": "Unknown",
        "mainboard_serial": "Unknown",
        "mainboard_version": "Unknown",
        "mainboard_firmware_vendor": "Unknown",
        "mainboard_firmware_date": "Unknown",
        "mainboard_firmware_version": "Unknown",
        "chassis_vendor": "Unknown",
        "chassis_type": "Unknown",
        "chassis_serial": "Unknown",
        "chassis_version": "Unknown"
    },
    "address_ttl": null,
    "memory_test_status": -1,
    "other_test_status_name": "Unknown",
    "osystem": "",
    "status_message": "Commissioning",
    "netboot": true,
    "physicalblockdevice_set": [],
    "node_type": 0,
    "cpu_test_status": -1,
    "memory_test_status_name": "Unknown",
    "bcaches": [],
    "storage_test_status": 0,
    "system_id": "bhxws3",
    "status": 1,
    "commissioning_status": 0,
    "power_type": "virsh",
    "locked": false,
    "numanode_set": [
        {
            "index": 0,
            "memory": 0,
            "cores": []
        }
    ],
    "bios_boot_method": null,
    "fqdn": "ace-swan.maas",
    "node_type_name": "Machine",
    "hostname": "ace-swan",
    "volume_groups": [],
    "testing_status": 0,
    "network_test_status": -1,
    "other_test_status": -1,
    "interface_test_status": -1,
    "hwe_kernel": null,
    "blockdevice_set": [],
    "testing_status_name": "Pending",
    "power_state": "unknown",
    "min_hwe_kernel": "",
    "owner": "admin",
    "distro_series": "",
    "storage_test_status_name": "Pending",
    "cpu_speed": 0,
    "swap_size": null,
    "cpu_test_status_name": "Unknown",
    "hardware_uuid": null,
    "architecture": "amd64/generic",
    "pool": {
        "name": "default",
        "description": "Default pool",
        "id": 0,
        "resource_uri": "/MAAS/api/2.0/resourcepool/0/"
    },
    "cache_sets": [],
    "pod": null,
    "iscsiblockdevice_set": [],
    "disable_ipv4": false,
    "status_action": "",
    "boot_interface": {
        "name": "eth0",
        "id": 10,
        "product": null,
        "system_id": "bhxws3",
        "effective_mtu": 1500,
        "children": [],
        "link_connected": true,
        "enabled": true,
        "interface_speed": 0,
        "numa_node": 0,
        "firmware_version": null,
        "parents": [],
        "discovered": null,
        "params": "",
        "links": [],
        "sriov_max_vf": 0,
        "tags": [],
        "type": "physical",
        "vlan": null,
        "vendor": null,
        "link_speed": 0,
        "mac_address": "52:54:00:15:36:f2",
        "resource_uri": "/MAAS/api/2.0/nodes/bhxws3/interfaces/10/"
    },
    "cpu_count": 0,
    "domain": {
        "authoritative": true,
        "ttl": null,
        "resource_record_count": 0,
        "name": "maas",
        "is_default": true,
        "id": 0,
        "resource_uri": "/MAAS/api/2.0/domains/0/"
    },
    "current_testing_result_id": 7,
    "default_gateways": {
        "ipv4": {
            "gateway_ip": null,
            "link_id": null
        },
        "ipv6": {
            "gateway_ip": null,
            "link_id": null
        }
    },
    "interface_set": [
        {
            "name": "eth0",
            "id": 10,
            "product": null,
            "system_id": "bhxws3",
            "effective_mtu": 1500,
            "children": [],
            "link_connected": true,
            "enabled": true,
            "interface_speed": 0,
            "numa_node": 0,
            "firmware_version": null,
            "parents": [],
            "discovered": null,
            "params": "",
            "links": [],
            "sriov_max_vf": 0,
            "tags": [],
            "type": "physical",
            "vlan": null,
            "vendor": null,
            "link_speed": 0,
            "mac_address": "52:54:00:15:36:f2",
            "resource_uri": "/MAAS/api/2.0/nodes/bhxws3/interfaces/10/"
        }
    ],
    "status_name": "Commissioning",
    "commissioning_status_name": "Pending",
    "owner_data": {},
    "ip_addresses": [],
    "raids": [],
    "network_test_status_name": "Unknown",
    "description": "",
    "current_commissioning_result_id": 6,
    "interface_test_status_name": "Unknown",
    "current_installation_result_id": null,
    "zone": {
        "name": "default",
        "description": "",
        "id": 1,
        "resource_uri": "/MAAS/api/2.0/zones/default/"
    },
    "resource_uri": "/MAAS/api/2.0/machines/bhxws3/"
}
```
</details>

[/tab]
[/tabs]

** Which BMC drivers are supported

MAAS supports many types of BMC hardware, though not all the drivers have the same capabilities. See the below table for a feature comparison of the BMC drivers currently supported by MAAS.

<details><summary>Tell me about BMC</summary>

BMC, or "Baseboard Management Controller," is an extra micro-controller on the motherboard of a server which forms the interface between system-management software and the device's hardware.  The BMC can collect data from attached sensors, alert administrators to issues, and respond to remote-control commands to control system operation or power state, independent of the system's CPU.

In the context of MAAS, the BMC is generally controlled by SNMP commands.  Any given BMC will function in the context of one or more "power types," which are physical interfaces that permit use of the IPMI ("Intelligent Platform Management Interface") protocol.  Each power type has a different set of expected parameters required to access and command the BMC.

</details>

<table>
<colgroup>
<col width="35%" />
<col width="12%" />
<col width="10%" />
<col width="14%" />
<col width="15%" />
<col width="11%" />
</colgroup>
<thead>
<tr class="header">
<th align="left">Power Driver (<em>X=supported</em>)</th>
<th>PXE Next Boot</th>
<th>Power Querying</th>
<th>Chassis/Pod Configuration</th>
<th>Enhanced UI Error Reporting</th>
<th>BMC Enlistment</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">American Power Conversion (APC) - PDU</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr class="even">
<td align="left">Cisco UCS Manager</td>
<td>X</td>
<td>X</td>
<td>X</td>
<td></td>
<td></td>
</tr>
<tr class="odd">
<td align="left">Digital Loggers, Inc. - PDU</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr class="even">
<td align="left">Facebook's Wedge <code>*</code></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr class="odd">
<td align="left">HP Moonshot - iLO Chassis Manager</td>
<td>X</td>
<td>X</td>
<td>X</td>
<td></td>
<td></td>
</tr>
<tr class="even">
<td align="left">HP Moonshot - iLO4 (IPMI)</td>
<td>X</td>
<td>X</td>
<td></td>
<td></td>
<td>X</td>
</tr>
<tr class="odd">
<td align="left">IBM Hardware Management Console (HMC)</td>
<td>X</td>
<td>X</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr class="even">
<td align="left">IPMI</td>
<td>X</td>
<td>X</td>
<td></td>
<td>X</td>
<td>X</td>
</tr>
<tr class="odd">
<td align="left">Intel AMT</td>
<td>X</td>
<td>X</td>
<td></td>
<td>X</td>
<td></td>
</tr>
<tr class="even">
<td align="left">Manual</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr class="odd">
<td align="left">Microsoft OCS - Chassis Manager</td>
<td>X</td>
<td>X</td>
<td>X</td>
<td></td>
<td></td>
</tr>
<tr class="even">
<td align="left">OpenStack Nova</td>
<td></td>
<td>X</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr class="odd">
<td align="left">Rack Scale Design</td>
<td>X</td>
<td>X</td>
<td>X</td>
<td></td>
<td></td>
</tr>
<tr class="even">
<td align="left">Redfish</td>
<td>X</td>
<td>X</td>
<td></td>
<td></td>
<td>X</td>
</tr>

<tr class="odd">
<td align="left">SeaMicro 15000</td>
<td>X</td>
<td>X</td>
<td>X</td>
<td></td>
<td></td>
</tr>
<tr class="even">
<td align="left">Sentry Switch CDU - PDU</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr class="odd">
<td align="left">VMWare</td>
<td>X</td>
<td>X</td>
<td>X</td>
<td></td>
<td></td>
</tr>
<tr class="even">
<td align="left">Virsh (virtual systems)</td>
<td>X</td>
<td>X</td>
<td>X</td>
<td></td>
<td></td>
</tr>
</tbody>
</table>

`*` The 'Facebook's Wedge' OpenBMC power driver is considered experimental at this time.

[tabs]
[tab version="v3.4 Snap,v3.4 Packages,v3.3 Snap,v3.3 Packages,v3.2 Snap,v3.2 Packages,v3.1 Snap,v3.1 Packages,v3.0 Snap,v3.0 Packages"]
** How to configure and use IBM Z with MAAS

The IBM Z or LinuxONE system can host MAAS controllers and is able to deploy predefined logical partitions (LPARs) KVM host(s), and virtual machines, if the mainframe is set up properly for MAAS.

The basic architecture is similar to this:

<a href="https://discourse.maas.io/uploads/default/original/2X/d/d78aec0bd5d5f485697701ed7316944f918fef94.png" target = "_blank"><img src="https://discourse.maas.io/uploads/default/original/2X/d/d78aec0bd5d5f485697701ed7316944f918fef94.png"></a>

Networking would be structured like this:

<a href="https://discourse.maas.io/uploads/default/original/2X/8/841305949182ba64037f9806396a0e60fdc46d23.png" target = "_blank"><img src="https://discourse.maas.io/uploads/default/original/2X/8/841305949182ba64037f9806396a0e60fdc46d23.png"></a>

Note that net-booting the KVM guests (through the two bridges) can be problematic.  There are two options:

1. Adding VNIC characteristics to enable "learning" on the network interface that's the base for bridge "br2."  This is the recommended approach.

2. Enable full promiscuous bridge port mode at the network interface that's the base for bridge "br2."  This approach is not recommended because it has some built-in limitations.

MAAS will automatically configure option 1 for you, in case an LPAR is deployed as KVM host (the bridge names may differ).

In order to achieve this configuration, there are a number of steps that must be executed; specifically, you must know how to:

- [Evaluate IBM Z requirements](#heading--ibm-z-requirements)
- [Login to the IBM Z](#heading--ibm-z-login)
- [Set up a suitable partition for MAAS](#heading--set-up-ibm-z-partition)
- [Set up networking for MAAS](#heading--set-up-ibm-z-networking)
- [Set up storage for MAAS](#heading--set-up-ibm-z-storage)
- [Set the partition boot parameters](#heading--set-the-boot-parameters)
- [Set up your IBM Z virtual machines for enlistment](#heading--set-up-ibm-z-enlistment)

The MAAS controller does not necessarily need to run on an LPAR on the IBM Z system itself, but can also run on a different system outside. But since the MAAS controller requires a network connection to the hardware management console (HMC), it is recommended to keep it co-located and (for security reasons) as close as possible to the HMC and run it in a dedicated LPAR.

Such a MAAS controller LPAR should have at least two SMT hardware threads (one IFL), since it runs several services (bind, rack-, region-controller and more), 16 GB RAM and 100 GB disk space - recommended is to use the double amount of these resources.

The resources of the LPARs to deploy on ('machines' in terms of MAAS) depending on the use case. LPARs that are deployed as KVM host would of course require significantly more resources to be able to host KVM guest on top.

There are several constraints on the definition and setup of the 'machine' LPARs - please see below.

*** Evaluate IBM Z requirements

The system requirements to host MAAS and its virtual machines on the IBM Z platform are as follows:

- IBM z14 GA2 (or newer) or IBM LinuxONE III (or newer)
- HMC running in DPM mode (mandatory, traditional mode is not supported!)
- HMC firmware level H39 - (HMC at H40 and SE at S55) 
- HMCs Rest-API enabled 
- python-zhmcclient (0.29 or later) running on the MAAS controller system, connected to the HMC
- HMC user ID for the zhmcclient access to the HMC API (must have permissions for the “Manage Web Services API Logs” role and “Manage Web Services API Logs” role)
- I/O auto-configuration enabled for the ‘machine’ LPARs
- zFCP (SCSI) disk storage (only, no DASD support), recommended are two disks, one defined as type ‘boot,’ the second as type ‘data’
- a dedicated storage group per ‘machine’ LPAR; these must include the dedicated zFCP storage for this particular managed LPAR only (‘boot’ and ‘data’ for LPAR n) - but no additional shared storage!
- qeth network devices (Hipersockets or OSA, recommended); at least one qeth NIC, recommended two (or more)
- Ubuntu Server 20.04 installed on a dedicated system (LPAR or PC), that acts as MAAS Controller
- one or more LPARs as ‘machines’ (aka MAAS deployment targets)

Be aware that these are minimum system requirements.

*** Access the HMC and login to the IBM Z

To login to the HMC, you must have at least “system programmer” privileges. Gaining that level of access is beyond the scope of this document. Once you are sure that you have the necessary access, you first need to navigate to the Hardware Management Console (HMC) application in your Web browser:

<a href="https://discourse.maas.io/uploads/default/original/2X/d/d085c8113e403546484778c858c27344e8986597.png" target = "_blank"><img src="https://discourse.maas.io/uploads/default/original/2X/d/d085c8113e403546484778c858c27344e8986597.png"></a>

Click on the "Log on..." link, which will bring you to a login screen:

<a href="https://discourse.maas.io/uploads/default/original/2X/5/5ccdfac4dc985260dcedd01284d24c5e8e5199d9.png" target = "_blank"><img src="https://discourse.maas.io/uploads/default/original/2X/5/5ccdfac4dc985260dcedd01284d24c5e8e5199d9.png"></a>

Upon successfully logging on, you will land on the Welcome Screen:

<a href="https://discourse.maas.io/uploads/default/original/2X/d/d18afe140a1971621ed44fa5fae36033927e293e.png" target = "_blank"><img src="https://discourse.maas.io/uploads/default/original/2X/d/d18afe140a1971621ed44fa5fae36033927e293e.png"></a>

Select the "Tasks Index" on the left-hand navigation:

<a href="https://discourse.maas.io/uploads/default/original/2X/c/c030c8280b0a6dcfdd0365f9cf50238ae708e34b.jpeg" target = "_blank"><img src="https://discourse.maas.io/uploads/default/original/2X/c/c030c8280b0a6dcfdd0365f9cf50238ae708e34b.jpeg"></a>

From here, you will be able to access the commands needed to prepare your IBM Z to host MAAS.

*** Set up a suitable IBM Z partition for a MAAS machine

In order to prevent MAAS from taking over the entire system, you must assign both the controller and the ‘machines’ / KVM hosts to specific partitions, with suitable limitations. To set up suitable IBM Z partitions for hosting MAAS, you must choose “Partition Details” from the “Tasks Index,” which will bring you to a screen like this one:

<a href="https://discourse.maas.io/uploads/default/original/2X/2/29e0cc00d68a5add1b13b1d50313ff6966f251a9.png" target = "_blank"><img src="https://discourse.maas.io/uploads/default/original/2X/2/29e0cc00d68a5add1b13b1d50313ff6966f251a9.png"></a>

You must then choose the “target object” (in this case we’ve chosen TA05) to be operated upon:

<a href="https://discourse.maas.io/uploads/default/original/2X/7/754c4926ecf5d9330b60c9b58bdd15bde6f24144.png" target = "_blank"><img src="https://discourse.maas.io/uploads/default/original/2X/7/754c4926ecf5d9330b60c9b58bdd15bde6f24144.png"></a>

Click “OK,” and you’ll arrive at a screen similar to the one below:

<a href="https://discourse.maas.io/uploads/default/original/2X/0/0ecf9bd89c132fd2c7ff8b879dd6c1b4d3090a99.png" target = "_blank"><img src="https://discourse.maas.io/uploads/default/original/2X/0/0ecf9bd89c132fd2c7ff8b879dd6c1b4d3090a99.png"></a>

Make sure you’re on the “Partitions” tab, and select the desired object (“TA05…”):

<a href="https://discourse.maas.io/uploads/default/original/2X/0/018d8309a1a16571df56a6672cff26e60f42075a.jpeg" target = "_blank"><img src="https://discourse.maas.io/uploads/default/original/2X/0/018d8309a1a16571df56a6672cff26e60f42075a.jpeg"></a>

Right-click on the selected object and select “Partition Details:”

<a href="https://discourse.maas.io/uploads/default/original/2X/5/5a7f696435b504eb212234acdd09c928f16b1670.jpeg" target = "_blank"><img src="https://discourse.maas.io/uploads/default/original/2X/5/5a7f696435b504eb212234acdd09c928f16b1670.jpeg"></a>

On the “General” tab, edit the partition details to suit your proposed MAAS deployment:

<a href="https://discourse.maas.io/uploads/default/original/2X/6/60ff5ca98d8b615ee4a947607872c973cf2c7f41.png" target = "_blank"><img src="https://discourse.maas.io/uploads/default/original/2X/6/60ff5ca98d8b615ee4a947607872c973cf2c7f41.png"></a>

Next, you will set up the networking details for this partition, as shown in the following section.

*** Set up IBM Z networking for a MAAS machine

To properly enable networking within the IBMZ partitions, you must change to the “Network” tab under “Partition Details:”

<a href="https://discourse.maas.io/uploads/default/original/2X/d/daf386497781df42ba7ffaa518c1f186ebef66ee.png" target = "_blank"><img src="https://discourse.maas.io/uploads/default/original/2X/d/daf386497781df42ba7ffaa518c1f186ebef66ee.png"></a>

Click on the NIC of interest to bring up the “NIC Details” screen:

<a href="https://discourse.maas.io/uploads/default/original/2X/e/e9b65711cf97dd722db1b1df4b69d4f590166a99.png" target = "_blank"><img src="https://discourse.maas.io/uploads/default/original/2X/e/e9b65711cf97dd722db1b1df4b69d4f590166a99.png"></a>

Confirm that the parameters on this screen are consistent with your planned MAAS deployment, then bring up the network adapter(either OSA or Hipersockets) by selecting it:

<a href="https://discourse.maas.io/uploads/default/original/2X/0/0a0873d7cd40147884c861d1fcde15ddc37c8853.png" target = "_blank"><img src="https://discourse.maas.io/uploads/default/original/2X/0/0a0873d7cd40147884c861d1fcde15ddc37c8853.png"></a>

Ensure that all settings on the “General” tab conform to your planned MAAS deployment; then select the “Network Interface Cards” tab on the left-hand navigation:

<a href="https://discourse.maas.io/uploads/default/original/2X/0/0a0873d7cd40147884c861d1fcde15ddc37c8853.png" target = "_blank"><img src="https://discourse.maas.io/uploads/default/original/2X/0/0a0873d7cd40147884c861d1fcde15ddc37c8853.png"></a>

Again, ensure that the parameters associated with the networking arrangement are consistent with your planned MAAS deployment.

Next, you will set up the storage layout for your MAAS partition(s).

*** Set up IBM Z storage for a MAAS machine

To set up suitable storage for a MAAS deployment, you should bring up the “Partition Details” for your chosen MAAS partition and select the “Storage” tab from the left-hand navigation:

<a href="https://discourse.maas.io/uploads/default/original/2X/c/c25792eeacd5aef57ca74a68b203c23ed74268d7.png" target = "_blank"><img src="https://discourse.maas.io/uploads/default/original/2X/c/c25792eeacd5aef57ca74a68b203c23ed74268d7.png"></a>

Choose the “VOLUMES” sub-tab, and lick on the hyperlinked partition name to bring up the storage configuration tab:

<a href="https://discourse.maas.io/uploads/default/original/2X/c/cf8d1427abda94ccd3b79966d06bee210ac1240b.png" target = "_blank"><img src="https://discourse.maas.io/uploads/default/original/2X/c/cf8d1427abda94ccd3b79966d06bee210ac1240b.png"></a>

Click on “GET DETAILS” for the Boot Volume in the Volume list to bring up the “Configuration details” screen:

<a href="https://discourse.maas.io/uploads/default/original/2X/a/a081c97b8196e708495156187b983b70c32fcdc5.png" target = "_blank"><img src="https://discourse.maas.io/uploads/default/original/2X/a/a081c97b8196e708495156187b983b70c32fcdc5.png"></a>

Ensure that the Boot Volume is configured appropriately for your planned MAAS deployment, then click “Done.” Then return to the storage configuration tab and choose the Data Volume, and tune it to the appropriate parameters.

Next, choose the “ADAPTERS” sub-tab to bring up information on the storage adapters:

<a href="https://discourse.maas.io/uploads/default/original/2X/8/821edff17e3fe8f2fbf9b5cb1682928dc9bb34d7.png" target = "_blank"><img src="https://discourse.maas.io/uploads/default/original/2X/8/821edff17e3fe8f2fbf9b5cb1682928dc9bb34d7.png"></a>

Set any necessary parameters to conform to your planned MAAS deployment.

*** Set the partition boot parameters

Return to the “Partition Details” screen and select the “Boot” tab in the left-hand navigation:

<a href="https://discourse.maas.io/uploads/default/original/2X/c/c5df4937135c1a9a1758b20855742bd038700c65.png" target = "_blank"><img src="https://discourse.maas.io/uploads/default/original/2X/c/c5df4937135c1a9a1758b20855742bd038700c65.png"></a>

Change any settings as necessary to support your planned MAAS deployment.

*** Set up your IBM Z virtual machine for enlistment

To cause IBM Z KVM partition guests to enlist, it’s necessary to manually put in the BMC information for each guest. MAAS can then detect the guest, enlist it, and boot it as necessary.
[/tab]
[tab version="v2.9 Snap,v2.9 Packages"]
MAAS 2.9 does not support IBM-Z.  To obtain a version of MAAS which supports IBM-Z, please upgrade to MAAS version 3.0 or greater.
[/tab]
[/tabs]

* Python API client reference
The `python-libmaas` client library is an asyncio-based client library whose purpose is to allow developers, integrators and administrators to better interact with MAAS.  This software is in development and does not yet support all MAAS endpoints (nor all operations).

**** This article will help you learn:

- [About library endpoints](#heading--library-endpoints)
- [How to install and use python-libmaas](#heading--installation-and-usage-of-python-libmaas)

** About library endpoints

At this time, the client library supports these eight endpoints:

- account
- boot-sources, boot-resources
- machines, devices, region controllers, rack controllers
- events
- configuration
- tags
- version
- zones

There are two resources that will help you better understand the above terms and how they are used:

- [Concepts and terms](/t/maas-glossary/5416)
- [API documentation](https://maas.io/docs/api)`↗`

** How to install and use python-libmaas

For installation and initial steps, see these external links:

- [Python libmaas on github](https://github.com/maas/python-libmaas)`↗`
- [Python libmass in the MAAS github](http://maas.github.io/python-libmaas/index.html)`↗`

For examples, see the [Python libmaas client guide](https://maas.github.io/python-libmaas/client/index.html)`↗`.

For pypi information, see [this external link](https://pypi.python.org/pypi/python-libmaas)`↗`.

* Rack controllers

A rack controller provides four services:

- DHCP
- TFTP
- HTTP (for images)
- power management

A rack controller is attached to each "fabric". As the name implies, a typical setup is to have a rack controller in each data centre server rack. The rack controller will cache large items for performance, such as operating system install images, but maintains no independent state other than the credentials required to talk to the region controller.

** Tell me about fabrics

A fabric is simply a way of linking [VLANs](/t/maas-glossary/5416#heading--vlans) (Virtual LANs) together.  If you're familiar with a VLAN, you know that it's designed to limit network traffic to specific ports (e.g., on a [switch](/t/maas-glossary/5416#heading--switch)) or by evaluating labels called "tags" (unrelated to MAAS tags).  By definition, this would mean that two VLANs can't communicate with each other -- it would defeat the purpose of the VLAN -- unless you implement some extraordinary measures.

For example, let's say that your [hospital](/t/bootstrap-maas/5092) has three key functions: Patient management, Accounting, and Facilities, each on their own VLAN.  Let's say that there are some situations in which you need to share data between all three of these functions.  To accomplish this, you can create a fabric that joins these three VLANS.  Since this fabric just makes it possible for these VLANs to communicate, you can manage the cross-VLAN access with additional software, or permissions, depending on your application software architecture.

You can learn more about fabrics in the [Concepts and terms](/t/maas-glossary/5416#heading--fabrics) section of this documentation.


* RBAC and candid
** Multi-tenancy in MAAS

Likewise, you want to grant fine-grained access-controls to different users, based on assigned roles.  Working in concert with RBAC and Candid, MAAS can restrict user access and actions based on four roles:

- Administrator: can access all settings and perform any operation on any machine in any resource pool; equivalent to a MAAS administrator.
- Operator: can act as a MAAS administrator, but only within an assigned resource pool.  Machines in other resource pools -- and system settings -- are not accessible.
- User: can access and manipulate machines that are not currently allocated to other users, within the confines of an assigned resource pool; equivalent to a MAAS user, but only over the resource pool.  Users cannot access any settings.
- Auditor: can view information for all machines within an assigned resource pool; cannot make any changes or alter any settings.

MAAS controls access with the help of RBAC (Role-Based Access Control), Candid (Canonical Identity Manager), and an identity service of your choice, such as SSO (Single Sign-On).  From this point on, we'll refer to this solution as MAAS/RBAC, even though it uses more than two tools.  You can design and deploy a carefully-controlled MAAS environment using MAAS/RBAC.

As a MAAS/RBAC administrator or designer, you should understand the concept of multi-tenancy.  Multi-tenancy means that groups of users own a group of resources (machines), but without knowing about other groups of users -- or their machines.  A common multi-tenancy use case provides different sets of machines for different users or groups of users. 

MAAS alone can achieve this, to some degree, by allowing users to allocate machines, but this approach has some drawbacks:

- Other administrative users can still see the allocated machines, as well as release and reallocate someone else's machines for themselves.
- Other administrative users can take administrative actions on someone else's machines (e.g., deployment).

With MAAS/RBAC, an operator can act as administrator for one resource pool, without the ability to manipulate someone else's machines. Users can only operate within the confines of their resource pool, and auditors can review actions without making any changes.

*** How resource pools are integrated into MAAS/RBAC

Resource pools are just a way of grouping machines.  Any given machine can be assigned to exactly one resource pool.  If you control access to resource pools, and you assign roles properly, you can effectively control user access.

Note that just using resource pools to hide machines is a flawed access control approach, known as "security by obscurity."  What users don't know will hurt you when the users figure it out. More often than not, users figure these things out entirely by accident, and hurt you unintentionally while trying to do the right thing.

Consequently, there must also be some means of active authorisation that allows access to a resource pool.  That authorisation must be based on user identity. There must be some way of controlling what the user can do with that resource pool.  In the parlance of standard security model, "what the user can do" would be called "privilege", but for the purposes of MAAS, we simply call them "permissions."  

*** About identity services

MAAS/RBAC will interface with many identity services, using Candid as a mediator.  While the choice of identity service is up to you, we should cover some general principles of identity servers as they relate to MAAS/RBAC.  Let's take a closer look at [Ubuntu Single Sign-On (SSO)](https://help.ubuntu.com/community/SingleSignOn)`↗`

SSO permits users to log in once to access many network services.  SSO centralises authentication (via Kerberos), account management (via LDAP), resource-sharing (via `pam_mount`), and limited authorisation through group memberships in LDAP, coupled with file system permissions.

RBAC (Role-based access control) does not authenticate users or verify their identity; that function is assigned to the identity service you choose.  RBAC does receive an identity token or "macaroon" (via Candid) that RBAC uses to assign user roles.  MAAS uses these roles, in turn, to control user access and actions.

** How Candid fits into the picture

Direct authentication involves a user entering something unique in response to a challenge, in order to gain access.  "Something unique" means "something you know, something you have, or something you are", e.g., a password, a hardware key, or a fingerprint, respectively.  Authentication can be automated with private/public key exchanges, protected with a password on the first exchange.  Adding another access point (another trusted client) usually means providing a public key, setting a password, or registering some biometric data.  Direct authentication works well when there are a limited number of clients and not a lot of user turnover.

Increase the number of users and services that need to authenticate, and direct authentication becomes an IT nightmare: generating access requests; validating requests; setting up authentication; and then managing access as users move around the organisation.  [Candid](https://github.com/canonical/candid)`↗`, the Canonical identity service, was designed to meet this need.  Candid acts as an authentication gateway that connects a service like RBAC to your chosen identity service.

Candid manages authenticated users via special access tokens ([macaroons](https://askubuntu.com/questions/940640/what-is-a-macaroon))`↗` that confirm user identity.  Unlike standard access tokens, macaroons can be verified independently, in a standard way, so they reduce the network traffic and delays of repeatedly querying the identity server.  Traditional access tokens must be short-lived; macaroons are valid for much longer and they can be refreshed easily.  Macaroons can also be bound to TLS certificates.  And macaroons can be used by multiple clients and services with no loss of security.

Candid can do the following things:

- find users by various identity parameters, such as e-mail, full name, last login time, etc.
- show details for a user, based on e-mail or username.
- add or remove users from ACLs (access control lists), or list members of an ACL.
- add or remove users from arbitrary groups.
- clear the multi-factor authentication (MFA) credentials for a specific user.
- manage Candid agents.

Candid can use certificates and agents, if desired.  You specify the identity provider by URL when instantiating the program.

When a user tries to log into a MAAS which is working with RBAC, MAAS redirects that login to the RBAC server.  RBAC, in turn, requests authentication via Candid, which then consults the specified identity server (at the URL provided on startup).  If the user is authenticated, Candid constructs a macaroon, which is then passed to RBAC and on to MAAS.  This macaroon serves as the user's authentication token until it expires.

** About RBAC

RBAC uses a database to associate a given role with a properly-authenticated user identity.  With RBAC, permissions are associated with roles, not individual users. Within a given resource pool, the role assigned to a properly authenticated user controls what they can and cannot do within that pool.

In the parlance of RBAC, MAAS would be a service, while each resource pool would be considered a separate scope. RBAC/MAAS also recognises scopes that are not tied to machines, including:

- DNS
- Availability zones
- Images
- System settings

RBAC can help MAAS also control access to these "non-machine resources".

Note that it is possible for a given user to be an operator for one resource pool, a user for another, and an auditor for still another, but have no ability to change system settings or manipulate images.  Nothing about RBAC prohibits this arrangement.

*** The MAAS/RBAC permissions model

Here is a thumbnail sketch of the permissions model:

- MAAS maintains resource pools, which are a machine attribute.  Every machine can be assigned to one and only one resource pool.  
- RBAC maintains roles, which are associated with user identities.  For a given user identity to carry out a particular operation in MAAS, that user identity must correspond to a role that has permission to carry out that operation in a given resource pool.
- Candid vouches for the user with a macaroon.
- Some identity service (e.g., SSO) authenticates the user to Candid, so that macaroons are not generated for unrecognised users.

Relationships between roles, resource pools, and users is maintained by RBAC as a source of truth.  MAAS mediates access to resource pools, based on user roles, using information obtained from RBAC. 

*** The MAAS/RBAC security architecture

The following diagram will give you a graphical view of how MAAS, RBAC, Candid, and an identity provider work together to control access to MAAS resources:

<a href="https://discourse.maas.io/uploads/default/original/2X/4/4433c6995c342efebe565f4888a46d7107d1525f.png" target = "_blank"><img src="https://discourse.maas.io/uploads/default/original/2X/4/4433c6995c342efebe565f4888a46d7107d1525f.png"></a>

The step-by-step walk-through of the MAAS/RBAC relationship goes like this:

- When MAAS is initiated with RBAC connected, MAAS pushes a list of resource pools and a global resource key to RBAC.  The global resource key covers things that are not added to resource pools, such as devices or settings.
- When a user tries to login, MAAS redirects that login request to RBAC.
- RBAC, in turn, requests an authentication check from Candid.
- Candid attempts to authenticate the user via whatever identity provider was configured when Candid was started (e.g., SSO).
- If Candid successfully authenticates the user, Candid creates a macaroon as a temporary identity token for the user.
- Candid passes the macaroon back to RBAC.
- RBAC passes the macaroon, in turn, to MAAS, along with a dictionary of groups, role(s) and resource pools corresponding to that user.
- As needed, MAAS then mediates access to resource pools, using the macaroon to recognise the user and their group(s), and using the role/resource pool pairs to adjudicate access.

Note that RBAC does not adjudicate individual permissions against resource pools. RBAC only sends MAAS the combination of users, roles, and related resource pools to MAAS when requested.  The MAAS code has a built-in understanding of the four roles (user, administrator, operator, and auditor) and what those roles can and cannot do with a given item.  

*** How the four MAAS RBAC roles protect MAAS resources

The most important thing to understand about MAAS RBAC roles is that restricted users cannot see or interact with machines in resource pools that aren't permitted for them. This is more than just "security by obscurity," because even if a user knows the name or system ID of a machine in a non-permitted resource pool, that user can't access it.  Removing non-permitted machines from view, though, prevents confusion about what the user can and can't do.

Here is a quick breakdown of how the four roles experience MAAS:

- Administrator: an administrator can do anything that a normal MAAS administrator can do in the absence of RBAC.  This means an admin can see all resource pools, take any action against any machine, and change any MAAS settings.

- Operator: an operator can do almost anything that a normal MAAS administrator can do, but only against machines in their permitted resource pools.  An operator cannot see or change system settings.

- User: a user can do just what a normal MAAS user can do.  They can only view and allocate machines that aren't allocated to someone else, even if that someone else is another user in the same resource pool.  Users can't change or access settings at all.

- Auditor: an auditor can view anything about machines in the resource pool(s) for which they are permitted.  Auditors cannot change or access settings.

MAAS makes no assumptions about how these roles might be used in the day-to-day operation of your MAAS instance.  The capabilities listed above form the complete set of what these roles can do.


* General reference

The reference material in this section provides technical descriptions of MAAS and related tools.

|                    |                                                                           |
|--------------------|---------------------------------------------------------------------------|
| General reference  | Release notes, settings. glossary, and the like                           |
| API reference      | Documentation on the MAAS API, its clients, and authentication mechanisms |
| Scripts            | Information about scripts for commissioning, cloud-init, curtin, etc.     |
| Logs               | Details on the many MAAS log files and what they contain                  |
| Machine parameters | Everything you need to know about power drivers and storage layouts       |

Make sure to also check out the [Tutorials](/t/tutorials/6140) for step-by-step instructions that help you get familiar with MAAS, the [How-to guides](/t/how-to-guides/6663) for instructions on how to achieve specific goals when using MAAS, and the [Explanation](/t/explanation/6141) section for background information.


* Region controllers

A region controller consists of the following components:

- REST API server (TCP port 5240)
- PostgreSQL database
- DNS
- caching HTTP proxy
- web UI

Region controllers are responsible for either a data centre or a single region. Multiple fabrics are used by MAAS to accommodate subdivisions within a single region, such as multiple floors in a data centre.


* Region-rack communication

MAAS communication happens in a strict hierarchy, flowing from the UI/API through the region controller, to the rack controller, to the machines (and back).  While [high availability](/t/how-to-enable-high-availability/5120) (HA) may add controllers, it does not change the flow of communication through the MAAS system.  Understanding this message flow may help you with the machine topics which follow.

**** How machines communicate with the rack controller

All machine communication with MAAS is proxied through rack controllers, including HTTP metadata, DNS, syslog and APT (cache-and-forward proxies via Squid). 

MAAS creates an internal DNS domain, not manageable by the user, and a unique DNS resource for each subnet that is managed by MAAS. Each subnet includes all rack controllers that have an IP on that subnet. Booting machines use the subnet DNS resource to resolve the rack controller available for communication. If multiple rack controllers belong to the same subnet, MAAS uses a round-robin algorithm to balance the load across numerous rack controllers. This arrangement ensures that machines always have a rack controller.

Machines use this internal domain for HTTP metadata queries, APT (proxying via Squid), and Syslog. DNS queries, PXE booting, and NTP polls use IP addresses.

The rack controller installs and configures `bind` as a forwarder. All machines communicate via the rack controller directly.

[note]
Zone management and maintenance still happen within the region controller.
[/note]

<a href="https://discourse.maas.io/uploads/default/original/1X/02a7ca58b989c67c74421b9d5e0c8b32907a2de1.jpeg" target = "_blank"><img src="https://discourse.maas.io/uploads/default/original/1X/02a7ca58b989c67c74421b9d5e0c8b32907a2de1.jpeg"></a>

**** How region and rack controllers communicate

The MAAS region and rack controllers interact in a number of different ways, depending upon the operation you've requested.  Consider the process of commissioning a machine, that is, taking over the machine and gathering information on its available resources, including CPU, RAM, storage, and MIB information (obtainable via LLDP requests).  Here's a rough idea of what that sequence looks like -- a sequence that is representative of the communication between rack and region controllers:

1. An operator makes a request of MAAS, either via the Web UI or the API.  
2. MAAS translates this to an API request to the region controller.
3. The region controller locates the rack controller that has BMC access to the machine in question, that is, the rack controller that can power on that machine.
4. That same rack controller powers on the machine via IPMI request.
5. The rack controller tasked with providing DHCP handles assigning an IP address to the machine via the [DORA](/t/maas-glossary/5416#heading--dhcp) sequence (Discover, Offer, Request, Acknowledge).  **Note** that this rack controller doesn't have to be the same one that powers on the machine.
6. The DHCP-managing rack controller inserts itself as the DHCP "next-server" and requests a network boot.
7. (Still) the same rack controller RPCs the region controller to get what's needed to boot an ephemeral Ubuntu kernel, namely the kernel, any kernel parameters, an initrd daemon, and a squashfs load.
8. That same rack controller transforms the RPC response from the region controller into a valid PXE config and tells the machine to come get its files.
9. The booting machine loads the kernel and initrd, boots with that initrd, and then loads the squashfs, eventually making its way up to an ephemeral Ubuntu instance.
10. The booted machine pulls cloud-init metadata from the region controller, proxying through the rackd.
11. cloud-init uses this metadata to gather resource information about the machine and pass it back to the region controller, again proxied by the rackd.
12. The region controller (regiond or "region daemon") stores this machine information in a postgres database that is accessible only to the regiond, making MAAS truly stateless with respect to machines.

Again, this list doesn't represent every interaction between the controllers and machines, but it gives you a good idea of how MAAS works.


* Search and filter operations
** About the MAAS search parameter

A valid MAAS search parameter looks like this:

<a href="https://discourse.maas.io/uploads/default/original/2X/d/dcf5037cdd886eb85a2d305fd3df111b38865cea.png" target = "_blank"><img src="https://discourse.maas.io/uploads/default/original/2X/d/dcf5037cdd886eb85a2d305fd3df111b38865cea.png"></a>

Each search attribute is placed in the Search box, separated by spaces (if it's an "AND" search operation), or by parentheses and commas (if's it's an "OR" operation).  Specific search parameters use the notation "parameter-name" followed by a colon (":").  For example, here we're searching for "pod:", which equates to the pod name (VM name).  

You can match exactly by using the equals sign ("=") with a filter fragment, or make a partial match by excluding the "=".  You can get an idea how these parameters work by using the filter dropdowns in the UI, which generate a valid search expression in the "Search" box.  There is also a "not" operator ("!") which can be applied to partial or exact matches.

*** Multiple search terms

MAAS uses Boolean AND logic to evaluate multiple search terms. For example, when you type `pod:able,cattle cpu:=5`, MAAS displays machines that belong to pods with names containing `able` OR `cattle` AND having 5 CPU cores. Similarly, if you enter multiple words into the search tool, e.g., `steady able`, MAAS will display machines matching both terms (e.g., `steady` AND `able`).


* Storage layouts reference
There are several possible MAAS storage layouts:

- [Flat layout storage](#heading--flat-storage-layout-reference)
- [LVM storage layout](#heading--lvm-storage-layout-reference)
- [bcache storage layout](#heading--bcache-storage-layout-reference)
- [VMFS6 storage layout](#heading--vmfs6-storage-layout-reference)
- [Blank storage layout](#heading--blank-storage-layout-reference)

The reference descriptions below include the EFI partition. If your system is not using UEFI, regard `sda2` as `sda1` (with an additional 512 MB available to it).

** Flat layout storage reference

With the Flat layout, a partition spans the entire boot disk. The partition is formatted with the ext4 filesystem and uses the `/` mount point:

| Name | Size        | Type | Filesystem | Mount point |
|:----:|------------:|:----:|:----------:|:------------|
| sda  | -           | disk |            |             |
| sda1 | 512 MB      | part | FAT32      | /boot/efi   |
| sda2 | rest of sda | part | ext4       | /           |

The following three options are supported:

1. `boot_size`: Size of the boot partition on the boot disk. Default is 0, meaning not to create the boot partition. The '/boot' will be placed on the root filesystem.

2. `root_device`: The block device on which to place the root partition. The default is the boot disk.

3. `root_size`: Size of the root partition. Default is 100%, meaning the entire size of the root device.

** LVM storage layout reference

The LVM layout creates the volume group `vgroot` on a partition that spans the entire boot disk. A logical volume `lvroot` is created for the full size of the volume group; is formatted with the ext4 filesystem; and uses the `/` mount point:

| Name   | Size        | Type | Filesystem     | Mount point |
|:----:|------------:|:----:|:----------:|:------------|
| sda    | -           | disk |                |             |
| sda1   | 512 MB      | part | FAT32          | /boot/efi   |
| sda2   | rest of sda | part | lvm-pv(vgroot) |             |
| lvroot | rest of sda | lvm  | ext4           | /           |
| vgroot | rest of sda | lvm  |                |             |

The following six options are supported:

1. `boot_size`: Size of the boot partition on the boot disk. Default is 0, meaning not to create the boot partition. The '/boot' will be placed on the root filesystem.
2. `root_device`: The block device on which to place the root partition. The default is the boot disk.
3. `root_size`: Size of the root partition. Default is 100%, meaning the entire size of the root device.
4. `vg_name`: Name of the created volume group. Default is `vgroot`.
5. `lv_name`: Name of the created logical volume. Default is `lvroot`.
6. `lv_size`: Size of the created logical volume. Default is 100%, meaning the entire size of the volume group.

** bcache storage layout reference

A bcache layout will create a partition that spans the entire boot disk as the backing device. It uses the smallest block device tagged with 'ssd' as the cache device. The bcache device is formatted with the ext4 filesystem and uses the `/` mount point. If there are no 'ssd' tagged block devices on the machine, then the bcache device will not be created, and the Flat layout will be used instead:

| Name      | Size        | Type | Filesystem | Mount point |
|:----:|------------:|:----:|:----------:|:------------|
| sda       | -           | disk |            |             |
| sda1      | 512 MB      | part | FAT32      | /boot/efi   |
| sda2      | rest of sda | part | bc-backing |             |
| sdb (ssd) | -           | disk |            |             |
| sdb1      | 100% of sdb | part | bc-cache   |             |
| bcache0   | per sda2    | disk | ext4       | /           |

The following seven options are supported:

1. `boot_size`: Size of the boot partition on the boot disk. Default is 0, meaning not to create the boot partition. The '/boot' will be placed on the root filesystem.
2. `root_device`: The block device upon which to place the root partition. The default is the boot disk.
3. `root_size`: Size of the root partition. Default is 100%, meaning the entire size of the root device.
4. `cache_device`: The block device to use as the cache device. Default is the smallest block device tagged ssd.
5. `cache_mode`: The cache mode to which MAAS should set the created bcache device. The default is `writethrough`.
6. `cache_size`: The size of the partition on the cache device. Default is 100%, meaning the entire size of the cache device.
7. `cache_no_part`: Whether or not to create a partition on the cache device. Default is false, meaning to create a partition using the given `cache_size`. If set to true, no partition will be created, and the raw cache device will be used as the cache.

** VMFS6 storage layout reference

The VMFS6 layout is used for VMware ESXi deployments only. It is required when configuring VMware VMFS Datastores. This layout creates all operating system partitions, in addition to the default datastore. The datastore may be modified.  New datastores may be created or extended to include other storage devices. The base operating system partitions may not be modified because VMware ESXi requires them. Once applied another storage layout must be applied to remove the operating system partitions.

| Name | Size      | Type    | Use               |
|:-----|------------:|:----:|:----------|
| sda  | -         | disk    |                   |
| sda1 | 3 MB      | part    | EFI               |
| sda2 | 4 GB      | part    | Basic Data        |
| sda3 | Remaining | part    | VMFS Datastore 1  |
| sda4 | -         | skipped |                   |
| sda5 | 249 MB    | part    | Basic Data        |
| sda6 | 249 MB    | part    | Basic Data        |
| sda7 | 109 MB    | part    | VMware Diagnostic |
| sda8 | 285 MB    | part    | Basic Data        |
| sda9 | 2.5 GB    | part    | VMware Diagnostic |

The following options are supported:

1. `root_device`: The block device upon which to place the root partition. Default is the boot disk.

2. `root_size`: Size of the default VMFS Datastore. Default is 100%, meaning the remaining size of the root disk.

** Blank storage layout reference

The blank layout removes all storage configuration from all storage devices. It is useful when needing to apply a custom storage configuration.

[note]
Machines with the blank layout applied are not deployable; you must first configure storage manually.
[/note]
* Tags
Tags are short, descriptive, searchable words that can be applied to various MAAS objects, including:

- machines (physical and virtual)
- VM hosts
- controllers (rack and region)
- storage (virtual and physical; block devices or partitions)
- network interfaces
- devices
- nodes (in the CLI only)

Tags serve to help you identify, group, and find objects easily, especially when you routinely deploy hundreds of machines.

** About tags and scripts

As with general tag management, tags make scripts easier to manage; grouping scripts together for commissioning and testing, for example:

``` bash
maas $PROFILE node-script add-tag $SCRIPT_NAME tag=$TAG
maas $PROFILE node-script remove-tag $SCRIPT_NAME tag=$TAG
```

MAAS runs all commissioning scripts by default. However, you can select which custom scripts to run during commissioning by name or tag:

``` bash
maas $PROFILE machine commission \
commissioning_scripts=$SCRIPT_NAME,$SCRIPT_TAG
```

You can also select which testing scripts to run by name or tag:

``` bash
maas $PROFILE machine commission \
testing_scripts=$SCRIPT_NAME,$SCRIPT_TAG
```

Any testing scripts tagged with commissioning will also run during commissioning.




* A TCP/IP primer
Good understanding of networking fundamentals makes it much easier to design, operate, and debug MAAS networks. You may have a good grasp of these fundamentals, but not everyone does, and that's perfectly normal.  Here's a quick refresher on TCP/IP networking, with references.  Feel free to pick and choose sections as you need them.

TCP/IP networks evolved to meet a specific need: How can we keep a network functioning if important nodes go offline?  Well, you build the ARPAnet -- now called the Internet -- which relies heavily on TCP/IP networks.  The OSI model underlying TCP/IP can adapt to changing loads, handle significant failures, and strictly limit the network "blast radius" (yes, that's what it's called) when things go wrong.   

This article is designed to help you learn:

- [About the Internet](#heading--about-the-internet)
- [About the OSI model](#heading--about-the-osi-model)
- [About bond interfaces](#heading--about-bond-interfaces)
- [About bridge interfaces](#heading--about-bridge-interfaces)
- [About ARP](#heading--about-arp)
- [About TCP](#heading--about-tcp)
- [About DNS](#heading--about-dns)
- [About other network elements](#heading--other-network-elements)

[note]
While some standard networking concepts, such as PXE booting, RPC, subnets, power drivers, and proxies are not unique to MAAS, they are sometimes used in a unique way by MAAS -- so we explain these concepts in the article entitled "[About MAAS networking](#heading--How-MAAS-networks)".
[/note]

*** About the Internet

A network could be nothing more than a bunch of computers connected with wires. This isn't efficient or affordable.  Obviously, many wires are hard to keep straight. Also, impedance would dampen the signals after a relatively short run of wire.  An easier way is the Internet infrastructure, which is an [access-aggregation-core (AAC)](https://www.cisco.com/c/en/us/td/docs/solutions/Enterprise/Data_Center/DC_Infra2_5/DCInfra_2.html)`↗` network.  AAC looks something like this: 

<a href="https://discourse.maas.io/uploads/default/original/2X/e/e15a35da43b2788883ec014efb1832b8f641e872.jpeg" target="_blank"><img src="https://discourse.maas.io/uploads/default/original/2X/e/e15a35da43b2788883ec014efb1832b8f641e872.jpeg"></a>

Routers are the multiplexing elements in AAC networks.  Ladders of lateral paths aren't theoretical; they exist mostly for performance reasons, like latency, redundancy, cost, and so on.  This means an AAC network incorporates redundant loops or "packet-traps".  TCP/IP has a dedicated solution for this problem, called ["Time To Live"](https://en.wikipedia.org/wiki/Time_to_live#firstHeading)`↗`.

Issues with ladder networks drove the development of cloud network architectures (also known as Clos architectures), which address financial and performance impacts of large networks in a much simpler way. [Cloud networking](#About-cloud-networks) is covered elsewhere, and should be next on your reading list.

[Network switching](https://en.wikipedia.org/wiki/Network_switch#firstHeading)`↗` is a very large topic unto itself.  It's worth catching up if you're weak in this area, since some elements of switching are exposed in MAAS networks.  Also important to review are [routers](https://en.wikipedia.org/wiki/Router_%28computing%29#firstHeading)`↗`, [bridges](https://en.wikipedia.org/wiki/Network_bridge#firstHeading)`↗`, and bonded NICs, aka [link aggregation](https://en.wikipedia.org/wiki/Link_aggregation#firstHeading)`↗`.  All of these come into play every time a MAAS network is modified.

**** Yesterday's phone network is today's Internet

Most of today's modern networking is a direct translation of the landline telephone system into the digital space.  Network switching is really just an outgrowth of [crossbar](https://en.wikipedia.org/wiki/Number_Five_Crossbar_Switching_System#firstHeading)`↗`, which is how local phone calls were "switched" or "routed" to the correct telephone line.  In most cases, every number dialled closed one more relay, with all seven relays making a connection to the target phone line.

Small exchanges often "swallowed" dialled digits. For example, if every local phone number had the exchange "881", those numbers wouldn't trigger any relays beyond just sending the call to the "881" [frameset](https://en.wikipedia.org/wiki/Distribution_frame#firstHeading)`↗`.  In some small exchanges, it wasn't even necessary to dial the exchange, just the four digits of the phone number, if the caller had the same exchange.  Essentially, these were the early [subnets](https://en.wikipedia.org/wiki/Subnetwork#firstHeading)`↗`.

Over time, the increasing density of telephone coverage and self-service long-distance changed all this.  More wires had to be installed, and repeaters were needed to get signals across longer distances as local exchanges were replaced by centralised exchanges called "central offices" (CO).  A CO would have frames for 8 or 10 exchanges, essentially serving the same function as today's network routers.

Repeaters had to be installed at regular intervals to overcome the impedance associated with longer wires.  Those color-camouflaged, "go-away-grey" metal cans (called pedestals) popped up everywhere, partly for easy re-routing of wires, and partly to house repeaters.  We still have those in today's networks, but they're called racks or patch-panels.

In the very early days of long-haul networking, most of the repeaters were owned by the local telephone companies. T1 lines, as they were called, couldn't compete with today's fibre connections, but they did provide a speedy (at the time) 1.5Mbps connection.  For example, in the oil and gas industry of the early 1990s, many of the city offices had wall after wall of T1 lines wired directly into the building.

T1 wasn't designed for network traffic, exactly,  The idea was to multiplex phone calls on one line via Time-Division Multiplexing.  TDM split up the call traffic into little digital packets that were sent on a rotating basis.  The first T1 lines, which showed up around 1962, could handle about 24 calls without the average telephone user noticing.  Telephone linemen generally could tell by the "clipped" nature of the call, as there is a distinctive flatness to the conversation over a digital TDM circuit.  These "little digital packets" formed the model for the packet networking we have today.

The T1 lines used ordinary, double-twisted-pair copper wiring, with repeaters at roughly one-mile intervals (about every fourth pedestal). When WAN and MAN networking became a thing, the phone company just repurposed some of those pairs to carry data traffic.  Many other key elements of TCP/IP, like twisted-pair Ethernet cables, packet-based messaging, and multiplexing, are all just holdovers of the original telephone system, repurposed for computer networking.

**** More about Internet infrastructure

The Internet is theoretically survivable because every computer can connect to every other computer.  That's not standard operating procedure.  High-level networks [(Network Service Providers, NSPs)](https://en.wikipedia.org/wiki/Internet_service_provider#firstHeading)`↗` connect to at least three top level nodes called Network Access Points (NAPs), aka [Internet Exchange Points](https://en.wikipedia.org/wiki/Internet_exchange_point#firstHeading)`↗`.  At these points, packets to jump from one NSP to another. NAPs are public access points, Metropolitan Area Exchanges are private. These are virtually indistinguishable for the purposes of this discussion.

As an aside, many of the MAEs are the residue of the phone company's early T1 lines, which was the initial backbone for the Internet.  These MAEs act just like a NAP for the purposes of this discussion.

A simpler picture of the Internet infrastructure looks like this:

<a href="https://discourse.maas.io/uploads/default/original/2X/b/b8da34432dd443cd3592487f53887f12889cef06.jpeg" target="_blank"><img src="https://discourse.maas.io/uploads/default/original/2X/b/b8da34432dd443cd3592487f53887f12889cef06.jpeg"></a>

In theory, the Internet infrastructure and a cloud network should be very similar.  In practice, they diverge greatly.  The real Internet has horizontal connections running everywhere, based on drivers like cost, security, and performance.  Those vertical connections introduce an interface cost (heterogeneous hardware), a performance hit (varying network speeds), and bandwidth variance (differing node architectures).  Contracts for lateral connections change daily, which changes the routing, which changes the way the Internet behaves for users.

**** About Internet network traffic

On-the-fly, Internet network paths can become very complicated and somewhat unpredictable.  As a result, there's rarely a reason to even count how many hops a message takes, or where it hops, unless you're trying to debug a broken route with, say, [traceroute](https://linux.die.net/man/8/traceroute)`↗`‘↗‘.  From a TCP/IP point of view, it's much easier to ignore the specific network, since each one is custom built, so to speak.  The path can theoretically change every time a message is sent, even between the same two computers.

When it comes to designing and troubleshooting networks, knowing the specific route (almost) never helps.  What we do want to know about is the network traffic between computers.  We have to understand what kind of data travels between computers, besides just the data we send.  Internet data flows are governed by the OSI model.   

*** About the OSI model

This subsection will help you learn about the OSI model and:

- [About the physical layer (L1)](#heading--about-the-physical-layer)
- [About the datalink layer (L2)](#heading--about-the-datalink-layer)
- [About the network layer (L3)](#heading--about-the-network-layer)
- [About the transport layer (L4)](#heading--about-the-transport-layer)
- [About the session layer (L5)](#heading--the-session-layer)
- [About the presentation layer (L6)](#heading--the-presentation-layer)
- [About the application layer (L7)](#heading--the-application-layer)

Networks are really just continuous wires.  We need to understand what travels on those wires, which depends on our perspective -- our level of magnification.  At the highest "zoom" level, all we'll see are electrons travelling down the wire; that's a level of abstraction that isn't comprehensible for debugging purposes.  About all we can tell is whether or not the circuit's dead.

The [Open Systems Interconnection model](https://en.wikipedia.org/wiki/OSI_model)`↗` was created to standardise on a few different levels.  The OSI model looks something like this:

<a href="https://discourse.maas.io/uploads/default/original/2X/7/765cd90cffcecfcc83593cde0483e64977a48223.jpeg" target="_blank"><img src="https://discourse.maas.io/uploads/default/original/2X/7/765cd90cffcecfcc83593cde0483e64977a48223.jpeg"></a>

The OSI model starts just above the raw physics, with the physical layer, also known as Layer 1.

The choice of "1" makes sense, because this is the lowest level we consider. Layers are normally added on top of each other.  For example, if you put six coats of varnish on a piece of furniture, you're going to have six layers.  The first layer you put on wouldn't sensibly be called "layer 6"; neither network layering work that way.

Here's a quick rundown of what each layer does.  Most likely, we won't get into details about all the layers, because the higher you go, the more widely they vary with the user application.  Higher layers don't really help us better understand MAAS networking, any more than watching electrons travel through a wire help us debug a missing packet.

**** About the physical layer (L1)

The phrase "physical layer" may conjure up notions of physics, but don't worry: we look at signals, not electrons.  Here you can learn about the basics of the physical layer, and also:

- [About variable latency](#heading--about-variable-latency)
- [Why the physical layer is not very interesting](#heading--physical-layer-uninteresting)

At the physical layer, we're looking for binary (on/off) signals, set to the cadence of a clock.  Every computer brings its own clock to the party, so we definitely need a way to "synchronise our watches".  [NTP](https://en.wikipedia.org/wiki/Network_Time_Protocol)`↗`, the network time protocol, does the trick.  

***** About variable latency

Variable latency is the important thing to know about the physical layer, because it affects the timing of network traffic. In order to understand variable latency, we need to understand network latency.  Packets aren't sent without some delay, because of:

1. The processing delay - how long it takes the router to process the packet header.
2. A queuing delay - how long the packet sits idle in routing queues.
3. Transmission delay - how long it takes layer 1 to push the packet's bits onto the link.
4. Propagation delay - how long it takes the bits to travel through the wire to the other end.

The size of the queue directly influences how fast data can get onto the link.  The processing and transmission delays are real, though relatively constant.  The propagation delay doesn't just depend on the speed of light, because there may be lots of other "relay" computers in the link.  Propagation depends on network architecture, network congestion, and the number of hops (how many routers between source and destination), among other things.  As we'll see later on, within your enterprise, [modern cloud architectures](#About-cloud-networks) usually create significantly less propagation delay.

Variable-latency networks are "variable" because of the density of network traffic and the complexity of the route between hosts.  We can't predict congestion or routing, although we can influence local routing by choosing the right network architecture.  We can't predict transmission delays, though we can statistically bound them.  Almost all digital networks are considered "variable-latency".

***** The physical layer is not very interesting

Other than verifying that signals are flowing, the physical layer doesn't usually tell us much about what happened to that DHCP request that never made it to the router.  Consequently, we really won't talk that much about the physical layer.  Just know that it's the thing that's passing bits back and forth between hosts, and very occasionally, we need to scan it to debug network issues.

**** About the datalink layer (L2)

The datalink layer (the "link" layer, layer 2 or "L2") has one purpose: send and receive IP datagrams.  L2 doesn't maintain a connection with the target host; it's intentionally "connectionless", and it doesn't guarantee delivery or integrity of packets.  It just ships them between source and destination.  This subsection will help you learn:

- [About MAC frames](#heading--about-frames)
- [About Ethernet](#heading--about-ethernet)
- [About Media Access Control (MAC)](#heading--about-media-access-control)
- [About trunking VLANs](#heading--about-trunking-vlans)
- [About VLANs, subnets, and fabrics](#heading--about-vlans-subnets-and-fabrics)
- [How to visualise the link layer](#heading--visualising-the-link-layer)

At first, the message-agnostic state of the link layer may seem a little weird.  L2 is not without error-checking and recovery code, but it functions efficiently because it isn't concerned with the data or the message containing the data.  That fact can be surprising, since L3 packets are called "datagrams".

A datagram is just a basic network transfer unit -- the indivisible unit for a given layer.  If we're talking about the data-link layer (aka the "link" layer), it's an IEEE 802.xx frame.  At the network layer, it's a data packet.  For the transport layer, it would be called a segment.  Indivisible units in the physical layer are called chips, which are spread-spectrum pulses in the CDMA, noise-utilising transmission system that operates at that layer.

Since datagram isn't carefully used by everyone (think of User Datagram Protocol), we'll agree to call these indivisible layer units PDUs (protocol data units).  This avoids conflation with other uses and reminds you that it's the atomic unit at the current network layer.  Just remember that, at the link layer (L2), it's a frame.

***** About MAC frames

A MAC frame, or just "frame", encapsulates the packets from the network layer so that they can be transmitted on the physical layer.  A frame can be small or big, anywhere from 5 bytes up to the kilobyte range.  The upper size limit is called the maximum transmission unit (MTU) or maximum frame size.  This size is set by the particular network technology in use.

This last observation brings up a good point: In order to talk sensibly about frames, we'd need to say what kind of frame.  With MAAS, we're always talking about packet-switched networks, so there are potentially four frame types to consider: Ethernet, fibre channel, V.42, and PPP (point-to-point protocol).

Happily, MAAS networks almost exclusively use Ethernet, as defined in the [IEEE 802 standards](https://www.ieee802.org/)`↗`, so we'll stick to that particular frame type for this discussion.  Where other frame types may come into play, we'll discuss those as special cases.  

***** About Ethernet

Before explaining an Ethernet Frame, we need to give a little background information about how Ethernet works; otherwise a lot of the frame components either won't make sense, or you'll wonder how it works at all.

Remember earlier, when we talked about voice radio, and the need to say "over"?  Well, Ethernet at the link layer is all about controlling the conversation, so that computers don't "talk over each other".  Ethernet implements an algorithm called CSMA/CD, which stands for "carrier sense multiple access with collision detection."  This algorithm controls which computers can access the shared medium (an Ethernet cable) without any special synchronisation requirements.

"Carrier sense" means that every NIC does what humans do when we're talking: it waits for quiet.  In this case, it's waiting for the network to be quiet, that is, when no signal is being sent on the network.

"Collision detection" means that, should two NICs both start to send on a shared network at the same time (because the network was quiet), they each receive a jam signal.  This signal tells them to wait a specific, randomly-generated amount of time before attempting again.  Every time subsequent messages collide, the NIC waits twice the amount of time it previously waited.  When it waits some maximum number of times, the NIC will declare a failure and report that the message didn't go.  This ensures that only one frame is traversing the network at any given time.

***** About Media Access Control (MAC)

Systems like CSMA/CD are a subset of the Media Access Control (MAC) protocol kit.  MAC is one-half of the link layer, with Logical Link Control (LLC) being the other half -- though these are sometimes called sub-layers. LLC mostly just defines the frame format for the 802.xx protocols, like WiFi, so we can safely ignore it for the purposes of MAAS networking.

If you've worked with networks at all, you've heard of MAC addresses.  Those are basically unique serial numbers assigned to network interface devices (like NICs) at the time of manufacture.  Theoretically, they are unique in the world, not counting virtual NICs in virtual machine environments.  [MAC address collisions](https://kb.vmware.com/s/article/219)`↗` do happen when using VMs, and there are ways to fix it, assuming that your VMs are confined to a subnet.

The MAC sub-layer is connected to the physical layer by a media-independent interface (MII), independent of the actual link protocol (e.g, cellular broadband, Wi-Fi radio, Bluetooth, Cat5e, T1, ...).  You can learn more about the [MII](https://en.wikipedia.org/wiki/Media-independent_interface)`↗` if you're so inclined, but we won't address it again in the context of MAAS networks.

Essentially, the MAC sub-layer grabs higher-level frames and makes them digestible by the physical layer, by encoding them into an MII format.  It adds some synchronisation info and pads the message (if needed).   The MAC sub-layer also adds a frame check sequence that makes it easier to identify errors.

In conventional Ethernet networks, all this looks something like the following:

<a href="https://discourse.maas.io/uploads/default/original/2X/1/14f6847ed92339061eb4515c12ac2b6117d5cd7c.jpeg" target="_blank"><img src="https://discourse.maas.io/uploads/default/original/2X/1/14f6847ed92339061eb4515c12ac2b6117d5cd7c.jpeg"></a>

Let's decode those blocks of bits:

- The **Preamble** is 7 bytes of clock sync, basically just zeroes and ones like this: ...0101010101....  This gives the receiving station a chance to catch up and sync their clock, so the following data isn't out of sync (and thus misinterpreted).  To delve just a little deeper, the Preamble helps the receiving NIC figure out the exact amount of time between encoded bits (it's called clock recovery).  NTP is nice, but Ethernet is an asynchronous LAN protocol, meaning it doesn't expect perfectly synchronised clocks between any two NICs.  The Preamble is similar to the way an orchestra conductor might "count the players in" so they all get the same rhythm to start.  Before clock recovery, there was MPE. Clock recovery is much more reliable than trying to get computers all over the world synced up to the same clock frequency and the same downbeat (starting point).  Ethernet actually started out that way with something called Manchester Encoding or Manchester Phase Encoding (MPE).  This was important because electrical frequency varies not only across the world, but also from moment to moment when the power is slightly "dirty".  MPE involved bouncing a bit between two fractional voltages using a 20MHz oscillator to generate a reference square wave.  It works, but it's not very efficient, so MPE was scrapped in favour of using the Preamble, the way that projectionists use alignment marks on reels of movie film.
   
- The **Start Frame Delimiter (SFD)** is the last chance for clock sync.  It is exactly 10101011, where the "11" tells the receiving station that the real header content starts next.  The receiving NIC has to recover the clock by the time it hits the SFD, or abandon the frame to the bit bucket.

- The **Destination Address (DAddr)** is six bytes long, and gives the physical address -- the MAC address -- of the next hop.  Be aware that the next hop might be the destination, but it's also possible that the next hop might be a NAP, MAE, NSP, or intermediate ISP.  It's basically the next address in the direction of the destination that the sender knows about.  Unlike the Source Address, the Destination Address can be in a broadcast format (similar to a subnet like 192.18.0.0, but using MAC addresses).

- The **Source Address (SAddr)** is also a six-byte MAC address, this time the MAC address of the sender, which does not change as long as the message is traversing only layer-2 (Ethernet) switches and routers.

- The **PDU Length (PDULen)** gives the byte length of the entire frame, assuming that it's 1500 or less.  If it's longer than that, it indicates a message type, like IPv4, ARP, or a "q-tagged" frame, which carries a VLAN ID.

- The **DSAP**, **SSAP**, and **Control** elements are each one byte in length, and help define devices and protocols.  For the most part, we won't be worried about these with MAAS networks.  Just know that as more and more 802 point-standards come out (e.g., 802.11, WiFi), these elements get longer and more complex.

- The **Data** or "Payload" is the actual packet being sent, which in the case of TCP/IP, is just a TCP header attached to a fixed-length chunk of the application's data.  It's passed on from the layer above.  It cannot be less than 46 bytes, and in conventional Ethernet, it cannot be larger than 1500 bytes.  If the actual data is too small, it's padded out to 46 bytes.

- The **CRC** or "Frame Checksum" (FCS) is a standard checksum, used to verify that the message hasn't been corrupted along the way.

The Preamble and SFD are often considered to be part of the IEEE "packet", so some people start counting the "frame" at the Destination Address.  That distinction shouldn't affect anything we do with MAAS networks, but it's nice to keep in mind, in case you run into someone who groups packets differently than you do.

***** About Trunking VLANs

There is a crucial modification to the basic frame format called a P/Q or VLAN Tag.  This allows something called VLAN trunking, which means sending all the VLAN data over the same wire and port, but giving the NICs a field (the P/Q tag) to control access.  On paper, it looks something like this:

<a href="https://discourse.maas.io/uploads/default/original/2X/6/6e58930520fe4ed38d5ea49fab7a337627a88a55.jpeg" target="_blank"><img src="https://discourse.maas.io/uploads/default/original/2X/6/6e58930520fe4ed38d5ea49fab7a337627a88a55.jpeg"></a>

As you can see in the modified P/Q frame, the following fields replace part of the frame:

- Sixteen bits of tags or a protocol ID.

- Three bits representing a priority.

- One bit is used as a Canonical Format Indicator (CFI), which is 0 if the following VLAN ID is in Ethernet format, or 1 if it's in Token Ring format.

- Twelve bits of VLAN ID.

This matters when we're building complex MAAS networks with lots of VLANS that probably cross over switches.  After all, VLANs were initially controlled with ports and switches, although they more commonly use tags now.  When more than one VLAN spans multiple switches, frames need to carry VLAN information that can be used by switches to sort or "trunk" the traffic.

**** The origin of "trunking"

The word "trunking" is derived from the telephone network term trunk lines, which are lines connecting switchboards.

In the original telephone company model, each telephone had a subscriber line, which was a wire that went straight from the local Central Office (CO) to that subscriber's telephone.  Each CO had one switchboard, though it might have many seats.

Connections between Central Offices were handled by trunk lines, because they ran between phone company facilities.  You'd have a thick cable with lots of pairs running from CO to CO, basically enough wires to handle something like 35% of the possible calls.  If you ever got the message, "All circuits are busy now; please try your call again later", you've heard what happens when the system is "trunking above capacity" or "TAC'd", as it was called.

At the CO, the wires would "branch" and run all over the place: First to junction points (those five-foot-tall boxes you see from time to time on the road), then to interface points (the square cans beside the road every half mile or so, also called "pedestals") and from there to subscriber homes.  When you draw out this network, it looks like a tree, where the bundles of cables between COs look like the trunks of trees.

With VLAN trunking, by the way, we're not just multiplexing packets, we're actually multiplexing LAN channels, so to speak.  

In the parlance of networks, especially VLANs, the term "trunking" is used to indicate the sharing of network routes.  This sharing is made possible by the Ethernet VLAN tags, which make the VLAN-bound messages less dependent on switches and routers to get the traffic to the right place.  Otherwise, you'd have to designate complicated port configurations for switches, which is particularly easy to misconfigure.

Note that the MAC sub-layer is responsible for managing CDMA/CD, that is, detecting a clear line, initiating re-transmission if there's a jam signal, etc.  On the way in, the MAC sub-layer verifies the CRC/FCS and removes frame elements before passing the data up to the higher layers.  Basically, anything that some other MAC layer did to encapsulate the message for sending, the receiving MAC layer un-does on the way in.

***** About VLANs, subnets, and fabrics

When working with MAAS networks, you will frequently be concerned with VLANs, subnets, and fabrics, which are all network groupings:

- Subnets define (group) a range of IP addresses.
   
- VLANs group subnets.

- Fabrics group VLANs.

Let's give each of these terms a MAAS context.

**Subnets**

A subnet is a range or collection of IP addresses.  A subnet just means "sub-network," and that's exactly what it is: a subset of IP addresses that can be treated like a single block for some operations.

Subnets are defined in CIDR (Classless Inter-Domain Routing) notation. If you want to use the addresses from 192.168.13.0 to 192.168.13.255 in a subnet, you can specify that with 192.168.13.0/24.  The "24" refers to the number of bits in the subnet address, with the remainder out of 32 bits free to address hosts.  Since 8 bits can represent 256 things, that means /24 gives you the last octet, or 255 host IP addresses.

**Whatever happened to subnet classes?**  Subnets used to be defined in terms of subnet classes, like A, B, and C. That got to be a limitation, because those three classes define a fixed number of bits of the IP address that represent the split between subnet addresses and host addresses.  In other words, the class defined how many hosts could be in the network, and three classes wasn't really adequate to address all the possible permutations that network architects needed.  The change to CIDR notation made subnets more granular, allowing many more subnets from the same network.

**VLANs**

A VLAN used to be a series of IP addresses that could access a given port on a specific switch, generally the switch that gated some protected resource.  With the advent of VLAN trunking (see above), VLANs are marked with the 802.1Q (P/Q) bits in the MAC frame.  In theory, any set of addresses can be associated with any VLAN.

MAAS encourages a correspondence of subnets to VLANs. Every IP should be in exactly one subnet, and every subnet should be part of exactly one VLAN.  You don't have to do that: you could, for example, have two different subnets that overlap, like 192.168.43.0/24 and 192.168.43.0/26.  The ".26" subnet would use fewer bits for the host addresses, so only some of the addresses would overlap.  MAAS generally prevents this kind of address overlap.

Likewise, putting one subnet in two different VLANs might be possible, but it isn't practical or easy to debug when conflicts happen.  MAAS endeavours to enforce a clean "fan-out" across the network, with no possibility of conflicting IP addresses.

**Fabrics**

A fabric just collects VLANs together.  If you stick to the clean fan-out, that also means that a fabric collects subnets.  A fabric provides a higher level grouping.

Consider a MAAS-centric example.  Suppose you have one VLAN for HR, and one VLAN for payroll, so that nobody else can see HR's private files, and likewise you've got payroll data limited to just those people who should see it.

Some executives are entitled to see anything and everything about the corporation.  An "executive" fabric would group all VLANs together, so that people admitted to that fabric can access the VLANs without having to be explicitly added to each one.  That's very handy in really large organisations, saving a lot of time and effort.

***** Visualising the link layer

Let's start with a message coming on Layer 1 from SanDiego to Bangor. When the message comes in, the link layer does the following things:

1. It synchronises the NIC, so that bits will indeed be recognised as bits and the message can be properly decoded.

2. It handles the source and destination addresses, using ARP as necessary. 

3. It interprets the length/type bytes and uses them, which means it must judge the length of a frame, and of the data in a frame, or, alternatively, decide whether a frame is IPv4, ARP, or VLAN ("q-tagged").  

4. It processes VLAN tags, which means, at the very least, dealing with the message priority, deciding whether the VLAN frame is Ethernet or Token Ring, and capturing and using the VLAN ID.  The layer handles messages by priority, knows how and when to send Ethernet or Token Ring frames, and knows how to route traffic to a specific VLAN.

5. It computes the checksum to make sure the message is valid.

Next, we'll take a look at the network layer, where most of the message transactions take place, and where most of our debugging will be done.

**** Interlayer addressing: ARP

One frequently asked question is this: Is ARP a layer 2 or layer 3 protocol?  Actually, it's both, as you'll [discover later](#heading--about-arp), but it does all of its work at L2.  One way to distinguish L2 from L3 is to find out what happens inside the firmware of the Network Interface Card (NIC), and that's usually where ARP takes place.   ARP maps MAC addresses, which is how things are addressed in L2, into other addresses (e.g., IP addresses), which is how L3 finds things.

**** About the network layer

You might have noticed in the original OSI model that "IP" was part of Layer 3, and protocol stacks like UDP and TCP were part of Layer 4.  It's a little bit confusing that we say "TCP/IP" when the "IP" really applies to so many other protocols like UDP and ICMP.  There are certainly other protocols and protocol stacks, but for the purposes of MAAS networks, we're talking almost exclusively about TCP/IP.

This subsection will help you learn:

- [About packets](#heading--about-packets)
- [About fixed packet lengths and segmented messaging](#heading--about-fixed-packet-lengths)
- [About IP packets](#heading--about-ip-packets)
- [About routing](#heading--about-routing)

The network layer does not guarantee delivery.  Essentially, it makes every effort to deliver IP datagrams (packets) to the destination, but it's error-handling is pretty simple: just toss the packet into the bit-bucket.

It's also a connectionless layer, meaning the packets making up a message aren't part of an ongoing conversation.  They can be split up, encoded, and sent separately, by different routes, and arrive completely out of order.  And packets can get duplicated or corrupted.  Figuring all this out is the job of the protocol stack (e.g., TCP) in layer 4.  The network layer, L3, just delivers packets.  

**Network byte order**: A rarely needed (but useful) fact is that the network sends bytes in big endian order.  That means bytes are transmitted starting with bit 0 and working down to bit 31, usually eight bits at a time.  A lot of the computers on the Internet use little endian encoding, which starts at the other end of the word.  In those cases, the byte order has to be reversed somewhere between the computer's memory and Layer 3. For most situations, that fact isn't particularly useful, but there is the occasional fault that involves failure to reverse byte order along the path from RAM to NIC.

***** About packets

Packets are basic Internet Protocol (IP) message units.  A message will probably be split into multiple packets by L4 (the transport layer) so it can be efficiently sent.

For example, imagine that you're sending a very long letter to your friend, and all you have are lots of envelopes and first-class stamps.  If you've ever done a lot of mailing, you'll know that mailing a one-ounce letter costs you, say, fifty-eight cents.  If you add another ounce of paper to it, that second ounce only costs you, say, twenty cents.  But all you have are first class (i.e., fifty-eight-cent) stamps.

If you don't want to waste your money, you can either cram more pages in the envelope, until you're at three ounces (the most you can get with two stamps), or send two letters, each with one ounce in it.  The way envelopes go through the mailing system, you're better off not over-stuffing an envelope.  So what do you do?

You sit down and write the letter to your friend, carefully numbering the pages.  Then you divide it into piles of pages that are just under one ounce. Finally, you put each pile into an addressed, stamped envelope and mail each letter separately.  When your friend gets the letters, it doesn't matter which one gets there first, because they can reassemble your message, using the page numbers.

***** About fixed packet lengths and segmented messaging

We could have designed computer networks to take messages of indeterminate lengths, but that presents some unique challenges when trying to manage network traffic.  For example, suppose you send seven overstuffed letters to your friend, and so does everyone else on your block?  All these huge letters aren't going to fit in one letter-carrier's bag, so they'll have to either send out two delivery people, or wait until tomorrow to send out someone's letters.

Choosing a fixed (relatively short) length makes it statistically possible for everyone's letters (everyone's messages) to be delivered at a fairly constant, reliable rate.  That rate will vary with the size of the overall message, not with who threw their message on the Internet first.  A larger message takes longer to send.

Messages are split into packets of consistent length before they're passed to L3, so larger messages take longer.  It's statistically more efficient to split messages into equally-sized packets than any other arrangement -- the method that gets the highest count of complete messages through the network in a given amount of time. In network terminology, it's the highest-throughput approach to network traffic.  Specifically, this technique is called multiplexing.

***** About IP packets

The IP datagram (packet) is the backbone of most modern networks.  The following diagram depicts an IPv4 header, which attaches to the front of data packets up to about 65K long:

<a href="https://discourse.maas.io/uploads/default/original/2X/f/fc349972f6b7509b5b2459bf3bb44419961f0bcd.jpeg" target="_blank"><img src="https://discourse.maas.io/uploads/default/original/2X/f/fc349972f6b7509b5b2459bf3bb44419961f0bcd.jpeg"></a>

Note that IPv6 headers have only the version field in common with IPv4 headers; otherwise, they are completely different.  Here are the header fields and what information they carry:

- **IP Protocol Version**  This is "4" for IPv4 and "6" for IPv6.  There are [lots of others](https://www.iana.org/assignments/protocol-numbers/protocol-numbers.xhtml)`↗`, but they generally don't touch a MAAS network.
- **Internet Header Length** The number of 32-bit words in the header, including the options (but not including the data, since it's just the header).  Most of the time, this will have the value "5", but options do exist and are sometimes included.
- **Differentiated Services Code Point** This is used to specify special classes of service.  Normally, IP packets are delivered on a "best-effort" basis, that is, Layer 3 will try everything possible to make sure a packet gets delivered.  You can cause L3 to deliver packets with higher priority (implying more certainty) by using a different DSCP.
- **ECN = Explicit Congestion Notification** These bits are both set by an ECN-capable router when that router is above a certain traffic threshold.  They are there to alert a sender to slow down (or expect delays) when the network segment in use is particularly congested.
- **Total Length of IP Packet** This field indicates the length of the entire packet, including the data.  This makes it possible to calculate the byte offset of the data within the datagram.
- **Identification**  This is a serial number, generated by the sending NIC, that helps the participants uniquely identify the datagram.  In a sense, it works like the little "take-a-number" tickets you get at the hamburger stand: Eventually, the number will repeat, but the repeat cycle is so long that there's no chance of confusing packets.  The sequential nature of this field, when used in concert with the Flags and Fragmentation Offset field, helps the protocol stack correctly reassemble the message.
- **Flags**  This field is basically used to indicate that a packet is a fragment of a longer message.
- **Fragmentation Offset**  Used with the Identification sequence number, this field allows the system to know which packets precede or follow this one when re-assembling the message.
- **Time to Live (TTL)**  This indicates the number of routers that a datagram can pass through before it's discarded.  Since routers function by replacing their own destination address with the IP address of the next hop, this essentially limits the number of times a packet's destination IP can be changed.  Most RFC documents suggest keeping this number at 64, it's more often set to something like 255 without any real bottlenecks.
- **Protocol** This field indicates the higher level protocol (the protocol stack) that generated this message.  Examples are given for TCP and UDP in the figure.
- **Header Checksum** This calculates a checksum for the header only.  It's only used in IPv4.  Doing integrity-checking on the data is the responsibility of Layer 4.
- **Source Address** This is the IP address of the sender of the packet, for this hop only.  As shown in the figure below, routers will change this address so they can get the answer back.
- **Destination Address**  This is the IP address of the destination, for this hop only.  As shown below, routers change this address to act as brokers in the IP chain.

***** About routing

We now have enough concepts in play to talk about routing.  Routing takes place at the network layer, by changing the source and destination addresses (without losing track of the replaced address).  The process looks something like this:

<a href="https://discourse.maas.io/uploads/default/original/2X/7/73c49cb76573e42036c59832f5f16b6383fbc0bd.jpeg" target="_blank"><img src="https://discourse.maas.io/uploads/default/original/2X/7/73c49cb76573e42036c59832f5f16b6383fbc0bd.jpeg"></a>

The router typically assigns a unique port number to the outbound message, and records the source IP against that port number.  When the message comes back to it on that port number, it can look up the IP address of the NIC that sent the packet and route the answer back.

**** About the transport layer

Layer 4 brings us to protocols implemented only by the end hosts (i.e., not by the routers or other switching gear that connect the network).  This layer handles things like redundancy, confirmed delivery, managing packets on an unreliable network, and so forth.  This is the last layer that TCP/IP has anything to say about; layers above this are unique to specific applications.  Troubleshooting this level would involve knowing about entire protocol sets, like UDP or TCP.

**** About the session layer

Layer 5, the session layer, is where ongoing interactions between applications happen. The data is couched in terms of things an application might understand (e.g., cookies for a Web browser).    This is also the layer where check-pointing (i.e., saving work finished so far) happens.  At this layer, we'd discuss things like RPC, SQL, or NetBIOS.

**** About the presentation layer

The presentation layer converts data between formats and ensures standard encodings are used to present the information to the application. This layer is all about file formats: ASCII, EBCDIC, JPEG, GIF, and HTML, to name just a few.

**** About the application layer

The top layer, layer 7, is totally the province of the application(s) involved in processing messages.  Two techs talking about this layer would be swapping stories about application protocols, like FTP, DNS, SMTP, or NFS.  Almost nothing that happens at this layer -- except for throughput estimates or fouled daemon code -- filters into designing or debugging MAAS networks.


*** About bond interfaces

A bond interface is used to aggregate two or more physical interfaces into a single logical interface. Combining multiple network connections in parallel can increase network throughput beyond what a single NIC will allow.  It also provides some redundancy in case one of the NICs should fail.

Bonded interfaces use a special frame format called LACPDU, or "Link Aggregation Control Protocol Data Unit.  More information about these special frames, and about the theory behind bonded NICs, can be found in the [relevant IEEE standard](https://1.ieee802.org/tsn/802-1ax-rev/)`↗`.

*** About bridge interfaces

Let's take a short diversion to talk about bridge interfaces.  These will be important when we discuss ARP a little further down.

A network bridge may be useful if you intend to use virtual machines or containers with MAAS.  Bridges are, to some extent, artefacts of the AAC network.  Frequently, people ask about the difference between a switch and a bridge.  The core answer lies in the number of ports: switches have as many ports as you can afford; bridges usually have fewer ports, often only two.

Switches traditionally forward packets, without storing them, to a specific host on a specific port, building up a table of host vs. port in the process to reduce broadcast transmissions. Switches traditionally use ASICs (Application Specific Integrated Circuits), otherwise known as "merchant silicon", designed especially for that purpose.

Bridges, on the other hand, provide store and forward packets between two LANs, generally using software.  They might still integrate merchant silicon, or even start as an ODM (original design manufacturer) box with no NOS (network operating system), similar to the way some mid-range switches are built.  Software added on top, plus the low port count and the store-and-forward approach.

Bridges can use MAC addresses (L2) to direct packets, can essentially emulate L3 routing (recording IP addresses of bridged messages for correct return trips), or some combination of both.  Switches usually just route packets between ports.

You can create a network bridge with MAAS; via netplan; or by any other established method to create a network bridge.  For example, when you're using LXD, you typically create a virtual bridge, called `lxdbr0` by default, that bridges between the MAAS host and the LXD instance.  On the host side, the LXD bridge looks like this:

``` nohighlight
7: lxdbr0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP group default qlen 1000
    link/ether 00:16:3e:e4:2b:fe brd ff:ff:ff:ff:ff:ff
    inet 10.90.194.1/24 scope global lxdbr0
       valid_lft forever preferred_lft forever
    inet6 fd42:1fc6:f588:d0b8::1/64 scope global 
       valid_lft forever preferred_lft forever
    inet6 fe80::216:3eff:fee4:2bfe/64 scope link 
       valid_lft forever preferred_lft forever
```

On the MAAS side, the virtual bridge looks like this:

<a href="https://discourse.maas.io/uploads/default/original/2X/f/fbdef2010657939f5f25ee2f157ba5a92af72090.png" target="_blank"><img src="https://discourse.maas.io/uploads/default/original/2X/f/fbdef2010657939f5f25ee2f157ba5a92af72090.png"></a>

Where the 10.190.94.0/24 network is usually part of a VLAN offering MAAS-provided DHCP:

<a href="https://discourse.maas.io/uploads/default/original/2X/1/1067a379ba9411713ed1a67e78e249535771b08c.png" target="_blank"><img src="https://discourse.maas.io/uploads/default/original/2X/1/1067a379ba9411713ed1a67e78e249535771b08c.png"></a>

There are other DHCP configurations possible with MAAS (we'll cover this later).  The important point here is not to get overly hung up on the terms "switch", "bridge", and "router".  Instead, figure out whether you want to forward messages based on port numbers, MAC addresses, or IP addresses -- or some combination -- and then find real or virtual devices that will do this for you.

*** About ARP
 
In theory, every NIC card in the world has a unique identifier, called a /MAC address/.  "MAC" stands for "Media Access Control" -- you can find a [little history of this](https://en.wikipedia.org/wiki/Medium_access_control)`↗` on Wikipedia, if you're interested.

This subsection will help you learn:

- [About TCP/IP vs. MAC addresses](#heading--tcp-ip-does-not-use-mac-addresses)
- [About fixed versus assigned addressing](#heading--fixed-versus-assigned-addressing)
- [About address resolution](#heading--address-resolution)
- [That messages are sent to MAC addresses](#heading--messages-sent-to-mac-addresses)
- [About the ARP frame](#heading--about-the-arp-frame)
- [About the ARP cache](#heading--about-the-arp-cache)
- [More about ARP](#heading--more-about-arp)

When you're assigning MAC addresses with virtual machines, of course, you may be re-using one that's actually assigned to a network device out there somewhere.  Inside your Layer 2 network, that isn't a problem, because only devices connected to a physical switch -- that's actually connected to the physical Internet -- care about unique MAC addresses.  Inside your network, the only conflicts you need to worry about are the ones you create by hand-assigning MAC addresses.

The shorter answer to that implied question is this: MAC addresses must be unique across the domain where they're used.  

**** TCP/IP does not use MAC addresses

If we look at the IP datagram again, we see that it doesn't know about MAC addresses at all:

<a href="https://discourse.maas.io/uploads/default/original/2X/f/fc349972f6b7509b5b2459bf3bb44419961f0bcd.jpeg" target="_blank"><img src="https://discourse.maas.io/uploads/default/original/2X/f/fc349972f6b7509b5b2459bf3bb44419961f0bcd.jpeg"></a>

TCP, UDP, and a number of other protocol stacks are written to use IP addresses.  Routers depend on IP addresses, as we've already seen. This creates a bit of a conundrum: How do we map between MAC and IP addresses, and what does the mapping?  Is it a layer 2 or layer 3 operation?

The first thing to remember is that the MAC address is "ROM-burned" into the NIC card.  IP addresses, on the other hand, are assigned to a NIC by a DHCP server or an administrator.  This intentional separation of addressing schemes is what makes the Internet flexible.

**** Fixed versus assigned addressing

Here's an analogy.  Your postal address doesn't /actually/ define where your house is located.  There are two layers of other addressing schemes that are actually used by government organisations, like your county tax assessor or the local air ambulance company.

One is your land survey location.  Depending on where you live, this is defined by a series of coordinates that go something like this: county, township, section, plat, lot, etc.  If you've ever looked at your property tax bill, it will have your postal address on it, but it will not actually use your postal address to define the taxable property.  Instead, it uses this unique set of (rather obscure) coordinates to place you exactly on land survey maps.

But that's not good enough for the air ambulance, for two reasons.  First, the survey maps are huge, complex, and hard to interpret, and they change somewhat as property is bought or sold.  Second, helicopter navigation is intentionally independent of political boundaries.  Instead, the air ambulance will use your latitude and longitude, which allows them to uniquely locate you on the earth.  Granted, the ambulance company has a tool somewhere that automatically does the maths of translating your postal address to lat/long coordinates, but the principle holds.

In terms of your local network, each of these "address levels" applies.  Your postal address corresponds to the IP address of a machine.  That IP address may or may not be unique, depending on the domain.  For example, you can use Google Maps to try and locate something like "20 Main Street", and you'll get a really long list of responses that vary by city.

Likewise, there are probably hundreds of thousands of local networks using addresses in the "192.168...." subnet, since it's so common for local IP addressing.  As mentioned above, routers at the network layer take care of protecting these unique local addresses when going out on the Internet. On the other hand, your NIC's MAC address is like the GPS lat/long coordinates; it's unique across the entire world.

What about the analogue of survey maps?  Well, it's not hard to argue that these are more like the MAC addresses that you assign to your VMs.  Every county in a state like, say, Mississippi has the coordinates Township 1, Section 1, Parcel 1 -- but the outer domain (the county) makes those coordinates unique.  Granted, we don't use a different format for MAC addresses for VMs than we do for Internet-connected NICs, but you get the idea.

**** Address resolution

Address resolution is what we call the process of mapping between IP addresses and MAC addresses.  It's done with something called ARP, which stands for "Address Resolution Protocol".  Oddly enough, ARP takes on a life of its own, so you may hear it discussed in unusual ways.  Some people call it "the ARP", others speak of "arpd" (the ARP daemon), although if you look at [the man page for arpd](https://linux.die.net/man/8/arpd)`↗`, you'll see those characterisations are not precisely correct.

A frequent question is, "Where does ARP take place?"  Maybe the better question is, "Where is ARP implemented?"  As always with Internet-related things, the answer can vary, but normally, ARP is implemented as part of the embedded code in the NIC.  Technically, this means that ARP operates at Layer 2.  More often, you'll see vendors hedge their bets on this, with phrases such as "operates below the Network Layer", as in [this explanation](https://support.hpe.com/hpesc/public/docDisplay?docId=emr_na-c00686597#:~:text=ARP%20is%20a%20protocol%20used,network%20and%20OSI%20link%20layer)`↗`.

In reality, in order to work correctly, ARP has to map IP and MAC addresses, since the ARP message looks something like this:

<a href="https://discourse.maas.io/uploads/default/original/2X/1/18a95f7ed64b83ec1302c92e41696d137783e7bc.jpeg" target="_blank"><img src="https://discourse.maas.io/uploads/default/original/2X/1/18a95f7ed64b83ec1302c92e41696d137783e7bc.jpeg"></a>

To better understand, let's walk an ARP call.  It begins in say, a Web browser, when the browser makes a call to parse the URL.  In most cases, that URL contains a hostname (not an IP address), so the following sort of dance takes place:

<a href="https://discourse.maas.io/uploads/default/original/2X/1/1365006a1692e4788df733c58e1435e67da57536.jpeg" target="_blank"><img src="https://discourse.maas.io/uploads/default/original/2X/1/1365006a1692e4788df733c58e1435e67da57536.jpeg"></a>

We won't go into this in much detail now, just know that the browser is able to gather an IP address, if it exists.  To make the walk-through less confusing, let's assume that we're looking for a host with IP 192.168.17.4.

Next, the browser requests a connection with 192.168.17.4, using the TCP protocol, which sends a connection request, as an IP packet, to 192.168.17.4.  Along the way, there is probably more than one router hop.  

ARP sends a broadcast request to everything on the relevant subnet.  This request looks like the ARP message above, but it's encoded as a MAC frame, which helps to answer the often-fuzzy question, "Is ARP Layer 2 or Layer 3?"  As you see, this is an L2 message.  Incidentally, ARP only works as a broadcast, by the way; that is, it only works on a broadcast network.

A very important note for some systems like MAAS: ARP requests don't typically span VLANs.  

Essentially, this ARP message contains the IP address 192.168.17.4, but no corresponding MAC address in the message.  This tells the owner of 192.168.17.4 that it should reply with a similar ARP message, including its MAC address.  When the sender receives the ARP reply, it can send the datagram directly to the destination host, embedded in an Ethernet frame, using the MAC address.

By the way, for efficiency, the sending host and the intermediate routers are all doing ARP caching.  They copy down the mapping between IP and MAC addresses, holding onto it for about twenty minutes.  In terms of most network transactions, twenty minutes is an eternity.

***** Messages are sent to MAC addresses

We often speak of TCP/IP as if messages are sent from one IP address to another, but that's actually not strictly true.  Messages are sent to MAC addresses.  IP addresses are only used to get MAC addresses, so the message can go through.

We can return to the air ambulance company to see a practical analogy.  A 911 call comes in for "20 Main Street, Yourtown, Yourstate, Yourpostalcode".  The address is sent to the pilot of the helicopter, who punches the address into his GPS.  The GPS uses the postal address to retrieve the lat/long coordinates, which are then used to guide the helicopter, via satellite navigation.

The same sort of thing happens when you use the GPS navigator in your car.  The navigator is translating a logical (postal) address to a physical (lat/long) address on the surface of the earth, calculating a route, and translating that route back to logical landmarks (street names) to let you know how to get there.

By the way, You should also note that ARP only works with IPv4.  Certain other protocols, like Point-to-Point Protocol (PPP), don't make use of ARP at all.

***** About the ARP frame

ARP sends requests as an Ethernet frame, using the MAC address.  If you remember the MAC frame from earlier:

<a href="https://discourse.maas.io/uploads/default/original/2X/1/14f6847ed92339061eb4515c12ac2b6117d5cd7c.jpeg" target="_blank"><img src="https://discourse.maas.io/uploads/default/original/2X/1/14f6847ed92339061eb4515c12ac2b6117d5cd7c.jpeg"></a>

The ARP frame is just a special case of the MAC frame, replacing everything the DSAP, SSAP, control bits, and data with the ARP message shown above.  The resulting ARP frame looks something like this:

<a href="https://discourse.maas.io/uploads/default/original/2X/f/f430da9c144bda4cf2e2901181988f3444d335f8.jpeg" target="_blank"><img src="https://discourse.maas.io/uploads/default/original/2X/f/f430da9c144bda4cf2e2901181988f3444d335f8.jpeg"></a>

Based on the above diagram, you can see how the ARP request fits into the Ethernet frame to make an ARP frame.

***** About the ARP cache

Let's take a look at the ARP cache on a local system, ~cloudburst~. We can do that like this:

``` nohighlight
stormrider@cloudburst:~$ arp
Address                  HWtype  HWaddress           Flags Mask            Iface
192.168.1.24             ether   0c:8b:7d:f1:51:d3   C                     wlo1
10.250.204.17                    (incomplete)                              mpqemubr0
192.168.1.24             ether   0c:8b:7d:f1:51:d3   C                     enx606d3c645a57
192.168.117.16                   (incomplete)                              virbr0
```

The columns are mostly obvious, but just in case:

- **Address**: the IP address that's been cached.
- **HWtype**: the Hardware Type field, which is blank when there's no MAC address (as is the case in a number of these entries).
- **HWaddress**: the MAC address of the device.  The "incomplete" entries are meant to indicate that an ARP request was sent for that address, but no response was received.
- **Flags Mask**: this field can have three values: "C" indicates that ARP learned this on its own, based on ARP responses; "M" means that the data was manually entered in the ARP table by a user; and "P" means "Publish," which just tells the host how to respond to incoming ARP packets.
- **Iface**: the interface name.

In this case, ~virbr0~ and ~mpqemubr0~ are virtual bridges used for different sets of libvirsh VMs that haven't been used for anything in a while. Also note that something called ~lxdbr0~, which is an LXD bridge, doesn't even show up.

Let's see if we can influence that.  First, let's take a look using ~ip a~:

``` nohighlight
stormrider@cloudburst:~$ ip a show up
5: virbr0: <NO-CARRIER,BROADCAST,MULTICAST,UP> mtu 1500 qdisc noqueue state DOWN group default qlen 1000
    link/ether 52:54:00:d6:70:6c brd ff:ff:ff:ff:ff:ff
    inet 192.168.117.1/24 brd 192.168.117.255 scope global virbr0
       valid_lft forever preferred_lft forever
6: mpqemubr0: <NO-CARRIER,BROADCAST,MULTICAST,UP> mtu 1500 qdisc noqueue state DOWN group default qlen 1000
    link/ether 52:54:00:84:0a:4c brd ff:ff:ff:ff:ff:ff
    inet 10.250.204.1/24 brd 10.250.204.255 scope global mpqemubr0
       valid_lft forever preferred_lft forever
7: lxdbr0: <NO-CARRIER,BROADCAST,MULTICAST,UP> mtu 1500 qdisc noqueue state DOWN group default qlen 1000
    link/ether 00:16:3e:e4:2b:fe brd ff:ff:ff:ff:ff:ff
    inet 10.90.195.1/24 scope global lxdbr0
       valid_lft forever preferred_lft forever
    inet6 fd42:1fc6:f588:d0b8::1/64 scope global 
       valid_lft forever preferred_lft forever
```

You'll see that all three bridges are in a DOWN state, and again, ~lxdbr0~ is so cold that it doesn't even show up in the ARP table.  Let's bring up a LXD VM connected to ~lxdbr0~ and look at the ARP table again:

``` nohighlight
stormrider@cloudburst:~$ arp
Address                  HWtype  HWaddress           Flags Mask            Iface
192.168.1.24             ether   0c:8b:7d:f1:51:d3   C                     wlo1
10.250.204.17                    (incomplete)                              mpqemubr0
192.168.1.24             ether   0c:8b:7d:f1:51:d3   C                     enx606d3c645a57
192.168.117.16                   (incomplete)                              virbr0
10.90.195.16             ether   00:16:3e:fc:71:98   C                     lxdbr0
```

Note that the ~lxdbr0~ bridge now shows up and has a MAC address, too -- no incomplete entry here.  If we look at the MAC address of ~lxdbr0~ in the ~ip~ listing, we'll see it matches up.

Those "(incomplete)" entries are old.  They've been cached, but no traffic has passed through those bridges in a really long time.  The cache is just persistent in holding onto the IP addresses, but not the MAC addresses (since they could be stale).  We can prove this to ourselves by clearing the cache:

``` nohighlight
stormrider@cloudburst:~$ sudo ip -s neigh flush all
[sudo] password for stormrider: 

*Round 1, deleting 18 entries ***
*Flush is complete after 1 round ***
```

...and rebuilding the ARP table:

``` nohighlight
stormrider@cloudburst:~$ arp
Address                  HWtype  HWaddress           Flags Mask            Iface
192.168.1.24             ether   0c:8b:7d:f1:51:d3   C                     wlo1
192.168.1.24             ether   0c:8b:7d:f1:51:d3   C                     enx606d3c645a57
10.90.195.16             ether   00:16:3e:fc:71:98   C                     lxdbr0
```

***** More about ARP

Another form of ARP is promiscuous ARP, in which some proxy host pretends to be the destination host and provides an ARP response on behalf of the actual destination host.  You shouldn't use this form of ARP unless there's no other choice.  You can Google it (and use it at your own risk), but it won't be described here.

There is also gratuitous ARP, when the source and destination IP addresses are the same.  This can be used for at least two purposes:

1. To find out if someone else already has the source machine's IPv4 address, a technique called Address Conflict Detection by some references.

2. To update the source machine's new MAC address (e.g., a new NIC card was installed) in upstream ARP cache entries.  This is something akin to pre-caching MAC addresses before they're actually needed.

You can [read more about](https://en.wikipedia.org/wiki/Address_Resolution_Protocol)`↗` these (and many more) nuances of ARP, but this introduction should answer most of the immediate questions. 

*** About TCP

If the Internet Protocol (IP) is connectionless, the transport layer is all about connections.  The transport-layer protocol in use -- we'll talk exclusively about Transmission Control Protocol or TCP here -- the L4 protocol is the last place in the stack where the entire message exists in one piece.  L4 breaks up larger messages into segments.  Each segment gets a TCP header, and gets passed on to L3 where it becomes an IP packet.

**** About the TCP header

Here's a diagram of the L4-to-L3 hand-off:

<a href="https://discourse.maas.io/uploads/default/original/2X/a/a8f2aba29b1299bba2630177c0301dd518cacb57.jpeg" target="_blank"><img src="https://discourse.maas.io/uploads/default/original/2X/a/a8f2aba29b1299bba2630177c0301dd518cacb57.jpeg"></a>

We can get a pretty good idea what happens at Layer 4 just by decoding the contents of the TCP header. It contains the following fields:

- **Source port**: the application port number of the host sending the data.  For example, if this is an FTP message, the source port would probably be 21.

- **Destination port**: the port number of the application requested on the destination host.  If this is FTP, again this port would likely be 21.

- **Sequence number**: the sequence number of this segment of data, to help the other end put the data back together in the correct order, as well as help Level 4 on the receiving end know whether a packet's been dropped or lost.

- **Acknowledgement number**: essentially, the next sequence number the destination host is expecting; used to "gate" packets through the connection.

- **TCP header length**: given to know where the data begins.

- **Reserved**: reserved for future use, basically; currently always set to 0.

- **Code bits**: essentially a set of flags; see the list below.

- **Window**: used to negotiate the "window" size, that is, how many bytes the destination host is willing to receive at once; this allows for the most efficient transmission possible, based on the characteristics of the two communicating hosts.

- **Checksum (CRC)**: used to check the integrity of the segment.

- **Urgent pointer**: data byte count where urgent data ends; used if the urgent flag is set (see below).


The code bits can indicate the following things:

- **URG**: indicates that the urgent pointer field is meaningful, used to prioritise this message over other messages.

- **ACK**: used to acknowledge successful delivery of the previous segment.

- **PSH**: push notification; tells the receiving host the message is complete, you can push the data to the application.

- **RST**: request a connection reset when the error level reaches a certain threshold; basically, "let's try that again from the top."  This is considered an abnormal termination of the TCP connection.

- **SYN**: used for a three-way TCP handshake; this handshake is how sender and receiver sync up; it serves a purpose similar to the preamble in a MAC frame, but at a different level of synchronisation.

- **FIN**: we're done, close the connection.  This is considered a normal termination of a TCP connection.

- **NS/CWR/ECE**: used to provide Explicit Congestion Notification; note that OSI provides several methods for endpoints to know that the network is congested.

**** TCP is like a phone call

As you can see from the bytes above, TCP is all about the state of a connection, which is basically the same as a phone call.  When you pick up the receiver, you and the caller exchange information.  You say "bye" when the call is over.  If it's a bad connection or one end suddenly gets noisy (think jack-hammers outside), one of you can reset the connection by saying, "Let me call you back in a minute."  Take a minute and try to see how the other header bytes fit this analogy.

Also like a telephone call, TCP provides a connection (the call, however long it lasts), flow control (provided by the two parties on the call), multiplexing (handled by the two handsets, basically letting through multiple frequencies and sounds, so that you can get the tone and breath sounds of the other person, not just their raw words).  Likewise, the two parties try to handle the reliability of the connection by making sure you understand each other.

The analogy spreads a little because some of the items (connection, multiplexing) are handled by the telephone, and some are handled by the people operating the telephone (flow control, reliability).  In the network, the Level 4 protocol stack handles it all.

There is a lot more to know about TCP, but most of it isn't directly relevant for MAAS networking.  Instead, we direct you to the excellent [Wikipedia article about TCP](https://en.wikipedia.org/wiki/Transmission_Control_Protocol)`↗`.  Going forward, we'll only bring up specific transport layer elements as we need them.

*** About DNS

Because IP addresses are hard for humans to remember, the Internet supports the use of host names to identify hosts.  These host names are associated with the host's actual IP address in a server known as a Domain Name server.  The overall protocol is known as the [Domain Name System](https://en.wikipedia.org/wiki/Domain_Name_System)`↗`.  

*** About other network elements

This section summarises a number of other networking elements that may occasionally come up when working with MAAS networks.  These include:

- [Client](#heading--client)
- [Hub](#heading--hub)
- [LAN](#heading--lan)
- [MAC address](#heading--mac-address)
- [MAN](#heading--MAN)
- [Network cable](#heading--network-cable)
- [Patch panel](#heading--patch-panel)
- [Repeater](#heading--repeater)
- [Router](#heading--router)
- [Server](#heading--server)
- [Switch](#heading--switch)
- [Network topology](#heading--network-topology)
- [WAN](#heading--WAN)

**** Client

In the client/server age, the lines between client and server are blurred and sometimes reversible.  For the purposes of MAAS and general networking principles, we can define a [client](https://en.wikipedia.org/wiki/Client_%28computing%29#firstHeading)`↗` as a node that uses shared resources via a network.  If that same client provides shared resources to other nodes, it could also be considered a server.  

**** Server

A [server](https://en.wikipedia.org/wiki/Server_%28computing%29#Classes_of_computers)`↗` is a node that provides shared resources to clients via a network.  If that same server uses shared resources from other nodes, it could also be considered a client, but only in that context.

**** Hub

[Hubs](https://en.wikipedia.org/wiki/Ethernet_hub#firstHeading)`↗` essentially started as repeaters.  While they may be able to connect more than two computers together (i.e., multiple RJ45 ports)`↗`, they provide no improvement over simple bus networks, since every connected NIC must examine every packet.  They are rarely used anymore.

**** MAC address

A [MAC address](https://en.wikipedia.org/wiki/MAC_address#firstHeading)`↗` or "media access control" address is a unique address or "physical address" associated with a network interface.  They are 48 bits in length, which allows for 280 trillion devices, arranged into six hexadecimal octets, separated by colons or dashes.  Every computer in the world theoretically has a unique MAC address.  You can identify a node's IP address with the command `ipconfig /all`.

**** Network cable

[Network cables](https://en.wikipedia.org/wiki/Category_5_cable#firstHeading)`↗` are special cables that connect non-wireless-based nodes.  They consist of our pairs of insulated, 24-gauge wire, colour-coded (solid/striped)`↗`, usually in four colours: blue, green, orange, and brown.  The matching colour pairs are twisted together, each pair at a different turn rate to prevent electromagnetic interference between pairs.
These twists must be maintained all the way up to the (RJ45) connector.

Even with insulation, careful twisting, and connector-termination, natural losses in the cable cause the network signals to become too weak to maintain reliable data rates after a certain length.  In the case of Cat 5e cable, the maximum cable length is 100 meters to maintain 1Gb per second.  For Cat 6, the max length to maintain 10Gb per second is 55 meters.  These limits are overcome with [repeaters](#heading--repeater), which amplify the signal and relay it to the next repeater or NIC.

**** Repeater

Technically, a [repeater](https://en.wikipedia.org/wiki/Repeater#firstHeading)`↗` is a network signal amplifier with two RJ45 connectors which adds one maximum length (for the cable type) to the network connection or "run."  In practice, repeaters usually come in the form of [hubs](#heading--hub) or [switches](#heading--switch)`↗`, which can usually perform other functions as well.

**** Switch

A [switch](https://en.wikipedia.org/wiki/Network_switch#firstHeading)`↗` is a "smart" device that connects cables from nodes to make networks.  Like a hub, a switch amplifies signals, that is, it acts as a repeater.  Switches learn by induction which cables receive which IP addresses.  Over time a switch will direct each packet only to devices which indicate that they will accept the addresses associated with those packets.

**** Network topology

[Topology](https://en.wikipedia.org/wiki/Network_topology#mw-content-text)`↗` describes how nodes are connected to a network, specifically referring to the shapes made by the cables and the paths that packets can take.  There are probably as many topologies are there are shapes, but here are some of the most common:

- Bus topology: the most basic network topology, a group of computers connected to a single, long cable.  In this configuration, every computer sees every packet.  A [hub](#heading--hub) network, for instance, is still a bus topology, because every machine sees every packet.

- Star or switch topology: a group of computers connected to a [switch](#heading--switch).  As the switch learns where packets are supposed to go, the star quickly evolves so that only packets are only seen by computers that are intended to receive the packet.

- Backbone topology: a hybrid network configuration in which several stars are connected to a bus.

- Daisy-chain topology: stars connected to stars, or more accurately, switches connected to switches.

- Mesh topology: nodes with multiple interfaces and multiple connections.  Useful where wide-area networks ([WAN](#heading--wan)) where there would otherwise be lots of intermediaries.  Not popular or particularly cost effective for [LAN](#heading--lan) networks.

**** Patch panel

A [patch panel](https://en.wikipedia.org/wiki/Patch_panel#firstHeading)`↗` is simply a 24- to 48-port panel of connectors that can link together three- to ten-foot cables.  A patch panel allows jumpers from network runs to devices in racks, without putting strain and "cable creep" on long runs.

**** LAN

Besides topology, networks can also be classified by their size, range, or "reach."  One such classification is the [Local Area Network (LAN)](https://en.wikipedia.org/wiki/Local_area_network#firstHeading)`↗`, which connects computers in close proximity (about 300 feet)`↗`.

**** WAN

A [WAN (wide area network)](https://en.wikipedia.org/wiki/Wide_area_network#firstHeading)`↗` is a network which connects LANs across large geographic distances, e.g., thousands of miles.

**** MAN

A [metro area network or MAN](https://en.wikipedia.org/wiki/Metropolitan_area_network#firstHeading)`↗` connects LANs over a smaller area, like a city or urban footprint.  Basically, if it isn't really a WAN, but you can't connect it with cables, it's usually considered a MAN.

**** Router

A [router](https://en.wikipedia.org/wiki/Router_%28computing%29#firstHeading)`↗` is a device that transfers packets from one network to another.  Unlike switches, which only ensure that pre-addressed packets get to the correct recipient machines, routers actually modify or encapsulate packets to ensure that they can travel on other networks to reach a remote destination.  Choices about routing are so important that we'll spend a [great deal of time on the subject](#heading--routing-still-rules)`↗` when we learn about cloud networking.

* Test log reference
For any machine which has undergone testing, the Tests log screen -- under the "Tests" tab -- shows a list of tests that have been run for that machine, along with a timestamp and result.  You can view the detailed logs for each test script from this tab.

By default, every MAAS machine is tested with the `smartctl-validate` script, which is a script prepared by Canonical using the [smartmontools](https://www.smartmontools.org)`↗` kit.  This is a disk integrity test.  Typical output includes the following:

```
INFO: Verifying SMART support for the following drive: /dev/sda
INFO: Running command: sudo -n smartctl --all /dev/sda
INFO: SMART support is available; continuing...
INFO: Verifying SMART data on /dev/sda
INFO: Running command: sudo -n smartctl --xall /dev/sda
SUCCESS: SMART validation has PASSED for: /dev/sda
--------------------------------------------------------------------------------
smartctl 6.6 2016-05-31 r4324 [x86_64-linux-4.15.0-115-generic] (local build)
Copyright (C) 2002-16, Bruce Allen, Christian Franke, www.smartmontools.org

=== START OF INFORMATION SECTION ===
Device Model:     QEMU HARDDISK
Serial Number:    QM00001
Firmware Version: 2.5+
User Capacity:    5,368,709,120 bytes [5.36 GB]
Sector Size:      512 bytes logical/physical
Device is:        Not in smartctl database [for details use: -P showall]
ATA Version is:   ATA/ATAPI-7, ATA/ATAPI-5 published, ANSI NCITS 340-2000
Local Time is:    Wed Sep  2 22:29:12 2020 UTC
SMART support is: Available - device has SMART capability.
SMART support is: Enabled
AAM feature is:   Unavailable
APM feature is:   Unavailable
Rd look-ahead is: Unavailable
Write cache is:   Enabled
ATA Security is:  Unavailable
Wt Cache Reorder: Unavailable

=== START OF READ SMART DATA SECTION ===
SMART overall-health self-assessment test result: PASSED

General SMART Values:
Offline data collection status:  (0x82)	Offline data collection activity
					was completed without error.
					Auto Offline Data Collection: Enabled.
Self-test execution status:      (   0)	The previous self-test routine completed
					without error or no self-test has ever 
					been run.
Total time to complete Offline 
data collection: 		(  288) seconds.
Offline data collection
capabilities: 			 (0x19) SMART execute Offline immediate.
					No Auto Offline data collection support.
					Suspend Offline collection upon new
					command.
					Offline surface scan supported.
					Self-test supported.
					No Conveyance Self-test supported.
					No Selective Self-test supported.
SMART capabilities:            (0x0003)	Saves SMART data before entering
					power-saving mode.
					Supports SMART auto save timer.
Error logging capability:        (0x01)	Error logging supported.
					No General Purpose Logging support.
Short self-test routine 
recommended polling time: 	 (   2) minutes.
Extended self-test routine
recommended polling time: 	 (  54) minutes.

SMART Attributes Data Structure revision number: 1
Vendor Specific SMART Attributes with Thresholds:
ID# ATTRIBUTE_NAME          FLAGS    VALUE WORST THRESH FAIL RAW_VALUE
  1 Raw_Read_Error_Rate     PO----   100   100   006    -    0
  3 Spin_Up_Time            PO----   100   100   000    -    16
  4 Start_Stop_Count        -O----   100   100   020    -    100
  5 Reallocated_Sector_Ct   PO----   100   100   036    -    0
  9 Power_On_Hours          PO----   100   100   000    -    1
 12 Power_Cycle_Count       PO----   100   100   000    -    0
190 Airflow_Temperature_Cel PO----   069   069   050    -    31 (Min/Max 31/31)
                            ||||||_ K auto-keep
                            |||||__ C event count
                            ||||___ R error rate
                            |||____ S speed/performance
                            ||_____ O updated online
                            |______ P prefailure warning

Read SMART Log Directory failed: scsi error badly formed scsi parameters

General Purpose Log Directory not supported

SMART Extended Comprehensive Error Log (GP Log 0x03) not supported

SMART Error Log Version: 1
No Errors Logged

SMART Extended Self-test Log (GP Log 0x07) not supported

SMART Self-test log structure revision number 1
No self-tests have been logged.  [To run self-tests, use: smartctl -t]

Selective Self-tests/Logging not supported

SCT Commands not supported

Device Statistics (GP/SMART Log 0x04) not supported

SATA Phy Event Counters (GP Log 0x11) not supported
```

* Testing scripts

* The MAAS log file

* The machine lifecycle

One of the most important things to understand about machines is their life-cycle. In this subsection, you will learn:

- [Introduction to the machine life-cycle](#heading--Introduction-to-the-machine-life-cycle)
- [About machine states](#heading--about-machine-states)
- [About enlistment](#heading--about-enlistment)
- [About commissioning machines](#heading--about-commissioning-machines)
- [About allocation and deployment](#heading--about-allocation-and-deployment)

*** Introduction to the machine life-cycle

Everything that happens to a machine under MAAS control conforms to a specific life-cycle.  All MAAS machines are in a named state, or in transition between states.  Most of these transitions are user-controlled.  Only the "failure" state is reached under the direction of MAAS, when a user's request for certain state changes can't be successfully completed.

In general, the various states and transitions can be summarised in a diagram:

<a href="https://discourse.maas.io/uploads/default/original/2X/b/bd9e5e225ffee4b2e88104e5bbd363dd2ef61a88.jpeg" target = "_blank"><img src="https://discourse.maas.io/uploads/default/original/2X/b/bd9e5e225ffee4b2e88104e5bbd363dd2ef61a88.jpeg"></a>

The central state flow at the bottom of the diagram is the "standard" life-cycle.  If all goes well, you won't have to deviate much from this flow:

- Machines start as servers in your environment, attached to a network or subnet that MAAS can reach and manage.  If those machines are configured to netboot, MAAS can discover them and enlist them, assigning a status of "NEW". By definition, NEW machines are: (2) enabled to network boot, and (2) on a subnet accessible to MAAS. 

- Once you've pared the list to machines that you want MAAS to control, you can choose to commission them.  You can select any machine that is marked "NEW" and tell MAAS to commission it, or, if you add machine manually, MAAS will automatically commission it.  Commissioning PXE boots the machine and loads an ephemeral version of the Ubuntu operating system into the machine's RAM.  MAAS then uses that OS to scan the machine to determine its hardware configuration: CPUs, RAM, storage layouts, PCI and USB devices, and so forth.  Commissioning can be customised -- more on that in a later section.  If a machine fails to properly commission, either because of a commissioning error, or because the commissioning process timed out, that machine enters a "FAILED" state.
  
- MAAS next tests the machine to make sure it's working properly. These basic tests just assure that the discovered hardware works as expected.  Testing can also be customised, if you wish.  Machines that don't pass these tests are moved to a "FAILED" state.

- Having tested it, MAAS then places that machine in the "READY" state, meaning that MAAS should be able to deploy it, based on the gathered hardware information.

- Before you deploy a machine, you should allocate it.  This step essentially involves taking ownership of the machine, so that no other users can deploy it.

- Having allocated a machine, you can deploy it.  When deploying, MAAS again loads an ephemeral Ubuntu OS onto the machine, uses `curtin` to configure the hardware in the way you've specified, and then loads and boots the OS image you've requested.  Deployment also runs some `cloud-init` steps to finish machine configuration, before leaving it up and ready for use.  

Once deployed, there are a couple of minor state changes you can effect without releasing the machine:

- You can lock a machine, if desired, to provide a little extra insurance that it won't accidentally be changed by you -- or anyone.

- Depending upon the machine's duty cycle, you can also power it on, power it off, or even power-cycle it (to effect a reboot, for example).

Note that these minor state changes are not shown in the diagram above.  There are also some exceptional states you can command:

- For any machine that is ready, allocated, or deployed, you can cycle it through a battery of tests at any time.  Be aware, of course, that testing causes the machine to be unavailable for normal use for the duration of the text cycle.

- Machines that are ready, allocated, or deployed can also be placed in "rescue mode".  Essentially, rescue mode is the same as walking to a malfunctioning or mis-configured machine, taking it off the network, and fixing whatever may be wrong with it -- except that you're doing so via SSH, or by running tests from MAAS, rather than standing in front of the machine.  Machines in rescue mode can't enter normal life cycle states until you remove them from rescue mode.  You can, of course, delete them, modify their parameters (tags, zone, and so on), power them off, and mark them broken.  Rescue mode is like a remote repair state that you can control from wherever you are.

- Machines that are allocated or deployed can also be marked broken.  A broken machine powers off by default.  You can still power it on, delete it, or enter rescue mode, but you can't log into it via SSH.  This state is intended for machines that experience catastrophic hardware or software failures and need direct repairs.

There is one more state that a machine can get into: "failed".  This state is entered when commissioning, allocation, or deployment are not successful.  Getting out of a failed state means figuring out what went wrong, correcting it, and retrying the failed operation.  For example, when a machine fails, you can try and commission it again, hopefully after you've found the bug in your custom commissioning script that's causing it to fail (for instance).

Now that we have a solid overview of the life-cycle, let's break down some of these states and transitions in greater detail.

*** About enlistment

MAAS is built to manage machines, including the operating systems on those machines. Enlistment and commissioning are features that make it easier to start managing a machine -- as long as that machine has been configured to netboot. Enlistment enables users to simply connect a machine, configure the firmware properly, and power it on so that MAAS can find it and add it.

Enlistment happens when MAAS starts; it reaches out on connected subnets to locate any nodes -- that is, devices and machines -- that reside on those subnets. MAAS finds a machine that's configured to netboot (e.g., via PXE), boots that machine into Ubuntu, and then sends cloud-init user data which runs standard (i.e., built-in) commissioning scripts. The machine actually adds itself over the MAAS API, and then requests permission to send commissioning data.

Since MAAS doesn't know whether you might intend to actually include these discovered machines in your cloud configuration, it won't automatically take them over, but it will read them to get an idea how they're set up. MAAS then presents these machines to you with a MAAS state of "New." This allows you to examine them and decide whether or not you want MAAS to manage them.

When you configure a machine to netboot -- and turn it on while connected to the network -- MAAS will enlist it, giving it a status of "New."  You can also [add a machine manually](/t/how-to-make-machines-available/5160#heading--how-to-add-a-machine-manually). In either case, the next step is *commissioning*, which boots the machine into an ephemeral Ubuntu kernel so that resource information can be gathered.  You can also run custom commissioning scripts to meet your specific needs.

**** About the enlistment process

When MAAS enlists a machine, it first contacts the DHCP server, so that the machine can be assigned an IP address.  An IP address is necessary to download a kernel and initrd via TFTP, since these functions can't accept domain names.  Once the machine has a bootable kernel, MAAS boots it:

<a href="https://discourse.maas.io/uploads/default/original/1X/76f7113545e6950fec60bdeac06cfaf79b14b3ff.jpeg" target = "_blank"><img src="https://discourse.maas.io/uploads/default/original/1X/76f7113545e6950fec60bdeac06cfaf79b14b3ff.jpeg"></a>

Next, initrd mounts a Squashfs image, ephemerally via HTTP, so that cloud-init can execute:

<a href="https://discourse.maas.io/uploads/default/original/1X/500f9bd2d070790a4007085705035366bee88a4a.jpeg" target = "_blank"><img src="https://discourse.maas.io/uploads/default/original/1X/500f9bd2d070790a4007085705035366bee88a4a.jpeg"></a>

Finally, cloud-init runs enlistment and setup scripts:

<a href="https://discourse.maas.io/uploads/default/original/1X/bd87f78c8ee668a22640bf15607c9e3e532d46bb.jpeg" target = "_blank"><img src="https://discourse.maas.io/uploads/default/original/1X/bd87f78c8ee668a22640bf15607c9e3e532d46bb.jpeg"></a>

The enlistment scripts send information about the machine to the region API server, including the architecture, MAC address and other details.  The API server, in turn, stores these details in the database. This information-gathering process is known as [automatic discovery or network discovery](/t/how-to-set-up-networks/6174#heading--about-network-discovery).

Typically, the next step will be to commission the machine. As an alternative to enlistment, an administrator can add a machine manually. Typically this is done when enlistment doesn't work for some reason. Note that when you manually add a machine, MAAS automatically commissions the machine as soon as you've added it.

After the commissioning process, MAAS places the machine in the ‘Ready’ state. ‘Ready’ is a holding state for machines that are commissioned, waiting to be deployed when needed.

[note]
MAAS runs built-in commissioning scripts during the enlistment phase. When you commission a machine, any customised commissioning scripts you add will have access to data collected during enlistment. Follow the link above for more information about commissioning and commission scripts.
[/note]

**** About BMC enlistment

For IPMI machines, you only need to provide IPMI credentials. MAAS automatically discovers the machine and runs enlistment configuration by matching the BMC address.  For non-IPMI machines, you must specify a non-PXE MAC address. MAAS automatically discovers the machine and runs enlistment configuration by matching the non-PXE MAC address.

**** About adding machines

There are two ways to add a machine to MAAS:

1. If you place the machine on a connected network, and the machine is configured to netboot, MAAS will automatically enlist it.

2. If you add a machine manually, MAAS will automatically commission it.  There are also ways to turn off this automatic commissioning, should you desire to do so.

MAAS typically adds a machine via a combination of DHCP, TFTP, and PXE. By now, you should have enabled MAAS to automatically add devices and machines to your environment. This unattended method of adding machines is called enlistment.

Configuring a computer to boot over PXE is done via its BIOS, often referred to as "netboot" or "network boot". Normally, when you add a machine manually, MAAS will immediately attempt to commission the machine. Note that you will need to configure the underlying machine to netboot, or commissioning will fail. MAAS cannot handle this configuration for you.  While the correct method for configuring network boot depends heavily on your server, there are two common elements:

1. The network card on your server must be able to support PXE, i.e., your NIC -- whether independent or integrated on a motherboard -- must have a boot PROM that supports network booting.  You'll need to consult the documentation for the machine in question to determine this. Note that in MAAS versions before 2.5, you are required to provide the MAC address of the PXE interface when adding a new machine manually.

2. You usually have to interrupt the boot process and enter the BIOS/UEFI menu to configure the network card's PXE stack.  Again, you may need to consult your machine's documentation to pin down this step.

Additional steps will vary widely by machine type and architecture.

Regardless of how MAAS adds a machine, there are no special requirements for the underlying machine itself, other than being able to netboot. In particular, there is no need to install an operating system on it.

**** About cloning machines

[tabs]
[tab version="v3.4 Snap,v3.4 Packages,v3.3 Snap,v3.3 Packages,v3.2 Snap,v3.2 Packages,v3.1 Snap,v3.1 Packages"]
MAAS 3.1 provides the ability to quickly clone or copy configuration from one machine to one or more machines, via the MAAS UI, providing convenient access to an existing API feature.. This is a step towards machine profile templating work. 

Creating a machine profile is a repetitive task. Based on the responses to our survey -- and multiple forum posts, we have learned that most users create multiple machines of the same configuration in batches. Some users create a machine profile template and loop them through the API, while some create a script to interface with the CLI. However, there is no easy way to do this in the UI except by going through each machine and configuring them individually.   

MAAS API already has the cloning functionality, but it was never exposed in the UI. Hence, users may not know that this API feature exists, nor is there any current documentation about how to use this feature.  Although the current cloning API feature does not solve all machine profile templating problems, it is a great place for us to start moving in the direction of machine templates.

**** About copying machine configurations

As a MAAS user -- API or UI -- you may want to copy the configuration of a given machine and apply it to multiple existing machines. Assuming that at least one machine is already set to the desired configuration, you should be able to apply these same settings to a list of destination machines.  This means that a user should be able to:

- select the source machine to copy from.
- validate that the source machine exists.
- select at least 1 destination machine.
- validate that the destination machine(s) exist.
- edit the source machine or destination machines, if needed.
- know at all times which machines are affected.
- see the cloned machines when cloning is successful, or
- get clear failure information, if cloning fails. 

**** About choosing configuration items to copy

As a MAAS user, you will likely want to select whether storage, network, or both configurations should be cloned. The cloning API allows users to choose interfaces and storage separately.  Thus, this new feature should allow the user to:

- clone only the interface (network) configuration.
- clone only the storage configuration.
- clone both configurations.

**** About cloning restrictions

In order for cloning to succeed, a few restrictions must be met:

- The destination interface names must be the same source.
- The destination drive must be equal to or larger than the source drive.
- For static IPs, a new IP will be allocated to the interface on the destination machine
[/tab]
[tab version="v3.0 Snap,v3.0 Packages,v2.9 Snap,v2.9 Packages"]
Cloning machines is available starting with MAAS version 3.1.
[/tab]
[/tabs]

**** About enlisting deployed machines

[tabs]
[tab version="v3.4 Snap,v3.4 Packages,v3.3 Snap,v3.3 Packages,v3.2 Snap,v3.2 Packages,v3.1 Snap,v3.1 Packages"]
In general, when adding a machine to MAAS, it network boots the machine into an ephemeral environment to collect hardware information about the machine. While this is not a destructive action, it doesn’t work if you have machines that are already running a workload.

For one, you might not be able to disrupt the workload in order to network boot it. But also, the machine would be marked as Ready, which is incorrect.

When adding a machine, you may specify that the machine is already deployed. In that case, it won’t be going through the normal commissioning process and will be marked as being deployed.

Such machines lack hardware information. In order to update the information, a script is provided to run a subset of the commissioning scripts and send them back to MAAS.

Because already-deployed machines were not deployed by MAAS, most of the standard MAAS commands will not affect the machine and may, at times, return some odd results.  This is not errant behaviour; the goal of enlisting deployed machines is to avoid disturbing their workload. 
[/tab]
[tab version="v3.0 Snap,v3.0 Packages"]
MAAS version 3.0 cannot enlist deployed machines. Please upgrade to MAAS version 3.1 or greater to gain this capability.
[/tab]
[tab version="v2.9 Snap,v2.9 Packages"]
MAAS version 2.9 cannot enlist deployed machines. Please upgrade to MAAS version 3.1 or greater to gain this capability.
[/tab]
[/tabs]

*** About commissioning machines

When MAAS commissions a machine, the following sequence of events takes place:

1.  DHCP server is contacted
2.  kernel and initrd are received over TFTP
3.  machine boots
4.  initrd mounts a Squashfs image ephemerally over HTTP
5.  cloud-init runs built-in and custom commissioning scripts
6.  machine shuts down

The commissioning scripts will talk to the region API server to ensure that everything is in order and that eventual deployment will succeed.

MAAS chooses the latest Ubuntu LTS release as the default image for commissioning.  If desired, you can select a different image in the "Settings" page of the web UI, by selecting the "General" tab and then scrolling down to the Commissioning section.

[note]
Commissioning requires 60 seconds.
[/note]

**** About commissioning NUMA and SR-IOV nodes

If you are using the NUMA architecture, MAAS versions 2.7 and higher guarantee that machines are assigned to a single NUMA node that contains all the machine's resources. Node boundaries are critical, especially in vNUMA situations.  Splitting nodes can create unnecessary latency.  You want the NUMA node boundaries to match VM boundaries if at all possible.

[note]
You must recommission NUMA/SR-IOV machines that were previously commissioned under version 2.6 or earlier.
[/note]

<a href="https://discourse.maas.io/uploads/default/original/1X/7b47235ff57a570ccba6a6ed09186a3d7483f5a4.png" target = "_blank"><img src="https://discourse.maas.io/uploads/default/original/1X/7b47235ff57a570ccba6a6ed09186a3d7483f5a4.png"></a>

When using these nodes, you can specify a node index for interfaces and physical block devices.  MAAS will display the NUMA node index and details, depending upon your configuration, to include the count of NUMA nodes, number of CPU cores, memory, NICs, and node spaces for bonds and block devices.  You can also filter machines by CPU cores, memory, subnet, VLAN, fabric, space, storage, and RAID, among others.

**** About MAAS commissioning scripts

MAAS runs scripts during enlistment, commissioning and testing to collect data about nodes. Both enlistment and commissioning run all builtin commissioning scripts, though enlistment runs only built-ins. Commissioning also runs any user-uploaded commissioning scripts by default, unless the user manually provides a list of scripts to run. MAAS uses these commissioning scripts to configure hardware and perform other tasks during commissioning, such as updating the firmware. Similarly, MAAS employs hardware testing scripts to evaluate system hardware and report its status.

Scripts can be selected to run from web UI during commissioning, by testing hardware,  or from the command line. Note that MAAS only runs built-in commissioning scripts during enlistment. Custom scripts can be run when you explicitly choose to commission a machine.  A typical administrator workflow (with machine states), using customised commissioning scripts, can be represented as:

Add machine -&gt; Enlistment (runs built-in commissioning scripts MAAS) -&gt; New -&gt; Commission (runs built-in and custom commissioning scripts) -&gt; Ready -&gt; Deploy

NOTE: Scripts are run in alphabetical order in an ephemeral environment.  We recommend running your scripts after any MAAS built-in scripts.  This can be done by naming your scripts 99-z*.  It is possible to reboot the system during commissioning using a script, however, as the environment is ephemeral, any changes to the environment will be destroyed upon reboot (barring, of course, firmware type updates).

When a machine boots, MAAS first instructs it to run cloud-init to set up SSH keys (during commissioning only), set up NTP, and execute a script that runs other commissioning scripts.  Currently, the sequence of MAAS-provided commissioning scripts proceeds like this:

- **maas-support-info:** MAAS gathers information that helps to identify and characterise the machine for debugging purposes, such as the kernel, versioning of various components, etc.  **Runs in parallel with other scripts.**

- **maas-lshw:** this script pulls system BIOS and vendor info, and generates user-defined tags for later use.  **Runs in parallel with other scripts.**

- **20-maas-01-install-lldpd:** this script installs the link layer discovery protocol (LLDP) daemon, which will later capture networking information about the machine.  This script provides some extensive logging.

- **maas-list-modaliases:** this script figures out what hardware modules are loaded, providing a way to autorun certain scripts based on which modules are loaded.  **Runs in parallel with other scripts.**

- **20-maas-02-dhcp-unconfigured-ifaces:** MAAS will want to know all the ways the machine is connected to the network.  Only PXE comes online during boot; this script brings all the other networks online so they can be recognised.  This script provides extensive logging.

- **maas-get-fruid-api-data:** this script gathers information for the Facebook wedge power type.  **Runs in parallel with other scripts.**

- **maas-serial-ports:** this script lists what serial ports are available on the machine.  **Runs in parallel with other scripts.**

[tabs]
[tab version="v3.4 Snap,v3.4 Packages,v3.3 Snap,v3.3 Packages,v3.2 Snap,v3.2 Packages,v3.1 Snap,v3.1 Packages,v3.0 Snap,v3.0 Packages"]
[note]
As of MAAS version 3.0, **40-maas-01-network-interfaces** is no longer used by MAAS.
[/note]
[/tab]
[tab version="v2.9 Snap,v2.9 Packages"]
- **40-maas-01-network-interfaces:** this script is just used to get the IP address, which can then be associated with a VLAN/subnet.
[/tab]
[/tabs]

[tabs]
[tab version="v3.4 Snap,v3.4 Packages,v3.3 Snap,v3.3 Packages,v3.2 Snap,v3.2 Packages,v3.1 Snap,v3.1 Packages,v3.0 Snap,v3.0 Packages"]
- **50-maas-01-commissioning:** this script is the main MAAS tool, gathering information on machine resources, such as storage, network devices, CPU, RAM, details about attached USB and PCI devices, etc.  We currently pull this data using lxd: We use a Go binary built from lxd source that just contains the minimum source to gather the resource information we need.  This script also checks whether the machine being commissioning is a virtual machine, which may affect how MAAS interacts with it.
[/tab]
[tab version="v2.9 Snap,v2.9 Packages"]
- **50-maas-01-commissioning:** this script is the main MAAS tool, gathering information on machine resources, such as storage, network devices, CPU, RAM, etc.  We currently pull this data using lxd: We use a Go binary built from lxd source that just contains the minimum source to gather the resource information we need. This script also checks whether the machine being commissioning is a virtual machine, which may affect how MAAS interacts with it.
[/tab]
[/tabs]

- **maas-capture-lldp:** this script gathers LLDP network information to be presented on the logs page; this data is not used by MAAS at all.  **Runs in parallel with other scripts.**

- **maas-kernel-cmdline:** this script is used to update the boot devices; it double-checks that the right boot interface is selected.

Commissioning runs the same dozen or so scripts as enlistment, gathering all the same information, but with these caveats:

- Commissioning also runs user-supplied commissioning scripts, if present.  Be aware that these scripts run as root, so they can execute any system command.

- Commissioning runs test scripts which are not run during enlistment.

- Commissioning scripts can send BMC configuration data, and can be used to configure BMC data.

- The environment variable BMC_CONFIG_PATH is passed to serially run commissioning scripts; these scripts may write BMC power credentials to BMC_CONFIG_PATH in YAML format, where each key is a power parameter.  The first script to write BMC_CONFIG_PATH is the only script allowed to configure the BMC, allowing you to override MAAS' built-in BMC detection.  If the script returns 0, that value will be send to MAAS.

- All built-in commissioning scripts have been migrated into the database.

- `maas-run-remote-scripts` is capable of enlisting machines, so enlistment `user-data` scripts have been removed.

- The metadata endpoints `http://<MAAS>:5240/<latest or 2012-03-01>/` and `http://<MAAS>:5240/<latest or 2012-03-01>/meta-data/` are now available anonymously for use during enlistment.

In both enlistment and commissioning, MAAS uses either the MAC address or the UUID to identify machines.  Currently, because some machine types encountered by MAAS do **not** use unique MAC addresses, we are trending toward using the UUID.

[note]
To commission a node, it must have a status of "New".
[/note]

You have the option of setting some parameters to change how commissioning runs:

- `enable_ssh`: Optional integer. Controls whether to enable SSH for the commissioning environment using the user's SSH key(s). '1' == True, '0' == False. Roughly equivalent to the **Allow SSH access and prevent machine powering off** in the web UI.

- `skip_bmc_config`: Optional integer.  Controls whether to skip re-configuration of the BMC for IPMI based machines. '1' == True, '0' == False.

- `skip_networking`: Optional integer.  Controls whether to skip re-configuring the networking on the machine after the commissioning has completed. '1' == True, '0' == False. Roughly equivalent to **Retain network configuration** in the web UI.

- `skip_storage`: Optional integer.  Controls whether to skip re-configuring the storage on the machine after the commissioning has completed. '1' == True, '0' == False.  Roughly equivalent to **Retain storage configuration** in the web UI.

- `commissioning_scripts`: Optional string.  A comma separated list of commissioning script names and tags to be run. By default all custom commissioning scripts are run. Built-in commissioning scripts always run. Selecting `update_firmware` or `configure_hba` will run firmware updates or configure HBA's on matching machines.

- `testing_scripts`: Optional string.  A comma separated list of testing script names and tags to be run. By default all tests tagged `commissioning` will be run. Set to `none` to disable running tests.

- `parameters`: Optional string.  Scripts selected to run may define their own parameters. These parameters may be passed using the parameter name. Optionally a parameter may have the script name prepended to have that parameter only apply to that specific script.

**** About machine commissioning logs

MAAS keeps extensive logs of the commissioning process for each machine. These logs present an extremely detailed, timestamped record of completion and status items from the commissioning process.

**** About disabling individual boot methods

[tabs]
[tab version="v3.4 Snap,v3.4 Packages,v3.3 Snap,v3.3 Packages,v3.2 Snap,v3.2 Packages,v3.1 Snap,v3.1 Packages,v3.0 Snap,v3.0 Packages"]
It is possible to disable individual boot methods.  This must be done via the CLI. When a boot method is disabled MAAS will configure MAAS controlled `isc-dhcpd` to not respond to the associated [boot architecture code](https://www.iana.org/assignments/dhcpv6-parameters/dhcpv6-parameters.xhtml#processor-architecture)`↗`. External DHCP servers must be configured manually.

To allow different boot methods to be in different states on separate physical networks using the same VLAN ID configuration is done on the subnet in the UI or API. When using the API boot methods to be disabled may be specified using the MAAS internal name or [boot architecture code](https://www.iana.org/assignments/dhcpv6-parameters/dhcpv6-parameters.xhtml#processor-architecture)`↗` in octet or hex form. 

For MAAS 3.0 and above, the following boot method changes have been implemented:

- UEFI AMD64 HTTP(00:10) has been re-enabled.
- UEFI ARM64 HTTP(00:13) has been enabled.
- UEFI ARM64 TFTP(00:0B) and UEFI ARM64 HTTP(00:13) will now provide a shim and GRUB signed with the Microsoft boot loader keys.
- grub.cfg for all UEFI platforms has been updated to replace the deprecated `linuxefi` and `initrdefi` commands with the standard `linux` and `initrd` commands.
- GRUB debug may now be enabled by enabling rackd debug logging.

[/tab]
[tab version="v2.9 Snap,v2.9 Packages"]
Disabling boot methods is available in MAAS version 3.0 or greater.
[/tab]
[/tabs]


**** About automatic script selection by hardware type

When selecting multiple machines, scripts which declare the `for_hardware` field will only run on machines with matching hardware. To automatically run a script when 'Update firmware' or 'Configure HBA' is selected, you must tag the script with 'update_firmware' or 'configure_hba'.

Similarly, scripts selected by tag on the command line which specify the `for_hardware` field will only run on matching hardware.

**** About script results

A script can output its results to a YAML file, and those results will be associated with the hardware type defined within the script. MAAS provides the path for the results file in an environment variable, `RESULT_PATH`. Scripts should write YAML to this file before exiting.

If the hardware type is storage, for example, and the script accepts a storage type parameter, the result will be associated with a specific storage device.

The YAML file must represent a dictionary with these two fields:

1. `result`: The completion status of the script. This status can be `passed`, `failed`, `degraded`, or `skipped`. If no status is defined, an exit code of `0` indicates a pass while a non-zero value indicates a failure.

2. `results`: A dictionary of results. The key may map to a results key defined as embedded YAML within the script. The value of each result must be a string or a list of strings.

Optionally, a script may define what results to return in the YAML file in the metadata fields.. The `results` field should contain a dictionary of dictionaries. The key for each dictionary is a name which is returned by the results YAML. Each dictionary may contain the following two fields:

1. `title` - The title for the result, used in the UI.

2. `description` - The description of the field used as a tool-tip in the UI.

Here is an example of "degrade detection":

``` python
#!/usr/bin/env python3

# --- Start MAAS 1.0 script metadata ---
# name: example
# results:
#   memspeed:
#     title: Memory Speed
#     description: Bandwidth speed of memory while performing random read writes
# --- End MAAS 1.0 script metadata ---

import os
import yaml

memspeed = some_test()

print('Memspeed: %s' % memspeed)
results = {
   'results': {
       'memspeed': memspeed,
   }
}
if memspeed < 100:
   print('WARN: Memory test passed but performance is low!')
   results['status'] = 'degraded'

result_path = os.environ.get("RESULT_PATH")
if result_path is not None:
   with open(result_path, 'w') as results_file:
       yaml.safe_dump(results, results_file)
```


**** About tags and scripts

As with general tag management, tags make scripts easier to manage; grouping scripts together for commissioning and testing, for example:

``` bash
maas $PROFILE node-script add-tag $SCRIPT_NAME tag=$TAG
maas $PROFILE node-script remove-tag $SCRIPT_NAME tag=$TAG
```

MAAS runs all commissioning scripts by default. However, you can select which custom scripts to run during commissioning by name or tag:

``` bash
maas $PROFILE machine commission \
commissioning_scripts=$SCRIPT_NAME,$SCRIPT_TAG
```

You can also select which testing scripts to run by name or tag:

``` bash
maas $PROFILE machine commission \
testing_scripts=$SCRIPT_NAME,$SCRIPT_TAG
```

Any testing scripts tagged with commissioning will also run during commissioning.

**** About debugging script failures

You can individually access the output from both completed and failed scripts.

<a href="https://assets.ubuntu.com/v1/855015e5-nodes-hw-scripts__2.2_fail.png" target = "_blank"><img src="https://assets.ubuntu.com/v1/855015e5-nodes-hw-scripts__2.2_fail.png"></a>

If you need further details, especially when writing and running your own scripts, you can connect to a machine and examine its logs and environment.

<a href="https://assets.ubuntu.com/v1/da793c67-nodes-hw-scripts__2.4_ssh.png" target = "_blank"><img src="https://assets.ubuntu.com/v1/da793c67-nodes-hw-scripts__2.4_ssh.png"></a>

Because scripts operate within an ephemeral version of Ubuntu, enabling this option stops the machine from shutting down, allowing you to connect and probe a script's status.

As long as you've added your [SSH key](/t/how-to-manage-user-accounts/5184#heading--ssh-keys) to MAAS, you can connect with SSH to the machine's IP with a username of `ubuntu`. Type `sudo -i` to get root access.

**** About testing hardware

If you wish, you can tell MAAS to test machine hardware using well-known Linux utilities.  MAAS can test machines that have a status of **Ready**, **Broken**, or **Deployed**.  You can include testing as part of the commissioning process. When you choose the 'Commission' action, MAAS will display the dialog described below.  Be aware, though, that if the hardware tests fail, the machine will become unavailable for Deployment.

[note]
The majority of testing scripts only work with machines that are backed by physical hardware (e.g. they may be incompatible with VM-based machines).
[/note]

With MAAS, you can easily write, upload and execute your hardware testing scripts and see the results.

**** About machine hardware & test logs

MAAS logs test results and allows you to view a summary of tests run against a particular machine.  You can also example details on any particular tests:

<a href="https://discourse.maas.io/uploads/default/original/2X/e/e53a2c01b57df49e56bb4d95552b6a038249aa97.png" target = "_blank"><img src="https://discourse.maas.io/uploads/default/original/2X/e/e53a2c01b57df49e56bb4d95552b6a038249aa97.png"></a>

You can also examine the "raw" log output:

<a href="https://discourse.maas.io/uploads/default/original/2X/d/dc5bb5e6489a382e257dac605f2dbdc6fa1ca630.png" target = "_blank"><img src="https://discourse.maas.io/uploads/default/original/2X/d/dc5bb5e6489a382e257dac605f2dbdc6fa1ca630.png"></a>

Help interpreting these logs can be found under the [Logging](/t/how-to-work-with-log-files/5240) section of this documentation.

**** About testing machine networking

MAAS provides a comprehensive suite of network and link testing capabilities.  MAAS can check whether or not links are connected, detect slow links, and report link and interface speeds via UI or API. In addition, you can test Internet connectivity against a user-provided list of URLs or IP addresses.  Bonded NICS will be separated during this testing, so that each side of a redundant interface is fully evaluated.

Network testing also includes customisable network testing and commissioning scripts. There are no particular restrictions on these scripts, allowing you to test a wide variety of possible conditions and situations.

**** About post-commission configuration

Once commissioned, you can configure the machine's network interface(s). Specifically, when a machine's status is either "Ready" or "Broken", interfaces can be added/removed, attached to a fabric and linked to a subnet, and provided an IP assignment mode. Tags can also be assigned to specific network interfaces.

*** About allocation and deployment

Once a machine has been commissioned, the next logical step is to deploy it. Deploying a machine means, effectively, to [install an operating system on it](/t/how-to-acquire-images/6192#heading--how-images-deploy), along with any other application loads you wish to run on that machine.

 A detailed picture of deployment looks something like this:

<a href="https://discourse.maas.io/uploads/default/original/2X/f/f7e0fb1916bca084de75fc0479bfec3c95adf7b6.png" target = "_blank"><img src="https://discourse.maas.io/uploads/default/original/2X/f/f7e0fb1916bca084de75fc0479bfec3c95adf7b6.png"></a>

Before deploying a machine, MAAS must allocate it (status 'Allocated'). Allocating a machine reserves the machine for the exclusive use of the allocation process. The machine is no longer available to any other process, including another MAAS instance, or a process such as Juju.

The agent that triggers deployment may vary. For instance, if the machines are destined to run complex, inter-related services that scale up or down frequently, like a "cloud" resource, then [Juju](https://jaas.ai/)`↗` is the recommended deployment agent. Juju will also install and configure services on the deployed machines. If you want to use MAAS to install a base operating system and work on the machines manually, then you can deploy a machine directly with MAAS.

Machines deployed with MAAS will also be ready to accept connections via SSH, to the 'ubuntu' user account.  This connection assumes that you have imported an SSH key has to your MAAS account. This is explained in [SSH keys](/t/how-to-manage-user-accounts/5184#heading--ssh-keys).

[note]
Juju adds SSH keys to machines under its control.
[/note]

MAAS also supports machine customisation with a process called "preseeding." For more information about customising machines, see [How to customise machines](/t/how-to-customise-machines/5108).

To deploy, you must configure the underlying machine to netboot.  Such a machine will undergo the following process, outlined in the above diagram:

1. MAAS boots the machine via the machine's BMC, using whatever power driver is necessary to properly communicate with the machine.
2. The booted machine sends a DHCP Discover request.
3. The MAAS-managed DHCP server (ideally) responds with an IP address and the location of a MAAS-managed HTTP or TFTP boot server.
4. The machine uses the HTTP/TFTP location to request a usable Network Boot Program (NBP).
5. The machine recieves the NBP and boots.
6. The machine firmware requests a bootable image.
7. MAAS sends an ephemeral OS image, including an initrd; this ephemeral (RAM-only) image is necessary for ```curtin``` to carry out any hardware-prep instructions (such as disk paritioning) before the deployed OS is booted.
8. The initrd mounts a SquashFS image, also ephemerally, over HTTP.
9. The machine boots the emphemeral image.
10. The ephemeral image runs ```curtin```, with passed pre-seed information, to configure the machine's hardware.
11. The desired deployment (target) image is retrieved by ```curtin```, which installs and boots that deployment image.  Note that the curtin installer uses an image-based method and is now the only installer used by MAAS. Although the older debian-installer method has been removed, curtin continues to support preseed files. For more information about customising machines see [How to customise machines](/t/how-to-customise-machines/5108).
12. The target image runs its embedded ```cloud-init``` script set, including any customisations and pre-seeds.

Once this is done, the target image is up and running on the machine, and the machine can be considered successfully deployed.

Also note that, before deploying, you should take two key actions:

1. Review and possibly set the [Ubuntu kernels](/t/how-to-customise-machines/5108#heading--about-ubuntu-kernels) and the [Kernel boot options](/t/how-to-customise-machines/5108#heading--about-kernel-boot-options) that will get used by deployed machines.

2. Ensure any pertinent SSH keys are imported (see [SSH keys](/t/how-to-manage-user-accounts/5184#heading--ssh-keys)) to MAAS so it can connect to deployed machines.

* The machine list
The machine list is the basic dashboard for many MAAS operations.  In this subsection, you will learn:

- [About the machine summary](#heading--about-the-machine-summary)
- [Handling attached USB and PCI devices](#heading--usb-pci-devices)
- [About machine network info](#heading--about-machine-interfaces)
- [About machine configuration info](#heading--machine-config)
- [About resource pools](#heading--about-resource-pools)
- [About tags](#heading--about-tags)
- [About annotations](#heading--about-annotations)
- [About storage](#heading--about-storage)
  
In the illustration below, you see the machine list for a typical small hospital data centre, including servers ready and allocated for functions like Pharmacy, Orders, Charts, and so on:

[tabs]
[tab version="v3.4 Snap,v3.4 Packages,v3.3 Snap,v3.3 Packages,v3.2 Snap,v3.2 Packages,v3.1 Snap,v3.1 Packages,v3.0 Snap,v3.0 Packages,v2.9 Snap,v2.9 Packages" view="UI"]
<a href="https://discourse.maas.io/uploads/default/original/1X/30df04b0bcec5fcf6538590ed795cb0514a64675.jpeg" target = "_blank"><img src="https://discourse.maas.io/uploads/default/original/1X/30df04b0bcec5fcf6538590ed795cb0514a64675.jpeg"></a>

Rolling the cursor over status icons often reveals more details. For example, a failed hardware test script will place a warning icon alongside the hardware type tested by the script. Rolling the cursor over this will reveal which test failed.  Likewise, you can find some immediate options by rolling over the column data items in the machines table.

<a href="https://discourse.maas.io/uploads/default/original/1X/8f78a8877a029e7a44bcd4cf3d138499637fe790.jpeg" target = "_blank"><img src="https://discourse.maas.io/uploads/default/original/1X/8f78a8877a029e7a44bcd4cf3d138499637fe790.jpeg"></a>

The 'Add hardware' drop-down menu is used to add either new machines or a new chassis. This menu changes context when one or more machines are selected from the table, using either the individual checkboxes in the first column or the column title checkbox to select all.

<a href="https://discourse.maas.io/uploads/default/original/1X/9a0747649e6aff999d3c04335eb752accedaf3de.jpeg" target = "_blank"><img src="https://discourse.maas.io/uploads/default/original/1X/9a0747649e6aff999d3c04335eb752accedaf3de.jpeg"></a>

With one or more machines selected, the 'Add hardware' drop-down menu moves to the left, and is joined by the 'Take action' menu.  This menu provides access to the various [machine actions](/t/maas-glossary/5416#node-actions) that can be applied to the selected machine(s):

<a href="https://discourse.maas.io/uploads/default/original/1X/e03d5ac8de9ea4f4827ed057bb2dd83e241aac3b.jpeg" target = "_blank"><img src="https://discourse.maas.io/uploads/default/original/1X/e03d5ac8de9ea4f4827ed057bb2dd83e241aac3b.jpeg"></a>

[note]
The 'Filter by' section limits the machines listed in the table to selected keywords and machine attributes.
[/note]

[/tab]
[tab version="v3.4 Snap,v3.4 Packages,v3.3 Snap,v3.3 Packages,v3.2 Snap,v3.2 Packages,v3.1 Snap,v3.1 Packages,v3.0 Snap,v3.0 Packages,v2.9 Snap,v2.9 Packages" view="CLI"]
```nohighlight
FQDN               POWER  STATUS     OWNER  TAGS     POOL       NOTE     ZONE
----               -----  ------     -----  ----     ----       ----     ----
52-54-00-15-36-f2  off    Ready      -      Orders   Prescrbr   @md-all  Medications
52-54-00-17-64-c8  off    Ready      -      HRMgmt   StaffComp  @tmclck  Payroll
52-54-00-1d-47-95  off    Ready      -      MedSupp  SuppServ   @storag  Inventory
52-54-00-1e-06-41  off    Ready      -      PatPrtl  BusOfc     @bzstns  BizOffice
52-54-00-1e-a5-7e  off    Ready      -      Pharm    Prescrbr   @rxonly  Pharmacy
52-54-00-2e-b7-1e  off    Allocated  admin  NursOrd  NurServ    @nstns   Nursing
52-54-00-2e-c4-40  off    Allocated  admin  MedAdmn  NurServ    @rxonly  Nursing
52-54-00-2e-ee-17  off    Deployed   admin  Charts   ProServ    @md-all  Physician
```

You can generate a list similar to this for your machines with the command:

```nohighlight
maas admin machines read | jq -r '(["FQDN","POWER","STATUS",
"OWNER", "TAGS", "POOL", "NOTE", "ZONE"] | (., map(length*"-"))),
(.[] | [.hostname, .power_state, .status_name, .owner // "-", 
.tag_names[0] // "-", .pool.name, .description // "-", .zone.name]) | @tsv' | column -t
```
[/tab]
[/tabs]

These example machines would typically be duplicated in several different geographies, with a quick way to switch to a redundant node, should anything go wrong (e.g., high availability).  We used the word node there because, In the network language of MAAS, machines are one of several different types of nodes.  A node is simply a network-connected object or, more specifically, an object that can independently communicate on a network. MAAS nodes include controllers, network devices, and of course, machines.

Looking back at the example above, you can see that there are several columns in the machine list, depending on your view:

- **FQDN | MAC**: The fully qualified domain name or the MAC address of the machine.
- **Power**: 'On', 'Off' or 'Error' to highlight an error state.
- **Status**: The current status of the machine, such as 'Ready', 'Commissioning' or 'Failed testing'.
- **Owner**: The MAAS account responsible for the machine.
- **Cores**: The number of CPU cores detected on the machine.
- **RAM**: The amount of RAM, in GiB, discovered on the machine.
- **Disks**: The number of drives detected on the machine.
- **Storage**: The amount of storage, in GB, identified on the machine.

*** About the machine summary

Click a machine's FQDN or MAC address to open a detailed view of a machine's status and configuration.

<a href="https://discourse.maas.io/uploads/default/original/2X/a/a8ff4caf6362a3d695682499a74d64cb189dfc37.png" target = "_blank"><img src="https://discourse.maas.io/uploads/default/original/2X/a/a8ff4caf6362a3d695682499a74d64cb189dfc37.png"></a>

The default view is 'Machine summary', presented as a series of cards detailing the CPU, memory, storage and tag characteristics of the machine, as well as an overview of its current status. When relevant, 'Edit' links take you directly to the settings pane for the configuration referenced within the card.  The machine menu bar within the web UI also includes links to logs, events, and configuration options:

<a href="https://discourse.maas.io/uploads/default/original/2X/2/21e9f4dca3a3e0a6657b5b2a570c9fc68a3e4961.png" target = "_blank"><img src="https://discourse.maas.io/uploads/default/original/2X/2/21e9f4dca3a3e0a6657b5b2a570c9fc68a3e4961.png"></a>

The machine status card presents an overview of CPU, memory, storage, tags, and general settings:

<a href="https://discourse.maas.io/uploads/default/original/2X/a/a8ff4caf6362a3d695682499a74d64cb189dfc37.png" target = "_blank"><img src="https://discourse.maas.io/uploads/default/original/2X/a/a8ff4caf6362a3d695682499a74d64cb189dfc37.png"></a>

The first card presents some basics of the machine resources and configuration:

<a href="https://discourse.maas.io/uploads/default/original/1X/3e50fb21f4985db0a85519e2e933e24658770b9e.jpeg" target = "_blank"><img src="https://discourse.maas.io/uploads/default/original/1X/3e50fb21f4985db0a85519e2e933e24658770b9e.jpeg"></a>

Here are some details on what this card presents, with details on in-card links described in following sections:

- **OVERVIEW** the machine status (in this case "Deployed"), and lists OS version information.  

- **CPU** shows the specifics of the CPU(s), including a link to test the processor(s).

- **MEMORY** gives the total available RAM for this machine, along with a test link.

- **STORAGE** presents the total amount of storage available and the number of disks that provide that storage.  There are two links here: one gives the storage layout (with the opportunity to change it for devices that are in 'Ready' or 'Allocated' states.

- **Owner** identifies the owner of the machine.

- **Domain** indicates the domain in which the machine exists.

- **Zone** shows the AZ in which this machine resides, along with a link to edit the machine configuration (to change the AZ, if desired).

- **Resource pool** shows the pool to which this machine has been assigned, and an edit link.

- **Power type** gives the current power type, which links to the relevant edit form.

- **Tags** presents the list of tags associated with this machine, editable via the link.

Note that clicking any of the links in this card will either present a pop-up form or take you to another item in the machine menu -- so using the browser "back" button will take you completely away from this machine's page.  For example, you can choose the "Test CPU" option, which brings up this overlay:

<a href="https://discourse.maas.io/uploads/default/original/2X/6/6d7fe50e5b296a37a03269a1f5be3d25a2a2481a.png" target = "_blank"><img src="https://discourse.maas.io/uploads/default/original/2X/6/6d7fe50e5b296a37a03269a1f5be3d25a2a2481a.png"></a>

From this screen, you can choose test scripts and run the tests (in the background) as the interface returns to the Machine summary.  A linked note in the CPU block lets you know that the tests are in progress:

<a href="https://discourse.maas.io/uploads/default/original/2X/3/3e140872c407e5b9eb06960b5b42353765567192.png" target = "_blank"><img src="https://discourse.maas.io/uploads/default/original/2X/3/3e140872c407e5b9eb06960b5b42353765567192.png"></a> 

And you can watch the results under the "Tests" option in the Machine menu:

<a href="https://discourse.maas.io/uploads/default/original/2X/f/f398c9ed670af8c0886ccc1ed8bf586e3faf1e53.png" target = "_blank"><img src="https://discourse.maas.io/uploads/default/original/2X/f/f398c9ed670af8c0886ccc1ed8bf586e3faf1e53.png"></a> 

The rest of the cards on the Machine summary are either self-explanatory, or they're covered in the sections below.  The main point is this: You can see that nearly everything about machines takes place within the main menu's "Machines" option. 

*** Handling attached USB and PCI devices

[tabs]
[tab version="v3.4 Snap,v3.4 Packages,v3.3 Snap,v3.3 Packages,v3.2 Snap,v3.2 Packages,v3.1 Snap,v3.1 Packages,v3.0 Snap,v3.0 Packages"]
The machines in your MAAS may have devices attached to them via USB or PCI interface, such as keyboards, cameras, network cards, GPUs, etc.  MAAS will recognise these devices and make them visible to you when a machine is commissioned.

For example, the machine details presents USB and PCI devices like this:

<a href="https://discourse.maas.io/uploads/default/original/2X/8/87f42bafe321d45af94d73216f933a9067f01df2.png" target = "_blank"><img src="https://discourse.maas.io/uploads/default/original/2X/8/87f42bafe321d45af94d73216f933a9067f01df2.png"></a>

Note that this page now includes two new tabs: "PCI devices" and "USB."  For each USB/PCI device attached to your machine, these tabs will list:

- device type
- vendor ID
- a product description
- a product ID
- the driver name
- the containing NUMA node (if any)
- the device address

A typical PCI device tab would look something like this:

<a href="https://discourse.maas.io/uploads/default/original/2X/8/82e1e6f8bc511047ac5f773430f7e5812c7a24d4.png" target = "_blank"><img src="https://discourse.maas.io/uploads/default/original/2X/8/82e1e6f8bc511047ac5f773430f7e5812c7a24d4.png"></a>

The USB tab presents similar information in the same format.

[note]
If you are upgrading from a previous version of MAAS, PCI and USB devices aren't modelled, so you will have to recommission the machine to capture these devices.
[/note]


Once you've commissioned the machine, you have the option of deleting
PCI/USB devices from the machine in any machine state, via the CLI
only, using the following command:

```
maas $PROFILE node-device delete $SYSTEM_ID $DEVICE_ID
```

where:

- $PROFILE   = your user profile (e.g., "admin")
- $SYSTEM_ID = the ID of the machine in question (e.g., "ngx7ry")
- $DEVICE_ID = the ID of the device you want to delete 

If the device is still present in the system, it will be recognised again (and thus "recreated")
when the machine is commissioned again.

*** About machine network info

The Network "tab" provides you with a way to view/edit the network and interface configuration for a machine: 

<a href="https://discourse.maas.io/uploads/default/original/2X/c/c5316db130ae05a9cdabcd49ffaa69f0bb405d1d.png" target = "_blank"><img src="https://discourse.maas.io/uploads/default/original/2X/c/c5316db130ae05a9cdabcd49ffaa69f0bb405d1d.png"></a> 

In the case of this deployed machine, there are not many editing options.  If the machine is in a 'Ready' state, though, altering the network configuration is possible, as shown in the screenshot above.

Options on this tab are described in the introduction to [Networking](/t/how-to-set-up-networks/6174) article in this documentation set.

*** About machine configuration info

The final tab from the Machine menu allows you to update machine and power configuration options: 

<a href="https://discourse.maas.io/uploads/default/original/2X/7/7cfd77228a5cf1a6f779897d501f14fbf78fd4b4.png" target = "_blank"><img src="https://discourse.maas.io/uploads/default/original/2X/7/7cfd77228a5cf1a6f779897d501f14fbf78fd4b4.png"></a> 

There are two sections to this tab.  The "Machine configuration" section, shown above, offers some general parameters, mostly related to how this machine is grouped and categorised.  More information on these options are found in the relevant sections of the documentation (e.g., tags, resource pools, and so forth). 

The "Power configuration" supplies the parameters necessary for MAAS to access the machine to PXE-boot it: 

<a href="https://discourse.maas.io/uploads/default/original/2X/1/198898362285e4a1308535a4aa701156a67c9616.png" target = "_blank"><img src="https://discourse.maas.io/uploads/default/original/2X/1/198898362285e4a1308535a4aa701156a67c9616.png"></a> 

More information on Power configuration will be found in the [Power management](/t/power-management-reference/5246) section of this documentation.
[/tab]
[tab version="v2.9 Snap,v2.9 Packages"]
USB and PCI devices are supported in MAAS version 3.0 and greater.
[/tab]
[/tabs]

*** About resource pools

Resource pools allow administrators to logically group resources -- machines and VM hosts -- into pools. Pools can help you budget machines for a particular set of functions.  For example, if you're using MAAS to manage a hospital data centre, you may want to keep a certain number of machines reserved for provider use, whether that be for the charts, documentation, or orders application.  You can use resource pools to reserve those machines, regardless of which of the three applications you end up loading onto a particular machine at any given time. 

*** About tags

Tags are short, descriptive, searchable words that can be applied to various MAAS objects, including:

- machines (physical and virtual)
- VM hosts
- controllers (rack and region)
- storage (virtual and physical; block devices or partitions)
- network interfaces
- devices
- nodes (in the CLI only)

Tags serve to help you identify, group, and find objects easily, especially when you routinely deploy hundreds of machines.


*** About annotations

Annotations are descriptive, searchable phrases that apply only to machines.  There are two types of annotations: static (always present in any machine state), and dynamic (only present in allocated or deployed states).  Annotations help you identify, characterise, and inform others about your machines.


*** About storage

You have significant latitude when choosing the final storage configuration of a deployed machine. MAAS supports traditional disk partitioning, as well as more complex options such as LVM, RAID, and bcache. MAAS also supports UEFI as a boot mechanism.  This article explains boot mechanisms and layouts, and offers some advice on how to configure layouts and manage storage.

[note]
MAAS doesn’t currently support deploying with ZFS for devices other than the root one.  For this reason, ZFS is disrecommended.
[/note]

A machine's storage is dependant upon the underlying system's disks, but its configuration (i.e., disk usage) is the result of a storage template. In MAAS, this template is called a layout, and MAAS applies it to a machine during commissioning.  Once a layout is applied, a regular user can make modifications to a machine at the filesystem level to arrive at the machine's final storage configuration.  When a machine is no longer needed, a user can choose from among several disk erasure types before releasing it.

[note]
MAAS supports storage configuration for CentOS and RHEL deployments. Support includes RAID, LVM, and custom partitioning with different file systems (ZFS and bcache excluded). This support requires a newer version of Curtin, [available as a PPA](https://launchpad.net/ubuntu/+source/curtin)`↗`.
[/note]


* The rack log

* the region log

* TLS and MAAS
[tabs]
[tab version="v3.4 Snap,v3.4 Packages,v3.3 Snap,v3.3 Packages,v3.2 Snap,v3.2 Packages"]
** About MAAS Native TLS

MAAS version 3.2 has built-in TLS support for communicating with the UI and API over HTTPS. This eliminates the need to deploy a separate TLS-terminating reverse-proxy solution in front of MAAS to provide secure access to API and UI.

TLS versions 1.2 and 1.3 are supported by MAAS. For TLSv1.2, the following ciphers are accepted:

- AES256+EECDH
- AES256+EDH

You will need to obtain your own certificates via some provider, e.g., [small step](https://smallstep.com/docs/step-ca)`↗`.

*** About certificate auto-renewal

At the moment we don’t support automatic certificate renewal, because it depends on the PKI used at the organisation level.  We [do provide some examples](#heading--how-to-auto-renew-certificates) of how to set this up, as long as you understand that these are just gratuitous helps, not supported configurations.
[/tab]
[tab version="v3.1 Snap,v3.1 Packages,v3.0 Snap,v3.0 Packages,v2.9 Snap,v2.9 Packages"]
To obtain MAAS native TLS, upgrade to MAAS v3.2 or higher.
[/tab]
[/tabs]

* Try out the MAAS CLI

Let's get some machines up and running, using the MAAS CLI.  If you haven't installed MAAS yet, you can [follow these instructions](/t/how-to-install-maas/5128) first.

** Log into MAAS

In the CLI, logging in is a two-stepper:

```nohighlight
sudo maas apikey --username=admin > api-key-file
```

You can make sure you got a valid API key by displaying the contents of api-key-file:

```nohighlight
cat api-key-file
XXEjkeeqM:zXb7LkuPY7VxShFNhCFDaD8WnP8gLVL8V64GbSn:tTKdwWV64GbSn:tTKdwW
```

[note]
You need to run the `apikey` command to get an API key.  You can't cut and paste the one above, it won't work.
[/note]

Anyway, we can now login to MAAS.  But first, let's try "maas --help" -- there's an important distinction that you'll want to remember:

```nohighlight
maas --help
usage: maas [-h] COMMAND ...

optional arguments:
   -h, --help      show this help message and exit

drill down:
   COMMAND
      login         Log in to a remote API, and remember its description and credentials.
      logout        Log out of a remote API, purging any stored credentials.
      list          List remote APIs that have been logged-in to.
      refresh       Refresh the API descriptions of all profiles.
      init          Initialise MAAS in the specified run mode.
      config        View or change controller configuration.
      status        Status of controller services.
      migrate       Perform migrations on connected database.
      apikey        Used to manage a user's API keys. Shows existing keys unless --generate or --delete is passed.
      configauth    Configure external authentication.
      createadmin   Create a MAAS administrator account.
      changepassword
		    Change a MAAS user's password.
      admin         Interact with http://192.168.43.251:5240/MAAS/api/2.0/


```

This is the help you get (a) if you're not logged in, or (b) if you don't type a logged-in username (called a "profile") after "maas". What you see above isn't even half of what MAAS can do, but it's all you get as an unrecognized user.

So now, let's login and try that help again:

[note]
Your MAAS address will probably not be the same as the one I'm using; cutting and pasting won't work.
[/note]

```nohighlight
maas login admin http://192.168.43.251:5240/MAAS/api/2.0/ $(head -1 api-key-file)

You are now logged in to the MAAS server at
http://192.168.43.251:5240/MAAS/api/2.0/ with the profile name 'admin'.

For help with the available commands, try:

maas admin --help
```
** Check the detailed help

Now, having done that, we can get a much better idea what MAAS will do:

```nohighlight
maas admin --help

usage: maas admin [-h] COMMAND ...

Issue commands to the MAAS region controller at http://192.168.43.251:5240/MAAS/api/2.0/.

optional arguments:
   -h, --help            show this help message and exit

drill down:
   COMMAND
      account             Manage the current logged-in user.
      bcache-cache-set    Manage bcache cache set on a machine.
      bcache-cache-sets   Manage bcache cache sets on a machine.
      bcache              Manage bcache device on a machine.
      bcaches             Manage bcache devices on a machine.
      block-device        Manage a block device on a machine.
      block-devices       Manage block devices on a machine.
      boot-resource       Manage a boot resource.
      boot-resources      Manage the boot resources.
      boot-source         Manage a boot source.
      boot-source-selection
			  Manage a boot source selection.
      boot-source-selections
			  Manage the collection of boot source selections.
      boot-sources        Manage the collection of boot sources.
      commissioning-script
			  Manage a custom commissioning script.
      commissioning-scripts
			  Manage custom commissioning scripts.
      dhcpsnippet         Manage an individual DHCP snippet.
      dhcpsnippets        Manage the collection of all DHCP snippets in MAAS.
      dnsresource         Manage dnsresource.
      dnsresource-record  Manage dnsresourcerecord.
      dnsresource-records
			  Manage DNS resource records (e.g. CNAME, MX, NS, SRV, TXT)
      dnsresources        Manage dnsresources.
      device              Manage an individual device.
      devices             Manage the collection of all the devices in the MAAS.
      discoveries         Query observed discoveries.
      discovery           Read or delete an observed discovery.
      domain              Manage domain.
      domains             Manage domains.
      events              Retrieve filtered node events.
      fabric              Manage fabric.
      fabrics             Manage fabrics.
      fan-network         Manage Fan Network.
      fan-networks        Manage Fan Networks.
      file                Manage a FileStorage object.
      files               Manage the collection of all the files in this MAAS.
      ipaddresses         Manage IP addresses allocated by MAAS.
      iprange             Manage IP range.
      ipranges            Manage IP ranges.
      interface           Manage a node's or device's interface.
      interfaces          Manage interfaces on a node.
      license-key         Manage a license key.
      license-keys        Manage the license keys.
      maas                Manage the MAAS server.
      machine             Manage an individual machine.
      machines            Manage the collection of all the machines in the MAAS.
      network             Manage a network.
      networks            Manage the networks.
      node                Manage an individual Node.
      node-results        Read the collection of commissioning script results.
      node-script         Manage or view a custom script.
      node-script-result  Manage node script results.
      node-script-results
			  Manage node script results.
      node-scripts        Manage custom scripts.
      nodes               Manage the collection of all the nodes in the MAAS.
      notification        Manage an individual notification.
      notifications       Manage the collection of all the notifications in MAAS.
      package-repositories
			  Manage the collection of all Package Repositories in MAAS.
      package-repository  Manage an individual package repository.
      partition           Manage partition on a block device.
      partitions          Manage partitions on a block device.
      pod                 Manage an individual pod.
      pods                Manage the collection of all the pod in the MAAS.
      rack-controller     Manage an individual rack controller.
      rack-controllers    Manage the collection of all rack controllers in MAAS.
      raid                Manage a specific RAID (Redundant Array of Independent
			  Disks) on a machine.
      raids               Manage all RAIDs (Redundant Array of Independent Disks) on
			  a machine.
      region-controller   Manage an individual region controller.
      region-controllers  Manage the collection of all region controllers in MAAS.
      resource-pool       Manage a resource pool.
      resource-pools      Manage resource pools.
      sshkey              Manage an SSH key.
      sshkeys             Manage the collection of all the SSH keys in this MAAS.
      sslkey              Manage an SSL key.
      sslkeys             Operations on multiple keys.
      space               Manage space.
      spaces              Manage spaces.
      static-route        Manage static route.
      static-routes       Manage static routes.
      subnet              Manage subnet.
      subnets             Manage subnets.
      tag                 Tags are properties that can be associated with a Node and
			  serve as criteria for selecting and allocating nodes.
      tags                Manage all tags known to MAAS.
      user                Manage a user account.
      users               Manage the user accounts of this MAAS.
      version             Information about this MAAS instance.
      virtual-machine     Manage individual virtual machines.
      virtual-machines    Manage a collection of virtual machines.
      vlan                Manage a VLAN on a fabric.
      vlans               Manage VLANs on a fabric.
      vm-host             Manage an individual vm-host.
      vm-hosts            Manage the collection of all the vm-hosts in the MAAS.
      vmfs-datastore      Manage VMFS datastore on a machine.
      vmfs-datastores     Manage VMFS datastores on a machine.
      volume-group        Manage volume group on a machine.
      volume-groups       Manage volume groups on a machine.
      zone                Manage a physical zone.
      zones               Manage physical zones.

      This is a profile.  Any commands you issue on this profile will
      operate on the MAAS region server.

      The command information you see here comes from the region server's
      API; it may differ for different profiles.  If you believe the API may
      have changed, use the command's 'refresh' sub-command to fetch the
      latest version of this help information from the server.
```

** Set the DNS server IP address

The next step?  Set the DNS server IP address.  You can do this by issuing the CLI subcommand called "dnsresource".  Let's look at the help for that command:

```nohighlight
maas admin dnsresource --help
Usage: maas admin dnsresource [-h] COMMAND ...

Manage dnsresource.

optional arguments:
   -h, --help  show this help message and exit

drill down:
   COMMAND
      read      Read a DNS resource
      update    Update a DNS resource
      delete    Delete a DNS resource
```

That looks like what we want; let's be naive and try that:

```nohighlight
maas admin dnsresource read
Usage: maas admin dnsresource read [--help] [-d] [-k] id [data [data ...]]

Read a DNS resource

positional arguments:
   id
   data

optional arguments:
   --help, -h      Show this help message and exit.
   -d, --debug     Display more information about API responses.
   -k, --insecure  Disable SSL certificate check

Read a DNS resource by id.
   the following arguments are required: id, data
```

Well, that's weird, and not what we want.  But wait -- the CLI follows the "collection-instance" rule.  Listing DNS resources would be part of a collection, so they would be pluralized.  Let's try `dnsresources` (plural):

```nohighlight
maas admin dnsresources read
Success.
Machine-readable output follows:
[]
```

That isn't it either, but in the `maas set-config` command there is a parameter named `upstream_dns` for which we can set a value.  Let's try that:

```nohighlight
maas admin maas set-config name=upstream_dns value="8.8.8.8"
Success.
Machine-readable output follows:
OK
```

Yep, that's it. It isn't obvious whether we needed to type the IP address with quotes.  In fact, it doesn't matter; whatever makes you feel more comfortable:

```nohighlight
maas admin maas set-config name=upstream_dns value=8.8.8.8
Success.
Machine-readable output follows:
OK
```

** Import some images

Next, we need to import images.  Some images automatically sync, so let's bring in some other image (like Ubuntu 16.04 LTS) just to see how that works.

Reading the help, it says we can discover the images we've already downloaded, using the following command:

```nohighlight
maas admin boot-resources read
```

The JSON resulting from this command is rather lengthy, so we've collapsed it to a pop-down.  You can click the arrow to see the JSON returns.  Incidentally, we'll do that from now on with long listings:

<details><summary>Success.</summary>

```nohighlight
Success.
Machine-readable output follows:
[
    {
	"id": 7,
	"type": "Synced",
	"name": "grub-efi-signed/uefi",
	"architecture": "amd64/generic",
	"resource_uri": "/MAAS/api/2.0/boot-resources/7/"
    },
    {
	"id": 8,
	"type": "Synced",
	"name": "grub-efi/uefi",
	"architecture": "arm64/generic",
	"resource_uri": "/MAAS/api/2.0/boot-resources/8/"
    },
    {
	"id": 9,
	"type": "Synced",
	"name": "grub-ieee1275/open-firmware",
	"architecture": "ppc64el/generic",
	"resource_uri": "/MAAS/api/2.0/boot-resources/9/"
    },
    {
	"id": 10,
	"type": "Synced",
	"name": "pxelinux/pxe",
	"architecture": "i386/generic",
	"resource_uri": "/MAAS/api/2.0/boot-resources/10/"
    },
    {
	"id": 1,
	"type": "Synced",
	"name": "ubuntu/bionic",
	"architecture": "amd64/ga-18.04",
	"resource_uri": "/MAAS/api/2.0/boot-resources/1/",
	"subarches": "generic,hwe-p,hwe-q,hwe-r,hwe-s,hwe-t,hwe-u,hwe-v,hwe-w,ga-16.04,ga-16.10,ga-17.04,ga-17.10,ga-18.04"
    },
    {
	"id": 2,
	"type": "Synced",
	"name": "ubuntu/bionic",
	"architecture": "amd64/ga-18.04-lowlatency",
	"resource_uri": "/MAAS/api/2.0/boot-resources/2/",
	"subarches": "generic,hwe-p,hwe-q,hwe-r,hwe-s,hwe-t,hwe-u,hwe-v,hwe-w,ga-16.04,ga-16.10,ga-17.04,ga-17.10,ga-18.04"
    },
    {
	"id": 3,
	"type": "Synced",
	"name": "ubuntu/bionic",
	"architecture": "amd64/hwe-18.04",
	"resource_uri": "/MAAS/api/2.0/boot-resources/3/",
	"subarches": "generic,hwe-p,hwe-q,hwe-r,hwe-s,hwe-t,hwe-u,hwe-v,hwe-w,ga-16.04,ga-16.10,ga-17.04,ga-17.10,ga-18.04"
    },
    {
	"id": 4,
	"type": "Synced",
	"name": "ubuntu/bionic",
	"architecture": "amd64/hwe-18.04-edge",
	"resource_uri": "/MAAS/api/2.0/boot-resources/4/",
	"subarches": "generic,hwe-p,hwe-q,hwe-r,hwe-s,hwe-t,hwe-u,hwe-v,hwe-w,ga-16.04,ga-16.10,ga-17.04,ga-17.10,ga-18.04,hwe-18.10,hwe-19.04"
    },
    {
	"id": 5,
	"type": "Synced",
	"name": "ubuntu/bionic",
	"architecture": "amd64/hwe-18.04-lowlatency",
	"resource_uri": "/MAAS/api/2.0/boot-resources/5/",
	"subarches": "generic,hwe-p,hwe-q,hwe-r,hwe-s,hwe-t,hwe-u,hwe-v,hwe-w,ga-16.04,ga-16.10,ga-17.04,ga-17.10,ga-18.04"
    },
    {
	"id": 6,
	"type": "Synced",
	"name": "ubuntu/bionic",
	"architecture": "amd64/hwe-18.04-lowlatency-edge",
	"resource_uri": "/MAAS/api/2.0/boot-resources/6/",
	"subarches": "generic,hwe-p,hwe-q,hwe-r,hwe-s,hwe-t,hwe-u,hwe-v,hwe-w,ga-16.04,ga-16.10,ga-17.04,ga-17.10,ga-18.04,hwe-18.10,hwe-19.04"
    }
]
```
</details>

Okay, that's a lot of information, but it looks like we have a bunch of 18.04 images downloaded and synched.  Depending on what version of MAAS you're using, your list could be a lot different, and that doesn't matter here.

*** Try getting a more compact image list

Let's try to get a little fancy with  `grep` and see if we can make that list shorter:

```nohighlight
maas admin boot-resources read | grep architecture
```

This produces a quick list of the images we've successfully downloaded:<br />

```nohighlight
"architecture": "amd64/generic",
"architecture": "arm64/generic",
"architecture": "ppc64el/generic",
"architecture": "i386/generic",
"architecture": "amd64/ga-18.04",
"architecture": "amd64/ga-18.04-lowlatency",
"architecture": "amd64/hwe-18.04",
"architecture": "amd64/hwe-18.04-edge",
"architecture": "amd64/hwe-18.04-lowlatency",
"architecture": "amd64/hwe-18.04-lowlatency-edge",
```

That definitely confirms that we have some images.  But what are those three or four on top?  Looking at the massive JSON output, notice that they have names like "open-firmware," "uefi," and "pxe."  Those are images that can PXE-boot machines, basically.  But how can we sort this information out in a neat way?

*** Get familiar with jq

Well, if you're going to use anything with JSON-based output, you'll want to consider learning the command line tool [jq](https://stedolan.github.io/jq/)`↗`.  It's quite handy for parsing the JSON output of the MAAS CLI.  So, for example, if we want a lightly formatted table of names and architectures, we can run that last command through `jq` like this:

```nohighlight
maas admin boot-resources read | jq -r '.[] | "\(.name)\t\(.architecture)"'
```

This gives us a cleaner image list that looks something like this:

```nohighlight
grub-efi-signed/uefi         amd64/generic
grub-efi/uefi                arm64/generic
grub-ieee1275/open-firmware  ppc64el/generic
pxelinux/pxe                 i386/generic
ubuntu/bionic                amd64/ga-18.04
ubuntu/bionic                amd64/ga-18.04-lowlatency
ubuntu/bionic                amd64/hwe-18.04
ubuntu/bionic                amd64/hwe-18.04-edge
ubuntu/bionic                amd64/hwe-18.04-lowlatency
ubuntu/bionic                amd64/hwe-18.04-lowlatency-edge
```

So basically, we have images needed to boot and deloy machines.  That's a good start, but let's pull down another image for the practice.

We can select images with the `boot-source-selections` command, so let's try that with old "Trusty" (Xenial Xerus, aka 16.04):

```nohighlight
maas admin boot-source-selections create 1 \
  > os="ubuntu" release="trusty" arches="amd64" subarches="*" \
  > labels="*"
```

The results look like this:

```nohighlight
Success.
Machine-readable output follows:
{
    "os": "ubuntu",
    "release": "trusty",
    "arches": [
	"amd64"
    ],
    "subarches": [
	"*"
    ],
    "labels": [
	"*"
    ],
    "boot_source_id": 1,
    "id": 2,
    "resource_uri": "/MAAS/api/2.0/boot-sources/1/selections/2/"
}
```

And that worked, which is good, because that was a long command to type correctly.  Luckily, downloading (importing) them is a fairly simple command:

```nohighlight
maas admin boot-resources import
Success.
Machine-readable output follows:
Import of boot resources started
```

** Configure DHCP

The whole point here is to get machines deployed, so the next step is to get DHCP working.  We have to find the right VLAN, which isn't too hard, since at this point there's only one.

In order to turn on DHCP, we need to know two things besides the VLAN name ("untagged"): we need to know the fabric ID and the primary rack controller name. Actually, to start with, all the fabrics will be on the same untagged VLAN, so any fabric will do. We can find a usable fabric by picking a valid bridge IP address like this:


```nohighlight
maas admin subnet read 192.168.123.0/24 | grep fabric_id
"fabric_id": 2,
```

Then we need to find the name of the primary rack controller:

```nohighlight
 maas admin rack-controllers read | grep hostname | cut -d '"' -f 4
wintermute
```

So we should be able to turn on DHCP like this:

```nohighlight
maas admin vlan update 2 untagged dhcp_on=True primary_rack=wintermute
{"dhcp_on": ["dhcp can only be turned on when a dynamic IP range is defined."]}
```

Wait. We need to define a dynamic IP range for this to work.  Well, my virtual bridge is on 192.168.123.0/24. Yours will be in a different place, most likely.  We want to use that subnet, so let's choose a reasonable IP range, like, say, 192.168.123.190 to 192.168.123.253:

```nohighlight
 maas admin ipranges create type=dynamic start_ip=192.168.123.190 end_ip=192.168.123.253
```

Remember that your subnet address will probably be different, so you'll need to create a range within that subnet.  The result of this command is:

<details><summary>Success.</summary>

```nohighlight
Success.
Machine-readable output follows:
{
    "subnet": {
	"name": "192.168.123.0/24",
	"description": "",
	"vlan": {
	    "vid": 0,
	    "mtu": 1500,
	    "dhcp_on": false,
	    "external_dhcp": null,
	    "relay_vlan": null,
	    "fabric": "fabric-2",
	    "primary_rack": null,
	    "name": "untagged",
	    "id": 5003,
	    "space": "undefined",
	    "secondary_rack": null,
	    "fabric_id": 2,
	    "resource_uri": "/MAAS/api/2.0/vlans/5003/"
	},
	"cidr": "192.168.123.0/24",
	"rdns_mode": 2,
	"gateway_ip": null,
	"dns_servers": [],
	"allow_dns": true,
	"allow_proxy": true,
	"active_discovery": false,
	"managed": true,
	"id": 4,
	"space": "undefined",
	"resource_uri": "/MAAS/api/2.0/subnets/4/"
    },
    "type": "dynamic",
    "start_ip": "192.168.123.190",
    "end_ip": "192.168.123.253",
    "user": {
	"is_superuser": true,
	"username": "admin",
	"email": "admin@admin.com",
	"is_local": true,
	"resource_uri": "/MAAS/api/2.0/users/admin/"
    },
    "comment": "",
    "id": 1,
    "resource_uri": "/MAAS/api/2.0/ipranges/1/"
}
```
</details>

Okay, now let's try that DHCP switch-on one more time:

```nohighlight
 maas admin vlan update 2 untagged dhcp_on=True primary_rack=wintermute
```

Now it works:

```nohighlight
Success.
Machine-readable output follows:
{
    "vid": 0,
    "mtu": 1500,
    "dhcp_on": true,
    "external_dhcp": null,
    "relay_vlan": null,
    "fabric": "fabric-2",
    "space": "undefined",
    "primary_rack": "8dwnne",
    "secondary_rack": null,
    "name": "untagged",
    "fabric_id": 2,
    "id": 5003,
    "resource_uri": "/MAAS/api/2.0/vlans/5003/"
}
```

** Commission some machines

In order to deploy machines, we've got to create some, plain and simple, and then commission them.  We're using `virsh` for this example, but you can use LXD or any VM hosting tool you're comfortable with:

```nohighlight
 maas admin machines create
  > architecture=amd64
  > mac_addresses=52:54:00:15:36:f2
  > power_type=virsh
  > power_parameters_power_id=f677a842-571c-4e65-adc9-11e2cf92d363
  > power_parameters_power_address=qemu+ssh://stormrider@192.168.123.1/system
  > power_parameters_power_pass=xxxxxxxx

```

That seemed to work:

<details><summary>Success.</summary>

```nohighlight
Success.
Machine-readable output follows:
{
    "storage": 0.0,
    "tag_names": [],
    "special_filesystems": [],
    "memory": 0,
    "boot_disk": null,
    "virtualblockdevice_set": [],
    "hardware_info": {
	"system_vendor": "Unknown",
	"system_product": "Unknown",
	"system_family": "Unknown",
	"system_version": "Unknown",
	"system_sku": "Unknown",
	"system_serial": "Unknown",
	"cpu_model": "Unknown",
	"mainboard_vendor": "Unknown",
	"mainboard_product": "Unknown",
	"mainboard_serial": "Unknown",
	"mainboard_version": "Unknown",
	"mainboard_firmware_vendor": "Unknown",
	"mainboard_firmware_date": "Unknown",
	"mainboard_firmware_version": "Unknown",
	"chassis_vendor": "Unknown",
	"chassis_type": "Unknown",
	"chassis_serial": "Unknown",
	"chassis_version": "Unknown"
    },
    "address_ttl": null,
    "memory_test_status": -1,
    "other_test_status_name": "Unknown",
    "osystem": "",
    "status_message": "Commissioning",
    "netboot": true,
    "physicalblockdevice_set": [],
    "node_type": 0,
    "cpu_test_status": -1,
    "memory_test_status_name": "Unknown",
    "bcaches": [],
    "storage_test_status": 0,
    "system_id": "bhxws3",
    "status": 1,
    "commissioning_status": 0,
    "power_type": "virsh",
    "locked": false,
    "numanode_set": [
	{
	    "index": 0,
	    "memory": 0,
	    "cores": []
	}
    ],
    "bios_boot_method": null,
    "fqdn": "ace-swan.maas",
    "node_type_name": "Machine",
    "hostname": "ace-swan",
    "volume_groups": [],
    "testing_status": 0,
    "network_test_status": -1,
    "other_test_status": -1,
    "interface_test_status": -1,
    "hwe_kernel": null,
    "blockdevice_set": [],
    "testing_status_name": "Pending",
    "power_state": "unknown",
    "min_hwe_kernel": "",
    "owner": "admin",
    "distro_series": "",
    "storage_test_status_name": "Pending",
    "cpu_speed": 0,
    "swap_size": null,
    "cpu_test_status_name": "Unknown",
    "hardware_uuid": null,
    "architecture": "amd64/generic",
    "pool": {
	"name": "default",
	"description": "Default pool",
	"id": 0,
	"resource_uri": "/MAAS/api/2.0/resourcepool/0/"
    },
    "cache_sets": [],
    "pod": null,
    "iscsiblockdevice_set": [],
    "disable_ipv4": false,
    "status_action": "",
    "boot_interface": {
	"name": "eth0",
	"id": 10,
	"product": null,
	"system_id": "bhxws3",
	"effective_mtu": 1500,
	"children": [],
	"link_connected": true,
	"enabled": true,
	"interface_speed": 0,
	"numa_node": 0,
	"firmware_version": null,
	"parents": [],
	"discovered": null,
	"params": "",
	"links": [],
	"sriov_max_vf": 0,
	"tags": [],
	"type": "physical",
	"vlan": null,
	"vendor": null,
	"link_speed": 0,
	"mac_address": "52:54:00:15:36:f2",
	"resource_uri": "/MAAS/api/2.0/nodes/bhxws3/interfaces/10/"
    },
    "cpu_count": 0,
    "domain": {
	"authoritative": true,
	"ttl": null,
	"resource_record_count": 0,
	"name": "maas",
	"is_default": true,
	"id": 0,
	"resource_uri": "/MAAS/api/2.0/domains/0/"
    },
    "current_testing_result_id": 7,
    "default_gateways": {
	"ipv4": {
	    "gateway_ip": null,
	    "link_id": null
	},
	"ipv6": {
	    "gateway_ip": null,
	    "link_id": null
	}
    },
    "interface_set": [
	{
	    "name": "eth0",
	    "id": 10,
	    "product": null,
	    "system_id": "bhxws3",
	    "effective_mtu": 1500,
	    "children": [],
	    "link_connected": true,
	    "enabled": true,
	    "interface_speed": 0,
	    "numa_node": 0,
	    "firmware_version": null,
	    "parents": [],
	    "discovered": null,
	    "params": "",
	    "links": [],
	    "sriov_max_vf": 0,
	    "tags": [],
	    "type": "physical",
	    "vlan": null,
	    "vendor": null,
	    "link_speed": 0,
	    "mac_address": "52:54:00:15:36:f2",
	    "resource_uri": "/MAAS/api/2.0/nodes/bhxws3/interfaces/10/"
	}
    ],
    "status_name": "Commissioning",
    "commissioning_status_name": "Pending",
    "owner_data": {},
    "ip_addresses": [],
    "raids": [],
    "network_test_status_name": "Unknown",
    "description": "",
    "current_commissioning_result_id": 6,
    "interface_test_status_name": "Unknown",
    "current_installation_result_id": null,
    "zone": {
	"name": "default",
	"description": "",
	"id": 1,
	"resource_uri": "/MAAS/api/2.0/zones/default/"
    },
    "resource_uri": "/MAAS/api/2.0/machines/bhxws3/"
}
```
</details>

And just like that, it's already commissioning (MAAS does that if it can).

** Manually commission a machine

So now we have a machine in the "Ready" state, but let's also get familiar with commanding MAAS to commission it via the CLI.  All we really need for that is the system ID, which is the last parameter in the "resource uri" above.  But just for grins, let's go ahead and retrieve the system ID using the CLI.

There's only one in this example, so we don't have to worry about any other cross-referencing on this machine:

```nohighlight
 maas admin machines read | jq '.[] | .hostname, .system_id'
"ace-swan"
"bhxws3"
```

Okay, now we can use that system ID to commission the machine via the CLI:

```nohighlight
 maas admin machine commission bhxws3
```

And commissioning should start and return success:

<details><summary>Success.</summary>

```nohighlight
Success.
Machine-readable output follows:
{
    "storage_test_status_name": "Pending",
    "bcaches": [],
    "cpu_count": 1,
    "interface_set": [
	{
	    "params": "",
	    "numa_node": 0,
	    "tags": [],
	    "id": 10,
	    "mac_address": "52:54:00:15:36:f2",
	    "vendor": "Red Hat, Inc.",
	    "children": [],
	    "effective_mtu": 1500,
	    "discovered": [],
	    "links": [],
	    "link_speed": 0,
	    "link_connected": true,
	    "system_id": "bhxws3",
	    "enabled": true,
	    "interface_speed": 0,
	    "firmware_version": null,
	    "name": "ens3",
	    "sriov_max_vf": 0,
	    "product": null,
	    "vlan": {
		"vid": 0,
		"mtu": 1500,
		"dhcp_on": true,
		"external_dhcp": null,
		"relay_vlan": null,
		"fabric": "fabric-2",
		"primary_rack": "8dwnne",
		"name": "untagged",
		"id": 5003,
		"space": "undefined",
		"secondary_rack": null,
		"fabric_id": 2,
		"resource_uri": "/MAAS/api/2.0/vlans/5003/"
	    },
	    "parents": [],
	    "type": "physical",
	    "resource_uri": "/MAAS/api/2.0/nodes/bhxws3/interfaces/10/"
	}
    ],
    "network_test_status_name": "Unknown",
    "numanode_set": [
	{
	    "index": 0,
	    "memory": 985,
	    "cores": [
		0
	    ]
	}
    ],
    "locked": false,
    "hardware_uuid": "F677A842-571C-4E65-ADC9-11E2CF92D363",
    "default_gateways": {
	"ipv4": {
	    "gateway_ip": null,
	    "link_id": null
	},
	"ipv6": {
	    "gateway_ip": null,
	    "link_id": null
	}
    },
    "status_action": "",
    "status_message": "Commissioning",
    "cpu_test_status_name": "Unknown",
    "memory_test_status": -1,
    "virtualblockdevice_set": [],
    "pool": {
	"name": "default",
	"description": "Default pool",
	"id": 0,
	"resource_uri": "/MAAS/api/2.0/resourcepool/0/"
    },
    "current_testing_result_id": 9,
    "current_installation_result_id": null,
    "netboot": true,
    "description": "",
    "special_filesystems": [],
    "testing_status": 0,
    "memory": 1024,
    "current_commissioning_result_id": 8,
    "storage": 5368.70912,
    "commissioning_status": 0,
    "cpu_test_status": -1,
    "tag_names": [
	"virtual"
    ],
    "memory_test_status_name": "Unknown",
    "swap_size": null,
    "status_name": "Commissioning",
    "other_test_status": -1,
    "pod": null,
    "storage_test_status": 0,
    "blockdevice_set": [
	{
	    "id_path": "/dev/disk/by-id/ata-QEMU_HARDDISK_QM00001",
	    "size": 5368709120,
	    "block_size": 512,
	    "tags": [
		"ssd"
	    ],
	    "serial": "QM00001",
	    "uuid": null,
	    "numa_node": 0,
	    "available_size": 5368709120,
	    "id": 3,
	    "partition_table_type": null,
	    "model": "QEMU HARDDISK",
	    "path": "/dev/disk/by-dname/sda",
	    "storage_pool": null,
	    "used_for": "Unused",
	    "filesystem": null,
	    "system_id": "bhxws3",
	    "used_size": 0,
	    "partitions": [],
	    "name": "sda",
	    "type": "physical",
	    "resource_uri": "/MAAS/api/2.0/nodes/bhxws3/blockdevices/3/"
	}
    ],
    "other_test_status_name": "Unknown",
    "distro_series": "",
    "testing_status_name": "Pending",
    "ip_addresses": [],
    "address_ttl": null,
    "system_id": "bhxws3",
    "physicalblockdevice_set": [
	{
	    "firmware_version": "2.5+",
	    "serial": "QM00001",
	    "uuid": null,
	    "numa_node": 0,
	    "available_size": 5368709120,
	    "size": 5368709120,
	    "tags": [
		"ssd"
	    ],
	    "id": 3,
	    "partition_table_type": null,
	    "id_path": "/dev/disk/by-id/ata-QEMU_HARDDISK_QM00001",
	    "model": "QEMU HARDDISK",
	    "path": "/dev/disk/by-dname/sda",
	    "storage_pool": null,
	    "used_for": "Unused",
	    "filesystem": null,
	    "system_id": "bhxws3",
	    "used_size": 0,
	    "partitions": [],
	    "name": "sda",
	    "block_size": 512,
	    "type": "physical",
	    "resource_uri": "/MAAS/api/2.0/nodes/bhxws3/blockdevices/3/"
	}
    ],
    "fqdn": "ace-swan.maas",
    "osystem": "",
    "domain": {
	"authoritative": true,
	"ttl": null,
	"resource_record_count": 0,
	"name": "maas",
	"id": 0,
	"is_default": true,
	"resource_uri": "/MAAS/api/2.0/domains/0/"
    },
    "boot_interface": {
	"params": "",
	"numa_node": 0,
	"tags": [],
	"id": 10,
	"mac_address": "52:54:00:15:36:f2",
	"vendor": "Red Hat, Inc.",
	"children": [],
	"effective_mtu": 1500,
	"discovered": [],
	"links": [],
	"link_speed": 0,
	"link_connected": true,
	"system_id": "bhxws3",
	"enabled": true,
	"interface_speed": 0,
	"firmware_version": null,
	"name": "ens3",
	"sriov_max_vf": 0,
	"product": null,
	"vlan": {
	    "vid": 0,
	    "mtu": 1500,
	    "dhcp_on": true,
	    "external_dhcp": null,
	    "relay_vlan": null,
	    "fabric": "fabric-2",
	    "primary_rack": "8dwnne",
	    "name": "untagged",
	    "id": 5003,
	    "space": "undefined",
	    "secondary_rack": null,
	    "fabric_id": 2,
	    "resource_uri": "/MAAS/api/2.0/vlans/5003/"
	},
	"parents": [],
	"type": "physical",
	"resource_uri": "/MAAS/api/2.0/nodes/bhxws3/interfaces/10/"
    },
    "hostname": "ace-swan",
    "network_test_status": -1,
    "min_hwe_kernel": "",
    "power_state": "off",
    "interface_test_status_name": "Unknown",
    "owner_data": {},
    "volume_groups": [],
    "power_type": "virsh",
    "node_type": 0,
    "owner": "admin",
    "cache_sets": [],
    "architecture": "amd64/generic",
    "hwe_kernel": null,
    "zone": {
	"name": "default",
	"description": "",
	"id": 1,
	"resource_uri": "/MAAS/api/2.0/zones/default/"
    },
    "disable_ipv4": false,
    "boot_disk": {
	"firmware_version": "2.5+",
	"serial": "QM00001",
	"uuid": null,
	"numa_node": 0,
	"available_size": 5368709120,
	"size": 5368709120,
	"tags": [
	    "ssd"
	],
	"id": 3,
	"partition_table_type": null,
	"id_path": "/dev/disk/by-id/ata-QEMU_HARDDISK_QM00001",
	"model": "QEMU HARDDISK",
	"path": "/dev/disk/by-dname/sda",
	"storage_pool": null,
	"used_for": "Unused",
	"filesystem": null,
	"system_id": "bhxws3",
	"used_size": 0,
	"partitions": [],
	"name": "sda",
	"block_size": 512,
	"type": "physical",
	"resource_uri": "/MAAS/api/2.0/nodes/bhxws3/blockdevices/3/"
    },
    "status": 1,
    "iscsiblockdevice_set": [],
    "raids": [],
    "node_type_name": "Machine",
    "hardware_info": {
	"system_vendor": "QEMU",
	"system_product": "Standard PC (i440FX + PIIX, 1996)",
	"system_family": "Unknown",
	"system_version": "pc-i440fx-focal",
	"system_sku": "Unknown",
	"system_serial": "Unknown",
	"cpu_model": "Intel Core Processor (Skylake, IBRS)",
	"mainboard_vendor": "Unknown",
	"mainboard_product": "Unknown",
	"mainboard_serial": "Unknown",
	"mainboard_version": "Unknown",
	"mainboard_firmware_vendor": "SeaBIOS",
	"mainboard_firmware_date": "04/01/2014",
	"mainboard_firmware_version": "1.13.0-1ubuntu1",
	"chassis_vendor": "QEMU",
	"chassis_type": "Other",
	"chassis_serial": "Unknown",
	"chassis_version": "pc-i440fx-focal"
    },
    "commissioning_status_name": "Pending",
    "bios_boot_method": "pxe",
    "interface_test_status": -1,
    "cpu_speed": 0,
    "resource_uri": "/MAAS/api/2.0/machines/bhxws3/"
}
```
</details>

And that's it, it's that easy. It takes a minute to get all the parameters together to create a new machine, but it isn't that difficult.  

** Deploy some machines

The real value of MAAS, of course, is deploying machines without having to be there.  That's what we're going to do next.

*** Allocate a machine

When it's finished commissioning, we can allocate a machine like this:

```nohighlight
maas admin machines allocate system_id=bhxws3
```

Allocating assigns ownership of that machine to the requesting user, which prevents others from changing the state of your machine you aren't expecting it.  When successful, you'll get a return like this:

<details><summary>Success.</summary>

```nohighlight
Success.
Machine-readable output follows:
{
    "raids": [],
    "zone": {
	"name": "default",
	"description": "",
	"id": 1,
	"resource_uri": "/MAAS/api/2.0/zones/default/"
    },
    "current_commissioning_result_id": 8,
    "storage_test_status": 2,
    "current_testing_result_id": 9,
    "bcaches": [],
    "ip_addresses": [
	"192.168.123.190"
    ],
    "pool": {
	"name": "default",
	"description": "Default pool",
	"id": 0,
	"resource_uri": "/MAAS/api/2.0/resourcepool/0/"
    },
    "physicalblockdevice_set": [
	{
	    "firmware_version": "2.5+",
	    "id_path": "/dev/disk/by-id/ata-QEMU_HARDDISK_QM00001",
	    "system_id": "bhxws3",
	    "partition_table_type": "GPT",
	    "type": "physical",
	    "block_size": 512,
	    "id": 3,
	    "numa_node": 0,
	    "partitions": [
		{
		    "uuid": "8aa1164c-8a91-41d7-92e3-c411634355bb",
		    "size": 5360320512,
		    "bootable": false,
		    "tags": [],
		    "id": 3,
		    "used_for": "ext4 formatted filesystem mounted at /",
		    "device_id": 3,
		    "system_id": "bhxws3",
		    "path": "/dev/disk/by-dname/sda-part2",
		    "type": "partition",
		    "filesystem": {
			"fstype": "ext4",
			"label": "root",
			"uuid": "68487852-7e38-4605-a84e-d787532fd443",
			"mount_point": "/",
			"mount_options": null
		    },
		    "resource_uri": "/MAAS/api/2.0/nodes/bhxws3/blockdevices/3/partition/3"
		}
	    ],
	    "filesystem": null,
	    "available_size": 0,
	    "size": 5368709120,
	    "storage_pool": null,
	    "model": "QEMU HARDDISK",
	    "used_size": 5366611968,
	    "tags": [
		"ssd"
	    ],
	    "used_for": "GPT partitioned with 1 partition",
	    "uuid": null,
	    "name": "sda",
	    "path": "/dev/disk/by-dname/sda",
	    "serial": "QM00001",
	    "resource_uri": "/MAAS/api/2.0/nodes/bhxws3/blockdevices/3/"
	}
    ],
    "swap_size": null,
    "storage": 5368.70912,
    "node_type_name": "Machine",
    "system_id": "bhxws3",
    "owner_data": {},
    "special_filesystems": [],
    "tag_names": [
	"virtual"
    ],
    "cpu_test_status_name": "Unknown",
    "locked": false,
    "cpu_count": 1,
    "volume_groups": [],
    "storage_test_status_name": "Passed",
    "hardware_info": {
	"system_vendor": "QEMU",
	"system_product": "Standard PC (i440FX + PIIX, 1996)",
	"system_family": "Unknown",
	"system_version": "pc-i440fx-focal",
	"system_sku": "Unknown",
	"system_serial": "Unknown",
	"cpu_model": "Intel Core Processor (Skylake, IBRS)",
	"mainboard_vendor": "Unknown",
	"mainboard_product": "Unknown",
	"mainboard_serial": "Unknown",
	"mainboard_version": "Unknown",
	"mainboard_firmware_vendor": "SeaBIOS",
	"mainboard_firmware_date": "04/01/2014",
	"mainboard_firmware_version": "1.13.0-1ubuntu1",
	"chassis_vendor": "QEMU",
	"chassis_type": "Other",
	"chassis_serial": "Unknown",
	"chassis_version": "pc-i440fx-focal"
    },
    "node_type": 0,
    "other_test_status": -1,
    "hostname": "ace-swan",
    "interface_test_status": -1,
    "boot_interface": {
	"link_speed": 0,
	"params": "",
	"vendor": "Red Hat, Inc.",
	"firmware_version": null,
	"system_id": "bhxws3",
	"enabled": true,
	"type": "physical",
	"links": [
	    {
		"id": 15,
		"mode": "auto",
		"subnet": {
		    "name": "192.168.123.0/24",
		    "description": "",
		    "vlan": {
			"vid": 0,
			"mtu": 1500,
			"dhcp_on": true,
			"external_dhcp": null,
			"relay_vlan": null,
			"fabric": "fabric-2",
			"id": 5003,
			"secondary_rack": null,
			"primary_rack": "8dwnne",
			"name": "untagged",
			"fabric_id": 2,
			"space": "undefined",
			"resource_uri": "/MAAS/api/2.0/vlans/5003/"
		    },
		    "cidr": "192.168.123.0/24",
		    "rdns_mode": 2,
		    "gateway_ip": null,
		    "dns_servers": [],
		    "allow_dns": true,
		    "allow_proxy": true,
		    "active_discovery": false,
		    "managed": true,
		    "id": 4,
		    "space": "undefined",
		    "resource_uri": "/MAAS/api/2.0/subnets/4/"
		}
	    }
	],
	"id": 10,
	"discovered": [
	    {
		"subnet": {
		    "name": "192.168.123.0/24",
		    "description": "",
		    "vlan": {
			"vid": 0,
			"mtu": 1500,
			"dhcp_on": true,
			"external_dhcp": null,
			"relay_vlan": null,
			"fabric": "fabric-2",
			"id": 5003,
			"secondary_rack": null,
			"primary_rack": "8dwnne",
			"name": "untagged",
			"fabric_id": 2,
			"space": "undefined",
			"resource_uri": "/MAAS/api/2.0/vlans/5003/"
		    },
		    "cidr": "192.168.123.0/24",
		    "rdns_mode": 2,
		    "gateway_ip": null,
		    "dns_servers": [],
		    "allow_dns": true,
		    "allow_proxy": true,
		    "active_discovery": false,
		    "managed": true,
		    "id": 4,
		    "space": "undefined",
		    "resource_uri": "/MAAS/api/2.0/subnets/4/"
		},
		"ip_address": "192.168.123.190"
	    }
	],
	"numa_node": 0,
	"children": [],
	"parents": [],
	"link_connected": true,
	"effective_mtu": 1500,
	"tags": [],
	"sriov_max_vf": 0,
	"interface_speed": 0,
	"name": "ens3",
	"mac_address": "52:54:00:15:36:f2",
	"product": null,
	"vlan": {
	    "vid": 0,
	    "mtu": 1500,
	    "dhcp_on": true,
	    "external_dhcp": null,
	    "relay_vlan": null,
	    "fabric": "fabric-2",
	    "id": 5003,
	    "secondary_rack": null,
	    "primary_rack": "8dwnne",
	    "name": "untagged",
	    "fabric_id": 2,
	    "space": "undefined",
	    "resource_uri": "/MAAS/api/2.0/vlans/5003/"
	},
	"resource_uri": "/MAAS/api/2.0/nodes/bhxws3/interfaces/10/"
    },
    "memory": 1024,
    "memory_test_status_name": "Unknown",
    "default_gateways": {
	"ipv4": {
	    "gateway_ip": null,
	    "link_id": null
	},
	"ipv6": {
	    "gateway_ip": null,
	    "link_id": null
	}
    },
    "blockdevice_set": [
	{
	    "id_path": "/dev/disk/by-id/ata-QEMU_HARDDISK_QM00001",
	    "size": 5368709120,
	    "block_size": 512,
	    "tags": [
		"ssd"
	    ],
	    "system_id": "bhxws3",
	    "partition_table_type": "GPT",
	    "type": "physical",
	    "id": 3,
	    "numa_node": 0,
	    "partitions": [
		{
		    "uuid": "8aa1164c-8a91-41d7-92e3-c411634355bb",
		    "size": 5360320512,
		    "bootable": false,
		    "tags": [],
		    "id": 3,
		    "used_for": "ext4 formatted filesystem mounted at /",
		    "device_id": 3,
		    "system_id": "bhxws3",
		    "path": "/dev/disk/by-dname/sda-part2",
		    "type": "partition",
		    "filesystem": {
			"fstype": "ext4",
			"label": "root",
			"uuid": "68487852-7e38-4605-a84e-d787532fd443",
			"mount_point": "/",
			"mount_options": null
		    },
		    "resource_uri": "/MAAS/api/2.0/nodes/bhxws3/blockdevices/3/partition/3"
		}
	    ],
	    "filesystem": null,
	    "available_size": 0,
	    "storage_pool": null,
	    "model": "QEMU HARDDISK",
	    "used_size": 5366611968,
	    "used_for": "GPT partitioned with 1 partition",
	    "uuid": null,
	    "name": "sda",
	    "path": "/dev/disk/by-dname/sda",
	    "serial": "QM00001",
	    "resource_uri": "/MAAS/api/2.0/nodes/bhxws3/blockdevices/3/"
	}
    ],
    "interface_set": [
	{
	    "link_speed": 0,
	    "params": "",
	    "vendor": "Red Hat, Inc.",
	    "firmware_version": null,
	    "system_id": "bhxws3",
	    "enabled": true,
	    "type": "physical",
	    "links": [
		{
		    "id": 15,
		    "mode": "auto",
		    "subnet": {
			"name": "192.168.123.0/24",
			"description": "",
			"vlan": {
			    "vid": 0,
			    "mtu": 1500,
			    "dhcp_on": true,
			    "external_dhcp": null,
			    "relay_vlan": null,
			    "fabric": "fabric-2",
			    "id": 5003,
			    "secondary_rack": null,
			    "primary_rack": "8dwnne",
			    "name": "untagged",
			    "fabric_id": 2,
			    "space": "undefined",
			    "resource_uri": "/MAAS/api/2.0/vlans/5003/"
			},
			"cidr": "192.168.123.0/24",
			"rdns_mode": 2,
			"gateway_ip": null,
			"dns_servers": [],
			"allow_dns": true,
			"allow_proxy": true,
			"active_discovery": false,
			"managed": true,
			"id": 4,
			"space": "undefined",
			"resource_uri": "/MAAS/api/2.0/subnets/4/"
		    }
		}
	    ],
	    "id": 10,
	    "discovered": [
		{
		    "subnet": {
			"name": "192.168.123.0/24",
			"description": "",
			"vlan": {
			    "vid": 0,
			    "mtu": 1500,
			    "dhcp_on": true,
			    "external_dhcp": null,
			    "relay_vlan": null,
			    "fabric": "fabric-2",
			    "id": 5003,
			    "secondary_rack": null,
			    "primary_rack": "8dwnne",
			    "name": "untagged",
			    "fabric_id": 2,
			    "space": "undefined",
			    "resource_uri": "/MAAS/api/2.0/vlans/5003/"
			},
			"cidr": "192.168.123.0/24",
			"rdns_mode": 2,
			"gateway_ip": null,
			"dns_servers": [],
			"allow_dns": true,
			"allow_proxy": true,
			"active_discovery": false,
			"managed": true,
			"id": 4,
			"space": "undefined",
			"resource_uri": "/MAAS/api/2.0/subnets/4/"
		    },
		    "ip_address": "192.168.123.190"
		}
	    ],
	    "numa_node": 0,
	    "children": [],
	    "parents": [],
	    "link_connected": true,
	    "effective_mtu": 1500,
	    "tags": [],
	    "sriov_max_vf": 0,
	    "interface_speed": 0,
	    "name": "ens3",
	    "mac_address": "52:54:00:15:36:f2",
	    "product": null,
	    "vlan": {
		"vid": 0,
		"mtu": 1500,
		"dhcp_on": true,
		"external_dhcp": null,
		"relay_vlan": null,
		"fabric": "fabric-2",
		"id": 5003,
		"secondary_rack": null,
		"primary_rack": "8dwnne",
		"name": "untagged",
		"fabric_id": 2,
		"space": "undefined",
		"resource_uri": "/MAAS/api/2.0/vlans/5003/"
	    },
	    "resource_uri": "/MAAS/api/2.0/nodes/bhxws3/interfaces/10/"
	}
    ],
    "numanode_set": [
	{
	    "index": 0,
	    "memory": 985,
	    "cores": [
		0
	    ]
	}
    ],
    "min_hwe_kernel": "",
    "memory_test_status": -1,
    "power_type": "virsh",
    "power_state": "off",
    "status": 10,
    "testing_status_name": "Passed",
    "interface_test_status_name": "Unknown",
    "cache_sets": [],
    "constraints_by_type": {},
    "domain": {
	"authoritative": true,
	"ttl": null,
	"id": 0,
	"resource_record_count": 0,
	"name": "maas",
	"is_default": true,
	"resource_uri": "/MAAS/api/2.0/domains/0/"
    },
    "network_test_status": -1,
    "current_installation_result_id": null,
    "bios_boot_method": "pxe",
    "status_name": "Allocated",
    "address_ttl": null,
    "fqdn": "ace-swan.maas",
    "cpu_speed": 0,
    "hwe_kernel": null,
    "description": "",
    "commissioning_status_name": "Passed",
    "pod": null,
    "network_test_status_name": "Unknown",
    "hardware_uuid": "F677A842-571C-4E65-ADC9-11E2CF92D363",
    "commissioning_status": 2,
    "status_message": "Ready",
    "owner": "admin",
    "distro_series": "",
    "status_action": "",
    "testing_status": 2,
    "cpu_test_status": -1,
    "architecture": "amd64/generic",
    "netboot": true,
    "iscsiblockdevice_set": [],
    "disable_ipv4": false,
    "virtualblockdevice_set": [],
    "osystem": "",
    "boot_disk": {
	"firmware_version": "2.5+",
	"id_path": "/dev/disk/by-id/ata-QEMU_HARDDISK_QM00001",
	"system_id": "bhxws3",
	"partition_table_type": "GPT",
	"type": "physical",
	"block_size": 512,
	"id": 3,
	"numa_node": 0,
	"partitions": [
	    {
		"uuid": "8aa1164c-8a91-41d7-92e3-c411634355bb",
		"size": 5360320512,
		"bootable": false,
		"tags": [],
		"id": 3,
		"used_for": "ext4 formatted filesystem mounted at /",
		"device_id": 3,
		"system_id": "bhxws3",
		"path": "/dev/disk/by-dname/sda-part2",
		"type": "partition",
		"filesystem": {
		    "fstype": "ext4",
		    "label": "root",
		    "uuid": "68487852-7e38-4605-a84e-d787532fd443",
		    "mount_point": "/",
		    "mount_options": null
		},
		"resource_uri": "/MAAS/api/2.0/nodes/bhxws3/blockdevices/3/partition/3"
	    }
	],
	"filesystem": null,
	"available_size": 0,
	"size": 5368709120,
	"storage_pool": null,
	"model": "QEMU HARDDISK",
	"used_size": 5366611968,
	"tags": [
	    "ssd"
	],
	"used_for": "GPT partitioned with 1 partition",
	"uuid": null,
	"name": "sda",
	"path": "/dev/disk/by-dname/sda",
	"serial": "QM00001",
	"resource_uri": "/MAAS/api/2.0/nodes/bhxws3/blockdevices/3/"
    },
    "other_test_status_name": "Unknown",
    "resource_uri": "/MAAS/api/2.0/machines/bhxws3/"
}
```
</details>

*** Deploying a machine with the CLI

Finally, let's deploy that machine this way:

```nohighlight
maas admin machine deploy bhxws3
```

<details><summary>Success.</summary>

```nohighlight
Success.
Machine-readable output follows:
{
    "architecture": "amd64/generic",
    "cpu_speed": 0,
    "tag_names": [
	"virtual"
    ],
    "boot_interface": {
	"mac_address": "52:54:00:15:36:f2",
	"links": [
	    {
		"id": 15,
		"mode": "auto",
		"subnet": {
		    "name": "192.168.123.0/24",
		    "description": "",
		    "vlan": {
			"vid": 0,
			"mtu": 1500,
			"dhcp_on": true,
			"external_dhcp": null,
			"relay_vlan": null,
			"fabric_id": 2,
			"id": 5003,
			"fabric": "fabric-2",
			"secondary_rack": null,
			"name": "untagged",
			"space": "undefined",
			"primary_rack": "8dwnne",
			"resource_uri": "/MAAS/api/2.0/vlans/5003/"
		    },
		    "cidr": "192.168.123.0/24",
		    "rdns_mode": 2,
		    "gateway_ip": null,
		    "dns_servers": [],
		    "allow_dns": true,
		    "allow_proxy": true,
		    "active_discovery": false,
		    "managed": true,
		    "id": 4,
		    "space": "undefined",
		    "resource_uri": "/MAAS/api/2.0/subnets/4/"
		}
	    }
	],
	"numa_node": 0,
	"enabled": true,
	"params": "",
	"firmware_version": null,
	"sriov_max_vf": 0,
	"type": "physical",
	"children": [],
	"vendor": "Red Hat, Inc.",
	"system_id": "bhxws3",
	"parents": [],
	"vlan": {
	    "vid": 0,
	    "mtu": 1500,
	    "dhcp_on": true,
	    "external_dhcp": null,
	    "relay_vlan": null,
	    "fabric_id": 2,
	    "id": 5003,
	    "fabric": "fabric-2",
	    "secondary_rack": null,
	    "name": "untagged",
	    "space": "undefined",
	    "primary_rack": "8dwnne",
	    "resource_uri": "/MAAS/api/2.0/vlans/5003/"
	},
	"link_connected": true,
	"id": 10,
	"effective_mtu": 1500,
	"discovered": [
	    {
		"subnet": {
		    "name": "192.168.123.0/24",
		    "description": "",
		    "vlan": {
			"vid": 0,
			"mtu": 1500,
			"dhcp_on": true,
			"external_dhcp": null,
			"relay_vlan": null,
			"fabric_id": 2,
			"id": 5003,
			"fabric": "fabric-2",
			"secondary_rack": null,
			"name": "untagged",
			"space": "undefined",
			"primary_rack": "8dwnne",
			"resource_uri": "/MAAS/api/2.0/vlans/5003/"
		    },
		    "cidr": "192.168.123.0/24",
		    "rdns_mode": 2,
		    "gateway_ip": null,
		    "dns_servers": [],
		    "allow_dns": true,
		    "allow_proxy": true,
		    "active_discovery": false,
		    "managed": true,
		    "id": 4,
		    "space": "undefined",
		    "resource_uri": "/MAAS/api/2.0/subnets/4/"
		},
		"ip_address": "192.168.123.190"
	    }
	],
	"link_speed": 0,
	"name": "ens3",
	"product": null,
	"interface_speed": 0,
	"tags": [],
	"resource_uri": "/MAAS/api/2.0/nodes/bhxws3/interfaces/10/"
    },
    "ip_addresses": [
	"192.168.123.190"
    ],
    "testing_status_name": "Passed",
    "osystem": "ubuntu",
    "bcaches": [],
    "owner": "admin",
    "special_filesystems": [],
    "numanode_set": [
	{
	    "index": 0,
	    "memory": 985,
	    "cores": [
		0
	    ]
	}
    ],
    "node_type": 0,
    "cpu_test_status": -1,
    "storage_test_status_name": "Passed",
    "locked": false,
    "disable_ipv4": false,
    "status_message": "Deploying",
    "other_test_status_name": "Unknown",
    "interface_test_status_name": "Unknown",
    "status_name": "Deploying",
    "commissioning_status": 2,
    "hardware_uuid": "F677A842-571C-4E65-ADC9-11E2CF92D363",
    "fqdn": "ace-swan.maas",
    "min_hwe_kernel": "",
    "network_test_status": -1,
    "iscsiblockdevice_set": [],
    "current_testing_result_id": 9,
    "interface_test_status": -1,
    "status_action": "",
    "pool": {
	"name": "default",
	"description": "Default pool",
	"id": 0,
	"resource_uri": "/MAAS/api/2.0/resourcepool/0/"
    },
    "netboot": true,
    "distro_series": "bionic",
    "current_installation_result_id": 10,
    "memory_test_status_name": "Unknown",
    "cpu_count": 1,
    "hwe_kernel": "ga-18.04",
    "description": "",
    "current_commissioning_result_id": 8,
    "cpu_test_status_name": "Unknown",
    "storage_test_status": 2,
    "hardware_info": {
	"system_vendor": "QEMU",
	"system_product": "Standard PC (i440FX + PIIX, 1996)",
	"system_family": "Unknown",
	"system_version": "pc-i440fx-focal",
	"system_sku": "Unknown",
	"system_serial": "Unknown",
	"cpu_model": "Intel Core Processor (Skylake, IBRS)",
	"mainboard_vendor": "Unknown",
	"mainboard_product": "Unknown",
	"mainboard_serial": "Unknown",
	"mainboard_version": "Unknown",
	"mainboard_firmware_vendor": "SeaBIOS",
	"mainboard_firmware_date": "04/01/2014",
	"mainboard_firmware_version": "1.13.0-1ubuntu1",
	"chassis_vendor": "QEMU",
	"chassis_type": "Other",
	"chassis_serial": "Unknown",
	"chassis_version": "pc-i440fx-focal"
    },
    "bios_boot_method": "pxe",
    "storage": 5368.70912,
    "blockdevice_set": [
	{
	    "id_path": "/dev/disk/by-id/ata-QEMU_HARDDISK_QM00001",
	    "size": 5368709120,
	    "block_size": 512,
	    "tags": [
		"ssd"
	    ],
	    "numa_node": 0,
	    "partition_table_type": "GPT",
	    "storage_pool": null,
	    "type": "physical",
	    "filesystem": null,
	    "model": "QEMU HARDDISK",
	    "used_size": 5366611968,
	    "serial": "QM00001",
	    "system_id": "bhxws3",
	    "uuid": null,
	    "available_size": 0,
	    "path": "/dev/disk/by-dname/sda",
	    "id": 3,
	    "name": "sda",
	    "partitions": [
		{
		    "uuid": "8aa1164c-8a91-41d7-92e3-c411634355bb",
		    "size": 5360320512,
		    "bootable": false,
		    "tags": [],
		    "path": "/dev/disk/by-dname/sda-part2",
		    "device_id": 3,
		    "type": "partition",
		    "id": 3,
		    "system_id": "bhxws3",
		    "filesystem": {
			"fstype": "ext4",
			"label": "root",
			"uuid": "68487852-7e38-4605-a84e-d787532fd443",
			"mount_point": "/",
			"mount_options": null
		    },
		    "used_for": "ext4 formatted filesystem mounted at /",
		    "resource_uri": "/MAAS/api/2.0/nodes/bhxws3/blockdevices/3/partition/3"
		}
	    ],
	    "used_for": "GPT partitioned with 1 partition",
	    "resource_uri": "/MAAS/api/2.0/nodes/bhxws3/blockdevices/3/"
	}
    ],
    "system_id": "bhxws3",
    "boot_disk": {
	"firmware_version": "2.5+",
	"tags": [
	    "ssd"
	],
	"numa_node": 0,
	"partition_table_type": "GPT",
	"size": 5368709120,
	"storage_pool": null,
	"type": "physical",
	"block_size": 512,
	"filesystem": null,
	"model": "QEMU HARDDISK",
	"used_size": 5366611968,
	"serial": "QM00001",
	"system_id": "bhxws3",
	"uuid": null,
	"available_size": 0,
	"path": "/dev/disk/by-dname/sda",
	"id": 3,
	"id_path": "/dev/disk/by-id/ata-QEMU_HARDDISK_QM00001",
	"name": "sda",
	"partitions": [
	    {
		"uuid": "8aa1164c-8a91-41d7-92e3-c411634355bb",
		"size": 5360320512,
		"bootable": false,
		"tags": [],
		"path": "/dev/disk/by-dname/sda-part2",
		"device_id": 3,
		"type": "partition",
		"id": 3,
		"system_id": "bhxws3",
		"filesystem": {
		    "fstype": "ext4",
		    "label": "root",
		    "uuid": "68487852-7e38-4605-a84e-d787532fd443",
		    "mount_point": "/",
		    "mount_options": null
		},
		"used_for": "ext4 formatted filesystem mounted at /",
		"resource_uri": "/MAAS/api/2.0/nodes/bhxws3/blockdevices/3/partition/3"
	    }
	],
	"used_for": "GPT partitioned with 1 partition",
	"resource_uri": "/MAAS/api/2.0/nodes/bhxws3/blockdevices/3/"
    },
    "default_gateways": {
	"ipv4": {
	    "gateway_ip": null,
	    "link_id": null
	},
	"ipv6": {
	    "gateway_ip": null,
	    "link_id": null
	}
    },
    "raids": [],
    "cache_sets": [],
    "domain": {
	"authoritative": true,
	"ttl": null,
	"is_default": true,
	"id": 0,
	"name": "maas",
	"resource_record_count": 0,
	"resource_uri": "/MAAS/api/2.0/domains/0/"
    },
    "hostname": "ace-swan",
    "virtualblockdevice_set": [],
    "memory": 1024,
    "owner_data": {},
    "zone": {
	"name": "default",
	"description": "",
	"id": 1,
	"resource_uri": "/MAAS/api/2.0/zones/default/"
    },
    "power_state": "off",
    "status": 9,
    "address_ttl": null,
    "other_test_status": -1,
    "volume_groups": [],
    "power_type": "virsh",
    "pod": null,
    "testing_status": 2,
    "physicalblockdevice_set": [
	{
	    "firmware_version": "2.5+",
	    "tags": [
		"ssd"
	    ],
	    "numa_node": 0,
	    "partition_table_type": "GPT",
	    "size": 5368709120,
	    "storage_pool": null,
	    "type": "physical",
	    "block_size": 512,
	    "filesystem": null,
	    "model": "QEMU HARDDISK",
	    "used_size": 5366611968,
	    "serial": "QM00001",
	    "system_id": "bhxws3",
	    "uuid": null,
	    "available_size": 0,
	    "path": "/dev/disk/by-dname/sda",
	    "id": 3,
	    "id_path": "/dev/disk/by-id/ata-QEMU_HARDDISK_QM00001",
	    "name": "sda",
	    "partitions": [
		{
		    "uuid": "8aa1164c-8a91-41d7-92e3-c411634355bb",
		    "size": 5360320512,
		    "bootable": false,
		    "tags": [],
		    "path": "/dev/disk/by-dname/sda-part2",
		    "device_id": 3,
		    "type": "partition",
		    "id": 3,
		    "system_id": "bhxws3",
		    "filesystem": {
			"fstype": "ext4",
			"label": "root",
			"uuid": "68487852-7e38-4605-a84e-d787532fd443",
			"mount_point": "/",
			"mount_options": null
		    },
		    "used_for": "ext4 formatted filesystem mounted at /",
		    "resource_uri": "/MAAS/api/2.0/nodes/bhxws3/blockdevices/3/partition/3"
		}
	    ],
	    "used_for": "GPT partitioned with 1 partition",
	    "resource_uri": "/MAAS/api/2.0/nodes/bhxws3/blockdevices/3/"
	}
    ],
    "interface_set": [
	{
	    "mac_address": "52:54:00:15:36:f2",
	    "links": [
		{
		    "id": 15,
		    "mode": "auto",
		    "subnet": {
			"name": "192.168.123.0/24",
			"description": "",
			"vlan": {
			    "vid": 0,
			    "mtu": 1500,
			    "dhcp_on": true,
			    "external_dhcp": null,
			    "relay_vlan": null,
			    "fabric_id": 2,
			    "id": 5003,
			    "fabric": "fabric-2",
			    "secondary_rack": null,
			    "name": "untagged",
			    "space": "undefined",
			    "primary_rack": "8dwnne",
			    "resource_uri": "/MAAS/api/2.0/vlans/5003/"
			},
			"cidr": "192.168.123.0/24",
			"rdns_mode": 2,
			"gateway_ip": null,
			"dns_servers": [],
			"allow_dns": true,
			"allow_proxy": true,
			"active_discovery": false,
			"managed": true,
			"id": 4,
			"space": "undefined",
			"resource_uri": "/MAAS/api/2.0/subnets/4/"
		    }
		}
	    ],
	    "numa_node": 0,
	    "enabled": true,
	    "params": "",
	    "firmware_version": null,
	    "sriov_max_vf": 0,
	    "type": "physical",
	    "children": [],
	    "vendor": "Red Hat, Inc.",
	    "system_id": "bhxws3",
	    "parents": [],
	    "vlan": {
		"vid": 0,
		"mtu": 1500,
		"dhcp_on": true,
		"external_dhcp": null,
		"relay_vlan": null,
		"fabric_id": 2,
		"id": 5003,
		"fabric": "fabric-2",
		"secondary_rack": null,
		"name": "untagged",
		"space": "undefined",
		"primary_rack": "8dwnne",
		"resource_uri": "/MAAS/api/2.0/vlans/5003/"
	    },
	    "link_connected": true,
	    "id": 10,
	    "effective_mtu": 1500,
	    "discovered": [
		{
		    "subnet": {
			"name": "192.168.123.0/24",
			"description": "",
			"vlan": {
			    "vid": 0,
			    "mtu": 1500,
			    "dhcp_on": true,
			    "external_dhcp": null,
			    "relay_vlan": null,
			    "fabric_id": 2,
			    "id": 5003,
			    "fabric": "fabric-2",
			    "secondary_rack": null,
			    "name": "untagged",
			    "space": "undefined",
			    "primary_rack": "8dwnne",
			    "resource_uri": "/MAAS/api/2.0/vlans/5003/"
			},
			"cidr": "192.168.123.0/24",
			"rdns_mode": 2,
			"gateway_ip": null,
			"dns_servers": [],
			"allow_dns": true,
			"allow_proxy": true,
			"active_discovery": false,
			"managed": true,
			"id": 4,
			"space": "undefined",
			"resource_uri": "/MAAS/api/2.0/subnets/4/"
		    },
		    "ip_address": "192.168.123.190"
		}
	    ],
	    "link_speed": 0,
	    "name": "ens3",
	    "product": null,
	    "interface_speed": 0,
	    "tags": [],
	    "resource_uri": "/MAAS/api/2.0/nodes/bhxws3/interfaces/10/"
	}
    ],
    "node_type_name": "Machine",
    "commissioning_status_name": "Passed",
    "network_test_status_name": "Unknown",
    "memory_test_status": -1,
    "swap_size": null,
    "resource_uri": "/MAAS/api/2.0/machines/bhxws3/"
}
```
</details>

Okay then. We've installed and configured MAAS, started DHCP, created a machine, commissioned it, acquired it, and deployed it, with minimal hassle.  There's just one more useful thing to experience before diving into MAAS head-first.

** Log into running machines

What if we just set ourselves up to SSH into our MAAS machines?  Making this work will also allow us to `scp` files in -- and I'm sure you can see how we'd provision a machine from there.  We can also do the provisioning with MAAS, but that's a more complex topic for later.

First things first: we need to build the MAAS infrastructure necessary to play with this feature.

*** Create a VM host

We start by creating a vm-host.  Let's play dumb and walk our way through this; first, we'll just try creating a vm-host, like this:

```nohighlight
maas admin vm-host create
```

This doesn't give anything like the expected result:

```nohighlight
usage: maas admin vm-host [-h] COMMAND ...

Manage an individual vm-host.

optional arguments:
  -h, --help  show this help message and exit

drill down:
  COMMAND
    refresh   Refresh a pod
    parameters
	      Obtain pod parameters
    compose   Compose a pod machine
    add-tag   Add a tag to a pod
    remove-tag
	      Remove a tag from a pod
    read
    update    Update a specific pod
    delete    Deletes a pod

A vm-host is identified by its id.

argument COMMAND: invalid choice: 'create' (choose from 'refresh', 'parameters', 'compose', 'add-tag', 'remove-tag', 'read', 'update', 'delete')
```

We forgot about the collective pluralism of the MAAS CLI. We need to use `vm-hosts` to create one, because we're adding to the collection, so the correct command should look something like this:

```nohighlight
maas admin vm-hosts create
```

Still not quite what we expected, but we're failing forward fast, which is a great way to learn.  MAAS tells us we need to specify a `type`:

```nohighlight
{"type": ["This field is required."]}
```

We have to specify what kind of `vm-host` we want; in this case, it's going to be an LXD `vm-host`, so we modify our previous command like this:

```nohighlight
maas admin vm-hosts create type=lxd
```

Hmm, still one more thing to enter: the `power_address`:

```nohighlight
{"power_address": ["This field is required."]}
```

We need to update our command to tell MAAS what LXD instance we're going to use.  The `power_address` for an LXD vm-host is of the form `https://<gateway-ip-address>:8443`.  The `8443` is the default port when you ran `lxd init` to get LXD started, after installing it.  In my case, the LXD gateway is `10.38.31.1` at the moment, so my modified command would be:

```nohighlight
maas admin vm-hosts create type=lxd power_address=https://10.38.31.1:8443
```

Within seconds, we get a success message and JSON output.  From now on, I'll leave the JSON output for you to generate and view on your own, unless it bears specifically on the discussion.  In this case, all that we'll need is the last non-bracket line of the JSON return, which is:

```nohighlight
"resource_uri": "/MAAS/api/2.0/pods/7/"
```

Your address will vary, so check carefully.

What we'll need in the next step is the `vm-host ID`, which is the number on the end of the `resource_uri`. In this case that's "7".

*** Compose a machine

Having a VM host is great, but we can't demonstrate `ssh/scp` machine actions without a virtual machine running on that host.  Let's create one.  This may get tricky, so let's start by looking at the MAAS CLI help:

```nohighlight
maas admin --help
```

This gives us the following, very long command list:

```nohighlight
usage: maas admin [-h] COMMAND ...

Issue commands to the MAAS region controller at http://192.168.56.91:5240/MAAS/api/2.0/.

optional arguments:
  -h, --help            show this help message and exit

drill down:
  COMMAND
    account             Manage the current logged-in user.
    bcache-cache-set    Manage bcache cache set on a machine.
    bcache-cache-sets   Manage bcache cache sets on a machine.
    bcache              Manage bcache device on a machine.
    bcaches             Manage bcache devices on a machine.
    block-device        Manage a block device on a machine.
    block-devices       Manage block devices on a machine.
    boot-resource       Manage a boot resource.
    boot-resources      Manage the boot resources.
    boot-source         Manage a boot source.
    boot-source-selection
			Manage a boot source selection.
    boot-source-selections
			Manage the collection of boot source selections.
    boot-sources        Manage the collection of boot sources.
    commissioning-script
			Manage a custom commissioning script.
    commissioning-scripts
			Manage custom commissioning scripts.
    dhcpsnippet         Manage an individual DHCP snippet.
    dhcpsnippets        Manage the collection of all DHCP snippets in MAAS.
    dnsresource         Manage dnsresource.
    dnsresource-record  Manage dnsresourcerecord.
    dnsresource-records
			Manage DNS resource records (e.g. CNAME, MX, NS, SRV,
			TXT)
    dnsresources        Manage dnsresources.
    device              Manage an individual device.
    devices             Manage the collection of all the devices in the MAAS.
    discoveries         Query observed discoveries.
    discovery           Read or delete an observed discovery.
    domain              Manage domain.
    domains             Manage domains.
    events              Retrieve filtered node events.
    fabric              Manage fabric.
    fabrics             Manage fabrics.
    fan-network         Manage Fan Network.
    fan-networks        Manage Fan Networks.
    file                Manage a FileStorage object.
    files               Manage the collection of all the files in this MAAS.
    ipaddresses         Manage IP addresses allocated by MAAS.
    iprange             Manage IP range.
    ipranges            Manage IP ranges.
    interface           Manage a node's or device's interface.
    interfaces          Manage interfaces on a node.
    license-key         Manage a license key.
    license-keys        Manage the license keys.
    maas                Manage the MAAS server.
    machine             Manage an individual machine.
    machines            Manage the collection of all the machines in the MAAS.
    network             Manage a network.
    networks            Manage the networks.
    node                Manage an individual Node.
    node-results        Read the collection of commissioning script results.
    node-script         Manage or view a custom script.
    node-script-result  Manage node script results.
    node-script-results
			Manage node script results.
    node-scripts        Manage custom scripts.
    nodes               Manage the collection of all the nodes in the MAAS.
    notification        Manage an individual notification.
    notifications       Manage the collection of all the notifications in
			MAAS.
    package-repositories
			Manage the collection of all Package Repositories in
			MAAS.
    package-repository  Manage an individual package repository.
    partition           Manage partition on a block device.
    partitions          Manage partitions on a block device.
    pod                 Manage an individual pod.
    pods                Manage the collection of all the pod in the MAAS.
    rack-controller     Manage an individual rack controller.
    rack-controllers    Manage the collection of all rack controllers in MAAS.
    raid                Manage a specific RAID (Redundant Array of Independent
			Disks) on a machine.
    raids               Manage all RAIDs (Redundant Array of Independent
			Disks) on a machine.
    region-controller   Manage an individual region controller.
    region-controllers  Manage the collection of all region controllers in
			MAAS.
    resource-pool       Manage a resource pool.
    resource-pools      Manage resource pools.
    sshkey              Manage an SSH key.
    sshkeys             Manage the collection of all the SSH keys in this
			MAAS.
    sslkey              Manage an SSL key.
    sslkeys             Operations on multiple keys.
    space               Manage space.
    spaces              Manage spaces.
    static-route        Manage static route.
    static-routes       Manage static routes.
    subnet              Manage subnet.
    subnets             Manage subnets.
    tag                 Tags are properties that can be associated with a Node
			and serve as criteria for selecting and allocating
			nodes.
    tags                Manage all tags known to MAAS.
    user                Manage a user account.
    users               Manage the user accounts of this MAAS.
    version             Information about this MAAS instance.
    virtual-machine     Manage individual virtual machines.
    virtual-machines    Manage a collection of virtual machines.
    vlan                Manage a VLAN on a fabric.
    vlans               Manage VLANs on a fabric.
    vm-host             Manage an individual vm-host.
    vm-hosts            Manage the collection of all the vm-hosts in the MAAS.
    vmfs-datastore      Manage VMFS datastore on a machine.
    vmfs-datastores     Manage VMFS datastores on a machine.
    volume-group        Manage volume group on a machine.
    volume-groups       Manage volume groups on a machine.
    zone                Manage a physical zone.
    zones               Manage physical zones.

This is a profile.  Any commands you issue on this profile will
operate on the MAAS region server.

The command information you see here comes from the region server's
API; it may differ for different profiles.  If you believe the API may
have changed, use the command's 'refresh' sub-command to fetch the
latest version of this help information from the server.
```

We're looking to compose a machine here, so where would you look instinctively?  Well, the first thought might be `machines`, so we can give that help screen a try:

```nohighlight
maas admin machines --help
```

This produces a few commands:

```nohighlight
sage: maas admin machines [-h] COMMAND ...

Manage the collection of all the machines in the MAAS.

optional arguments:
  -h, --help            show this help message and exit

drill down:
  COMMAND
    is-registered       MAC address registered
    set-zone            Assign nodes to a zone
    power-parameters    Get power parameters
    accept              Accept declared machines
    accept-all          Accept all declared machines
    release             Release machines
    list-allocated      List allocated
    allocate            Allocate a machine
    add-chassis         Add special hardware
    clone               Clone storage and/or interface configurations
    read                List Nodes visible to the user
    create              Create a new machine
    is-action-in-progress
			MAC address of deploying or commissioning node
```

This list is interesting, but there isn't a specific `compose` command here.  We could go down the garden path with `maas admin machines create`, but first, let's see if the vm-host command has anything we're seeking:

```nohighlight
maas admin vm-host --help
```

Bingo. Found the command; do you see it in this list?

```nohighlight
usage: maas admin vm-host [-h] COMMAND ...

Manage an individual vm-host.

optional arguments:
  -h, --help  show this help message and exit

drill down:
  COMMAND
    refresh   Refresh a pod
    parameters
	      Obtain pod parameters
    compose   Compose a pod machine
    add-tag   Add a tag to a pod
    remove-tag
	      Remove a tag from a pod
    read
    update    Update a specific pod
    delete    Deletes a pod

A vm-host is identified by its id.
```

Okay, so `maas admin vm-host compose` is the root command; let's see what it requires:

```nohighlight
maas admin vm-host compose --help
```

This command is robust:

```nohighlight
usage: maas admin vm-host compose [--help] [-d] [-k] id [data [data ...]]

Compose a pod machine


Positional arguments:
	id


This method accepts keyword arguments.  Pass each argument as a
key-value pair with an equals sign between the key and the value:
key1=value1 key2=value key3=value3.  Keyword arguments must come after
any positional arguments.

Compose a new machine from a pod.

:param cores: Optional.  The minimum number of CPU cores.
:type cores: Int

 :param memory: Optional.  The minimum amount of memory,
specified in MiB (e.g. 2 MiB == 2*1024*1024).
:type memory: Int

 :param hugepages_backed: Optional.  Whether to request
hugepages backing for the machine.
:type hugepages_backed: Boolean

 :param pinned_cores: Optional.  List of host CPU cores
to pin the VM to. If this is passed, the "cores" parameter is ignored.
:type pinned_cores: Int

 :param cpu_speed: Optional.  The minimum CPU speed,
specified in MHz.
:type cpu_speed: Int

 :param architecture: Optional.  The architecture of
the new machine (e.g. amd64). This must be an architecture the pod
supports.
:type architecture: String

 :param storage: Optional.  A list of storage
constraint identifiers in the form ``label:size(tag,tag,...),
label:size(tag,tag,...)``. For more information please see the CLI
pod management page of the official MAAS documentation.
:type storage: String

 :param interfaces: Optional.  A
labeled constraint map associating constraint labels with desired
interface properties. MAAS will assign interfaces that match the
given interface properties.

Format: ``label:key=value,key=value,...``

Keys:

- ``id``: Matches an interface with the specific id
- ``fabric``: Matches an interface attached to the specified fabric.
- ``fabric_class``: Matches an interface attached to a fabric
  with the specified class.
- ``ip``: Matches an interface whose VLAN is on the subnet implied by
  the given IP address, and allocates the specified IP address for
  the machine on that interface (if it is available).
- ``mode``: Matches an interface with the specified mode. (Currently,
  the only supported mode is "unconfigured".)
- ``name``: Matches an interface with the specified name.
  (For example, "eth0".)
- ``hostname``: Matches an interface attached to the node with
  the specified hostname.
- ``subnet``: Matches an interface attached to the specified subnet.
- ``space``: Matches an interface attached to the specified space.
- ``subnet_cidr``: Matches an interface attached to the specified
  subnet CIDR. (For example, "192.168.0.0/24".)
- ``type``: Matches an interface of the specified type. (Valid
  types: "physical", "vlan", "bond", "bridge", or "unknown".)
- ``vlan``: Matches an interface on the specified VLAN.
- ``vid``: Matches an interface on a VLAN with the specified VID.
- ``tag``: Matches an interface tagged with the specified tag.
:type interfaces: String

 :param hostname: Optional.  The hostname of the newly
composed machine.
:type hostname: String

 :param domain: Optional.  The ID of the domain in which
to put the newly composed machine.
:type domain: Int

 :param zone: Optional.  The ID of the zone in which to
put the newly composed machine.
:type zone: Int

 :param pool: Optional.  The ID of the pool in which to
put the newly composed machine.
:type pool: Int


Common command-line options:
    --help, -h
	Show this help message and exit.
    -d, --debug
	Display more information about API responses.
    -k, --insecure
	Disable SSL certificate check
```

We could get fancy, but for these purposes, we just need a machine.  The only thing that's absolutely required is the vm-host ID.  Remember that line of JSON from above?  In this example, the ID is "7"; remember to check your own machine for the correct ID.  We'll enter this command:

```nohighlight
maas admin vm-host compose 7
```

We got some feedback with a machine `system_id`:

```nohighlight
Success.
Machine-readable output follows:
{
    "system_id": "xttpfx",
    "resource_uri": "/MAAS/api/2.0/machines/xttpfx/"
}
```

We can use this, along with some jq tricks, to see if this machine is commissioning (as expected):

```nohighlight
maas admin machines read | jq -r '(["HOSTNAME","SYSID",
"POWER","STATUS","OWNER", "TAGS", "POOL","VLAN","FABRIC",
"SUBNET"] | (., map(length*"-"))),(.[] | [.hostname, .system_id, 
.power_state, .status_name, .owner // "-",.tag_names[0] // "-", 
.pool.name,.boot_interface.vlan.name,.boot_interface.vlan.fabric,
.boot_interface.links[0].subnet.name]) | @tsv' | column -t
```

This gives the following output on my machine:

```nohighlight
HOSTNAME    SYSID   POWER  STATUS   OWNER  TAGS     POOL     VLAN      FABRIC    SUBNET
--------    -----   -----  ------   -----  ----     ----     ----      ------    ------
native-cub  xttpfx  on     Testing  admin  virtual  default  untagged  fabric-1  10.38.31.0/24
```

By the time I got this command typed in, commissioning had already nearly finished, and the machine was in the "testing" phase.  If we run this command again now, we should see that it's in the "Ready" state:

```nohighlight
HOSTNAME    SYSID   POWER  STATUS  OWNER  TAGS     POOL     VLAN      FABRIC    SUBNET
--------    -----   -----  ------  -----  ----     ----     ----      ------    ------
native-cub  xttpfx  off    Ready   -      virtual  default  untagged  fabric-1  10.38.31.0/24
```

*** Get the machine to a login state

We can't SSH into it, because it automatically turned off after commissioning, and anyway, we didn't have a chance to ask for SSH keys to be loaded during the commissioning process.  Let's run that commissioning again, with SSH keys enabled, and making sure that it's left on after it's done.  For this operation, we just use the standard `machine` commands, because the `vm-host` is now hosting a MAAS machine:

```nohighlight
maas admin machine commission xttpfx enable_ssh=1
```

This will return a success message (be sure to substitute the "xttpfx" with whatever your composed machine `system_id` turns out to be; remember, your mileage may vary).  After a little while, the machine should return to a "Ready" state again, but this time, with the power left /on/, and with SSH keys passed to the machine, so that we can login to it.

We can check this again, with the above jq command:

```nohighlight
HOSTNAME    SYSID   POWER  STATUS  OWNER  TAGS     POOL     VLAN      FABRIC    SUBNET
--------    -----   -----  ------  -----  ----     ----     ----      ------    ------
native-cub  xttpfx  on     Ready   -      virtual  default  untagged  fabric-1  10.38.31.0/24
```

*** Logging into a commissioned machine

So it's "Ready" and it's powered on, that's good.  In order to log in, we'll need to know the machine's IP address.  There are several ways to get this, but by far the easiest (with LXD VMs) is just using the `lxc` command:

```nohighlight
lxc list
```

This will give us the following output:

```nohighlight
+------------+---------+---------------------+-----------------------------------------------+-----------------+-----------+
|    NAME    |  STATE  |        IPV4         |                     IPV6                      |      TYPE       | SNAPSHOTS |
+------------+---------+---------------------+-----------------------------------------------+-----------------+-----------+
| first-one  | RUNNING | 10.38.31.193 (eth0) |                                               | CONTAINER       | 0         |
+------------+---------+---------------------+-----------------------------------------------+-----------------+-----------+
| native-cub | RUNNING | 10.38.31.202 (eth0) | fd42:fd4c:6ab9:19bc:216:3eff:fe9e:bc7b (eth0) | VIRTUAL-MACHINE | 0         |
+------------+---------+---------------------+-----------------------------------------------+-----------------+-----------+
```

This brings up some important nuances about the LXD list.  Note that there are two machines, one of which is a `CONTAINER`, not useful for this tutorial.  The other, "native-cub," is the `VIRTUAL-MACHINE` we just created, and that's the one whose IP address we want for SSH purposes: `10.38.31.202`.

Okay, so now we can try logging in via SSH, using the "ubuntu" user (always):

```nohighlight
ssh ubuntu@10.38.31.202
```

We get the expected first-login response:

```nohighlight
The authenticity of host '10.38.31.202 (10.38.31.202)' can't be established.
ECDSA key fingerprint is SHA256:hkKRDyRDG9JcsSmAQ0ir5jy0UKQ+PrU/FTJr36U3bvw.
Are you sure you want to continue connecting (yes/no/[fingerprint])? 
```

And if we say "yes," we should get this result:

```nohighlight
Warning: Permanently added '10.38.31.202' (ECDSA) to the list of known hosts.
Welcome to Ubuntu 20.04.1 LTS (GNU/Linux 5.4.0-64-generic x86_64)

 * Documentation:  https://help.ubuntu.com
 * Management:     https://landscape.canonical.com
 * Support:        https://ubuntu.com/advantage

  System information as of Mon Feb  1 00:11:52 UTC 2021

  System load:    0.0       Processes:               127
  Usage of /home: unknown   Users logged in:         0
  Memory usage:   10%       IPv4 address for enp5s0: 10.38.31.202
  Swap usage:     0%

14 updates can be installed immediately.
2 of these updates are security updates.
To see these additional updates run: apt list --upgradable

tmpfs-root /media/root-rw tmpfs rw,relatime 0 0
overlayroot / overlay rw,relatime,lowerdir=/media/root-ro,upperdir=/media/root-rw/overlay,workdir=/media/root-rw/overlay-workdir/_ 0 0
/dev/loop0 /media/root-ro squashfs ro,relatime 0 0


The programs included with the Ubuntu system are free software;
the exact distribution terms for each program are described in the
individual files in /usr/share/doc/*/copyright.

Ubuntu comes with ABSOLUTELY NO WARRANTY, to the extent permitted by
applicable law.

To run a command as administrator (user "root"), use "sudo &lt;command&gt;".
See "man sudo_root" for details.

ubuntu@native-cub:`$ 
```

*** Using SCP

We can jump out of this machine and use its IP address to copy files over to it.  First, let's make sure that there isn't anything in the local directory on the machine:

```nohighlight
ls
```

And we get what we'd expect:

```nohighlight
ubuntu@native-cub:`$ ls
ubuntu@native-cub:`$ 
```

So now, let's exit the machine with `exit`, and just `touch` a file called "zork" (a very uncommon filename) in the CWD on the local machine:

```nohighlight
ubuntu@native-cub:`$ exit
logout
Connection to 10.38.31.202 closed.
stormrider@wintermute:`$ touch zork
stormrider@wintermute:`$ ls
 api-key-file   Credentials   Dropbox   Pictures      snap            Templates
 Backups        Desktop       git       Public        stormrider.io   Videos
 BRF            Documents     mnt      '#scratch#'    temp            Websites
 Code           Downloads     Music     Show-n-Tell   temp`           zork
stormrider@wintermute:`$ 
```

Now, let's try to `scp` (secure copy) the file over to the machine, login, and see if the file made it:

```nohighlight
stormrider@wintermute:`$ scp ./zork ubuntu@10.38.31.202:
zork                                          100%    0     0.0KB/s   00:00    
stormrider@wintermute:`$ ssh ubuntu@10.38.31.202
Welcome to Ubuntu 20.04.1 LTS (GNU/Linux 5.4.0-64-generic x86_64)

 * Documentation:  https://help.ubuntu.com
 * Management:     https://landscape.canonical.com
 * Support:        https://ubuntu.com/advantage

  System information as of Mon Feb  1 00:20:42 UTC 2021

  System load:    0.0       Processes:               123
  Usage of /home: unknown   Users logged in:         0
  Memory usage:   10%       IPv4 address for enp5s0: 10.38.31.202
  Swap usage:     0%


14 updates can be installed immediately.
2 of these updates are security updates.
To see these additional updates run: apt list --upgradable

tmpfs-root /media/root-rw tmpfs rw,relatime 0 0
overlayroot / overlay rw,relatime,lowerdir=/media/root-ro,upperdir=/media/root-rw/overlay,workdir=/media/root-rw/overlay-workdir/_ 0 0
/dev/loop0 /media/root-ro squashfs ro,relatime 0 0

Last login: Mon Feb  1 00:19:34 2021 from 10.38.31.1
To run a command as administrator (user "root"), use "sudo &lt;command&gt;".
See "man sudo_root" for details.

ubuntu@native-cub:`$ ls
zork
ubuntu@native-cub:`$ 
```

Good, we can copy files to a MAAS-managed machine.

*** Copying files to a deployed machine

Copying files to a commissioned machine doesn't do us much good, of course, since the machine gets wiped out and reloaded on deployment.  Let's acquire and deploy that same machine, and then try logging in and copying files again.

First, we have to acquire and deploy the machine:

```nohighlight
maas admin machines allocate system_id=xttpfx
(Success message and JSON data stream)

maas admin machine deploy xttpfx
(Success message and JSON data stream)

maas admin machines read | jq -r '(["HOSTNAME","SYSID", 
"POWER","STATUS","OWNER", "TAGS", "POOL","VLAN","FABRIC",
"SUBNET"] | (., map(length*"-"))),(.[] | [.hostname, .system_id, 
.power_state, .status_name, .owner // "-",.tag_names[0] // "-", 
.pool.name,.boot_interface.vlan.name,.boot_interface.vlan.fabric,
.boot_interface.links[0].subnet.name]) | @tsv' | column -t

HOSTNAME    SYSID   POWER  STATUS     OWNER  TAGS     POOL     VLAN      FABRIC    SUBNET
--------    -----   -----  ------     -----  ----     ----     ----      ------    ------
native-cub  xttpfx  on     Deploying  admin  virtual  default  untagged  fabric-1  10.38.31.0/24
```

When it finally reaches the "Deployed" state, we can try and log into it:

```nohighlight
stormrider@wintermute:`$ ssh ubuntu@10.38.31.2
@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@
@    WARNING: REMOTE HOST IDENTIFICATION HAS CHANGED!     @
@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@
IT IS POSSIBLE THAT SOMEONE IS DOING SOMETHING NASTY!
Someone could be eavesdropping on you right now (man-in-the-middle attack)!
It is also possible that a host key has just been changed.
The fingerprint for the ECDSA key sent by the remote host is
SHA256:AsOdI357mZdmymQG/bmZzbtrDwZPKNYwdUDgCecHHhI.
Please contact your system administrator.
Add correct host key in /home/stormrider/.ssh/known_hosts to get rid of this message.
Offending ECDSA key in /home/stormrider/.ssh/known_hosts:20
  remove with:
  ssh-keygen -f "/home/stormrider/.ssh/known_hosts" -R "10.38.31.2"
ECDSA host key for 10.38.31.2 has changed and you have requested strict checking.
Host key verification failed.
```

No surprise.  On deployment, the SSH key just got updated, so just do what the message suggests, and you can SSH in normally:

```nohighlight
stormrider@wintermute:`$ ssh-keygen -f "/home/stormrider/.ssh/known_hosts" -R "10.38.31.2"
# Host 10.38.31.2 found: line 20
/home/stormrider/.ssh/known_hosts updated.
Original contents retained as /home/stormrider/.ssh/known_hosts.old
stormrider@wintermute:`$ ssh ubuntu@10.38.31.2
The authenticity of host '10.38.31.2 (10.38.31.2)' can't be established.
ECDSA key fingerprint is SHA256:AsOdI357mZdmymQG/bmZzbtrDwZPKNYwdUDgCecHHhI.
Are you sure you want to continue connecting (yes/no/[fingerprint])? yes
Warning: Permanently added '10.38.31.2' (ECDSA) to the list of known hosts.
Welcome to Ubuntu 20.04.1 LTS (GNU/Linux 5.4.0-65-generic x86_64)

 * Documentation:  https://help.ubuntu.com
 * Management:     https://landscape.canonical.com
 * Support:        https://ubuntu.com/advantage

  System information as of Mon Feb  1 00:34:27 UTC 2021

  System load:  0.08              Processes:               133
  Usage of /:   48.2% of 6.78GB   Users logged in:         0
  Memory usage: 10%               IPv4 address for enp5s0: 10.38.31.2
  Swap usage:   0%

14 updates can be installed immediately.
2 of these updates are security updates.
To see these additional updates run: apt list --upgradable



The programs included with the Ubuntu system are free software;
the exact distribution terms for each program are described in the
individual files in /usr/share/doc/*/copyright.

Ubuntu comes with ABSOLUTELY NO WARRANTY, to the extent permitted by
applicable law.

To run a command as administrator (user "root"), use "sudo &lt;command&gt;".
See "man sudo_root" for details.

ubuntu@native-cub:`$ 
```

*** Copying a script to a machine and running it

So first, let's verify that the script we want to copy over there isn't /already/ there.  In fact, to keep it simple, let's just create a simple and fun script to see what `scp` can get us.  First, we'll need to install a couple of software packages on the deployed machine:

```nohighlight
ubuntu@native-cub:`$ fortune

Command 'fortune' not found, but can be installed with:

sudo apt install fortune-mod

ubuntu@native-cub:`$ sudo apt install fortune-mod
Reading package lists... Done
Building dependency tree       
Reading state information... Done
The following additional packages will be installed:
  fortunes-min librecode0
Suggested packages:
  fortunes x11-utils
The following NEW packages will be installed:
  fortune-mod fortunes-min librecode0
0 upgraded, 3 newly installed, 0 to remove and 17 not upgraded.
Need to get 615 kB of archives.
After this operation, 2135 kB of additional disk space will be used.
Do you want to continue? [Y/n] Y
Get:1 http://archive.ubuntu.com/ubuntu focal/main amd64 librecode0 amd64 3.6-24 [523 kB]
Get:2 http://archive.ubuntu.com/ubuntu focal/universe amd64 fortune-mod amd64 1:1.99.1-7build1 [37.3 kB]
Get:3 http://archive.ubuntu.com/ubuntu focal/universe amd64 fortunes-min all 1:1.99.1-7build1 [55.1 kB]
Fetched 615 kB in 3s (203 kB/s)    
Selecting previously unselected package librecode0:amd64.
(Reading database ... 71387 files and directories currently installed.)
Preparing to unpack .../librecode0_3.6-24_amd64.deb ...
Unpacking librecode0:amd64 (3.6-24) ...
Selecting previously unselected package fortune-mod.
Preparing to unpack .../fortune-mod_1%3a1.99.1-7build1_amd64.deb ...
Unpacking fortune-mod (1:1.99.1-7build1) ...
Selecting previously unselected package fortunes-min.
Preparing to unpack .../fortunes-min_1%3a1.99.1-7build1_all.deb ...
Unpacking fortunes-min (1:1.99.1-7build1) ...
Setting up librecode0:amd64 (3.6-24) ...
Setting up fortunes-min (1:1.99.1-7build1) ...
Setting up fortune-mod (1:1.99.1-7build1) ...
Processing triggers for man-db (2.9.1-1) ...
Processing triggers for libc-bin (2.31-0ubuntu9.1) ...
ubuntu@native-cub:`$ ddate

Command 'ddate' not found, but can be installed with:

sudo apt install ddate

ubuntu@native-cub:`$ sudo apt install ddate
Reading package lists... Done
Building dependency tree       
Reading state information... Done
The following NEW packages will be installed:
  ddate
0 upgraded, 1 newly installed, 0 to remove and 17 not upgraded.
Need to get 10.8 kB of archives.
After this operation, 34.8 kB of additional disk space will be used.
Get:1 http://archive.ubuntu.com/ubuntu focal/universe amd64 ddate amd64 0.2.2-1build1 [10.8 kB]
Fetched 10.8 kB in 1s (20.0 kB/s)
Selecting previously unselected package ddate.
(Reading database ... 71424 files and directories currently installed.)
Preparing to unpack .../ddate_0.2.2-1build1_amd64.deb ...
Unpacking ddate (0.2.2-1build1) ...
Setting up ddate (0.2.2-1build1) ...
Processing triggers for man-db (2.9.1-1) ...
ubuntu@native-cub:`$ cowsay

Command 'cowsay' not found, but can be installed with:

sudo apt install cowsay

ubuntu@native-cub:`$ sudo apt install cowsay
Reading package lists... Done
Building dependency tree       
Reading state information... Done
Suggested packages:
  filters cowsay-off
The following NEW packages will be installed:
  cowsay
0 upgraded, 1 newly installed, 0 to remove and 17 not upgraded.
Need to get 18.5 kB of archives.
After this operation, 93.2 kB of additional disk space will be used.
Get:1 http://archive.ubuntu.com/ubuntu focal/universe amd64 cowsay all 3.03+dfsg2-7 [18.5 kB]
Fetched 18.5 kB in 2s (7603 B/s) 
Selecting previously unselected package cowsay.
(Reading database ... 71431 files and directories currently installed.)
Preparing to unpack .../cowsay_3.03+dfsg2-7_all.deb ...
Unpacking cowsay (3.03+dfsg2-7) ...
Setting up cowsay (3.03+dfsg2-7) ...
Processing triggers for man-db (2.9.1-1) ...
ubuntu@native-cub:`$ 
```

Now we can drop back and write a script that uses these three packages to produce an interesting result.  Here's what should be in the script:

```nohighlight
#!/bin/bash
ddate &gt; /tmp/foo
echo '   ' &gt;&gt; /tmp/foo
fortune -s &gt;&gt; /tmp/foo
cat /tmp/foo | cowsay
```

Add the text above to a script called `motd.sh`, and then `chmod 777 motd.sh`.  Then, use the following command to copy the script to the deployed machine:

```nohighlight
scp ./motd.sh ubuntu@10.38.31.2:
```

Then we can log back into the deployed machine and check the permissions on `motd.sh` in the arriving CWD:

```nohighlight
ssh ubuntu@10.38.31.2
...
ls -lsa motd.sh

ubuntu@native-cub:`$ ls -lsa motd.sh
4 -rwxrwxr-x 1 ubuntu ubuntu 97 Feb  1 00:49 motd.sh
ubuntu@native-cub:`$ 
```

On my machine, it didn't copy the permissions precisely, but it is executable by me, so I can run it and get the highly-important output:

```nohighlight
 ________________________________________
/ Today is Boomtime, the 32nd day of     \
| Chaos in the YOLD 3187                 |
|                                        |
| Water, taken in moderation cannot hurt |
| anybody.                               |
|                                        |
\ -- Mark Twain                          /
 ----------------------------------------
	\   ^__^
	 \  (oo)\_______
	    (__)\       )\/\
		||----w |
		||     ||
```

** Summary

As you see, it's not that difficult to install MAAS, deploy a machine, and then load usable software on it. Now you can go through the MAAS documenation to learn more about what you just did.
* Tutorials

Here are some to tutorials to help you build confidence using MAAS:

- [Bootstrap MAAS](/t/bootstrap-maas/5092)
- [Try out the MAAS CLI](/t/try-out-the-maas-cli/5236)
- [Create a custom image](/t/create-a-custom-image/6102)
- [Get fancy CLI output](/t/get-fancy-cli-output/6027)

* Understanding MAAS events

Events are state changes that happen to MAAS elements, such as controllers, networks, or machines.  These state changes can be caused by MAAS itself, some external agent (such as an external DHCP server), or by users (such as when commissioning a machine).  Being able to review events is often essential to debugging or verifying your MAAS system.

Events can be seen in the MAAS logs, in the UI event log, and in output from the CLI `events query` command.  These three sources provide analogous (but somewhat different information). For example, consider the following log listing, obtained by doing a `grep "fun-zebra" *.log | grep "transition from"` in the MAAS log directory:

```nohighlight
maas.log:2022-09-29T15:04:07.795515-05:00 neuromancer maas.node: [info] fun-zebra: Status transition from COMMISSIONING to TESTING
maas.log:2022-09-29T15:04:17.288763-05:00 neuromancer maas.node: [info] fun-zebra: Status transition from TESTING to READY
```

This information appears this way when events are queried from the CLI:

```nohighlight
        {
            "username": "unknown",
            "node": "bk7mg8",
            "hostname": "fun-zebra",
            "id": 170,
            "level": "INFO",
            "created": "Thu, 29 Sep. 2022 20:04:17",
            "type": "Ready",
            "description": ""
        },
        {
            "username": "unknown",
            "node": "bk7mg8",
            "hostname": "fun-zebra",
            "id": 167,
            "level": "INFO",
            "created": "Thu, 29 Sep. 2022 20:04:07",
            "type": "Running test",
            "description": "smartctl-validate on sda"
        },
```

And it appears like this in the UI events log:

| Time	| Event |
|---|---|
|**Thu, 29 Sep. 2022 20:04:17**	|**Node changed status - From 'Testing' to 'Ready'** |
|Thu, 29 Sep. 2022 20:04:17	|Ready |
|Thu, 29 Sep. 2022 20:04:17	|Script result - smartctl-validate on sda changed status from 'Running' to 'Skipped' |
|Thu, 29 Sep. 2022 20:04:16	|Script result - smartctl-validate on sda changed status from 'Installing dependencies' to 'Running' |
|Thu, 29 Sep. 2022 20:04:07	|Running test - smartctl-validate on sda |
|**Thu, 29 Sep. 2022 20:04:07**	|**Node changed status - From 'Commissioning' to 'Testing'** |

You can see that all three outputs are sources of truth, but the messages are somewhat different, include different information, and contain different levels of detail.

** Using the logs directly

By the way, if you're interested in reading the logs, and you're using snaps, you'll find what you need here:

- /var/snap/maas/common/log/maas.log
- /var/snap/maas/common/log/regiond.log
- /var/snap/maas/common/log/rackd.log
- /var/snap/maas/common/log/rsyslog/$MACHINE_NAME/$RELEVANT_DATE/messages

If you’re using packages, you’ll find the log files in these locations:

- /var/log/maas/maas.log
- /var/log/maas/regiond.log
- /var/log/maas/rackd.log
- /var/log/maas/rsyslog/$MACHINE_NAME/$RELEVANT_DATE/messages

These logs can be very large and hard to search, and the web UI does not separate events by type. For instance, commissioning a simple VM produces logging information like this:

```nohighlight
maas.log:2022-09-29T15:00:16.461402-05:00 neuromancer maas.interface: [info] eth0 (physical) on fun-zebra: IP address automatically unlinked: None:type=AUTO
maas.log:2022-09-29T15:00:16.553396-05:00 neuromancer maas.node: [info] fun-zebra: Status transition from NEW to COMMISSIONING
maas.log:2022-09-29T15:00:16.754265-05:00 neuromancer maas.power: [info] Changing power state (on) of node: fun-zebra (bk7mg8)
maas.log:2022-09-29T15:00:16.759676-05:00 neuromancer maas.node: [info] fun-zebra: Commissioning started
maas.log:2022-09-29T15:00:18.039441-05:00 neuromancer maas.power: [info] Changed power state (on) of node: fun-zebra (bk7mg8)
maas.log:2022-09-29T15:03:26.946507-05:00 neuromancer maas.node: [info] fun-zebra: Storage layout was set to flat.
maas.log:2022-09-29T15:04:07.795515-05:00 neuromancer maas.node: [info] fun-zebra: Status transition from COMMISSIONING to TESTING
maas.log:2022-09-29T15:04:17.288763-05:00 neuromancer maas.node: [info] fun-zebra: Status transition from TESTING to READY
regiond.log:2022-09-29 20:00:16 maasserver.models.signals.interfaces: [info] eth0 (physical) on fun-zebra: deleted IP addresses due to VLAN update (5001 -> 5002).
regiond.log:2022-09-29 20:00:32 maasserver.region_controller: [info] Reloaded DNS configuration; ip 10.103.114.192 connected to fun-zebra on eth0
regiond.log:2022-09-29 20:01:09 maasserver.rpc.leases: [info] Lease update: commit for 10.103.114.192 on 0:16:3e:a2:73:5c at 2022-09-29 20:01:09 (lease time: 600s) (hostname: fun-zebra)
regiond.log:2022-09-29 20:01:14 maasserver.region_controller: [info] Reloaded DNS configuration; ip 10.103.114.192 connected to fun-zebra on eth0
regiond.log:     * node fun-zebra renamed interface eth0 to enp5s0
regiond.log:     * ip 10.103.114.192 connected to fun-zebra on eth0
regiond.log:     * ip 10.103.114.192 disconnected from fun-zebra on eth0
```

Not all of this output is relevant, nor does it all trigger a recorded MAAS event.  Interpreting MAAS logs is a matter of practice with known events in a controlled environment.

** MAAS CLI events query command

In fact, probably the best way to review events is via the CLI sub-command, `events query`. This sub-command can help you filter and summarise events.  Let's take a look at how this tool works.

*** Basic queries

MAAS events can be queried with the simple CLI command:

```nohighlight
maas $PROFILE events query
```

where `$PROFILE` is your login name for your MAAS CLI.  This command produces a very long JSON listing, something like this:

```nohighlight
Success.
Machine-readable output follows:
{
    "count": 100,
    "events": [
        {
            "username": "unknown",
            "node": "ebd7dc",
            "hostname": "new-name",
            "id": 588448,
            "level": "WARNING",
            "created": "Tue, 27 Sep. 2022 20:49:37",
            "type": "Failed to query node's BMC",
            "description": "Failed to login to virsh console."
        },
        {
            "username": "unknown",
            "node": "mm3tc8",
            "hostname": "fair-marten",
            "id": 588447,
            "level": "WARNING",
            "created": "Tue, 27 Sep. 2022 20:49:37",
            "type": "Failed to query node's BMC",
            "description": "Failed to login to virsh console."
        },
		[... goes on for 100 events, by default ...]
        {
            "username": "unknown",
            "node": "ebd7dc",
            "hostname": "new-name",
            "id": 588442,
            "level": "WARNING",
            "created": "Tue, 27 Sep. 2022 20:39:22",
            "type": "Failed to query node's BMC",
            "description": "Failed to login to virsh console."
        }
    ],
    "next_uri": "/MAAS/api/2.0/events/?op=query&limit=5&after=588448",
    "prev_uri": "/MAAS/api/2.0/events/?op=query&limit=5&before=588442"
}
```

These listings can be very long and very hard to read.  You'll also notice that this particular MAAS has over 500,000 events, so parsing these logs by hand is certainly not practical.  There are two things you should do to make events easier to interpret:

- use the `jq` command, with some invocations we'll give you, to make neat tables out of your event lists.

- use the various filters -- supplied as part of the `events query` command -- to limit your output.

Let's explore both of these things in turn.

*** Using jq with events

We offer a [more complete tutorial on jq](/t/using-jq-with-the-maas-cli/6027) in this documentation set, but for now, we can give you some invocations that will make events much easier to read.  Let's take our example command above and add some `jq` to it to make the output more readable:

```nohighlight
maas $PROFILE events query limit=20 \
| jq -r '(["USERNAME","NODE","HOSTNAME","LEVEL","DATE","TYPE","EVENT"] | 
(., map(length*"-"))),
(.events[] | [.username,.node,.hostname,.level,.created,.type,.description]) 
| @tsv' | column -t -s$'\t'
```

This might give us output something like this:

```nohighlight
USERNAME  NODE    HOSTNAME       LEVEL    DATE                        TYPE                        EVENT
--------  ----    --------       -----    ----                        ----                        -----
unknown   ebd7dc  new-name       WARNING  Thu, 10 Mar. 2022 21:59:52  Failed to query node's BMC  Failed to login to virsh console.
unknown   mm3tc8  fair-marten    WARNING  Thu, 10 Mar. 2022 21:59:52  Failed to query node's BMC  Failed to login to virsh console.
unknown   pbpncx  ruling-bobcat  WARNING  Thu, 10 Mar. 2022 21:59:22  Failed to query node's BMC  Pod pbpncx: Failed to connect to the LXD REST API.
unknown   ebd7dc  new-name       WARNING  Thu, 10 Mar. 2022 21:54:34  Failed to query node's BMC  Failed to login to virsh console.
unknown   mm3tc8  fair-marten    WARNING  Thu, 10 Mar. 2022 21:54:34  Failed to query node's BMC  Failed to login to virsh console.
unknown   pbpncx  ruling-bobcat  WARNING  Thu, 10 Mar. 2022 21:54:05  Failed to query node's BMC  Pod pbpncx: Failed to connect to the LXD REST API.
unknown   ebd7dc  new-name       WARNING  Thu, 10 Mar. 2022 21:49:21  Failed to query node's BMC  Failed to login to virsh console.
unknown   mm3tc8  fair-marten    WARNING  Thu, 10 Mar. 2022 21:49:19  Failed to query node's BMC  Failed to login to virsh console.
unknown   pbpncx  ruling-bobcat  WARNING  Thu, 10 Mar. 2022 21:48:49  Failed to query node's BMC  Pod pbpncx: Failed to connect to the LXD REST API.
unknown   mm3tc8  fair-marten    WARNING  Thu, 10 Mar. 2022 21:44:08  Failed to query node's BMC  Failed to login to virsh console.
admin     ebd7dc  new-name       AUDIT    Thu, 09 Jun. 2022 21:39:54  Node                        Tagging 'new-name'.
unknown   pbpncx  contr-105      ERROR    Rack import error           Unable to import boot images: ('Connection broken: IncompleteRead(4096 bytes read)', IncompleteRead(4096 bytes read))
unknown   mm3tc8  fair-marten    WARNING  Thu, 10 Mar. 2022 21:38:50  Failed to query node's BMC  Failed to login to virsh console.
unknown   ebd7dc  contr-105      DEBUG    Thu, 12 May. 2022 21:38:26  Rack import info            Starting rack boot image import
unknown   pbpncx  ruling-bobcat  WARNING  Thu, 10 Mar. 2022 21:38:21  Failed to query node's BMC  Pod pbpncx: Failed to connect to the LXD REST API.
admin     pbpncx  ruling-bobcat  AUDIT    Thu, 16 Jun. 2022 21:35:16  Node                        Started commissioning on 'ruling-bobcat'.
unknown   mm3tc8  fair-marten    WARNING  Thu, 10 Mar. 2022 21:33:44  Failed to query node's BMC  Failed to login to virsh console.
unknown   pbpncx  ruling-bobcat  WARNING  Thu, 10 Mar. 2022 21:33:16  Failed to query node's BMC  Pod pbpncx: Failed to connect to the LXD REST API.
unknown   knpge8  bolla          INFO     Thu, 10 Mar. 2022 20:21:41  Ready                         
unknown   pbpncx  ruling-bobcat  WARNING  Thu, 10 Mar. 2022 18:01:47  Failed to query node's BMC  <LXDAPIException instance at 0x7f0b53e21dc0 with str error:\n Traceback (most recent call last):\n  File "/snap/maas/19076/usr/lib/python3/dist-packages/twisted/python/reflect.py", line 448, in safe_str\n    return str(o)\n  File "/snap/maas/19076/usr/lib/python3/dist-packages/pylxd/exceptions.py", line 18, in __str__\n    if self.response.status_code == 200:  # Operation failure\nAttributeError: 'LXDAPIException' object has no attribute 'status_code'\n>
```

You'll notice, in this listing, we have a mix of event types and responses.  In one case, the log even recorded a code exception.  You can probably see from this listing that events can be very helpful in tracking behaviours and resolving issues with your MAAS instance.  Even limited to 20 records, though, this output is still hard to parse, so let's explore ways to filter this table.

*** Filter parameters

The `events query` command accepts several different filters, all of them optional:

- *hostname*: Only events relating to the node with the matching hostname will be returned. This can be specified multiple times to get events relating to more than one node.

- *mac_address*: Only nodes with matching MAC addresses will be returned. Note that MAC address is not part of the standard output, so you'd need to look it up elsewhere.

- *id*: Only nodes with matching system IDs will be returned.  This corresponds to the `node` parameter in the JSON listing, not the `id` parameter there, which is a serial event number.

- *zone*: Only nodes in the zone will be returned.  Note that zones are not part of the standard output, so you'd need to look these up elsewhere.

- *level*: The event level to capture.  You can choose from AUDIT, CRITICAL, DEBUG, ERROR, INFO, or WARNING.  The default is INFO.

- *limit*: Number of events to return. The default is 100, the maximum in one command is 1000.

- *before*: Defines an event id to start returning older events.  This is the "id" part of the JSON, not the system ID or "node".  Note that `before` and `after` cannot be used together, as the results are unpredictable.

- *after*: Defines an event id to start returning newer events.  This is the "id" part of the JSON, not the system ID or "node".  Note that `before` and `after` cannot be used together, as the results are unpredictable.

This list of filters gives us a few different ways to simplify the output.  Let's try some of these combinations on the sample data, above.

*** Hostname, system ID, and MAC address filters

We can limit the hostname to, say, "new-name" by entering the following:

```nohighlight
maas $PROFILE events query limit=5 hostname=new-name\
| jq -r '(["USERNAME","NODE","HOSTNAME","LEVEL","DATE","TYPE","EVENT"] | 
(., map(length*"-"))),
(.events[] | [.username,.node,.hostname,.level,.created,.type,.description]) 
| @tsv' | column -t -s$'\t'
```

This might give us the following output:

```nohighlight
USERNAME  NODE    HOSTNAME  LEVEL    DATE                        TYPE                        EVENT
--------  ----    --------  -----    ----                        ----                        -----
unknown   ebd7dc  new-name  WARNING  Tue, 27 Sep. 2022 21:41:37  Failed to query node's BMC  Failed to login to virsh console.
unknown   ebd7dc  new-name  WARNING  Tue, 27 Sep. 2022 21:36:22  Failed to query node's BMC  Failed to login to virsh console.
unknown   ebd7dc  new-name  WARNING  Tue, 27 Sep. 2022 21:31:22  Failed to query node's BMC  Failed to login to virsh console.
unknown   ebd7dc  new-name  WARNING  Tue, 27 Sep. 2022 21:26:07  Failed to query node's BMC  Failed to login to virsh console.
unknown   ebd7dc  new-name  WARNING  Tue, 27 Sep. 2022 21:21:07  Failed to query node's BMC  Failed to login to virsh console.
```

We would get similar results with this command, using the "id" filter:

```nohighlight
maas $PROFILE events query limit=5 id=ebd7dc\
| jq -r '(["USERNAME","NODE","HOSTNAME","LEVEL","DATE","TYPE","EVENT"] | 
(., map(length*"-"))),
(.events[] | [.username,.node,.hostname,.level,.created,.type,.description]) 
| @tsv' | column -t -s$'\t'
```

We can also get similar results by looking up this machine's MAC address (52:54:00:32:8b:ea) and filtering by that parameter instead:

```nohighlight
maas $PROFILE events query limit=5 mac_address=52:54:00:32:8b:ea\
| jq -r '(["USERNAME","NODE","HOSTNAME","LEVEL","DATE","TYPE","EVENT"] | 
(., map(length*"-"))),
(.events[] | [.username,.node,.hostname,.level,.created,.type,.description]) 
| @tsv' | column -t -s$'\t'
```

In this particular case, all three would yield identical outputs.

*** Zone filter

We can look up one of the zones (using the Web UI or other CLI commands), and formulate a filter like this:

```nohighlight
maas $PROFILE events query limit=5 zone=twilight\
| jq -r '(["USERNAME","NODE","HOSTNAME","LEVEL","DATE","TYPE","EVENT"] | 
(., map(length*"-"))),
(.events[] | [.username,.node,.hostname,.level,.created,.type,.description]) 
| @tsv' | column -t -s$'\t'
```

Note that this query yields slightly different records:

```nohighlight
USERNAME  NODE    HOSTNAME     LEVEL    DATE                        TYPE                        EVENT
--------  ----    --------     -----    ----                        ----                        -----
unknown   mm3tc8  fair-marten  WARNING  Tue, 27 Sep. 2022 21:52:07  Failed to query node's BMC  Failed to login to virsh console.
unknown   mm3tc8  fair-marten  WARNING  Tue, 27 Sep. 2022 21:46:52  Failed to query node's BMC  Failed to login to virsh console.
unknown   mm3tc8  fair-marten  WARNING  Tue, 27 Sep. 2022 21:41:37  Failed to query node's BMC  Failed to login to virsh console.
unknown   mm3tc8  fair-marten  WARNING  Tue, 27 Sep. 2022 21:36:22  Failed to query node's BMC  Failed to login to virsh console.
unknown   mm3tc8  fair-marten  WARNING  Tue, 27 Sep. 2022 21:31:22  Failed to query node's BMC  Failed to login to virsh console.
```

*** Level filter

We can choose to look at specific events that match a logging level.  For example, we can repeat this command with `level=AUDIT`:

```nohighlight
maas $PROFILE events query limit=5 level=AUDIT\
| jq -r '(["USERNAME","NODE","HOSTNAME","LEVEL","DATE","TYPE","EVENT"] | 
(., map(length*"-"))),
(.events[] | [.username,.node,.hostname,.level,.created,.type,.description]) 
| @tsv' | column -t -s$'\t'
```

This will yield dramatically different results:

```nohighlight
USERNAME  NODE    HOSTNAME       LEVEL  DATE                        TYPE  EVENT
--------  ----    --------       -----  ----                        ----  -----
admin     ebd7dc  new-name       AUDIT  Thu, 22 Sep. 2022 15:25:55  Node  Overrode failed testing on 'new-name'.
admin     87pnsc  solid-tick     AUDIT  Thu, 22 Sep. 2022 15:22:33  Node  Aborted 'commissioning' on 'solid-tick'.
admin     pbpncx  ruling-bobcat  AUDIT  Thu, 22 Sep. 2022 15:19:00  Node  Started commissioning on 'ruling-bobcat'.
admin     87pnsc  solid-tick     AUDIT  Thu, 22 Sep. 2022 15:18:59  Node  Started commissioning on 'solid-tick'.
zorkobob  mm3tc8  fair-marten    AUDIT  Wed, 21 Sep. 2022 14:21:25  Node  Tagging 'fair-marten'.
zorkobob  mm3tc8  fair-marten    AUDIT  Wed, 21 Sep. 2022 14:10:38  Node  Untagging 'fair-marten'.
```

In fact, there are several different levels associated with MAAS events:

- INFO: the default, used if no `level=` is specified; shows `INFO` and `ERROR` events.  A typical `INFO` event is "Ready", indicating that a machine has reached the "Ready" state.
- CRITICAL: critical MAAS failures; shows only `CRITICAL` events.  These events usually represent severe error conditions that should be immediately remedied.
- ERROR: MAAS errors; shows only `ERROR` events. Typical `ERROR` events include such things as power on/off failures, commissioning timeouts, and image import failures.
- WARNING: failures which may or may not affect MAAS performance; shows `WARNING` and `ERROR` events.  A typical warning event, for example, might include the inability to find and boot a machine.
- DEBUG: information which would help debug MAAS behaviour; shows `DEBUG` and `INFO` events.  Typical `DEBUG` events involve routine image import activities, for example.
- AUDIT: information which helps determine settings and user actions in MAAS; shows only `AUDIT` events.  They are [covered in more detail elsewhere](/t/understanding-maas-audit-events/6372).

*** Combining filters

We can combine the `level` parameter with the `zone` parameter:

```nohighlight
maas $PROFILE events query limit=5 level=AUDIT zone=twilight\
| jq -r '(["USERNAME","NODE","HOSTNAME","LEVEL","DATE","TYPE","EVENT"] | 
(., map(length*"-"))),
(.events[] | [.username,.node,.hostname,.level,.created,.type,.description]) 
| @tsv' | column -t -s$'\t'
```

This combination gives us a very different output:

```nohighlight
USERNAME     NODE    HOSTNAME     LEVEL  DATE                        TYPE  EVENT
--------     ----    --------     -----  ----                        ----  -----
bobslidell   7h3cw7  polong       AUDIT  Tue, 27 Sep. 2022 21:53:38  Node  Set the zone to 'twilight' on 'polong'.
theotherbob  8r6pw7  karura       AUDIT  Tue, 27 Sep. 2022 21:53:34  Node  Set the zone to 'twilight' on 'karura'.
miltwaddams  mm3tc8  fair-marten  AUDIT  Wed, 21 Sep. 2022 14:21:25  Node  Tagging 'fair-marten'.
mikebolton   mm3tc8  fair-marten  AUDIT  Wed, 21 Sep. 2022 14:10:38  Node  Untagging 'fair-marten'.
admin        8r6pw7  karura       AUDIT  Wed, 21 Sep. 2022 14:00:47  Node  Untagging 'karura'.
```

These various filters can be combined, and even repeated as necessary:

```nohighlight
$ maas $PROFILE events query level=AUDIT hostname=karura hostname=polong limit=5 \
| jq -r '(["USERNAME","NODE","HOSTNAME","LEVEL","DATE","TYPE","EVENT"] | 
(., map(length*"-"))),
(.events[] | [.username,.node,.hostname,.level,.created,.type,.description]) 
| @tsv' | column -t -s$'\t'
```

Again, this combination gives us a different view of the event data:

```nohighlight
USERNAME    NODE    HOSTNAME  LEVEL  DATE                        TYPE  EVENT
--------    ----    --------  -----  ----                        ----  -----
stormrider  7h3cw7  polong    AUDIT  Tue, 27 Sep. 2022 21:53:38  Node  Set the zone to 'twilight' on 'polong'.
stormrider  8r6pw7  karura    AUDIT  Tue, 27 Sep. 2022 21:53:34  Node  Set the zone to 'twilight' on 'karura'.
admin       8r6pw7  karura    AUDIT  Wed, 21 Sep. 2022 14:00:47  Node  Untagging 'karura'.
admin       8r6pw7  karura    AUDIT  Wed, 21 Sep. 2022 14:00:01  Node  Tagging 'karura'.
admin       8r6pw7  karura    AUDIT  Wed, 21 Sep. 2022 13:58:11  Node  Untagging 'karura'.
```

*** The limit filter

You can use the `limit` filter to restrict the number of records listed, as we have been doing in many of the examples above.  We can expand the last example to `limit=7`, for instance:

```nohighlight
$ maas $PROFILE events query level=AUDIT hostname=karura hostname=polong limit=7 \
| jq -r '(["USERNAME","NODE","HOSTNAME","LEVEL","DATE","TYPE","EVENT"] | 
(., map(length*"-"))),
(.events[] | [.username,.node,.hostname,.level,.created,.type,.description]) 
| @tsv' | column -t -s$'\t'
```

This gives us a slightly longer list:

```nohighlight
USERNAME    NODE    HOSTNAME  LEVEL  DATE                        TYPE  EVENT
--------    ----    --------  -----  ----                        ----  -----
stormrider  7h3cw7  polong    AUDIT  Tue, 27 Sep. 2022 21:53:38  Node  Set the zone to 'twilight' on 'polong'.
stormrider  8r6pw7  karura    AUDIT  Tue, 27 Sep. 2022 21:53:34  Node  Set the zone to 'twilight' on 'karura'.
admin       8r6pw7  karura    AUDIT  Wed, 21 Sep. 2022 14:00:47  Node  Untagging 'karura'.
admin       8r6pw7  karura    AUDIT  Wed, 21 Sep. 2022 14:00:01  Node  Tagging 'karura'.
admin       8r6pw7  karura    AUDIT  Wed, 21 Sep. 2022 13:58:11  Node  Untagging 'karura'.
admin       8r6pw7  karura    AUDIT  Wed, 21 Sep. 2022 13:57:48  Node  Tagging 'karura'.
admin       7h3cw7  polong    AUDIT  Tue, 13 Sep. 2022 14:14:24  Node  Powered on 'polong'.
```

*** The before and after filters

Let's suppose that we want to repeat the query in the last example, but we want to start from the beginning of the event log (whenever that might have been).  We could modify the above command to something like this:

```nohighlight
$ maas $PROFILE events query level=AUDIT hostname=karura hostname=polong after=0 limit=7 \
| jq -r '(["USERNAME","NODE","HOSTNAME","LEVEL","DATE","TYPE","EVENT"] | 
(., map(length*"-"))),
(.events[] | [.username,.node,.hostname,.level,.created,.type,.description]) 
| @tsv' | column -t -s$'\t'
```

This would give us a different view:

```nohighlight
USERNAME  NODE    HOSTNAME  LEVEL  DATE                        TYPE  EVENT
--------  ----    --------  -----  ----                        ----  -----
ed        8r6pw7  karura    AUDIT  Tue, 12 Jul. 2022 15:08:27  Node  Powered on 'karura'.
ed        8r6pw7  karura    AUDIT  Tue, 12 Jul. 2022 15:08:23  Node  Powered on 'karura'.
ed        8r6pw7  karura    AUDIT  Tue, 12 Jul. 2022 15:08:08  Node  Powered off 'karura'.
admin     8r6pw7  karura    AUDIT  Thu, 23 Jun. 2022 23:26:53  Node  Set the zone to 'asd' on 'karura'.
peter     8r6pw7  karura    AUDIT  Fri, 22 Apr. 2022 08:06:59  Node  Powered off 'karura'.
peter     8r6pw7  karura    AUDIT  Fri, 22 Apr. 2022 07:09:55  Node  Powered on 'karura'.
ed        7h3cw7  polong    AUDIT  Thu, 27 Jan. 2022 14:34:34  Node  Powered on 'polong'.
```

We could also retrieve very recent records using "before":

```nohighlight
$ maas $PROFILE events query level=AUDIT before=500000 limit=7 \
| jq -r '(["USERNAME","NODE","HOSTNAME","LEVEL","DATE","TYPE","EVENT"] | 
(., map(length*"-"))),
(.events[] | [.username,.node,.hostname,.level,.created,.type,.description]) 
| @tsv' | column -t -s$'\t'
```

This would return:

```nohighlight
USERNAME  NODE    HOSTNAME     LEVEL  DATE                        TYPE  EVENT
--------  ----    --------     -----  ----                        ----  -----
peter     8r6pw7  karura       AUDIT  Fri, 22 Apr. 2022 08:06:59  Node  Powered off 'karura'.
peter     8r6pw7  karura       AUDIT  Fri, 22 Apr. 2022 07:09:55  Node  Powered on 'karura'.
admin     ebd7dc  new-name     AUDIT  Thu, 14 Apr. 2022 02:24:02  Node  Aborted 'commissioning' on 'new-name'.
admin     ebd7dc  new-name     AUDIT  Thu, 14 Apr. 2022 02:23:44  Node  Started commissioning on 'new-name'.
ed        mm3tc8  fair-marten  AUDIT  Fri, 08 Apr. 2022 11:02:15  Node  Untagging 'fair-marten'.
ed        mm3tc8  fair-marten  AUDIT  Fri, 08 Apr. 2022 11:02:14  Node  Tagging 'fair-marten'.
admin     mm3tc8  fair-marten  AUDIT  Fri, 11 Feb. 2022 11:00:00  Node  Set the zone to 'twilight' on 'fair-marten'.
```

** Using different event levels

As mentioned earlier, the `AUDIT` events are [discussed elsewhere](/t/understanding-maas-audit-events/6372).  It may be useful, though to take a closer look at the other event levels here.

*** INFO and DEBUG events

We walked the MAAS machine `fun-zebra` through the following states:

- Commissioning
- Allocation
- Deployment
- Releasing
- Testing (with a premature manual abort)
- Rescue mode

The resulting `level=INFO` and `level=DEBUG` event sets are enlightening.

<details><summary>The raw log output, for reference only.</summary>

```nohighlight
maas.log:2022-09-29T15:00:16.461402-05:00 neuromancer maas.interface: [info] eth0 (physical) on fun-zebra: IP address automatically unlinked: None:type=AUTO
maas.log:2022-09-29T15:00:16.553396-05:00 neuromancer maas.node: [info] fun-zebra: Status transition from NEW to COMMISSIONING
maas.log:2022-09-29T15:00:16.754265-05:00 neuromancer maas.power: [info] Changing power state (on) of node: fun-zebra (bk7mg8)
maas.log:2022-09-29T15:00:16.759676-05:00 neuromancer maas.node: [info] fun-zebra: Commissioning started
maas.log:2022-09-29T15:00:18.039441-05:00 neuromancer maas.power: [info] Changed power state (on) of node: fun-zebra (bk7mg8)
maas.log:2022-09-29T15:03:26.946507-05:00 neuromancer maas.node: [info] fun-zebra: Storage layout was set to flat.
maas.log:2022-09-29T15:04:07.795515-05:00 neuromancer maas.node: [info] fun-zebra: Status transition from COMMISSIONING to TESTING
maas.log:2022-09-29T15:04:17.288763-05:00 neuromancer maas.node: [info] fun-zebra: Status transition from TESTING to READY
maas.log:2022-09-29T16:14:21.778320-05:00 neuromancer maas.node: [info] fun-zebra: Status transition from READY to ALLOCATED
maas.log:2022-09-29T16:14:21.793566-05:00 neuromancer maas.node: [info] fun-zebra: allocated to user case
maas.log:2022-09-29T16:14:27.662829-05:00 neuromancer maas.node: [info] fun-zebra: Status transition from ALLOCATED to DEPLOYING
maas.log:2022-09-29T16:14:31.019526-05:00 neuromancer maas.power: [info] Changing power state (on) of node: fun-zebra (bk7mg8)
maas.log:2022-09-29T16:14:32.334589-05:00 neuromancer maas.power: [info] Changed power state (on) of node: fun-zebra (bk7mg8)
maas.log:2022-09-29T16:22:41.935983-05:00 neuromancer maas.node: [info] fun-zebra: Status transition from DEPLOYING to DEPLOYED
maas.log:2022-09-29T16:23:37.084128-05:00 neuromancer maas.node: [info] fun-zebra: Releasing node
maas.log:2022-09-29T16:23:37.085876-05:00 neuromancer maas.node: [info] fun-zebra: Status transition from DEPLOYED to RELEASING
maas.log:2022-09-29T16:23:37.196437-05:00 neuromancer maas.power: [info] Changing power state (off) of node: fun-zebra (bk7mg8)
maas.log:2022-09-29T16:23:38.546649-05:00 neuromancer maas.node: [info] fun-zebra: Status transition from RELEASING to READY
maas.log:2022-09-29T16:23:38.591042-05:00 neuromancer maas.power: [info] Changed power state (off) of node: fun-zebra (bk7mg8)
maas.log:2022-09-29T16:23:51.876495-05:00 neuromancer maas.node: [info] fun-zebra: Status transition from READY to TESTING
maas.log:2022-09-29T16:23:51.997139-05:00 neuromancer maas.power: [info] Changing power state (on) of node: fun-zebra (bk7mg8)
maas.log:2022-09-29T16:23:52.001167-05:00 neuromancer maas.node: [info] fun-zebra: Testing starting
maas.log:2022-09-29T16:23:53.291863-05:00 neuromancer maas.power: [info] Changed power state (on) of node: fun-zebra (bk7mg8)
maas.log:2022-09-29T16:26:14.607386-05:00 neuromancer maas.power: [info] Changing power state (off) of node: fun-zebra (bk7mg8)
maas.log:2022-09-29T16:26:14.622643-05:00 neuromancer maas.node: [info] fun-zebra: Status transition from TESTING to READY
maas.log:2022-09-29T16:26:14.678433-05:00 neuromancer maas.node: [info] fun-zebra: Testing aborted, stopping node
maas.log:2022-09-29T16:26:16.051940-05:00 neuromancer maas.power: [info] Changed power state (off) of node: fun-zebra (bk7mg8)
maas.log:2022-09-29T16:26:23.081533-05:00 neuromancer maas.node: [info] fun-zebra: Status transition from READY to ENTERING_RESCUE_MODE
maas.log:2022-09-29T16:26:23.160687-05:00 neuromancer maas.power: [info] Changing power state (cycle) of node: fun-zebra (bk7mg8)
maas.log:2022-09-29T16:26:23.163274-05:00 neuromancer maas.node: [info] fun-zebra: Rescue mode starting
maas.log:2022-09-29T16:26:24.528007-05:00 neuromancer maas.power: [info] Changed power state (on) of node: fun-zebra (bk7mg8)
maas.log:2022-09-29T16:28:58.268558-05:00 neuromancer maas.node: [info] fun-zebra: Status transition from ENTERING_RESCUE_MODE to RESCUE_MODE
maas.log:2022-09-29T16:29:52.204837-05:00 neuromancer maas.node: [info] fun-zebra: Status transition from RESCUE_MODE to EXITING_RESCUE_MODE
maas.log:2022-09-29T16:29:52.323798-05:00 neuromancer maas.power: [info] Changing power state (off) of node: fun-zebra (bk7mg8)
maas.log:2022-09-29T16:29:53.708975-05:00 neuromancer maas.node: [info] fun-zebra: Status transition from EXITING_RESCUE_MODE to READY
maas.log:2022-09-29T16:29:53.745776-05:00 neuromancer maas.power: [info] Changed power state (off) of node: fun-zebra (bk7mg8)
maas.log:2022-09-29T16:32:20.147958-05:00 neuromancer maas.node: [info] fun-zebra: moved from default zone to new-zone zone.
regiond.log:2022-09-29 20:00:16 maasserver.models.signals.interfaces: [info] eth0 (physical) on fun-zebra: deleted IP addresses due to VLAN update (5001 -> 5002).
regiond.log:2022-09-29 20:00:32 maasserver.region_controller: [info] Reloaded DNS configuration; ip 10.103.114.192 connected to fun-zebra on eth0
regiond.log:2022-09-29 20:01:09 maasserver.rpc.leases: [info] Lease update: commit for 10.103.114.192 on 0:16:3e:a2:73:5c at 2022-09-29 20:01:09 (lease time: 600s) (hostname: fun-zebra)
regiond.log:2022-09-29 20:01:14 maasserver.region_controller: [info] Reloaded DNS configuration; ip 10.103.114.192 connected to fun-zebra on eth0
regiond.log:     * node fun-zebra renamed interface eth0 to enp5s0
regiond.log:     * ip 10.103.114.192 connected to fun-zebra on eth0
regiond.log:     * ip 10.103.114.192 disconnected from fun-zebra on eth0
regiond.log:2022-09-29 21:15:31 maasserver.rpc.leases: [info] Lease update: commit for 10.103.114.2 on 0:16:3e:a2:73:5c at 2022-09-29 21:15:31 (lease time: 600s) (hostname: fun-zebra)
regiond.log:2022-09-29 21:21:48 maasserver.models.node: [info] fun-zebra: Turning off netboot for node
regiond.log:2022-09-29 21:22:41 metadataserver: [info] No user data registered for node named fun-zebra
regiond.log:2022-09-29 21:23:37 maasserver.models.node: [info] fun-zebra: Turning on netboot for node
regiond.log:2022-09-29 21:23:37 maasserver.models.node: [info] fun-zebra: Turning ephemeral deploy off for node
regiond.log:2022-09-29 21:24:06 maasserver.region_controller: [info] Reloaded DNS configuration; ip 10.103.114.192 connected to fun-zebra on enp5s0
regiond.log:2022-09-29 21:24:43 maasserver.rpc.leases: [info] Lease update: commit for 10.103.114.192 on 0:16:3e:a2:73:5c at 2022-09-29 21:24:43 (lease time: 600s) (hostname: fun-zebra)
regiond.log:2022-09-29 21:24:46 maasserver.region_controller: [info] Reloaded DNS configuration; ip 10.103.114.192 connected to fun-zebra on enp5s0
regiond.log:2022-09-29 21:27:18 maasserver.rpc.leases: [info] Lease update: commit for 10.103.114.192 on 0:16:3e:a2:73:5c at 2022-09-29 21:27:18 (lease time: 600s) (hostname: fun-zebra)
```
</details>

First, let's try this command:

```nohighlight
 maas $PROFILE events query level=INFO hostname=fun-zebra limit=1000 | jq -r '(["USERNAME","NODE","HOSTNAME","LEVEL","DATE","TYPE","EVENT"] | (., map(length*"-"))),(.events[] | [.username,.node,.hostname,.level,.created,.type,.description]) | @tsv' | column -t -s$'\t'
 ```
 
 This will yield a surprisingly compact report:
 
 ```nohighlight
 USERNAME  NODE    HOSTNAME   LEVEL  DATE                        TYPE                   EVENT
--------  ----    --------   -----  ----                        ----                   -----
unknown   bk7mg8  fun-zebra  INFO   Thu, 29 Sep. 2022 21:29:53  Exited rescue mode     
unknown   bk7mg8  fun-zebra  INFO   Thu, 29 Sep. 2022 21:29:52  Powering off           
unknown   bk7mg8  fun-zebra  INFO   Thu, 29 Sep. 2022 21:28:58  Rescue mode            
unknown   bk7mg8  fun-zebra  INFO   Thu, 29 Sep. 2022 21:27:18  Loading ephemeral      
unknown   bk7mg8  fun-zebra  INFO   Thu, 29 Sep. 2022 21:26:40  Performing PXE boot    
unknown   bk7mg8  fun-zebra  INFO   Thu, 29 Sep. 2022 21:26:23  Power cycling          
unknown   bk7mg8  fun-zebra  INFO   Thu, 29 Sep. 2022 21:26:23  Entering rescue mode   
unknown   bk7mg8  fun-zebra  INFO   Thu, 29 Sep. 2022 21:26:14  Powering off           
unknown   bk7mg8  fun-zebra  INFO   Thu, 29 Sep. 2022 21:26:14  Aborted testing        
unknown   bk7mg8  fun-zebra  INFO   Thu, 29 Sep. 2022 21:24:08  Performing PXE boot    
unknown   bk7mg8  fun-zebra  INFO   Thu, 29 Sep. 2022 21:23:51  Powering on            
unknown   bk7mg8  fun-zebra  INFO   Thu, 29 Sep. 2022 21:23:51  Testing                
unknown   bk7mg8  fun-zebra  INFO   Thu, 29 Sep. 2022 21:23:38  Released               
unknown   bk7mg8  fun-zebra  INFO   Thu, 29 Sep. 2022 21:23:37  Powering off           
unknown   bk7mg8  fun-zebra  INFO   Thu, 29 Sep. 2022 21:23:37  Releasing              
unknown   bk7mg8  fun-zebra  INFO   Thu, 29 Sep. 2022 21:22:41  Deployed               
unknown   bk7mg8  fun-zebra  INFO   Thu, 29 Sep. 2022 21:21:49  Rebooting              
unknown   bk7mg8  fun-zebra  INFO   Thu, 29 Sep. 2022 21:18:42  Configuring OS         
unknown   bk7mg8  fun-zebra  INFO   Thu, 29 Sep. 2022 21:17:42  Installing OS          
unknown   bk7mg8  fun-zebra  INFO   Thu, 29 Sep. 2022 21:17:30  Configuring storage    
unknown   bk7mg8  fun-zebra  INFO   Thu, 29 Sep. 2022 21:15:31  Loading ephemeral      
unknown   bk7mg8  fun-zebra  INFO   Thu, 29 Sep. 2022 21:14:48  Performing PXE boot    
unknown   bk7mg8  fun-zebra  INFO   Thu, 29 Sep. 2022 21:14:31  Powering on            
unknown   bk7mg8  fun-zebra  INFO   Thu, 29 Sep. 2022 21:14:27  Deploying              
unknown   bk7mg8  fun-zebra  INFO   Thu, 29 Sep. 2022 20:04:17  Ready                  
unknown   bk7mg8  fun-zebra  INFO   Thu, 29 Sep. 2022 20:04:07  Running test           smartctl-validate on sda
unknown   bk7mg8  fun-zebra  INFO   Thu, 29 Sep. 2022 20:01:27  Gathering information  
unknown   bk7mg8  fun-zebra  INFO   Thu, 29 Sep. 2022 20:01:10  Loading ephemeral      
unknown   bk7mg8  fun-zebra  INFO   Thu, 29 Sep. 2022 20:00:35  Performing PXE boot    
unknown   bk7mg8  fun-zebra  INFO   Thu, 29 Sep. 2022 20:00:16  Powering on            
unknown   bk7mg8  fun-zebra  INFO   Thu, 29 Sep. 2022 20:00:16  Commissioning          
 ```
 
Note that most of the `INFO` events are either machine life-cycle events or key operations within those state changes, such as `Loading ephemeral` after a PXE boot.  `DEBUG` events, on the other hand, include `INFO` events for reference, but provide a much more extensive report of individual actions within each state change.  For instance, here is just the snippet of `DEBUG` information for the host's exit from rescue mode:

```nohighlight
USERNAME  NODE    HOSTNAME   LEVEL  DATE                        TYPE                              EVENT
--------  ----    --------   -----  ----                        ----                              -----
unknown   bk7mg8  fun-zebra  DEBUG  Thu, 29 Sep. 2022 21:29:53  Node powered off                  
unknown   bk7mg8  fun-zebra  DEBUG  Thu, 29 Sep. 2022 21:29:53  Node changed status               From 'Exiting rescue mode' to 'Ready'
unknown   bk7mg8  fun-zebra  INFO   Thu, 29 Sep. 2022 21:29:53  Exited rescue mode                
unknown   bk7mg8  fun-zebra  DEBUG  Thu, 29 Sep. 2022 21:28:58  Node status event                 'cloudinit' running config-power-state-change with frequency once-per-instance
unknown   bk7mg8  fun-zebra  DEBUG  Thu, 29 Sep. 2022 21:28:58  Node status event                 'cloudinit' running config-final-message with frequency always
unknown   bk7mg8  fun-zebra  DEBUG  Thu, 29 Sep. 2022 21:28:58  Node status event                 'cloudinit' running config-phone-home with frequency once-per-instance
unknown   bk7mg8  fun-zebra  DEBUG  Thu, 29 Sep. 2022 21:28:58  Node status event                 'cloudinit' running config-install-hotplug with frequency once-per-instance
unknown   bk7mg8  fun-zebra  DEBUG  Thu, 29 Sep. 2022 21:28:58  Node status event                 'cloudinit' running config-keys-to-console with frequency once-per-instance
unknown   bk7mg8  fun-zebra  DEBUG  Thu, 29 Sep. 2022 21:28:58  Node status event                 'cloudinit' running config-ssh-authkey-fingerprints with frequency once-per-instance
unknown   bk7mg8  fun-zebra  DEBUG  Thu, 29 Sep. 2022 21:28:57  Node status event                 'cloudinit' running config-scripts-user with frequency once-per-instance
unknown   bk7mg8  fun-zebra  DEBUG  Thu, 29 Sep. 2022 21:28:57  Node status event                 'cloudinit' running config-scripts-per-instance with frequency once-per-instance
unknown   bk7mg8  fun-zebra  DEBUG  Thu, 29 Sep. 2022 21:28:57  Node status event                 'cloudinit' running config-scripts-per-boot with frequency always
unknown   bk7mg8  fun-zebra  DEBUG  Thu, 29 Sep. 2022 21:28:57  Node status event                 'cloudinit' running config-scripts-per-once with frequency once
unknown   bk7mg8  fun-zebra  DEBUG  Thu, 29 Sep. 2022 21:28:57  Node status event                 'cloudinit' running config-scripts-vendor with frequency once-per-instance
unknown   bk7mg8  fun-zebra  DEBUG  Thu, 29 Sep. 2022 21:28:57  Node status event                 'cloudinit' running config-rightscale_userdata with frequency once-per-instance
unknown   bk7mg8  fun-zebra  DEBUG  Thu, 29 Sep. 2022 21:28:57  Node status event                 'cloudinit' running config-refresh_rmc_and_interface with frequency always
unknown   bk7mg8  fun-zebra  DEBUG  Thu, 29 Sep. 2022 21:28:57  Node status event                 'cloudinit' running config-reset_rmc with frequency once-per-instance
unknown   bk7mg8  fun-zebra  DEBUG  Thu, 29 Sep. 2022 21:28:57  Node status event                 'cloudinit' running config-salt-minion with frequency once-per-instance
unknown   bk7mg8  fun-zebra  DEBUG  Thu, 29 Sep. 2022 21:28:57  Node status event                 'cloudinit' running config-mcollective with frequency once-per-instance
unknown   bk7mg8  fun-zebra  DEBUG  Thu, 29 Sep. 2022 21:28:57  Node status event                 'cloudinit' running config-chef with frequency always
unknown   bk7mg8  fun-zebra  DEBUG  Thu, 29 Sep. 2022 21:28:57  Node status event                 'cloudinit' running config-puppet with frequency once-per-instance
unknown   bk7mg8  fun-zebra  DEBUG  Thu, 29 Sep. 2022 21:28:57  Node status event                 'cloudinit' running config-write-files-deferred with frequency once-per-instance
unknown   bk7mg8  fun-zebra  DEBUG  Thu, 29 Sep. 2022 21:28:57  Node status event                 'cloudinit' running config-ubuntu-drivers with frequency once-per-instance
unknown   bk7mg8  fun-zebra  DEBUG  Thu, 29 Sep. 2022 21:28:57  Node status event                 'cloudinit' running config-lxd with frequency once-per-instance
unknown   bk7mg8  fun-zebra  DEBUG  Thu, 29 Sep. 2022 21:28:57  Node status event                 'cloudinit' running config-landscape with frequency once-per-instance
unknown   bk7mg8  fun-zebra  DEBUG  Thu, 29 Sep. 2022 21:28:57  Node status event                 'cloudinit' running config-fan with frequency once-per-instance
unknown   bk7mg8  fun-zebra  INFO   Thu, 29 Sep. 2022 21:29:52  Powering off                      
unknown   bk7mg8  fun-zebra  DEBUG  Thu, 29 Sep. 2022 21:29:52  Node changed status               From 'Rescue mode' to 'Exiting rescue mode'
case      bk7mg8  fun-zebra  DEBUG  Thu, 29 Sep. 2022 21:29:52  User stopping rescue mode         (case)
```

Notice the detailed `cloudinit` actions necessary to change the machine's state.  The other state changes have similarly detailed outputs in `DEBUG`.

*** ERROR and WARNING events

Here are a few representative `ERROR` event descriptions taken from a live MAAS machine:

```nohighlight
Node has not been heard from for the last 30 minutes
Node operation 'Commissioning' timed out after 30 minutes.
Unable to import boot images: HTTPConnectionPool(host='localhost', port=5240): Read timed out.
Node operation 'Testing' timed out after 30 minutes.
Power on for the node failed: Failed talking to node's BMC: Failed to login to virsh console.
Unable to import boot images: Invalid sha256 Checksum at http://localhost:5240/MAAS/images-stream/ubuntu/amd64/ga-18.04-lowlatency/bionic/20200206/boot-initrd. Found 834c0eacb1a19526f715f9947bd47904b18ad8c733b0762e690edf6143e10561. Expected addfa86d7c054bd0dc085333ad2850e93223d511d04b59ee516d42d801522324. read 38 bytes expected 61715624 bytes. (size 38 expected 61715624)
``` 

Notice that these `ERROR` events flag failures that are probably going to prevent MAAS from operating properly.  Changing the level to `WARNING` picks up all `ERROR` events, but also includes warnings such as this one:

```nohighlight
Finished importing boot images, the region does not have any boot images available.
```

`WARNINGS` tend to be failures, as well, but failures which are more easily fixed (such as having not successfully downloaded any images).

*** CRITICAL errors

`CRITICAL` errors represent major failures, often code failures or trace-backs.  Any `CRITICAL` errors should be immediately examined and resolved, if possible, and [reported as a bug](/t/how-to-report-a-bug/4446) if not resolvable.
* User accounts
Presumably, you have already created an administrative user, but MAAS can also have regular users (who log in to the interface or use the CLI). What users you create depends on how you intend to use MAAS.  An administrator can manage all aspects of MAAS, whereas a non-administrator user can perform a subset of tasks on machines they allocate and deploy.  MAAS limits the details a non-admin user can view, such as nodes allocated to other users. Also, non-admin users cannot access the global settings page in the web UI, nor any of the equivalent API calls from the command line.

Additionally, in order for a user to log into a MAAS-deployed machine that user must have their public SSH key installed on it.  This article explains how to create users and add their public SSH keys to MAAS, so that every deployed machine will automatically have that key installed. 

[note]
A currently logged in user cannot delete themselves from the web UI.
[/note]

The user preferences page includes an API key for the currently active user. This key can be copied and regenerated as needed. The API key is used to login to the API from the [MAAS CLI](/t/try-out-the-maas-cli/5236).  Other services connecting to MAAS such as Juju will also need this key.

Before a user can deploy a machine, they must import at least one public SSH key into MAAS. This key allows the user to access the deployed machine with the corresponding private key, which the user must possess. See [Public key authentication](https://www.ssh.com/ssh/public-key-authentication) (ssh.com)`↗` if you're not familiar with SSH keys.



* Vault and MAAS

* Virtual machines
Virtual machines bring tremendous advantages to MAAS.  We use [LXD](https://linuxcontainers.org/lxd/introduction/)`↗` as our primary VM host, so everything about MAAS VMs is optimised for LXD VMs.  For reference, VM hosts are called "KVMs" in the MAAS Web UI.

If KVMs and LXD VMs are not new to you, feel free to go ahead and [set up LXD](/t/how-to-set-up-lxd/5208), create one or more [VM hosts](/t/how-to-create-vm-hosts/5140), and start [deploying virtual machines](/t/how-to-create-and-manage-vms/5148) to cover your workload.  The rest of this article provides a little theory about MAAS VM hosts, just in case you need to catch up.

** MAAS VM hosting

MAAS VM hosts allow for the dynamic composition of nodes from a pool of available hardware resources (e.g. disk space, memory, cores). You can create virtual machines (VMs) as needed within the limits of your resources, without concern for physical hardware.

This theory section will help you learn:

[tabs]
[tab version="v3.4 Snap,v3.4 Packages,v3.3 Snap,v3.3 Packages,v3.2 Snap,v3.2 Packages,v3.1 Snap,v3.1 Packages,v3.0 Snap,v3.0 Packages" view="UI"]
- [About VM hosts](#heading--about-vm-hosts)
- [About VM host storage pools](#heading--about-vm-host-storage-pools)
- [About LXD VM hosts](#heading--about-lxd-vm-hosts)
- [About LXD VM host project summaries](#heading--vm-host-project-summary)
- [About LXD VM host resource details](#heading--vm-host-resource-details)
- [About VM host settings](#heading--configuration)
- [About VMs and NUMA](#heading--about-vms-and-numa)
- [About support for NUMA, SR-IOV, and hugepages](#heading--about-support-for-numa-et-al)
- [About over-committed resources](#heading--overcommit-resources)
[/tab]
[tab version="v3.4 Snap,v3.4 Packages,v3.3 Snap,v3.3 Packages,v3.2 Snap,v3.2 Packages,v3.1 Snap,v3.1 Packages,v3.0 Snap,v3.0 Packages" view="CLI"]
- [About VM hosts](#heading--about-vm-hosts)
- [About VM host storage pools](#heading--about-vm-host-storage-pools)
- [About VMs and NUMA](#heading--about-vms-and-numa)
- [About support for NUMA, SR-IOV, and hugepages](#heading--about-support-for-numa-et-al)
- [About over-committed resources](#heading--overcommit-resources)
[/tab]
[tab version="v2.9 Snap,v2.9 Packages"]
- [About VM hosts](#heading--about-vm-hosts)
- [About VM host storage pools](#heading--about-vm-host-storage-pools)
- [About LXD vs. libvirt](#heading--about-lxd-vs-libvirt)
- [About VMs and NUMA](#heading--about-vms-and-numa)
- [About support for NUMA, SR-IOV, and hugepages](#heading--about-support-for-numa-et-al)
- [About over-committed resources](#heading--overcommit-resources)
[/tab]
[/tabs]

[tabs]
[tab version="v3.4 Snap,v3.4 Packages,v3.3 Snap,v3.3 Packages,v3.2 Snap,v3.2 Packages,v3.1 Snap,v3.1 Packages,v3.0 Snap,v3.0 Packages"]
MAAS currently supports VM hosts and VMs created with LXD VMs and VM hosts as the preferred VM hosting method. As a legacy offering, MAAS still supports VM hosts and VMs created via [libvirt](https://ubuntu.com/server/docs/virtualization-libvirt)`↗`.
[/tab]
[tab version="v2.9 Snap,v2.9 Packages"]
MAAS currently supports VM hosts and VMs created via [libvirt](https://ubuntu.com/server/docs/virtualization-libvirt)`↗`. MAAS also supports LXD VMs and VM hosts as a Beta feature.
[/tab]
[/tabs]

*** About VM hosts

A VM host is simply a machine which can run virtual machines (VMs) by allocating  resources across the VMs you want to create.  If needed, you can over-commit resources, allocating more resources than actually available, so long as you don't try to use more than the VM host has available at any one time. Once MAAS has enlisted, commissioned, and allocated a newly-added machine, you can deploy it as a VM host.  Alternatively, you can create a VM host from a machine you've already got running.

VM hosts are particularly useful for Juju integration, allowing for dynamic allocation of VMs with custom interface constraints. Alternatively, if you would like to use MAAS to manage a collection of VMs, the robust web UI allows you to easily create and manage VMs, logically grouped by VM host. Six conspicuous features include:

- Juju integration
- At-a-glance visual tools for easy resource management
- Set overcommit ratios for physical resources such as CPU and RAM
- Assign VMs to resource pools to segregate your VMs into logical groupings
- Track VM host storage pool usage and assign default storage pools
- Create VMs on multiple networks, specified by space, subnet, VLAN, or IP address

Simply put, a VM host is a machine which is designated to run virtual machines (VMs). A VM host divides its resources (CPU cores, RAM, storage) among the number of VMs you want to create, based on choices that you make when creating each VM. It is also possible to overcommit resources – that is, use more resources than the VM host actually has available – as long as you use the VMs carefully. Once MAAS has enlisted, commissioned, and allocated a newly-added machine, you can deploy it as a VM host.


*** About VM host storage pools

"Storage pools” are storage resources managed by a VM host. A storage pool is a given amount of storage set aside for use by VMs. A pool can be organised into storage volumes, assigned to VMs as individual block devices.

[note]
For LXD VM hosts, each VM can be assigned a single block device from the storage pool.
[/note]

[tabs]
[tab version="v3.4 Snap,v3.4 Packages,v3.3 Snap,v3.3 Packages,v3.2 Snap,v3.2 Packages,v3.1 Snap,v3.1 Packages,v3.0 Snap,v3.0 Packages,v2.9 Snap,v2.9 Packages" view="UI"]
The MAAS web UI displays information about each VM host's storage pools so you can understand your resource usage at a glance:

<a href="https://discourse.maas.io/uploads/default/original/1X/3387f256f9bd02f7fc2079f119377305256973c8.jpeg" target = "_blank"><img src="https://discourse.maas.io/uploads/default/original/1X/3387f256f9bd02f7fc2079f119377305256973c8.jpeg"></a>
[/tab]
[tab version="v3.4 Snap,v3.4 Packages,v3.3 Snap,v3.3 Packages,v3.2 Snap,v3.2 Packages,v3.1 Snap,v3.1 Packages,v3.0 Snap,v3.0 Packages,v2.9 Snap,v2.9 Packages" view="CLI"]
Retrieve VM host storage pool information with the following command:

```
maas $PROFILE vm-host read $VM_HOST_ID
```

or, to get tabular output, try:

```
maas admin vm-host read 5 \
| jq -r '(["NAME","TYPE","PATH","TOTAL","USED","AVAIL"]) 
| (,. map(length*"-"))), (.storage_pools[] 
| [.name, .type, .path, .total, used, .available]) | @tsv' \
| column -t
```

[/tab]
[/tabs]

*** About LXD VM hosts

[tabs]
[tab version="v3.4 Snap,v3.4 Packages,v3.3 Snap,v3.3 Packages,v3.2 Snap,v3.2 Packages,v3.1 Snap,v3.1 Packages,v3.0 Snap,v3.0 Packages" view="UI"]
**** About LXD VM host authentication

MAAS 3.1 provides a smoother experience when connecting an existing LXD server to MAAS, guiding the user through manual steps and providing increased connection security with use of certificates. Currently, each MAAS region/rack controller has its own certificate. To add a LXD VM host to MAAS, the user needs to either add the certificate for each controller that can reach the LXD server to the trust list in LXD, or use the trust_password (in which case the controller talking to LXD will automatically add its certificate to the trust).

This doesn’t provide a great user experience, as the former process is cumbersome, and the latter is not suggested for production use for security reasons.  To improve this, MAAS 3.1 manages per-LXD keys/certificates, and provide a way for users to get the content of certificates, to authorise MAAS in LXD.

**** About on-the-spot certificate creation

As a MAAS user, you want to register a LXD host into MAAS using certificates for authentication -- to follow LXD production deployment best practices.  The standard way for clients to authenticate with LXD servers is through certificates. The use of trust_password is *only* meant as a way to interact for initial setup.

While prior versions of MAAS support both ways of authentication (and automatically adds the certificate for the rack talking to LXD when registering the VM host), the user experience is lacking, since there's no control over the certificate being used.  In addition, each rack uses a different certificate, making it hard to manage scenarios where multiple racks can connect to a LXD server.

For these reasons, when adding a LXD host, MAAS 3.1 provides a way to generate a secret key and certificate pair to use specifically for that server, and show the certificate to the user, so that they can add it to the LXD server trust list.  The user experience changes to something like the following:

- MAAS generates a secret key and certificate pair for use with a LXD server.
- The user can see the certificate and is guided to add it to the LXD server trust list.
- The user can easily complete the registration of the LXD server once the certificate is trusted in LXD.
- All racks use the same key when talking to the LXD server. 
- If a new rack controller is added, it can communicate with the LXD server out of the box.
- If the trust password is used, it’s not stored in MAAS persistently.
- It’s possible to get the certificate for a LXD server from a URL (e.g. for curl use).

**** About bringing your own certificates

As a MAAS user, you may want to register a LXD host into MAAS by providing a private key for a certificate that’s already trusted by the LXD server.  For example, you may already have set up certificates in the server trust for MAAS to use, MAAS should provide a way to import it, instead of generating a new one.

With MAAS 3.1, it’s possible to import an existing key/certificate pair for use with a LXD server when registering it with MAAS.  MAAS stores the key/certificate instead of generating new ones.

[note]
The imported key must not have a passphrase; otherwise, MAAS will not be able to use it.
[/note]

*** About LXD VM host project summaries

Each LXD VM host provides a "Project" tab that summarises the current state of the LXD KVM:

<a href="https://discourse.maas.io/uploads/default/original/2X/e/e0cc264a17d67f9530ff8c2ef2bb9522fed0749a.png" target = "_blank"><img src="https://discourse.maas.io/uploads/default/original/2X/e/e0cc264a17d67f9530ff8c2ef2bb9522fed0749a.png"></a>

This tab identifies the project, shows its current resource state, and provides the ability to select existing VM hosts and perform specific actions on them -- as well as being able to compose new VMs on the spot.

*** About LXD VM host resource details

This tab presents a summary of the LXD VM host's resource usage:

<a href="https://discourse.maas.io/uploads/default/original/2X/d/d67cf384d6fe903274893eb50a098518d2c1295d.png" target = "_blank"><img src="https://discourse.maas.io/uploads/default/original/2X/d/d67cf384d6fe903274893eb50a098518d2c1295d.png"></a>

The only interactive option on this tab allows you to map or unmap resource usage to NUMA nodes.

*** About VM host settings

VM hosts have several settings. Modify these by selecting the 'Settings' tab and editing items directly. Options include a VM host's address, password, network zone, resource pool, and memory and CPU overcommit sliders.

<a href="https://discourse.maas.io/uploads/default/original/2X/2/253afc122d61145be656bb5c3811f9b6c6caa708.png" target = "_blank"><img src="https://discourse.maas.io/uploads/default/original/2X/2/253afc122d61145be656bb5c3811f9b6c6caa708.png"></a>
[/tab]
[tab version="v3.4 Snap,v3.4 Packages,v3.3 Snap,v3.3 Packages,v3.2 Snap,v3.2 Packages,v3.1 Snap,v3.1 Packages,v3.0 Snap,v3.0 Packages" view="CLI"]
Please use the UI interface to interact with LXD VM hosts, by selecting "UI" from the dropdown above.
[/tab]
[tab version="v2.9 Snap,v2.9 Packages view="UI,CLI"]
LXD VM hosts are not available in MAAS version 2.9.  Please upgrade to MAAS version 3.0 or greater to access this functionality.

*** About LXD (Beta) vs. libvirt

Libvirt KVMs and LXD VMs are both based on the same underlying virtualisation technology, QEMU. Unlike libvirt KVMs, though, LXD VMs can be managed without requiring SSH access to the VM host. LXD are remotely accessed via secure HTTP transport, which provides better security for LXD-based VMs. In addition, LXD has a better API, and is part of a much larger constellation of enterprise software, offering a wider range of future features and use cases.
[/tab]
[/tabs]

*** About VMs and NUMA

MAAS provides extensive optimisation tools for using NUMA with virtual machines. Earlier versions of MAAS guarantee that machines are assigned to a single NUMA node that contains all the machine's resources. As of 2.9, MAAS now allows you to see how many VMs are allocated to each NUMA node, along with the allocations of cores, storage, and memory. You can quickly spot a VM running in multiple NUMA nodes, and optimise accordingly, with instant updates on pinning and allocations. You can also tell which VMs are currently running.

In addition, you can get a bird's-eye view of network configuration:

- You can see which VM NIC/bond is connected to which NUMA node.
- You can tell when a NIC is connected to a different NUMA node.
- You can tell if one of multiple NICs is not in the correct node.
- You can confirm the subnet and space connecting to a VM.
- You can confirm that a VM has the desired network properties, such as latency and throughput.
- You can identify NICs that support SR-IOV and tell how many VFs are available.

MAAS also shows hugepages information (if they are in use) and prevents overcommit when using them. Hugepages essentially allow a much larger memory cache associated with the core. This obviously reduces the number of times a core has to access memory, but because the core must swap entire hugepages, optimising usage of them can be complex. MAAS helps you create these optimisations by giving you a discrete view of hugepages associated with your VM, helping you decide whether you need to use them or not.

*** About support for NUMA, SR-IOV, and hugepages

[tabs]
[tab version="v3.4 Snap,v3.4 Packages,v3.3 Snap,v3.3 Packages,v3.2 Snap,v3.2 Packages,v3.1 Snap,v3.1 Packages,v3.0 Snap,v3.0 Packages,v2.9 Snap,v2.9 Packages" view="UI"]
VM host management has been redesigned to support NUMA/SR-IOV configurations and hugepages from the API/CLI. Users can:

- See resources per NUMA node.
- See resources for VM hosts bearing NUMA nodes.
- See the alignment between VM host interfaces and NUMA nodes.

Via the CLI, users can see more details about NUMA-bearing VM host resources and configure hugepages. Select the relevant "CLI" link in the top menu to access this information.
[/tab]
[tab version="v3.4 Snap,v3.4 Packages,v3.3 Snap,v3.3 Packages,v3.2 Snap,v3.2 Packages,v3.1 Snap,v3.1 Packages,v3.0 Snap,v3.0 Packages,v2.9 Snap,v2.9 Packages" view="CLI"]
VM host management has been redesigned to support NUMA/SR-IOV configurations and hugepages from the API/CLI. Users can:

- See resources per NUMA node.
- See resources for VM hosts bearing NUMA nodes.
- See the alignment between VM host interfaces and NUMA nodes.
- Configure and use hugepages.

[/tab]
[/tabs]

**** About over-committed resources

Over-committed resources are those allocated beyond what's available in the physical resource. Using sliders on the configuration page, you can limit whether MAAS will attempt to overcommit CPU and memory. The input fields to the right of the sliders accept floating-point values from 0 to 10, with a default value of 1.

The following shows four theoretical examples of these ratios and how they affect physical resource allocation:

1.  `8 physical CPU cores  * 1 multiplier     = 8 virtual CPU cores`
2.  `8 physical CPU cores  * 0.5 multiplier   = 4 virtual CPU cores`
3.  `32 physical CPU cores * 10.0 multiplier  = 320 virtual CPU cores`
4.  `128GB physical memory  * 5.5 multiplier  = 704G virtual Memory`

[tabs]
[tab version="v3.4 Snap,v3.4 Packages,v3.3 Snap,v3.3 Packages,v3.2 Snap,v3.2 Packages,v3.1 Snap,v3.1 Packages,v3.0 Snap,v3.0 Packages,v2.9 Snap,v2.9 Packages" view="UI"]
<a href="https://discourse.maas.io/uploads/default/original/1X/27a8f21392af3d29a500e33f99e1f79c578cf29c.jpeg" target = "_blank"><img src="https://discourse.maas.io/uploads/default/original/1X/27a8f21392af3d29a500e33f99e1f79c578cf29c.jpeg"></a> 
[/tab]
[tab version="v3.4 Snap,v3.4 Packages,v3.3 Snap,v3.3 Packages,v3.2 Snap,v3.2 Packages,v3.1 Snap,v3.1 Packages,v3.0 Snap,v3.0 Packages,v2.9 Snap,v2.9 Packages" view="CLI"]
Please use the MAAS UI to view overcommit ratios for NUMA resources.
[/tab]
[/tabs]

Over-committing resources allows a user to compose many MAAS-managed machines without worrying about the physical limitations of the host. For example, on a physical host with four cores and 12 GB of memory, you could compose four libvirt machines, each using two cores and 4 GB of memory.  This arrangement over commits the available physical resources. Provided you never run all four VMs simultaneously, you would have all the benefits of MAAS-managed VMs without over-taxing your host.



* VM hosting

A VM host is simply a machine which can run virtual machines (VMs) by allocating  resources across the VMs you want to create.  If needed, you can over-commit resources, allocating more resources than actually available, so long as you don't try to use more than the VM host has available at any one time. Once MAAS has enlisted, commissioned, and allocated a newly-added machine, you can deploy it as a VM host.  Alternatively, you can create a VM host from a machine you've already got running.

VM hosts are particularly useful for Juju integration, allowing for dynamic allocation of VMs with custom interface constraints. Alternatively, if you would like to use MAAS to manage a collection of VMs, the robust web UI allows you to easily create and manage VMs, logically grouped by VM host. Six conspicuous features include:

- Juju integration
- At-a-glance visual tools for easy resource management
- Set overcommit ratios for physical resources such as CPU and RAM
- Assign VMs to resource pools to segregate your VMs into logical groupings
- Track VM host storage pool usage and assign default storage pools
- Create VMs on multiple networks, specified by space, subnet, VLAN, or IP address

Simply put, a VM host is a machine which is designated to run virtual machines (VMs). A VM host divides its resources (CPU cores, RAM, storage) among the number of VMs you want to create, based on choices that you make when creating each VM. It is also possible to overcommit resources – that is, use more resources than the VM host actually has available – as long as you use the VMs carefully. Once MAAS has enlisted, commissioned, and allocated a newly-added machine, you can deploy it as a VM host.


** About VM host storage pools

"Storage pools” are storage resources managed by a VM host. A storage pool is a given amount of storage set aside for use by VMs. A pool can be organised into storage volumes, assigned to VMs as individual block devices.

[note]
For LXD VM hosts, each VM can be assigned a single block device from the storage pool.
[/note]

[tabs]
[tab version="v3.4 Snap,v3.4 Packages,v3.3 Snap,v3.3 Packages,v3.2 Snap,v3.2 Packages,v3.1 Snap,v3.1 Packages,v3.0 Snap,v3.0 Packages,v2.9 Snap,v2.9 Packages" view="UI"]
The MAAS web UI displays information about each VM host's storage pools so you can understand your resource usage at a glance:

<a href="https://discourse.maas.io/uploads/default/original/1X/3387f256f9bd02f7fc2079f119377305256973c8.jpeg" target = "_blank"><img src="https://discourse.maas.io/uploads/default/original/1X/3387f256f9bd02f7fc2079f119377305256973c8.jpeg"></a>
[/tab]
[tab version="v3.4 Snap,v3.4 Packages,v3.3 Snap,v3.3 Packages,v3.2 Snap,v3.2 Packages,v3.1 Snap,v3.1 Packages,v3.0 Snap,v3.0 Packages,v2.9 Snap,v2.9 Packages" view="CLI"]
Retrieve VM host storage pool information with the following command:

```
maas $PROFILE vm-host read $VM_HOST_ID
```

or, to get tabular output, try:

```
maas admin vm-host read 5 \
| jq -r '(["NAME","TYPE","PATH","TOTAL","USED","AVAIL"]) 
| (,. map(length*"-"))), (.storage_pools[] 
| [.name, .type, .path, .total, used, .available]) | @tsv' \
| column -t
```

[/tab]
[/tabs]

* VMs and NUMA/SR-IOV

MAAS provides extensive optimisation tools for using NUMA with virtual machines. Earlier versions of MAAS guarantee that machines are assigned to a single NUMA node that contains all the machine's resources. As of 2.9, MAAS now allows you to see how many VMs are allocated to each NUMA node, along with the allocations of cores, storage, and memory. You can quickly spot a VM running in multiple NUMA nodes, and optimise accordingly, with instant updates on pinning and allocations. You can also tell which VMs are currently running.

In addition, you can get a bird's-eye view of network configuration:

- You can see which VM NIC/bond is connected to which NUMA node.
- You can tell when a NIC is connected to a different NUMA node.
- You can tell if one of multiple NICs is not in the correct node.
- You can confirm the subnet and space connecting to a VM.
- You can confirm that a VM has the desired network properties, such as latency and throughput.
- You can identify NICs that support SR-IOV and tell how many VFs are available.

MAAS also shows hugepages information (if they are in use) and prevents overcommit when using them. Hugepages essentially allow a much larger memory cache associated with the core. This obviously reduces the number of times a core has to access memory, but because the core must swap entire hugepages, optimising usage of them can be complex. MAAS helps you create these optimisations by giving you a discrete view of hugepages associated with your VM, helping you decide whether you need to use them or not.

** About support for NUMA, SR-IOV, and hugepages

[tabs]
[tab version="v3.4 Snap,v3.4 Packages,v3.3 Snap,v3.3 Packages,v3.2 Snap,v3.2 Packages,v3.1 Snap,v3.1 Packages,v3.0 Snap,v3.0 Packages,v2.9 Snap,v2.9 Packages" view="UI"]
VM host management has been redesigned to support NUMA/SR-IOV configurations and hugepages from the API/CLI. Users can:

- See resources per NUMA node.
- See resources for VM hosts bearing NUMA nodes.
- See the alignment between VM host interfaces and NUMA nodes.

Via the CLI, users can see more details about NUMA-bearing VM host resources and configure hugepages. Select the relevant "CLI" link in the top menu to access this information.
[/tab]
[tab version="v3.4 Snap,v3.4 Packages,v3.3 Snap,v3.3 Packages,v3.2 Snap,v3.2 Packages,v3.1 Snap,v3.1 Packages,v3.0 Snap,v3.0 Packages,v2.9 Snap,v2.9 Packages" view="CLI"]
VM host management has been redesigned to support NUMA/SR-IOV configurations and hugepages from the API/CLI. Users can:

- See resources per NUMA node.
- See resources for VM hosts bearing NUMA nodes.
- See the alignment between VM host interfaces and NUMA nodes.
- Configure and use hugepages.

[/tab]
[/tabs]

** About over-committed resources

Over-committed resources are those allocated beyond what's available in the physical resource. Using sliders on the configuration page, you can limit whether MAAS will attempt to overcommit CPU and memory. The input fields to the right of the sliders accept floating-point values from 0 to 10, with a default value of 1.

The following shows four theoretical examples of these ratios and how they affect physical resource allocation:

1.  `8 physical CPU cores  * 1 multiplier     = 8 virtual CPU cores`
2.  `8 physical CPU cores  * 0.5 multiplier   = 4 virtual CPU cores`
3.  `32 physical CPU cores * 10.0 multiplier  = 320 virtual CPU cores`
4.  `128GB physical memory  * 5.5 multiplier  = 704G virtual Memory`

[tabs]
[tab version="v3.4 Snap,v3.4 Packages,v3.3 Snap,v3.3 Packages,v3.2 Snap,v3.2 Packages,v3.1 Snap,v3.1 Packages,v3.0 Snap,v3.0 Packages,v2.9 Snap,v2.9 Packages" view="UI"]
<a href="https://discourse.maas.io/uploads/default/original/1X/27a8f21392af3d29a500e33f99e1f79c578cf29c.jpeg" target = "_blank"><img src="https://discourse.maas.io/uploads/default/original/1X/27a8f21392af3d29a500e33f99e1f79c578cf29c.jpeg"></a> 
[/tab]
[tab version="v3.4 Snap,v3.4 Packages,v3.3 Snap,v3.3 Packages,v3.2 Snap,v3.2 Packages,v3.1 Snap,v3.1 Packages,v3.0 Snap,v3.0 Packages,v2.9 Snap,v2.9 Packages" view="CLI"]
Please use the MAAS UI to view overcommit ratios for NUMA resources.
[/tab]
[/tabs]

Over-committing resources allows a user to compose many MAAS-managed machines without worrying about the physical limitations of the host. For example, on a physical host with four cores and 12 GB of memory, you could compose four libvirt machines, each using two cores and 4 GB of memory.  This arrangement over commits the available physical resources. Provided you never run all four VMs simultaneously, you would have all the benefits of MAAS-managed VMs without over-taxing your host.


* What is new with MAAS 2.7

** MAAS 2.7.3 released

On 24 August 2020, MAAS 2.7.3 was released, replacing the `2.7/stable` channel in snap and the [ppa:maas/2.7](https://launchpad.net/~maas/+archive/ubuntu/2.7)`↗`.  You can update your 2.7 release to 2.7.3 by with:

    snap refresh --channel=2.7/stable

or by using the aforementioned PPA.  The focus for this release has been [bugfixing](https://launchpad.net/maas/+milestone/2.7.3rc1)`↗` -- there were no changes to MAAS since RC1.

Thanks to everyone who reported the issues with previous 2.7 releases and helped us with the logs.

** MAAS 2.7.2 released

On 30 July 2020, MAAS 2.7.2 was released, replacing the `2.7/stable` channel in snap and the [ppa:maas/2.7](https://launchpad.net/~maas/+archive/ubuntu/2.7)`↗`.  You can update your 2.7 release to 2.7.2 by with:

    snap refresh --channel=2.7/stable

or by using the aforementioned PPA.  The focus for this release has been [bugfixing](https://launchpad.net/maas/+milestone/2.7.2rc1)`↗` -- there were no changes to MAAS since RC1.

Thanks to everyone who reported the issues with previous 2.7 releases and helped us with the logs.

** Upgrading from MAAS 2.6 snap 

If you are using the MAAS 2.6 snap, which had to be installed with `--devmode`, you can update to 2.7 with the following parameters:

    snap refresh maas --devmode --channel=2.7

Be aware that you will still be in `--devmode`, which means the snap won't automatically upgrade.  You'll have to check manually for updates (via `snap refresh`).  Once you’re upgraded to MAAS 2.7 using this method, future snap updates won’t require the devmode parameter. So, for example, when a later version of 2.7 (or even 2.8) is released, you will be able to `snap refresh` to those channels and get out of devmode.  Once refreshed out of devmode in this way, you'll get updates for point releases automatically.

An alternative to avoid devmode would be to do a clean install of MAAS 2.7, that is, removing 2.6 with `snap remove maas` and reinstalling MAAS 2.7 with:

    snap install --channel=2.7 maas

Note that you can check the devmode status of your snap with:

    snap list maas


---

** MAAS 2.7 released

Following on from MAAS 2.6.2, we are happy to announce that MAAS 2.7 is now available. This release features some critical bug fixes, along with some exciting new features:

*** CentOS 8 image support

Users can now deploy CentOS 8 images in MAAS. The Images page in the MAAS UI will now offer a choice to select and download CentOS 8. Note that users of previous versions may see CentOS 8 as an available option, but cannot download or deploy it.

*** Network testing features

MAAS 2.7 brings a slate of new network testing and link detection features, as detailed below.

**** Network link disconnect detection

MAAS 2.7 can check whether links are connected or disconnected. Previously, when commissioning, you couldn’t detect unplugged cables. Now you can. You will have to take a couple of steps for existing MAAS deployments: First, you will have to upgrade to 2.7, then run commissioning again to see if a link is disconnected. But you no longer have to puzzle over what’s broken when this happens.

MAAS will report disconnected network cables, and users will receive a warning when trying to configure a disconnected interface. Administrators will be able to change cable connection status through both API and UI after manually rectifying the situation.

**** Slow network link detection

MAAS 2.7 makes sure you’re getting the most out of your link speed. As servers and hardware get faster — 10G, 40G, even 100G NICS — the chances increase that you might plug your 10G NIC into a 1G switch, for example. Previously, with MAAS, you’d be stuck with the speed of the slowest link, but there wasn’t a way to verify your link speed without recommissioning. Depending on your physical hardware, that might still be an issue, but the MAAS UI can now warn you if your interface is connected to a link slower than what the interface supports. And all information shown in the UI is available via the API, as well. You can still replace a slow switch without recommissioning.

MAAS will automatically detect link and interface speed during commissioning and report them via the API/UI. Administrators will be able to change or update the link and interface speeds via the API/UI after manual changes to the connection. MAAS 2.7 will also report link speed, allowing users to filter and list machines by their link speed in the UI. Users can also employ this information to allocate machines by their link speed in the API.

**** Network validation scripts and testing

MAAS 2.7 allows you to configure network connectivity testing in a number of ways. If you’ve used MAAS, you know that if it can’t connect to the rack controller, deployment can’t complete. Now MAAS can check connectivity to the rack controller and warn you if there’s no link, long before you have to try and debug it. For example, if you can’t connect to your gateway controller, traffic can’t leave your network. MAAS can now check this link and recognise that there’s no connectivity, which alleviates a lot of annoying (and sometimes hard-to-detect) network issues.

Users can now test their network configuration to check for:

- Interfaces which have a broken network configuration
- Bonds that are not fully operational
- Broken gateways, rack controllers, and Internet links

In addition, Internet connectivity testing has been greatly expanded. Previously, MAAS gave a yes/no link check during network testing. Now you can give a list of URLs or IP addresses to check. In the ephemeral environment, standard DHCP is still applied, but when network testing runs, we can apply your specific configuration for the duration of the test. While all URLs / IPs are tested with all interfaces, we do test each of your interfaces individually, including breaking apart bonded NICS and testing each side of your redundant interfaces. You can also run different tests on each pass, e.g., a different set of URLs, although each run would be a different testing cycle. For testing individual interfaces, you can use the API.

Of course, the main network feature available in 2.7 is improved — and customisable — network testing. You can now create your own commissioning scripts and tests related to networking. You can create your own network tests (e.g., a network throughput test) and run them during the network testing portion of the MAAS workflow. There are no particular restrictions on these scripts, so you can test a wide variety of possible conditions and situations.

Administrators can upload network tests and test scripts, as well as create tests which accept an interface parameter, or scripts which apply custom network configurations. Users can then specify (unique) parameters using the API, override machines which fail network testing (allowing their use), and suppress individual failed network tests. All users benefit from enhanced reporting, as they are now able to see the overall status of all interfaces via the API, the UI Machine list, and the UI Interfaces tab; review the health status from all interface tests; and view the interface test results by interface name and MAC.

**** Live IP address detection to prevent address conflicts

In some cases, MAAS connects with subnet which are not empty. Normally, the user has to tell MAAS about IP addresses which are already assigned on that subnet, and if that step is skipped, MAAS may assign and in-use IP address to one of the machines under its control, leading to an IP conflict.

MAAS 2.7 alleviates this problem by detecting IPs in use on a subnet, so that it can avoid assigning that IP to a MAAS-managed machine. The system is not perfect; for example, if NIC on a subnet-connected machine is in a quiescent state -- or turned off -- MAAS may not detect it before duplicating the IP. Note that at least one rack controller must have access to the previously-assigned machine in order for this feature to work. MAAS 2.7 will also recognise when the subnet ARP cache is full and re-check the oldest IPs added to the cache to search for free IP addresses.

<a href="#heading--Introductory-NUMA-/-SR-IOV-support">*** id="heading--Introductory-NUMA-/-SR-IOV-support">Introductory NUMA / SR-IOV support

NUMA (Non-Uniform Memory Access) is a useful way of achieving high-efficiency computing, by pairing a CPU core with a very fast connection to RAM and PCI buses. Typically the CPU socket and the closest banks of DIMM constitute a NUMA node. Obviously, if you’re deploying a MAAS machine under NUMA to get maximum performance, you would like for that machine to be confined to a single NUMA node. MAAS 2.7 introduces this capability.

MAAS will display the NUMA node index and details. Users can also see the count of available NUMA nodes, along with CPU cores, memory, NICS, and node spans for bonds and block devices (although node-spanning may not produce suitable performance). From a reporting standpoint, users can filter machines by CPU cores, memory, subnet, VLAN, fabric, space, storage, and RAID.

Similarly, the SR-IOV (Single Root I/O Virtualisation) allows a PCIe device (e.g, a NIC) to appear to be multiple separate devices. A network adaptor can be subdivided into multiple adaptors by adding a Virtual Function (VF). MAAS 2.7 supports the use of multiple VF adaptors to intelligently use SR-IOV edge clouds, by allowing users to see that a NIC supports SR-IOV, along with the supported VF counts.

The goal of this feature is to help users choose the right machine to deploy an edge cloud.

*** Settings and user preferences redesign

As part of our efforts to make the UI faster and more responsive, we have completely redesigned the Settings and User preferences within the MAAS UI.

*** Strictly-confined Snap support

With 2.7, MAAS is fully confined within the Snap container. No need for installation qualifiers (such as “--devmode”) to permit use of external resources, i.e., outside the Snap container. This means that we will begin to transition to recommending the Snap install as the default (and primary) MAAS install method. This also means that MAAS now gains the benefit of confined snap security features.

*** Update to hardware information gathering methods

MAAS has switched hardware information gathering from lshw/lsblk to lxd output during commissioning, because it more easily provides the information needed to complete resource discovery. Note that this information may not be particularly reliable for your use, so you may need to create your own commissioning scripts if you need something more detailed or specific.

*** Bug fixes

A number of bug fixes (see the [list in Launchpad](https://bugs.launchpad.net/maas/+bugs?field.milestone%3Alist=87757&field.milestone%3Alist=89662&field.milestone%3Alist=89714&field.milestone%3Alist=89840&field.milestone%3Alist=89954&field.milestone%3Alist=89682&field.status%3Alist=FIXRELEASED))`↗`.

* What is new with MAAS 2.8
***>MAAS 2.8.4 released</h3>

MAAS 2.8.4 has been released, replacing the `2.8/stable` channel in snap and the [ppa:maas/2.8](https://launchpad.net/~maas/+archive/ubuntu/2.8)`↗` .  You can update your 2.8 release to 2.8.4 with the command:

    snap refresh --channel=2.8/stable

or by using the aforementioned PPA.  2.8.4 has a single [bug fix - LP:1917372 ](https://bugs.launchpad.net/maas/+bug/1917372)`↗` in it. No other changes have been made to MAAS with this release.

***>MAAS 2.8.3 released</h3>

MAAS 2.8.3 has been released, replacing the `2.8/stable` channel in snap and the [ppa:maas/2.8](https://launchpad.net/~maas/+archive/ubuntu/2.8)`↗`.  You can update your 2.8 release to 2.8.3 with the command:

    snap refresh --channel=2.8/stable

or by using the aforementioned PPA.  The focus for this release has been [bugfixing](https://bugs.launchpad.net/maas/+milestone/2.8.3rc1)`↗` and [more bugfixing](https://bugs.launchpad.net/maas/+milestone/2.8.3)`↗`.  No other changes have been made to MAAS with this release.

Here's a summary of the bugs that were fixed in 2.8.3:

- [DNS Servers not set as expected](https://bugs.launchpad.net/maas/+bug/1881133)`↗`: MAAS was using the region controller IP in dhcpd.conf when other DNS servers are present, effectively bypassing the rack controller proxy to machines.  The code was updated to use the region controller IP for DNS only if no other DNS servers are found.

- [not able to import new image after MAAS upgrade](https://bugs.launchpad.net/maas/+bug/1890468)`↗`: After upgrading from MAAS 2.6.2 to snap-MAAS 2.8.1, it is impossible to import a new image.  This was fixed in MAAS 2.8.3.

- [an unlogged chown permission error leaves a temporary file behind](https://bugs.launchpad.net/maas/+bug/1883748)`↗`: Fixed in MAAS 2.8.3.

- [smartctl-validate fails to detect that NVME device is SMART-capable](https://bugs.launchpad.net/maas/+bug/1904329)`↗`: MAAS 2.8.2 fails to realize that WD Black gaming NVMEs are smart devices, hence MAAS doesn't display attributes.  This is fixed in 2.8.3.

- [cannot use release API on stuck observed IPs](https://bugs.launchpad.net/maas/+bug/1898122)`↗`: The CLI/API provide commands for forcing the release of an IP, but MAAS 2.8.2 was not allowing these commands to run successfully.  This was fixed.  There is also a workaround for those who cannot upgrade to 2.8.3 right away:

```
    $ sudo -u postgres psql $MAAS_DB -c "UPDATE maasserver_staticipaddress SET alloc_type=5 WHERE ip = '$IP_ADDRESS' AND alloc_type=6;"
    $ maas $PROFILE ipaddresses release ip='$IP_ADDRESS' force=true
```
- [MAAS is unable to handle duplicate UUIDs](https://bugs.launchpad.net/maas/+bug/1893690)`↗`: The firmware for Dell servers (and possibly others)`↗` has a bug whereby they use the service number for the UUID, which is not guaranteed to be unique.  This caused MAAS commissioning to fail. The code was modified in 2.8.3 to detect and remove duplicate UUIDs, allowing MAAS to fall back to the MAC address.  There is also a database workaround for those who cannot upgrade to 2.8.3 right away:
```
     $ sudo -u postgres psql $MAAS_DB -c "UPDATE maasserver_node SET hardware_uuid=NULL where hardware_uuid='$DUPLICATE_UUID'";
```
- [Ubuntu 20.04 pxe installation fails...](https://bugs.launchpad.net/curtin/+bug/1876258)`↗`:
When trying to PXE install Ubuntu 20.04, the installation fails with "no such file or directory, /dev/disk/by-id exception." This was an issue with block devices being created without serial numbers, bug fixed in curtin and released with 2.8.3.

- [Failed to allocate the required AUTO IP addresses after 2 retries](https://bugs.launchpad.net/maas/+bug/1902425)`↗`: MAAS incorrectly perceives that there are no available IP addresses, when in fact, there are plenty still available.  This is fixed in 2.8.3.

- [maas 2.9 rc1 machines create error (backport)](https://bugs.launchpad.net/maas/+bug/1904398)`↗`: Adding `commission=true` to a CLI machine creation command produces an error.  This was fixed in 2.9 and backported to 2.8.3.

- [Lists of LXD nodes are represented in an incompatible data structure](https://bugs.launchpad.net/maas/+bug/1910473)`↗`: Fixed in 2.8.3.

- Deselecting all architectures in the Ubuntu extra archtectures repo [blocks all deployments (backport)](https://bugs.launchpad.net/maas/+bug/1894116)`↗`.  The default architectures have been changed to prevent this issue. This was fixed in 2.9 and backported to 2.8.3.

- [Can't commission without a test (backport)](https://bugs.launchpad.net/maas/+bug/1884278)`↗`: MAAS 2.8 does not allow machines to be commissioned with zero tests selected; this occurs only for multiple machines, and only when commissioning from the UI.  This was fixed in 2.9 and backported to 2.8.3.

Note that there is a workaround for those not ready to upgrade to 2.8.3, specifically, using the CLI to commission machines without testing them:

    maas $PROFILE machine commission $SYSTEM_ID testing_scripts=none

- [UI should not autoselect noauto commissioning scripts (backport)](https://bugs.launchpad.net/maas/+bug/1884827)`↗`: Previously, users gained the ability to upload commissioning scripts which do not automatically run, but the UI ignores the "noauto" tag and runs the script anyway.  This was fixed in 2.9 and backported to 2.8.3.

- [ipmi-config command not found in snap (backport)](https://bugs.launchpad.net/maas/+bug/1891331)`↗`: The `ipmi-config` cannot be found in a MAAS snap, due to path confusion in the wrapper script. This was fixed in 2.9 and backported to 2.8.3.

- [Admin users cannot change other user's passwords via UI (backport)](https://bugs.launchpad.net/maas/+bug/1894727)`↗`: An administrator is unable to change users passwords via the UI.  This was fixed in 2.9 and backported to 2.8.3.

- [all rack addresses in vlan are included in list of nameservers sent to deployed server (backport)](https://bugs.launchpad.net/maas/+bug/1896684)`↗`: From the Bug Description: "MAAS forces all rack addresses for all subnets in a single vlan to any system deployed into any of those subnets. If the deployed systems are isolated, with no gateway configured, they may end up with broken DNS due to having nameservers configured which are not reachable."
This was fixed in 2.9 and backported to 2.8.3.

***>MAAS 2.8.2 released</h3>

On 1 September 2020, MAAS 2.8.2 was released, replacing the `2.8/stable` channel in snap and the [ppa:maas/2.8](https://launchpad.net/~maas/+archive/ubuntu/2.8)`↗`.  You can update your 2.8 release to 2.8.2 with the command:

    snap refresh --channel=2.8/stable

or by using the aforementioned PPA.  The focus for this release has been [bugfixing](https://launchpad.net/maas/+milestone/2.8.2rc1)`↗` -- there were no changes to MAAS since RC1.

Thanks to everyone who reported the issues with previous 2.7 releases and helped us with the logs.

***>MAAS 2.8 released</h3>

Following on from MAAS 2.7, we are happy to announce that MAAS 2.8 is now available. This release features some critical bug fixes, along with some exciting new features.

****>Some questions you may have:</h4>

- [What are the new features & fixes for 2.8?](#heading--2-8-release-notes)
- [What known issues should I be aware of?](#heading--2-8-known-issues)
- [How do I install MAAS 2.8 as a snap?](https://maas.io/docs/snap/2.8/ui/installation)`↗`
- [How do I upgrade my MAAS 2.7 snap to a MAAS 2.8 snap?](https://maas.io/docs/snap/2.8/ui/installation#heading--upgrade-maas-snap)`↗`
- [How do I install MAAS 2.8 from packages?](https://maas.io/docs/deb/2.8/ui/installation)`↗`
- [What bugs were fixed in this release?](#heading--bug-fixes)

** LXD-based VM host support (Beta)</h2>

MAAS 2.8 adds the beta capability to use LXD-based VM hosts and virtual machines (VMs), in addition to the [libvirt](https://ubuntu.com/server/docs/virtualization-libvirt)-based`↗` VM hosts/VMs already available.  These new LXD VM hosts use the same underlying technology as libvirt (QEMU)`↗`. Unlike libvirt KVMs, though, LXD VMs can be managed without requiring SSH access to the VM host. LXD are remotely accessed via secure HTTP transport, which provides better security for LXD-based VMs. In addition, LXD has a better API, and is part of a much larger constellation of enterprise software, offering a wider range of future features and use cases.

** UI performance improvements for the machine listing page

Within MAAS 2.8, we have made a number of performance improvements to everything related to the machine listing.  Some of the most visible changes involve the way that long lists are presented within categories (see the example below), but there are a number of other changes that make the list easier and more efficient to use.

<a href="https://discourse.maas.io/uploads/default/original/1X/b4ec4124225f052fb8646f754c22d287fffcc850.jpeg" target = "_blank"><img src="https://discourse.maas.io/uploads/default/original/1X/b4ec4124225f052fb8646f754c22d287fffcc850.jpeg"></a> 

Among those other changes are persisting UI state for grouping, new grouping options, bookmark-able URLs with filter and search parameters, and many other performance improvements. If you're interested in more details, see this [blog post](https://ubuntu.com/blog/building-a-cross-framework-ui-with-single-spa-in-maas-2-8)`↗`.

** Support for using an external/remote PostgreSQL MAAS database with the snap version of MAAS

In order to make MAAS more scalable, we have separated the MAAS database from the MAAS snap, so that the DB can be situated separately.  MAAS 2.8 now allows the MAAS DB to be located outside the snap on localhost, or on a separate, external or remote server.  We complement this capability with extensive instructions for setting up and managing this configuration.  To support those who are testing MAAS, we've also provided a test DB configuration that embeds the database in a separate snap that can easily be connected to MAAS.

** Bug fixes</h2>

We've also fixed number of bugs (see the [list in Launchpad](https://bugs.launchpad.net/bugs/+bugs?field.milestone%3Alist=89978&field.milestone%3Alist=90576&field.milestone%3Alist=90599&field.milestone%3Alist=90640&field.milestone%3Alist=90645&field.milestone%3Alist=90722&field.milestone%3Alist=91005&field.milestone%3Alist=91123&field.milestone%3Alist=91124&field.milestone%3Alist=91180&field.status%3Alist=FIXRELEASED))`↗`.  Notable among these are the following:

- [MAAS event table](https://bugs.launchpad.net/maas/+bug/1860619)`↗`: Power events are now being logged differently to reduce log sizes and improve performance.

- [Unprivileged users controlling services](https://bugs.launchpad.net/maas/+bug/1864201)`↗`: Unprivileged users can no longer start, stop, or restart services via HTTP channels.

- [Adding KVMs to snap-installed MAAS](https://bugs.launchpad.net/maas/+bug/1852405)`↗`: SSH key usage has been updated so that KVMs can now be added to snap-installed MAAS without difficulty.

- [Trouble editing physical interfaces in GUI](https://bugs.launchpad.net/maas/+bug/1864241)`↗`: It is now possible to edit physical interface parameters, when appropriate, from the web UI.

- [Subnet pages slow to load](https://bugs.launchpad.net/maas/+bug/1873430)`↗`: Subnet pages now load more quickly and efficiently.

- [Trouble loading multiple MAC addresses](https://bugs.launchpad.net/maas/+bug/1865122)`↗`: You can now reliably load multiple MAC addresses using the web UI.

- [Disabling DNS on regiond subnet breaks DNS](https://bugs.launchpad.net/maas/+bug/1871584)`↗`: This problem has been resolved.

** Known issues</h2>

- **Browser caching issue:** There is a known issue with browser caching on some MAAS pages.  If you initially encounter a page which does not appear to be correctly formatted, please manually clear your browser cache (**not Ctrl-F5**) and it should restore the page to normal.  You manually clear your browser cache, for example, in the "History" section of the menu on a Chrome browser.

- **Extra power types when adding chassis:** ([see bug report](https://bugs.launchpad.net/maas/+bug/1883743))`↗` When adding a chassis, the "Power type" drop-down will show power types not supported by a chassis.  Selecting one of the non-supported power types will result in the UI blocking the action.  Here is a list of power types supported for chassis creation:
  * `mscm` - Moonshot Chassis Manager
  * `msftocs` - Microsoft OCS Chassis Manager
  * `powerkvm` - Virtual Machines on Power KVM, managed by Virsh
  * `recs_box` - Christmann RECS|Box servers
  * `sm15k` - SeaMicro 1500 Chassis
  * `ucsm` - Cisco UCS Manager
  * `virsh` - virtual machines managed by Virsh
  * `vmware` - virtual machines managed by VMware

- **MAAS keys count in user list is bogus:** ([see bug report](https://bugs.launchpad.net/maas/+bug/1884112))`↗` The count of keys shown in the User list in the UI is wrong.

- **Leftover lock files may be present under some conditions:** Even if you purge an old MAAS Debian package, it can leave lock files in `/run/lock/maas*`.  This can cause issues if you later reinstall MAAS, and the previous MAAS user UID has been reassigned.  At that point, MAAS can't remove those files and create new ones.  If this occurs, it is easily fixed by removing those files manually before reinstalling.

* What is new with MAAS 2.9

**>MAAS 2.9.2 release notes</h2>

We have released MAAS 2.9.2, which contains two new features, and some notable [bug fixes](https://launchpad.net/maas/+milestone/2.9.2)`↗`. The two new features are:

- Proxmox driver: A driver has been added to MAAS 2.9.2 which interacts with the Proxmox API.  Only one URL is needed, though a username and credentials are required.  Credentials can be either a password or an API token.  Note that if you use a token, you have to configure the permissions for the token.  Newly-created Proxmox tokens don't assign any permissions by default, so you must add `power on`, `power off`, and `query power` permissions to the token before using it.

- Power driver Webhook:  A webhook was added to 2.9.2, which allows MAAS to interface with another web service that's running the power commands.  This webhook is provided for interacting with objects that MAAS does not support, that is, the MAAS team supports the driver itself, but whatever is interfacing to the driver is not supported.  This webhook as three URLs, one each for power on, power off, and power query.  Optionally, this webhook also supports a power user and password or token (RFC 6717).  This gives you a way to add your own power drivers without waiting for the driver to be added to MAAS.  There is a [video tutorial](https://discourse.maas.io/t/maas-show-and-tell-proxmox-and-webhook/3754/3)`↗` available on this new feature.

You can also find a [digest](#heading--bug-fixes-2-9-2) of the 2.9.2 bug fixes below.

**>MAAS 2.9.1 release notes</h2>

Building upon MAAS 2.9, we have released 2.9.1, which contains some notable [bug fixes](https://launchpad.net/maas/+milestone/2.9.1)`↗`.  You can find a [digest](#heading--bug-fixes-2-9-1)`↗` of these fixes below.

**>MAAS 2.9 release notes</h2>

Following on from MAAS 2.8, we are happy to announce that MAAS 2.9 is now available.

**** What are the new features & fixes for MAAS 2.9 and MAAS 2.9.1?

- [Focal Fossa (20.04) as default commissioning/deployment release](#heading--focal-default)
- [Support for OpenVswitch bridge type](#heading--openvswitch)
- [Support for NUMA, SR-IOV, and hugepages](#heading--numa)
- [Improved performance for large MAAS installations](#heading--improved-perf-large-maas)
- [New release notifications](#heading--new-release-notifications)
- [IPMI configuration screens](#heading--ipmi-config-screens)
- [Descriptions when marking machines broken](#heading--descrip-mark-mach-broken)
- [Curtin 20.2 now included](#heading--curtin-20-2-included)
- [HTTP boot disabled](#heading--http-boot-disabled)
- [BMC/IPMI default parameter additions](#heading--bmc-param-additions)
- [New global IPMI configuration options](#heading--new-config-options)
- [Addition of IPMI config options to UI](#heading--global-config-settings)
- [New MAAS CLI power command](#heading--maas-power)
- [Commissioning speed improvements](#heading--commissioning-speed)
- [BMC improvements](#heading--bmc-improve)
- [IPMI power driver upgrades](#heading--ipmi-driver)
- [Enlistment script improvements](#heading--enlistment-scripts)
- [Commissioning script improvements](#heading--commissioning-scripts)
- [Commissioning script reordering](#heading--commissioning-reorder)
- [Reader Adaptive Documentation](#heading--rad)
- [Offline documentation](#heading--offline-docs)

****>Six other questions you may have:</h4>

- [What known issues should I be aware of?](#heading--known-issues)
- [How do I install MAAS 2.9?](/t/how-to-install-maas/5128)
- [How do I upgrade my MAAS 2.8 snap to a MAAS 2.9 snap?](/t/how-to-install-maas/5128#heading--upgrade-maas-snap)
- [How do I install MAAS 2.9 from packages?](/t/how-to-install-maas/5128#heading--install-from-packages)
- [How do I upgrade MAAS 2.8 to MAAS 2.9 using packages?](/t/how-to-install-maas/5128#heading--upgrade-via-packages)
- [What bugs are fixed so far in this release?](#heading--bug-fixes)

** Focal Fossa (Ubuntu 20.04 LTS) as default release</h2>

Ubuntu 20.04 LTS (Focal Fossa) is now the default commissioning and deployment release for new MAAS installations.  Machines deployed with Focal may now be registered as KVM hosts.

** Support for OpenVswitch bridge type</h2>

MAAS 2.9 allows you to create an OpenVswitch bridge type when creating a bridge.

** Support for NUMA, SR-IOV, and hugepages</h2>

MAAS 2.9 adds extensive optimisation tools for using NUMA with virtual machines. You can now see how many VMs are allocated to each NUMA node, along with the allocations of cores, storage, and memory. You can quickly spot a VM running in multiple NUMA nodes, and optimise accordingly, with instant updates on pinning and allocations. You can also tell which VMs are currently running.  Using the CLI, you can also pin nodes to specific cores, and configure hugepages for use by VMs.

Specifically, there are five new features available to support NUMA, SR-IOV, and hugepages:

- You can examine resources on a per-NUMA-node basis.
- You can pin nodes to specific cores (CLI only).
- You can see resources for VM hosts supporting NUMA nodes.
- You can see the alignment between VM host interfaces and NUMA nodes.
- You can configure and use hugepages (configurable in CLI only).

This functionality comes with an enhanced panel in the "KVM" details section:

<a href="https://discourse.maas.io/uploads/default/optimized/1X/57245bbbfe6d28e83c9b7fb30e52caf05714eb00_2_485x500.png" target = "_blank">![](upload://5qDhxTUUitJxRzlVYIhaxShZXS9.png)</a>

See the [VM hosting](/t/about-vm-hosting/5068) page for more details, and be sure to use the menu at the top of that page to select your desired build method and interface, so that you'll see the most relevant instructions.

** Improved performance for large MAAS installations</h2>

MAAS 2.9 includes changes to the machine batch size that the UI loads. Previously the UI loaded machines in batches of 25; it now pulls in 25 for the first call, then 100 at a time in subsequent batches.

You can see the results of the investigation in [this video podcast](https://discourse.maas.io/t/maas-show-and-tell-improving-ui-performance-for-large-maas-installs/3515)`↗`.

** New release notifications</h2>

MAAS now includes new release notifications for users and administrators.  These appear when a new release is available:

<a href="https://discourse.maas.io/uploads/default/original/1X/c4f426b9f493a970efcc59c4d948d24fa5f12860.png" target = "_blank"><img src="https://discourse.maas.io/uploads/default/original/1X/c4f426b9f493a970efcc59c4d948d24fa5f12860.png"></a>

Both regular and administrative users can snooze these notifications for two weeks at a time.  Administrative users can opt out of new release notifications completely, preventing notifications for any user of that MAAS.

** IPMI configuration screens</h2>

MAAS now includes UI panels corresponding to the [IPMI power driver upgrades](#heading--ipmi-driver) mentioned earlier:

<a href="https://discourse.maas.io/uploads/default/original/1X/433b28f5dd807caef7c7382f9a877607c2ea2dac.png" target = "_blank"><img src="https://discourse.maas.io/uploads/default/original/1X/433b28f5dd807caef7c7382f9a877607c2ea2dac.png"></a>

This screen can be reached from `Settings | Configuration | Commissioning`.

** Descriptions when marking machines broken</h2>

When marking a machine broken, a description can now be included:

<a href="https://discourse.maas.io/uploads/default/original/1X/69df48044c964d27caf59b60dcf5bf5210894c15.png?" target = "_blank"><img src="https://discourse.maas.io/uploads/default/original/1X/69df48044c964d27caf59b60dcf5bf5210894c15.png?"></a>

This description appears in that machine's row on the machine list.

** Curtin 20.2 now included</h2>

A number of MAAS issues have actually been issues with an older version of Curtin.  MAAS now includes Curtin 20.2, which fixes many of these issues, including [MAAS is changing my boot order!](https://discourse.maas.io/t/maas-is-changing-my-boot-order/3491)`↗`.

** HTTP boot disabled</h2>

MAAS 2.9 disables HTTP boot. There are known issues with HTTP boot in MAAS, as well as known issues for HTTP boot with grub (e.g. https://bugs.launchpad.net/maas/+bug/1899581 `↗`)  This shouldn’t affect machine boot, as machines will normally try PXE as a fallback boot method if HTTP boot fails.  Be aware, though, that machine boot will fail if the BIOS is configured to boot only over HTTP; those machines need to be reconfigured to use PXE.

** 30-maas-01-bmc-config parameter additions</h2>

Four new parameters have been added for IPMI BMC configuration.  These parameters will pull from the global defaults, eliminating the need to set the corresponding parameter in each instance.

- maas_auto_ipmi_user - The username for the MAAS created IPMI user. Default comes from the global configuration setting.
- maas_auto_ipmi_user_password - The password for the MAAS created IPMI user, by default a random password is generated.
- maas_auto_ipmi_k_g_bmc_key - he IPMI K_g pre-shared encryption key to be set when adding the MAAS IPMI user. Note not all IPMI BMCs support setting the k_g key, if MAAS is unable to set the key commissioning will fail. Default comes from the global configuration setting. If an IPMI K_g key is set but the key is rejected by the BMC MAAS will automatically retry without the K_g key. This works around an edge case where some BMCs will allow you to set an K_g key but don’t allow it to be used.
- maas_auto_ipmi_user_privilege_level - The IPMI user privilege level to use when adding the MAAS IPMI user. Possible options are USER, OPERATOR, or ADMIN. Default comes from the global configuration setting.

Note that MAAS will not capture the BMC MAC address when detecting IPMI BMCs.

** New global IPMI configuration options</h2>

Two new global IPMI configuration options have been added:

- maas_auto_ipmi_k_g_bmc_key - sets a global default IPMI BMC key.
- maas_auto_ipmi_user_privilege_level - sets a global default IPMI BMC user privilege level.
    
** Addition of IPMI config options to UI</h2>

You may now set the global configuration options `maas_auto_ipmi_user`, `maas_auto_ipmi_k_g_bmc_key`, and `maas_auto_ipmi_user_privilege_level` on the "Settings" page in the UI under "Commissioning."

** New maas.power CLI command</h2>

Available in all MAAS 2.9 releases is the new `maas.power` CLI command. This command interfaces directly with the supported MAAS power drivers. This command can be used to control the power on a machine before it has been added to MAAS, for all maas supported power drivers.  You can get power status, turn machines on or off, and cycle power.  The `maas.power --help` shows usage details, including syntax for naming each power type (consistent with other MAAS CLI commands).

** IPMI BMC detection improvements (RAD)</h2>

This release adds two improvements to IPMI BMC detection capability:

- The IPMI cipher suite ID will now be automatically detected. MAAS tries to find the most secure cipher suite available. Preference order is 17, 3, 8, 12. If detection fails MAAS will fall back to using freeipmi-tool default, 3, which is what previous versions of MAAS use.
- The IPMI K_g BMC key will now be automatically detected if previously set. 

*** Reader Adaptive Documentation (RAD)</h3>

This release features Reader Adaptive Documentation, which allows you to adapt individual pages to your install method (Snap vs. Deb), version (2.7/2.8/2.9), and preferred interface (CLI/UI). 

** Offline documentation</h2>

This release will include offline documentation for those users whose MAAS installations reside behind firewalls, unable to access the online documentation.

** BMC improvements</h2>

Three substantial improvements to BMC usage have been released:

- IPMI, HP Moonshot, and Facebook Wedge BMC detection and configuration scripts have been migrated to the commissioning script `30-maas-01-bmc-config `.
- BMC detection and configuration are now logged to commissioning results.
- If BMC configuration is skipped a ScriptResult will log this result, and indicate which user chose to skip the configuration step.

*** IPMI power driver upgrades</h3>

Three new configuration options have been added to the IPMI power driver:

- K_g - The BMC Key of the IPMI device. Used to encrypt all traffic to and from the device during communication.
- Cipher Suite ID - The cipher suite to use when communicating with the IPMI BMC. Only 3, 8, 12, and 17 are available as only those enable ciphers for authentication, integrity, and confidentiality. Defaults to 3, freeipmi-tools default. See http://fish2.com/ipmi/bp.pdf `↗` for more information.
- Privilege Level - The IPMI privilege level to use when communicating with the BMC. Defaults to OPERATOR.

See the [2.9 UI](https://maas.io/docs/power-management-reference#heading--ipmi)`↗` or [2.9 CLI](https://maas.io/doc/power-management-reference#heading--ipmi)`↗` power management pages for details.

** Improvements in enlistment scripting</h2>

Script flow and capabilities have been improved in three ways:

<ol>
<li>`maas-run-remote-scripts` can now enlist machines.</li>
<li>Enlistment `user_data` scripts have been removed.</li>
<li> The metadata endpoints `http://<MAAS>:5240/<latest or 2012-03-01>/` and `http://<MAAS>:5240/<latest or 2012-03-01>/meta-data/` are now available anonymously for use during enlistment.</li>
</ol>

** Major improvements to commissioning script capabilities</h2>

Seven major improvements were made to commissioning script flow and capabilities:

<ol>
<li>Commissioning scripts can now send BMC configuration data</li>
<li>Commissioning scripts can now be used to configure BMC data. </li>
<li>The environment variable BMC_CONFIG_PATH is passed to serially run commissioning scripts. </li>
<li>These scripts may write BMC power credentials to BMC_CONFIG_PATH in a YAML format where each key is the power parameter. </li>
<li>If the commissioning script returns 0, it will be sent to MAAS. </li>
<li>The first script to write BMC_CONFIG_PATH is the only script that may configure the BMC, allowing you to override MAAS's builtin BMC detection.</li>
<li>All builtin commissioning scripts have been migrated into the database.</li>
</ol>

** Commissioning script reordering</h2>

Commissioning scripts have been reordered and some are now set to run in parallel. You can now easily set a script to run before the builtin MAAS commissioning scripts. There are nine significant changes:

<ol>
<li>00-maas-03-install-lldpd -> 20-maas-01-install-lldpd</li>

<li>00-maas-05-dhcp-unconfigured-ifaces -> 20-maas-02-dhcp-unconfigured-ifaces</li>

<li>99-maas-05-kernel-cmdline -> maas -kernel-cmdline</li>

<li>00-maas-00-support-info -> maas-support-info(now runs in parallel)</li>

<li>00-maas-01-lshw -> maas-lshw(now runs in parallel)</li>

<li>00-maas-04-list-modaliases -> maas-list-modaliases(now runs in parallel)</li>

<li>00-maas-06-get-fruid-api-data -> maas-get-fruid-api-data(now runs in parallel)</li>

<li>00-maas-08-serial-ports -> maas-serial-ports(now runs in parallel)</li>

<li>99-maas-01-capture-lldp -> maas-capture-lldp(now runs in parallel)</li>
</ol>

See the [commissioning logs page](https://maas.io/docs/commissioning-logs)`↗` for more details on these changes.

** Improvements in commissioning speed and logging</h2>

Four improvements have been made to speed up the commissioning process, mostly by running scripts in parallel (see above):

<ol>
<li>Commissioning should now take 60s.</li>
<li>Logging has been added to 20-maas-01-install-lldpd  (commissioning log output).</li>
<li>Logging added to 20-maas-02-dhcp-unconfigured-ifaces (commissioning log output).</li>
<li>`user_data` can now be input directly into the UI.</li>
</ol>

** Bug fixes

*** Bugs fixed in 2.9.2 release

- In the MAAS UI, ARM servers based on the [Hi1620 ARM SoC appear as an "Unknown model"](https://bugs.launchpad.net/maas/+bug/1897946)`↗`.  A fix was added to [lxd-4.11]( https://discuss.linuxcontainers.org/t/lxd-4-11-has-been-released/10135)`↗`, released 2021-02-05.

- Debian package installs of MAAS [reached an "impossible situation"](https://bugs.launchpad.net/maas/+bug/1910910)`↗` trying to install the MAAS region controller. This is caused because of an unsupported move from the transitional MAAS PPA to the latest PPA.  The workaround is to purge the MAAS packages (and the snap, if installed)`↗`, and install clean with the latest PPA enabled, which will install the correct versions.

- CentOS/RHEL 7+ ship with an unsigned version of GRUB [which breaks UEFI secure boot](https://bugs.launchpad.net/curtin/+bug/1895067)`↗`.  This bug is believed to be fixed in curtin version 21.1, which is now supported by MAAS 2.9.2.

- Debug [could not be properly enabled for MAAS snap version 2.9.1](https://bugs.launchpad.net/maas/+bug/1914588)`↗`.  This has been remedied.

- The MAAS [Backup doc article](https://maas.io/docs/backup)`↗` [was not clearly written with respect to stopping critical services](https://bugs.launchpad.net/maas/+bug/1892998)`↗`.  The article has been reworked to make clear in what order steps should be performed so that services are not stopped before appropriate data has been retrieved for backup.

- Deselecting all architectures in the Ubuntu extra architectures repo [blocks all deployments](https://bugs.launchpad.net/maas/+bug/1894116)`↗`.  The default architectures have been changed to prevent this issue.

- MAAS does not allow [FQDNs to be used in place of IPs](https://bugs.launchpad.net/maas/+bug/1911825)`↗` when a BMC extracts the address from the `power_address`.  This incorrect behaviour was changed in 2.9.2.

- The Proxmox driver [uses a hard-coded port that cannot be customised](https://bugs.launchpad.net/maas/+bug/1914165)`↗`.  This port is now customisable in 2.9.2.

*** Bugs fixed in 2.9.1 release

- It is now possible to [delete an LXD VM in an offline state](https://bugs.launchpad.net/maas/+bug/1908434)`↗`.
- MAAS now handles multiple NUMA nodes even when there are [gaps in the numbering](https://bugs.launchpad.net/maas/+bug/1910473)`↗`.
- A [snap install issue](https://bugs.launchpad.net/maas/+bug/1910909)`↗` was fixed.
- The way MAAS handles [gateways WRT DHCP](https://bugs.launchpad.net/maas/+bug/1910909)`↗` was adjusted.
- A majority of the document [headings have been converted to links](https://bugs.launchpad.net/maas/+bug/1900010)`↗` for easy bookmarking.

*** Bugs fixed in 2.9 release

- MAAS 2.9 includes a fix for [Bug #1894727: Admin uses cannot change other user's passwords via the UI](https://bugs.launchpad.net/maas/+bug/1894727)`↗`.

** Known issues</h2>

**# RAD LHS menu

There is a known issue with the Reader Adaptive Documentation left-hand-side menu (navigation), in that the menu links cannot currently be adapted to the RAD parameters.  This means that selecting a different page in the LHS menu will take you the the RAD for the current recommended version.  Every page that is different in RAD, though, should present you with a top menu, so that you can choose the RAD parameters matching your own preferences.

**# Erroneous message about "missing migration"

When upgrading to any release above 2.8, using packages, you may receive a warning about missing migration(s) -- specifically something that looks like this:

```
Setting up maas-common (2.8.3~rc1-8583-g.9ddc8051f-0ubuntu1~18.04.1) ...
Setting up maas-region-api (2.8.3~rc1-8583-g.9ddc8051f-0ubuntu1~18.04.1) ...
Setting up maas-region-controller (2.8.3~rc1-8583-g.9ddc8051f-0ubuntu1~18.04.1) ...
Operations to perform:
  Apply all migrations: auth, contenttypes, maasserver, metadataserver, piston3, sessions, sites
Running migrations:
  No migrations to apply.
  Your models have changes that are not yet reflected in a migration, and so won't be applied.
  Run 'manage.py makemigrations' to make new migrations, and then re-run 'manage.py migrate' to apply them.
```

This warning message has no effect on the installation or operation of MAAS, so it can be safely ignored.
* What is new with MAAS 3.0
We are happy to announce the release of MAAS 3.0. This release provides new features, along with critical and high-priority [bug fixes](#heading--maas-3-bug-fixes).

**** Cumulative summary of new features in MAAS 3.0

- [PCI and USB devices are now modelled in MAAS](#heading--pci-usb-devices)
- [IBM Z DPM partition support](#heading--ibm-z-dpm)
- [Proxmox support](#heading--proxmox-support)
- [LXD projects support](#heading--lxd-projects-support)
- [PCI and USB device tabs in machine details](#heading--pci-usb-device-tabs)
- [Workload annotations](#heading--workload-annotations)
- [Fixed status bar](#heading--fixed-status-bar)
- [Registering a machine as a VM host during deployment](#heading--machine-register-vm-host-on-deployment)
- [Improvements to MAAS CLI help UX](#heading--maas-cli-ux-improved-help)
- [Disabling boot methods](#heading--disabling-boot-methods)
- [Consolidation of logs and events](#heading--log-consolidation)

MAAS 3.0 can be installed fresh (recommended) with:

```
sudo snap install --channel=3.0/stable maas
```

MAAS 3.0 can be installed from packages by adding the `3.0` PPA:

```
sudo add-apt-repository ppa:maas/3.0
sudo apt update
sudo apt install maas
```

You can then either install MAAS 3.0 fresh (recommended) with:

```
sudo apt-get -y install maas
```

Or, if you prefer to upgrade, you can do so with:

```
sudo apt upgrade maas
```

At this point, you may proceed with a normal installation.

**>Significant changes</h2>

With the advent of MAAS 3.0, we are removing support for RSD pods.  Registered pods and their machines will be removed by MAAS upon upgrading to MAAS 3.0.

Note that new features are categorised by the level of release at which they became accessible to users.

**>New features in MAAS 3.0 RC1</h2>

*** Consolidation of logs and events</h3>

The logs and events tabs have combined and now live under "Logs". In addition to a number of small improvements, navigating and displaying events has been made easier.

<a href="https://discourse.maas.io/uploads/default/optimized/2X/4/497fd5d03ece0308648db33cf144f4cfefc6e5ed_2_690x465.png" target = "_blank"><img src="https://discourse.maas.io/uploads/default/optimized/2X/4/497fd5d03ece0308648db33cf144f4cfefc6e5ed_2_690x465.png"></a>

**** Downloading logs

A helpful new feature is the ability to download the machine and installation output, and if a machine has failed deployment you can now download a full tar of the curtain logs.

<a href="https://discourse.maas.io/uploads/default/optimized/2X/f/fe9df81b810fa3dd502b303b08978d1c60bff933_2_690x465.png" target = "_blank"><img src="https://discourse.maas.io/uploads/default/optimized/2X/f/fe9df81b810fa3dd502b303b08978d1c60bff933_2_690x465.png"></a>

*** Disabling boot methods</h3>

Individual boot methods may now be disabled. When a boot method is disabled MAAS will configure MAAS controlled isc-dhcpd to not respond to the associated [boot architecture code](https://www.iana.org/assignments/dhcpv6-parameters/dhcpv6-parameters.xhtml#processor-architecture)`↗`. External DHCP servers must be configured manually.

To allow different boot methods to be in different states on separate physical networks using the same VLAN ID configuration is done on the subnet in the UI or API. When using the API boot methods to be disabled may be specified using the MAAS internal name or [boot architecture code](https://www.iana.org/assignments/dhcpv6-parameters/dhcpv6-parameters.xhtml#processor-architecture)`↗` in octet or hex form. For example the following disabled i386/AMD64 PXE, AMD64 UEFI TFTP, and AMD64 UEFI HTTP

```
maas $PROFILE subnet update $SUBNET disabled_boot_architectures="0x00 uefi_amd64_tftp 00:10"
```

**** GRUB

- UEFI AMD64 HTTP(00:10) has been re-enabled.
- UEFI ARM64 HTTP(00:13) has been enabled.
- UEFI ARM64 TFTP(00:0B) and UEFI ARM64 HTTP(00:13) will now provide a shim and GRUB signed with the Microsoft boot loader keys.
- grub.cfg for all UEFI platforms has been updated to replace the deprecated `linuxefi` and `initrdefi` commands with the standard `linux` and `initrd` commands.
- GRUB debug may now be enabled by enabling [rackd debug logging](https://discourse.maas.io/t/running-installed-maas-in-debug-logging-mode/168)`↗`.

**>New feature in MAAS 3.0 Beta 4</h2>

*** Improvements to MAAS CLI help UX</h3>

The MAAS CLI will now give you help in more places, supporting a more exploration-based interaction. Specifically, we now show `help` for cases where the required arguments are not met.

Say you're trying to find out how to list the details of a machine in MAAS e.g.

 ```bash
$ PROFILE=foo
$ maas login $PROFILE http://$MY_MAAS:5240/MAAS/ $APIKEY
$ maas $PROFILE
usage: maas $PROFILE [-h] COMMAND ...

Issue commands to the MAAS region controller at http://$MY_MAAS:5240/MAAS/api/2.0/.

optional arguments:
  -h, --help            show this help message and exit

drill down:
  COMMAND
    account             Manage the current logged-in user.
    bcache-cache-set    Manage bcache cache set on a machine.
    bcache-cache-sets   Manage bcache cache sets on a machine.
 
✂️--cut for brevity--✂️
    machine             Manage an individual machine.
    machines            Manage the collection of all the machines in the MAAS.
    node                Manage an individual Node.
    nodes               Manage the collection of all the nodes in the MAAS.
✂️--cut for brevity--✂️

too few arguments
$ maas $PROFILE node 
usage: maas $PROFILE node [-h] COMMAND ...

Manage an individual Node.

optional arguments:
  -h, --help        show this help message and exit

drill down:
  COMMAND
    details         Get system details
    power-parameters
                    Get power parameters
    read            Read a node
    delete          Delete a node

The Node is identified by its system_id.

too few arguments

$ maas $PROFILE node read
usage: maas $PROFILE node read [--help] [-d] [-k] system_id [data [data ...]]

Read a node

positional arguments:
  system_id
  data

optional arguments:
  --help, -h      Show this help message and exit.
  -d, --debug     Display more information about API responses.
  -k, --insecure  Disable SSL certificate check

Reads a node with the given system_id.

the following arguments are required: system_id, data
$ maas $PROFILE node read $SYSTEM_ID
{
    "system_id": "$SYSTEM_ID",
    "domain": {
        "authoritative": true,
        "ttl": null,
        "is_default": true,
        "id": 0,
        "name": "maas",
        "resource_record_count": 200,
        "resource_uri": "/MAAS/api/2.0/domains/0/"
✂️--cut for brevity--✂️
```

We can see at each stage `help` which gives us clues as to what the next step is, finally arriving at a complete CLI command.

**>New feature in MAAS 3.0 Beta 2</h2>

*** Registering a machine as a VM host during deployment</h3>

When deploying a machine through the API, it’s now possible to specify `register_vmhost=True` to have LXD configured on the machine and registered as a VM host in MAAS (similar to what happens with virsh if `install_kvm=True` is provided).

**>New features in MAAS 3.0 Beta 1</h2>

*** PCI and USB devices are now modelled in MAAS</h3>

MAAS 3.0 models all PCI and USB devices detected during commissioning:

- Existing machines will have to be recommissioned to have PCI and USB devices modelled
- PCI and USB devices are shown in the UI and on the API using the node-devices endpoint
- Node devices may be deleted on the API only

On the API using the allocate operation on the machines endpoint a machine may allocated by a device vendor_id, product_id, vendor_name, product_name, or commissioning_driver.

*** IBM Z DPM partition support</h3>

IBM Z14 GA2 (LinuxOne II) and above mainframe partitions are supported in MAAS 3.0.  Note that partitions (LPARS) must pre-configured and use qeth-based network devices (use HyperV sockets and properly-defined storage groups like Hipersockets or OSA adaptors) and properly-defined (FCP) storage groups..  IBM Z DPM Partitions can be added as a chassis, which allows you to add all partitions at once.

*** Proxmox support</h3>

MAAS 3.0 supports Proxmox as a power driver:

- Only Proxmox VMs are supported
- You may authenticate with Proxmox using a username and password or a username and API token
- If an API token is used, it must be given permission to query, start and stop VMs.
- Proxmox VMs can be added as a chassis; this allows you to add all VMs in Proxmox at once.

Note that proxmox support has also been back-ported to MAAS 2.9

*** LXD projects support</h3>

MAAS 3.0 supports the use of LXD projects:

- LXD VM hosts registered in MAAS are now tied to a specific LXD project which MAAS uses to manage VMs
- MAAS doesn’t create or manage machines for VMs in other projects
- MAAS creates the specified project when the VM host is registered, if it doesn't exist
- All existing VMs in the specified project are commissioned on registration
- Resource usage is reported at both project and global levels

*** PCI and USB device tabs in UI machine details</h3>

Tables for detected PCI and USB devices have been added to the machine details page for MAAS 3.0:

<a  href="https://discourse.maas.io/uploads/default/original/2X/8/87f42bafe321d45af94d73216f933a9067f01df2.png" target = "_blank"><img src="https://discourse.maas.io/uploads/default/original/2X/8/87f42bafe321d45af94d73216f933a9067f01df2.png"></a>

These tables include a new skeleton loading state while node devices are being fetched:

<a href="https://discourse.maas.io/uploads/default/original/2X/4/4faa1d8cd996a25ee5089ada924b405bc8903aa4.png" target = "_blank"><img src="https://discourse.maas.io/uploads/default/original/2X/4/4faa1d8cd996a25ee5089ada924b405bc8903aa4.png"></a>

The user is prompted to commission the machine if no devices are detected.

*** Workload annotations</h3>

Workload annotations have been added to the machine summary page in MAAS 3.0.  These allow you to apply `owner_data` to a machine and make it visible while the machine is in allocated or deployed state:

<a href="https://discourse.maas.io/uploads/default/original/2X/5/54682ae5f9c7bb449a1ad222679be0156f27d109.png" target = "_blank"><img src="https://discourse.maas.io/uploads/default/original/2X/5/54682ae5f9c7bb449a1ad222679be0156f27d109.png"></a>

This data is cleared once the machine state changes to something other than "allocated" or "deployed."  The machine list can be filtered by these workload annotations.  MAAS will warn you on the release page to remind you that workload annotations will be cleared upon releasing the machine.

*** Fixed status bar</h3>

In MAAS 3.0, a fixed status bar has been added to the bottom of the screen, which will always display the MAAS name and version on the left.  The right side of the status bar is intended to show contextual data, depending on the UI panel currently displayed. For now, the only data shown is a “last commissioned” timestamp when the user is on a machine details page:

<a href="https://discourse.maas.io/uploads/default/original/2X/3/3a15d7e1d7251f3e928e3054a2aab71f414503bd.png" target = "_blank"><img src="https://discourse.maas.io/uploads/default/original/2X/3/3a15d7e1d7251f3e928e3054a2aab71f414503bd.png"></a>


** MAAS 3.0 bug fixes</h2>

MAAS 3.0 incorporates a large number of bug fixes, summarised in the sections below. Please feel free to validate these fixes at your convenience and give us feedback if anything doesn't seem to work as presented in the bug request.

One particular bug, [#1916860](https://bugs.launchpad.net/maas/+bug/1916860)`↗`, involves failures in the IPMI cipher suite in MAAS 2.9.2 and up, on the Lenovo x3650 M5 (and others).  This particular bug is a not a MAAS bug, but a firmware issue with the subject machines.  While the MAAS team can't fix this (hence the assignment of "Won't Fix"), the team did provide a easy [workaround](https://bugs.launchpad.net/maas/+bug/1916860/comments/27)`↗` which helps circumvent this issue.

*** MAAS 3.0 bug fixes</h3>

Here are the bugs that were 'Fix Released' for the MAAS 3.0 release:

|Number | Description | Importance |
|:------|:------------|:-----------|
|[#1932136](https://bugs.launchpad.net/bugs/1932136)`↗`|interface with a warning is not configured properly| Critical|
|[#1896771](https://bugs.launchpad.net/bugs/1896771)`↗`|interfaces that are not connected are detected as 'connected to slow interface'|Medium|

*** MAAS 3.0 RC2 bug fixes</h3>

Here are the bugs that have been 'Fix Released' in MAAS 3.0 RC2:

| Number | Description | Importance |
|:-------|:------------|:-----------|
|[#1929552](https://bugs.launchpad.net/bugs/1929552)`↗`|Deb-based controller fails to run machine-resources|Critical|
|[#1929576](https://bugs.launchpad.net/bugs/1929576)`↗`|Machines fail to commission using the 3.0 snap due to possible? DNS issue|Critical|
|[#1930227](https://bugs.launchpad.net/bugs/1930227)`↗`|Failure to commission when interfaces has a /32 IP |Critical|  
|[#1930554](https://bugs.launchpad.net/bugs/1930554)`↗`|vm-host CLI command is now named vmhosts  |Critical| 
|[#1930587](https://bugs.launchpad.net/bugs/1930587)`↗`|Different disks with same LUN detected as multipath  |Critical|  
|[#1931215](https://bugs.launchpad.net/bugs/1931215)`↗`|[.0~rc2-10023 testing] two IPs assigned to one interface  |Critical| 
|[#1931838](https://bugs.launchpad.net/bugs/1931838)`↗`|Reverse DNS lookup fails for subnets smaller than /24  |Critical| 
|[#1835292](https://bugs.launchpad.net/bugs/1835292)`↗`|UI should add button to download curtin-logs.tar on deployment failure MAAS |High| 
|[#1908552](https://bugs.launchpad.net/bugs/1908552)`↗`|maas init fails; 'relation "maasserver_routable_pairs" does not exist'  |High|  
|[#1929086](https://bugs.launchpad.net/bugs/1929086)`↗`|LXD VM hosts can't be refreshed if VLANs interfaces aren't named $parent.$vid  |High| 
|[#1929643](https://bugs.launchpad.net/bugs/1929643)`↗`|MAAS often fails and and returns a Pickled object if request header is set to Accept: */*  |Medium|  
|[#1924820](https://bugs.launchpad.net/bugs/1924820)`↗`|Trying to edit a disconnected NIC, then cancelling the edit and connecting the NIC via its drop-down menu, many drop-down menu options then disappear|Undecided| 
*** MAAS 3.0 RC1 bug fixes</h3>

Here are the bugs that have been 'Fix Released' in MAAS 3.0 RC1:

| Number | Description |Importance|
|:-----|:-----|:-----:|
[#1774529](https://bugs.launchpad.net/bugs/1774529)`↗`|Cannot delete some instances of model 'Domain' because they are referenced through a protected foreign key|High|
[#1919001](https://bugs.launchpad.net/bugs/1919001)`↗`|Unable to network boot VM on IBM Z DPM Partition|High|
[#1925249](https://bugs.launchpad.net/bugs/1925249)`↗`|MAAS detects 0 cores, RAM available for KVM host, reports negative availability on pod compose|High|
[#1927292](https://bugs.launchpad.net/bugs/1927292)`↗`|Updating controller has vlan_ids error|High|
[#1927657](https://bugs.launchpad.net/bugs/1927657)`↗`|Global kernel command line options not passed with tags|High|
[#1928098](https://bugs.launchpad.net/bugs/1928098)`↗`|If a workload annotation has a key with spaces in it, filtering doesn't work|High|
[#1926140](https://bugs.launchpad.net/bugs/1926140)`↗`|maas_url not returned to the UI|Medium|
[#1926171](https://bugs.launchpad.net/bugs/1926171)`↗`|Failure processing network information when adding a rack|Medium|
[#1927036](https://bugs.launchpad.net/bugs/1927036)`↗`|Incorrect value "accept_ra" in interface definition|Medium|
[#1927340](https://bugs.launchpad.net/bugs/1927340)`↗`|Deb to snap migration script should support remote Postgres|Medium|
[#1928104](https://bugs.launchpad.net/bugs/1928104)`↗`|New workload annotations don't show up without a reload|Medium|
[#1928115](https://bugs.launchpad.net/bugs/1928115)`↗`|API still refers to "owner data" rather than "workload annotations"|Medium|
[#1922891](https://bugs.launchpad.net/bugs/1922891)`↗`|MAAS configures nodes with incorrect DNS server addresses when using multiple IP addresses|Undecided|
[#1923268](https://bugs.launchpad.net/bugs/1923268)`↗`|grubnet default grub.cfg should try /grub/grub.cfg-${net_default_mac} before /grub/grub.cfg|Undecided|
[#1926164](https://bugs.launchpad.net/bugs/1926164)`↗`|VLAN page shows odd "Rack controllers" value|Undecided|
[#1926510](https://bugs.launchpad.net/bugs/1926510)`↗`|dhcp subnet snippets are NOT inside the pool block|Undecided|
[#1927559](https://bugs.launchpad.net/bugs/1927559)`↗`|Default logical volume size too big in UI|Undecided|
[#1928024](https://bugs.launchpad.net/bugs/1928024)`↗`|UI states commissioning/testing scripts were never uploaded|Undecided|
[#1928226](https://bugs.launchpad.net/bugs/1928226)`↗`|Information "not available" indicates that it''s an error of some sort|Undecided|
[#1928235](https://bugs.launchpad.net/bugs/1928235)`↗`|notes field won't update properly: MAAS 3.0 RC]()`↗`|Undecided|
[#1928324](https://bugs.launchpad.net/bugs/1928324)`↗`|updating a machine zone or resource pool doesn't refresh details|Undecided|
*** MAAS 3.0 Beta 5 bug fixes</h3>

Here are the bugs that have been `Fix Released` in MAAS 3.0 Beta 5:

| Number | Description |Importance|
|:-----|:-----|:-----:|
|[#1925784](https://bugs.launchpad.net/bugs/1925784)`↗`|Processing LXD results failure with loopback|Critical|
|[#1923871](https://bugs.launchpad.net/bugs/1923871)`↗`|LXD vmhost project usage includes usage for other projects|High|
|[#1815084](https://bugs.launchpad.net/bugs/1815084)`↗`|MAAS web ui should perform Save action when Enter/Return is pressed|Medium|
|[#1923867](https://bugs.launchpad.net/bugs/1923867)`↗`|Commissioning fails if NIC gets different PCI address|Medium|

*** MAAS 3.0 Beta 4 bug fixes</h3>

Here are the bugs that have been `Fix Released` in MAAS 3.0 Beta 4:

| Number | Description |Importance|
|:-----|:-----|:-----:|
|[#1923246](https://bugs.launchpad.net/bugs/1923246)`↗`|Unable to compose LXD VM with multiple NICs |High |
|[#1918963](https://bugs.launchpad.net/bugs/1918963)`↗`|Controllers page out of sync with nodes |Undecided |
|[#1923685](https://bugs.launchpad.net/bugs/1923685)`↗`|Unable to deploy LXD VM host on S390X |Undecided |
|[#1923687](https://bugs.launchpad.net/bugs/1923687)`↗`|LXD VM host refresh failure is ignored |Undecided |
|[#1774529](https://bugs.launchpad.net/bugs/1774529)`↗`|Cannot delete some instances of model 'Domain' because they are referenced through a protected foreign key |High |
|[#1914762](https://bugs.launchpad.net/bugs/1914762)`↗`|test network configuration broken with openvswitch bridge |High |
|[#1919001](https://bugs.launchpad.net/bugs/1919001)`↗`|Unable to network boot VM on IBM Z DPM Partition |High |
|[#1917963](https://bugs.launchpad.net/bugs/1917963)`↗`|Add chassis lowers the case of added machines |Low |
|[#1915087](https://bugs.launchpad.net/bugs/1915087)`↗`|2.9 UI is broken, seems to loop between user intro and machines pages endlessly |High |
|[#1923842](https://bugs.launchpad.net/bugs/1923842)`↗`|Can't use action menu on machine details page |High |
|[#1917667](https://bugs.launchpad.net/bugs/1917669)`↗`|Commissioning/testing scripts no longer show ETA or progress |Undecided |
|[#1917669](https://bugs.launchpad.net/bugs/1917669)`↗`|No way to view previous commissioning or testing script results |Undecided |
|[#1917670](https://bugs.launchpad.net/bugs/1917670)`↗`|Storage and interface tests not assoicated with a device |Undecided |
|[#1917671](https://bugs.launchpad.net/bugs/1917671)`↗`|Commissioning/testing scripts not updated after starting commissioning or testing |Undecided |
|[#1917794](https://bugs.launchpad.net/bugs/1917794)`↗`|Unable to view full history of events in UI |Undecided |
|[#1918964](https://bugs.launchpad.net/bugs/1918964)`↗`|UI shows action unavailable after performing action |Undecided |
|[#1918966](https://bugs.launchpad.net/bugs/1918966)`↗`|Tabs aren't always underscorred |Undecided |
|[#1918971](https://bugs.launchpad.net/bugs/1918971)`↗`|UI does not autofill size on storage tab |Undecided |
|[#1923524](https://bugs.launchpad.net/bugs/1923524)`↗`|Unable to delete LXD composed machine on KVM page |Undecided |

*** MAAS 3.0 Beta 3 bug fixes</h3>

Here are the bugs that have been `Fix Released` in MAAS 3.0 Beta 3:

| Number | Description |Importance|
|:-----|:-----|:-----:|
|[#1922569](https://bugs.launchpad.net/bugs/1922569)`↗`| Create KVM fails in MAAS 3.0 Beta with a project error |High|
|[#1923251](https://bugs.launchpad.net/bugs/1923251)`↗`| Creating an LXD VM host now requires a project name |High|
|[#1809939](https://bugs.launchpad.net/bugs/1809939)`↗`| dhcp snippet create fail when dhcp subnet is relayed |Medium|
|[#1913460](https://bugs.launchpad.net/bugs/1913460)`↗`| Add option to pick whether to keep or decompose machines in a VM host |Undecided|
|[#1922787](https://bugs.launchpad.net/bugs/1922787)`↗`| make "LXD" the default VM host in MAAS UI (rather than virsh) |Undecided|
|[#1922876](https://bugs.launchpad.net/bugs/1922876)`↗`| Deploy KVM hosts with LXD by default |Undecided|
|[#1922972](https://bugs.launchpad.net/bugs/1922972)`↗`| MAAS 3.0 Beta2 UI says "machine cannot be deployed" while successfully deploying machine |Undecided|
|[#1923719](https://bugs.launchpad.net/bugs/1923719)`↗`| MAAS 3.0 : snap refresh maas from 3.0.0~beta2-9826-g.13cc184d5 |Undecided|

*** MAAS 3.0 Beta 2 bug fixes</h3>

Here are the bugs that have been `Fix Released` in MAAS 3.0 Beta 2:

| Number | Description |Importance|
|:-----|:-----|:-----:|
|[#1922107](https://bugs.launchpad.net/bugs/1922107)`↗`| Hugepages/pinning available for virsh and lack validation |High|
|[#1922433](https://bugs.launchpad.net/bugs/1922433)`↗`| Machine resources path set incorrectly in rackd when using snap |High|

*** MAAS 3.0 Beta 1 bug fixes</h3>

Here are the bugs that have been `Fix Released` in MAAS 3.0 Beta 1:

| Number | Description |Importance|
|:-----|:-----|:-----:|
|[#1896199](https://bugs.launchpad.net/maas/+bug/1896199)`↗` |API docs link is not offline|Critical|
|[#1904245](https://bugs.launchpad.net/bugs/1904245)`↗`|MAAS Snap fails to build on PPC64 on Launchpad |Critical|
|[#1912727](https://bugs.launchpad.net/bugs/1912727)`↗`|KVM Page Fails to load with error "An unexpected error has occurred, please try refreshing your browser window." |Critical|
|[#1915869](https://bugs.launchpad.net/bugs/1915869)`↗`| maas snap cli renders SyntaxWarning in the stderr |Critical|
|[#1916093](https://bugs.launchpad.net/bugs/1916093)`↗`|Unable to add more than 3 Promox VMs |Critical| 
|[#1883824](https://bugs.launchpad.net/bugs/1883824)`↗`|Support LXD projects in power control |High| 
|[#1884276](https://bugs.launchpad.net/bugs/1884276)`↗`|Terrible user experience adding existing LXD host |High| 
|[#1902425](https://bugs.launchpad.net/bugs/1902425)`↗`|Failed to allocate the required AUTO IP addresses after 2 retries |High| 
|[#1908087](https://bugs.launchpad.net/bugs/1908087)`↗`|Reverse DNS for non-maas RFC1918 zones fails inside maas |High| 
|[#1908356](https://bugs.launchpad.net/bugs/1908356)`↗`|Owner data websocket methods are not working |High|
|[#1908434](https://bugs.launchpad.net/bugs/1908434)`↗`|Can't delete LXD VM in offline state |High| 
|[#1913323](https://bugs.launchpad.net/bugs/1913323)`↗`|/MAAS/docs/ leads to 404 page |High| 
|[#1914588](https://bugs.launchpad.net/bugs/1914588)`↗`|Enabling debug from snap traceback |High| 
|[#1915021](https://bugs.launchpad.net/bugs/1915021)`↗`|Mapping subnet doesn't work from the MAAS snap |High| 
|[#1915022](https://bugs.launchpad.net/bugs/1915022)`↗`|The MAAS snap doesn't include nmap |High| 
|[#1915715](https://bugs.launchpad.net/bugs/1915715)`↗`|LXD VM additional disks all show 10Gb size |High| 
|[#1915970](https://bugs.launchpad.net/bugs/1915970)`↗`|Facebook Wedge BMC detection fails on non-x86 architectures |High| 
|[#1918997](https://bugs.launchpad.net/bugs/1918997)`↗`|MAAS does not set snap proxy |High| 
|[#1919000](https://bugs.launchpad.net/bugs/1919000)`↗`|Unable to connect MAAS to an LXD VM host |High| 
|[#1887797](https://bugs.launchpad.net/bugs/1887797)`↗`|Impossible to delete zombie LXD VM |Medium| 
|[#1894116](https://bugs.launchpad.net/bugs/1894116)`↗`|Machines can't be deployed after deselecting all archs in the "Ubuntu extra architectures" package repo |Medium| 
|[#1897946](https://bugs.launchpad.net/bugs/1897946)`↗`|hi1620-based ARM Servers are shown as "Unknown model" |Medium| 
|[#1906212](https://bugs.launchpad.net/bugs/1906212)`↗`|timeout in testing scripts ignores the days if set to greater than 24 hours |Medium| Hemanth Nakkina 
|[#1911825](https://bugs.launchpad.net/bugs/1911825)`↗`|Unable to use FQDN as power_address |Medium| 
|[#1914165](https://bugs.launchpad.net/bugs/1914165)`↗`|Proxmox does not allow custom port |Medium| 
|[#1917652](https://bugs.launchpad.net/bugs/1917652)`↗`|30-maas-01-bmc-config failing on commissioning Cisco UCSC-C220-M4L |Medium| 
|[#1335175](https://bugs.launchpad.net/bugs/1335175)`↗`|maas does not combine kernel_opts when nodes have multiple tags with kernel options |Low| 
|[#1915359](https://bugs.launchpad.net/bugs/1915359)`↗`|make sampledata can't find machine-resources |Low| 
|[#1916844](https://bugs.launchpad.net/bugs/1916844)`↗`|Removing a machine that is a vm host tells you to remove the "pod" |Low| 
|[#1920019](https://bugs.launchpad.net/bugs/1920019)`↗`|maas_remote_syslog_compress is unnecessarily chatty |Low| 
|[#1887558](https://bugs.launchpad.net/bugs/1887558)`↗`|Multipath JBOD storage devices are not shown via /dev/mapper but each path as a single device. |Wishlist| 
|[#1901944](https://bugs.launchpad.net/bugs/1901944)`↗`|tags field in machine edit page overtakes other fields |Undecided| 
|[#1909985](https://bugs.launchpad.net/bugs/1909985)`↗`|Add commission timestamp to machine websocket api |Undecided| 
|[#1913464](https://bugs.launchpad.net/bugs/1913464)`↗`|Drop RSD pods UI |Undecided| 
|[#1914590](https://bugs.launchpad.net/bugs/1914590)`↗`|Support composing LXD VMs with multiple disks in the UI |Undecided| 
|[#1915970](https://bugs.launchpad.net/bugs/1915970)`↗`|Facebook Wedge BMC detection fails on non-x86 architectures |Undecided| 
|[#1916073](https://bugs.launchpad.net/bugs/1916073)`↗`|MAAS should install qemu-efi-aarch64 on arm64 KVM pods |Undecided| 
|[#1916317](https://bugs.launchpad.net/bugs/1916317)`↗`|UI is using API to request scripts with full content |Undecided| 
|[#1919381](https://bugs.launchpad.net/bugs/1919381)`↗`|typo "veryiying" in info message in smartctl-validate |Undecided|

* What is new with MAAS 3.1

We are happy to announce that MAAS 3.1.1 is now available. This release provides some [additional bug fixes](#heading--MAAS-3-1-1-bug-list).

*** Cumulative summary of MAAS 3.1 features and fixes

- [Support for LXD clusters](#heading--lxd-clusters): MAAS 3.1 can use LXD clusters with MAAS KVMs.
 
- [Improved image sync performance](#heading--image-sync-performance): After images are downloaded, rack controllers sync new images more quickly.
 
- [Ability to enlist deployed machines](#heading--enlist-deployed-machines): Users can enlist deployed machines, a top feature poll request.

- [Static Ubuntu image upload and reuse](#heading--static-ubuntu-images): Users can upload, deploy and reuse a bootable ubuntu image

- [Machine configuration cloning UI](#heading--machine-cloning-ui): We have extended machine cloning to the UI.

- [LXD authentication UX improvements](#heading--lxd-auth-ux-improvements): LXD certificates are easier to use.

- [MAAS 3.1 cumulative bug fixes](#heading--maas-3-1-cumulative-bug-fixes)

Critical and high-priority fixes also extend or repair MAAS features:

- [Expanded proxies](https://bugs.launchpad.net/maas/+bug/1867394)`↗`: Some proxies require authentication; MAAS now respects peer proxy username and password

- [Accurate storage pool sizes](https://bugs.launchpad.net/bugs/1949410)`↗`: The UI now calculates storage pool sizes correctly for CEPH pools; shared pools are no longer stacked

- [Refresh wipeout bug](https://bugs.launchpad.net/bugs/1949485)`↗`: MAAS does not destroy existing VMs on a refresh, or when the memory overcommit ratio is changed
 
- [Cloning issue fixed](https://bugs.launchpad.net/bugs/1948500)`↗`: UI cloning has been repaired to prevent "unsuccessful cloning" of storage

*** How to install MAAS 3.1

MAAS 3.1 can be installed fresh from snaps (recommended) with:

```
sudo snap install --channel=3.1 maas
```

MAAS 3.1 can also be installed via packages, by adding the `3.1` PPA:

```
sudo add-apt-repository ppa:maas/3.1
sudo apt update
sudo apt install maas
```

You can then install MAAS 3.1 fresh (recommended) with:

```
sudo apt-get -y install maas
```

Or, if you prefer to upgrade, you can:

```
sudo apt upgrade maas
```

At this point, proceed with a normal installation.

*** LXD clusters

MAAS 3.1 takes advantage of the existing LXD clustering capability.

**** About LXD clusters

LXD clusters in MAAS allow you to view and manage existing VM host clusters and compose VMs within clusters.  MAAS will not create a cluster, but it will discover existing clusters.

**** How to add LXD clusters

MAAS assumes you have already configured an LXD cluster. You then need to configure the cluster with a single trust which MAAS uses to communicate it. Adding a LXD cluster is like adding a single LXD host: you provide authentication in a similar way, and then select a project. MAAS then connects to the provided host to discover other hosts within the cluster, renaming the cluster host with a name you supply.

<a href="https://discourse.maas.io/uploads/default/original/2X/3/3aba7d6e30eda61623f66cb162ca85814128864a.png" target = "_blank"><img src="https://discourse.maas.io/uploads/default/original/2X/3/3aba7d6e30eda61623f66cb162ca85814128864a.png"></a>

First, add an LXD KVM:

<a href="https://discourse.maas.io/uploads/default/original/2X/c/c7d35ad0d8e1d9038dd39a8965307a49f57d453a.png" target = "_blank"><img src="https://discourse.maas.io/uploads/default/original/2X/c/c7d35ad0d8e1d9038dd39a8965307a49f57d453a.png"></a>

Next, set up credentials and let LXD trust your MAAS certificate:

<a href="https://discourse.maas.io/uploads/default/original/2X/b/b3ea7559edc066e899e41f41846a268b2459b1a5.png" target = "_blank"><img src="https://discourse.maas.io/uploads/default/original/2X/b/b3ea7559edc066e899e41f41846a268b2459b1a5.png"></a>

Once connected, you can select the project in that cluster:

<a href="https://discourse.maas.io/uploads/default/original/2X/b/ba798351c1c2b37d0aa79bca8c44def38d4ab839.png" target = "_blank"><img src="https://discourse.maas.io/uploads/default/original/2X/b/ba798351c1c2b37d0aa79bca8c44def38d4ab839.png"></a>

If the KVM host address is part of a cluster, it will show as a cluster on the listing page. 

<a href="https://discourse.maas.io/uploads/default/original/2X/0/069bae193cbb09ead3c811fd1a1d28582b946ff4.png" target = "_blank"><img src="https://discourse.maas.io/uploads/default/original/2X/0/069bae193cbb09ead3c811fd1a1d28582b946ff4.png"></a>

**** How to compose VMs in LXD clusters

Composing a VM in a LXD cluster via MAAS is similar to composing a VM for a single VM host. MAAS does not provide any sort of scheduling of said VM, and will instead target the host you select for composing the VM.

From the KVM host listing page, click on the `+` icon to add a VM to a specific host:

<a href="https://discourse.maas.io/uploads/default/original/2X/2/219a302c245992a390cd44ada341cfe5a93a7b5a.png" target = "_blank"><img src="https://discourse.maas.io/uploads/default/original/2X/2/219a302c245992a390cd44ada341cfe5a93a7b5a.png"></a>

If you are in a specific KVM host page, you can click `+ add virtual machine`:

<a href="https://discourse.maas.io/uploads/default/original/2X/2/219a302c245992a390cd44ada341cfe5a93a7b5a.png" target = "_blank"><img src="https://discourse.maas.io/uploads/default/original/2X/2/219a302c245992a390cd44ada341cfe5a93a7b5a.png"></a>

*** How to delete LXD clusters

To delete an LXD cluster, simply delete any VM host within the cluster:

<a href="https://discourse.maas.io/uploads/default/original/2X/e/ea7cd2476ae8cafe6d8e78f2b029d0cd41afa592.png" target = "_blank"><img src="https://discourse.maas.io/uploads/default/original/2X/e/ea7cd2476ae8cafe6d8e78f2b029d0cd41afa592.png"></a>

*** Ability to enlist deployed machines

**** Ten words or fewer
Users can enlist deployed machines, a top feature poll request.

**** About this feature
When adding machines to MAAS, MAAS (non-destructively) boots them into an ephemeral environment to gather hardware information.  Previously, this didn't work for deployed machines, since you don't want to disrupt deployed workloads or mark deployed machines as ready.

Now you may add deployed machines, without normal commissioning process or relabelling the machine. In order to update the information, a script is provided to run minimal commissioning scripts and update to MAAS.

**** How to enlist a machine that’s already running a workload
In order to enlist a deployed machine, you have two options:

Via the API/CLI, you can create a machine, passing the deployed flag:

```
$ maas $profile machines create deployed=true hostname=mymachine \   
architecture=amd64 mac_addresses=00:16:3e:df:35:bb power_type=manual
```

On the machine itself (the recommended way, if the machine is running Ubuntu), you can download and run a helper script from MAAS:

```
$ wget http://$MAAS_IP:5240/MAAS/maas-run-scripts
$ chmod 755 maas-run-scripts
$ ./maas-run-scripts register-machine --hostname mymachine \
 > http://$MAAS_IP:5240/MAAS $MAAS_API_TOKEN
```

Now you have enlisted a deployed machine, with no hardware information yet.

**** How to update hardware information for a deployed machine

To update hardware information for a deployed machine, we recommend that you download and run the maas-run-scripts script on the machine:

```
$ wget http://$MAAS_IP:5240/MAAS/maas-run-scripts
$ chmod 755 maas-run-scripts
$ ./maas-run-scripts report-results --config mymachine-creds.yaml
```

If you created the machine with the maas-run-scripts, you should have such a mymachine-creds.yaml file already. If not, it should look like this:

```
reporting:
          maas:
            consumer_key: $CONSUMER_KEY
            endpoint: http://$MAAS_IP:5240/MAAS/metadata/status/$SYSTEM_ID
            token_key: $TOKEN_KEY
            token_secret: $TOKEN_SECRET
```

You may get the needed credentials from the MAAS API, for example:

```
$ maas $profile machine get-token wxwwga
Success.
Machine-readable output follows:
{
        "token_key": "Lyy9BS4tKsQakDQScy",
        "token_secret": "V8vta8Azwn6FZVkfHnuTvLGLScAvEufB",
        "consumer_key": "YGT6QKSH65aap4tGnw"
}
```

*** Static Ubuntu image upload and reuse

**** Ten words or fewer

Users can upload, deploy and reuse a bootable ubuntu image

**** About this feature

MAAS supports deploying custom OS images.  Canonical provides both [lp:maas-image-builder](https://launchpad.net/maas-image-builder)`↗` and [gh:canonical/packer-maas](https://github.com/canonical/packer-maas)`↗` to create custom images. With 3.1, these custom images can include static Ubuntu images, created with whatever tool you choose, deployed as described below. Canonical still suggests customising Ubuntu using cloud-init user_data or Curtin preseed data, if possible.

**** About static Ubuntu images

MAAS allows you to build an Ubuntu image to deploy with MAAS, using any chosen image-building tool.  You can create the image with a fixed configuration and deploy it to many machines.  This fixed configuration can contain anything a normal image would contain: users, packages, etc.

**** About uploading hand-built Ubuntu images

You can upload and deploy hand-built Ubuntu images, containing kernel, bootloader, and fixed configuration.  The image can be built by tool, e.g., [packer](https://github.com/canonical/packer-maas)`↗`, or by scripts. You can upload these images to the boot-resources endpoint, where they will be available for deployment.

The minimum image must contain a kernel, bootloader, and `/curtin/curtin-hooks` script that configures the network. Samples can be found in the [packer-maas repos](https://github.com/canonical/packer-maas/tree/master/ubuntu/scripts)`↗`. The image must be in raw img file format so MAAS will accept the upload.  When built, you will upload this img file to the boot-resources endpoint, specifying the image architecture.

**** About how MAAS handles these images

MAAS will save the image as a `tar.gz` file in the database.  MAAS can differentiate between image types and generate appropriate pre-seed configurations.  MAAS also recognises the base Ubuntu version, so it can apply the correct ephemeral OS for installation.  Custom images are always deployed with the ephemeral OS, where the `base_image` field indicates the appropriate ephemeral version to avoid errors, ensuring smooth deployment later.

**** About how MAAS boots these images

When you deploy a machine with your custom image, MAAS ensures that the machine receives the kernel, bootloader and root file system provided in the image. The initial boot loader then boots an ephemeral OS matching the Ubuntu version of the custom image, reducing compatibility issues.  Curtin then writes your entire custom image to disk, after which it is not modified by MAAS.

Note that custom non-Ubuntu images still use a standard Ubuntu ephemeral OS to boot.

**** About configuring deployed machine networking

If you deploy a machine with a custom Ubuntu image, MAAS allows you to configure the deployed machine's networks just like any other MAAS machine.  If you create an interface and assign it to a subnet or static address, this will be reflected in the deployed machine.

For this reason, MAAS also does some initial diagnostics while installing the custom image.  MAAS will warn you about a missing network configuration, by checking for `cloud-init` and `netplan` in the `curtin` images. MAAS won't deploy machine with such images.

**** About configuring deployed machine storage

If you deploy a machine with a custom Ubuntu image, you will also want to be able to configure storage, just like any other machine.  MAAS facilitates changes to the storage configuration, such as resizing `/root`, attaching and formatting block devices, etc.

**** About static image metrics

As a user, you want to track of deployed static images. The standard MAAS dashboard now reflects these metrics.

**** How to upload a custom Ubuntu image

Custom Ubuntu images can be uploaded with the MAAS CLI by creating a boot-resource:

```nohighlight                                                                                                        	 
	maas $PROFILE boot-resources create \
        name='custom/ubuntu-custom'  \
        architecture=amd64/generic \
        title=’custom ubuntu’ \
        base_image=ubuntu/focal \
        filetype=ddraw \
        content@=./custom-ubuntu.img
```	 

[note]
When uploading a custom image, there is a new required field: `base_image`. This is not required for non-custom images; any image with the `custom` prefix will require it.
[/note]

*** Machine configuration cloning UI

**** Ten words or fewer

Extend machine cloning to UI, moving toward machine profile templates.

**** About this feature 

MAAS 3.1 allows you to quickly clone or copy a configuration between machines, via the MAAS UI -- a step towards machine templating. 

Creating a machine profile is repetitive. We've learned that most users create multiple machines of the same configuration in batches. Some users loop a template through the API, while others rely on scripts. MAAS API cloning functionality is now being exposed in the UI.

**** About copying machine configurations

As a MAAS user, you may want to copy a machine configuration to multiple existing machines. Assuming that at least one machine is already configured, you should be able to apply these settings to a list of machines.  This means that a user should be able to:

 - select the source machine to copy from.
 - validate that the source machine exists.
 - select at least 1 destination machine.
 - validate that the destination machine(s) exist.
 - edit the source machine or destination machines, if needed.
 - know at all times which machines are affected.
 - see the cloned machines when cloning is successful, or
 - get clear failure information, if cloning fails. 

**** About choosing configuration items to copy

As a MAAS user, you likely want to select whether storage, network, or both configurations should be cloned. The cloning API allows users to choose interfaces and storage separately.  Thus, this new feature also allows you to:

 - clone only the interface (network) configuration.
 - clone only the storage configuration.
 - clone both configurations.

**** About cloning restrictions

In order for cloning to succeed, a few restrictions must be met:

1. The destination interface names must be the same source.
2. The destination drive must be equal to or larger than the source drive.
3. For static IPs, a new IP will be allocated to the interface on the destination machine

**** How to clone a machine from the UI

Assume you have two machines available, like this:

<a href="https://discourse.maas.io/uploads/default/original/2X/6/6f662618011e3eb1f8e0bfe85748825db4a6ac25.png" target = "_blank"><img src="https://discourse.maas.io/uploads/default/original/2X/6/6f662618011e3eb1f8e0bfe85748825db4a6ac25.png">

Select the machine *to which you want to clone configuration*, and select "Clone from..."

<a href="https://discourse.maas.io/uploads/default/original/2X/b/b4e42a59f1d4bc6d63f2cd24d77316eea3aada1b.png" target = "_blank"><img src="https://discourse.maas.io/uploads/default/original/2X/b/b4e42a59f1d4bc6d63f2cd24d77316eea3aada1b.png">

Under "1. Select the source machine" -- choose a machine from the attached list:

<a href="https://discourse.maas.io/uploads/default/original/2X/2/287bbf3db4bbc3253a976ecde8965c341fc1bee3.png" target = "_blank"><img src="https://discourse.maas.io/uploads/default/original/2X/2/287bbf3db4bbc3253a976ecde8965c341fc1bee3.png">

Under "2. Select what to clone", choose "Network", "Storage", or both (here, we've chosen "Storage"):

<a href="https://discourse.maas.io/uploads/default/original/2X/6/622afe3c0bcd4775ef4c19460cf0f1f480c11efb.png" target = "_blank"><img src="https://discourse.maas.io/uploads/default/original/2X/6/622afe3c0bcd4775ef4c19460cf0f1f480c11efb.png">

Click "Clone to machine". MAAS will report the status of the attempt.

*** LXD authentication UX improvements

**** Ten words or fewer

Easier MAAS to LXD connection that uses certificates for authentication.

**** About this feature

MAAS 3.1 provides a smoother experience when connecting an existing LXD server to MAAS, guiding the user through manual steps and providing increased connection security with use of certificates. Currently, each MAAS region/rack controller has its own certificate. To add a LXD VM host to MAAS, the user needs to either add the certificate for each controller that can reach the LXD server to the trust list in LXD, or use the trust_password (in which case the controller talking to LXD will automatically add its certificate to the trust).

This doesn’t provide a great user experience, as the former process is cumbersome, and the latter is not suggested for production use for security reasons.  To improve this, MAAS 3.1 manages per-LXD keys/certificates, and provide a way for users to get the content of certificates, to authorise MAAS in LXD.

**** About on-the-spot certificate creation

As a MAAS user, you want to register a LXD host into MAAS using certificates for authentication -- to follow LXD production deployment best practices.  The standard way for clients to authenticate with LXD servers is through certificates. The use of trust_password is *only* meant as a way to interact for initial setup.

While prior versions of MAAS support both ways of authentication (and automatically adds the certificate for the rack talking to LXD when registering the VM host), the user experience is lacking, since there's no control over the certificate being used.  In addition, each rack uses a different certificate, making it hard to manage scenarios where multiple racks can connect to a LXD server.

For these reasons, when adding a LXD host, MAAS 3.1 provides a way to generate a secret key and certificate pair to use specifically for that server, and show the certificate to the user, so that they can add it to the LXD server trust list.  The user experience changes to something like the following:

 - MAAS generates a secret key and certificate pair for use with a LXD server.
 - The user can see the certificate and is guided to add it to the LXD server trust list.
 - The user can easily complete the registration of the LXD server once the certificate is trusted in LXD.
 - All racks use the same key when talking to the LXD server. 
 - If a new rack controller is added, it can communicate with the LXD server out of the box.
 - If the trust password is used, it’s not stored in MAAS persistently.
 - It’s possible to get the certificate for a LXD server from a URL (e.g. for curl use).

**** About bringing your own certificates

As a MAAS user, you may want to register a LXD host into MAAS by providing a private key for a certificate that’s already trusted by the LXD server.  For example, you may already have set up certificates in the server trust for MAAS to use, MAAS should provide a way to import it, instead of generating a new one.

With MAAS 3.1, it’s possible to import an existing key/certificate pair for use with a LXD server when registering it with MAAS.  MAAS stores the key/certificate instead of generating new ones.

[note]
The imported key must not have a passphrase; otherwise, MAAS will not be able to use it.
[/note]

**** How to get started

Suppose that you're creating a new LXD KVM, beginning from the top tab in MAAS:

<a href="https://discourse.maas.io/uploads/default/optimized/2X/b/b7048c83a7d6e4dbca69a060a7b4bf8bc07e1953_2_690x165.png" target = "_blank"><img src="https://discourse.maas.io/uploads/default/optimized/2X/b/b7048c83a7d6e4dbca69a060a7b4bf8bc07e1953_2_690x165.png"></a>

Select "Add KVM", which brings you to the definition screen:

<a href="https://discourse.maas.io/uploads/default/optimized/2X/8/806d3577b11ed415574fd06de5f643f26ffb7928_2_690x257.png" target = "_blank"><img src="https://discourse.maas.io/uploads/default/optimized/2X/8/806d3577b11ed415574fd06de5f643f26ffb7928_2_690x257.png"></a>

From here, you'll continue by choosing your authentication method.

**** How to let MAAS create a certificate for you

If you choose "Generate new certificate", as shown above, you'll come to a screen like this one:

<a href="https://discourse.maas.io/uploads/default/optimized/2X/0/08a32d9221a73f0d6f84580ab9ebeeaaf84aeb65_2_690x325.png" target = "_blank"><img src="https://discourse.maas.io/uploads/default/optimized/2X/0/08a32d9221a73f0d6f84580ab9ebeeaaf84aeb65_2_690x325.png"></a>

You can still choose to use the LXD trust password (entered when you ran `lxd init` during LXD installation).  You can also, though, choose to use the certificate MAAS has just generated for you.  To do that, select the entire contents of the text box, copy it, and paste it into a terminal window -- then hit "Enter":

```
$ lxc config trust add - <<EOF
> -----BEGIN CERTIFICATE-----
> MIIErTCCApUCEQCGa86XdjYUGm8h8YOh4HAEMA0GCSqGSIb3DQEBDQUAMAAwHhcN
> MjEwOTI0MjE1NDQ4WhcNMzEwOTIyMjE1NDQ4WjApMScwJQYDVQQDDB5teTBuZXh0
> LTMuMS4wLWJldGExLWt2bUB3YWxkZW4wggIiMA0GCSqGSIb3DQEBAQUAA4ICDwAw
> ggIKAoICAQC1tmJbSYx3Jb5JcuqLvyO6P0RtYWCbjVYOSAIM1PKHZJRvako6QhjR
> 6wWNcVLAjDJIMuEBysrI8mcAv9D/AfT2qLQ/5mg7anbxfrd3YXG2nc70QJazpFaw
> INDc85wrdJD5NEd50iaka+PztIAWzoZWQr/pLb7hUDnArzSHp5J+w0dRCUh54SyW
> Du4mLpDks5UqMeONO1o7lbaQuBdzGtR4btdmvOkJfg/Pu3i/rzFZ1vvn1JhZTX96
> +xH7tJQiqOk0SXG7F2RmbYiYDhAkiysbMoyOHBCf/qFWq4Vtd/VMxOAT1WERrgWn
> 8nL5kRBozV94QocJaOe+GUSWLHsRpsVa8jiAj3LS2CFQfpaEsrzLSlQOeN2rNB9z
> DO9yGXGql4tUpgtyEvxB/zVrIGd04iTC3D4S9b1KyzTbSsyjTc/XJhUStnn49ySW
> Iwv1eHa2jMvIjRVm5sRfpf0EOZW27HLI1AqDOXR0DmlM2mWvndjvfacX+41I8vuG
> +RPq0ZjDhwfRmUaLiebzcExwPmSHAxqiaV+t0n6ivDWTNk6cNc38rZBh3x6I7JMR
> /85Rc1blLSF7QBMA1HxheCUYzBPTKsdE2btygq9vShRXCdSekV0jGoL1g0n6T59r
> +9nHShgc/Bzk42kcddQySlrqWWHrXX6Z2N1R3eYpuvSEaKsnsjqjwwIDAQABMA0G
> CSqGSIb3DQEBDQUAA4ICAQA4d1Xqi941ssyJoiovTzBgMDSp9kqjpB83BRqbF9oZ
> fQGkezn2jF7SnaXbTyR/K+nir5Rms8OZfUxyZJwYh/YCdnIF8hzC32mLJbP6jcJV
> LS0OD+EipwyRLSe9g2it68TtAhhVXKPx3tGQWWiXtJOF631sJRcRUZATc9nco5H2
> 91GKog4LdFeKD3ArOq1GkE9r95WauTV37x0c474XBt2mVcEvFW50oZbIBPaWLt8E
> q8NG0KYkfIHkhXDGqPDkUtdPJlkiGwqXdaqghuG31a4Or9IKcNmDlli47apaWWJW
> /gqZfFALbOrSJHg10PCqNsfoKmQr2YZzPlTjG39RA7sA1XR6y+lQZqwcXnXk2iAE
> n62OkRUrYVXzBo99zk5jQJVEg6zhfPH9zl6Jmn/vBu0p6RqmqNLTTlMOio8VOp9e
> 9Gyb9uRwzwZ9zgydgI4bHMvcIAq+46wTruOfXBNATWLC2YqXbc+9QqemJebcXULW
> Wf7Sc+SHHx2cVb4OUvUD8keZN37No/2vfZ9NI2SJOI4SxlV2yf6ZRyb7MYIwpm1h
> YTzyS+ywUN4C8p1PsU5iT8DGdcg7Kcso4/DDZeZkLKNeCKizkdMreF7qV0qHTW8z
> PyfZHcR/xWMkjxYZoFu4rVyxpsUJYItJNUNk6vZvSnSDfC2e2JJFfMws+fntNy14
> /w==
> -----END CERTIFICATE-----
> EOF
$ 
```

The certificate will be created for you.  When you click the "Check authentication" button, you will be brought to this screen:

<a href="https://discourse.maas.io/uploads/default/optimized/2X/a/ad3f6fd06fdef3ce5be467816b2fc3667550f397_2_690x204.png" target = "_blank"><img src="https://discourse.maas.io/uploads/default/optimized/2X/a/ad3f6fd06fdef3ce5be467816b2fc3667550f397_2_690x204.png"></a>

from which you can continue with normal LXD KVM setup.

**** How to use your own, existing certificate

Suppose that, after identifying your LXD KVM, you choose "Provide certificate and private key".  When you do so, the screen will extend to allow you to upload these items:

<a href="https://discourse.maas.io/uploads/default/optimized/2X/a/ad3f6fd06fdef3ce5be467816b2fc3667550f397_2_690x204.png" target = "_blank"><img src="https://discourse.maas.io/uploads/default/optimized/2X/a/ad3f6fd06fdef3ce5be467816b2fc3667550f397_2_690x204.png"></a>

Paste or upload your certificate and private key, then click "Next" to validate your authentication criteria, before continuing through the normal LXD KVM creation process.  If your certificate and/or key aren't usable for some reason, MAAS will return an error (in this case, the private key was entered as gibberish, to produce an error output):

<a href="https://discourse.maas.io/uploads/default/original/2X/2/286e648de20c9db3bb6c56c5855647c23a5d9e2e.png" target = "_blank"><img src="https://discourse.maas.io/uploads/default/original/2X/2/286e648de20c9db3bb6c56c5855647c23a5d9e2e.png"></a>

*** Improved image sync performance

**** Ten words or fewer

After downloading images, the rack controller syncs them much faster.

**** About this feature

Downloading and syncing images is a known delay element in MAAS.  While images aren't small, and do take some time to download, we decided to try to speed up the process as much as possible.  After the region has downloaded new images, the rack controllers are now much quicker at syncing the new images.

**** How to take advantage of this new feature

There is nothing required of our users to experience this improved sync performance, other than upgrading to 3.1.

*** MAAS 3.1.1 bug list

- [1938296](https://bugs.launchpad.net/maas/+bug/1938296)`↗`: MAAS 3.0 incorrectly calculates the amount of free space on drive
- [1982866](https://bugs.launchpad.net/maas/+bug/1982866)`↗`: MAAS Breaks historical custom images
- [1988759](https://bugs.launchpad.net/maas/+bug/1988759)`↗`: Provisioning LXD vmhost fails
- [1993289](https://bugs.launchpad.net/maas/+bug/1993289)`↗`: Pod storage pool path can't be blank
- [1961808](https://bugs.launchpad.net/maas/+bug/1961808)`↗`: Regression - unable to select proper subnet when adding interface alias

*** MAAS 3.1 cumulative bug fixes

MAAS 3.1 bug fixes can be found in the following milestones:

- [MAAS 3.1 Beta5 bug fixes](https://launchpad.net/maas/+milestone/3.1.0-beta5)`↗`
- [MAAS 3.1 Beta4 bug fixes](https://launchpad.net/maas/+milestone/3.1.0-beta4)`↗`
- [MAAS 3.1 Beta3 bug fixes](https://launchpad.net/maas/+milestone/3.1.0-beta3)`↗`
- [MAAS 3.1 Beta2 bug fixes](https://launchpad.net/maas/+milestone/3.1.0-beta2)`↗`
- [MAAS 3.1 Beta1 bug fixes](https://launchpad.net/maas/+milestone/3.1.0-beta1)`↗`

* What is new with MAAS 3.2

<a href="#heading--MAAS-3.2-release-notes">** id="heading--MAAS-3.2-release-notes">MAAS 3.2 release notes

Here you will find release notes for all releases under the 3.2 version.

** MAAS 3.2.7 release notes

We are happy to announce that MAAS 3.2.7 has been released.  This point releease of MAAS 3.2 provides a number of [high-profile bug fixes](#heading--MAAS-3-2-7-bug-fixes).

** MAAS 3.2.6 release notes

We are happy to announce that MAAS 3.2.6 has been released. This point release of MAAS 3.2 provides a fix for a critical bug that prevented MAAS from enlisting machines on subnets with active DNS:

- #1989970 [Can't enlist machines on subnets with DNS set](https://bugs.launchpad.net/bugs/1989970)`↗`

No other changes were made for this point release.

** MAAS 3.2.5 release notes

MAAS 3.2.5 was an attempt to fix a critical issue in 3.2.4.  This issue was resolved in MAAS 3.2.6, listed above. 

** MAAS 3.2.4 release notes

We are happy to announce that MAAS 3.2.4 has been released.  This point release of MAAS 3.2 provides a fix for a critical bug that prevented the controllers page from displaying under certain conditions:

-  #1983624 [Fresh MAAS 3.2 install failed to find controller](https://bugs.launchpad.net/bugs/1983624)`↗`

This release also addresses build issues found in prior point releases.

** MAAS 3.2.2 ~ MAAS 3.2.3

MAAS 3.2.2 and MAAS 3.2.3 were successive attempts to fix issues in MAAS.  These issues were resolved in MAAS 3.2.4, listed above.

** MAAS 3.2.1 release notes

We are happy to announce that MAAS 3.2.1 has been released.  This point release of MAAS 3.2.1 provides support for Rocky Linux UEFI ([bug number 1955671](https://bugs.launchpad.net/bugs/1955671))`↗`, along with fixes for a number of recently-reported bugs:

- #1955671: [support for rocky linux UEFI](https://bugs.launchpad.net/bugs/1955671)`↗`
- #1980436: [MAAS CLI with anonymous access fails when TLS is enabled](https://bugs.launchpad.net/bugs/1980436)`↗`
- #1980490: [MAAS regiond IPC crash due to a machine-resources binary crash when parsing some VPDs](https://bugs.launchpad.net/bugs/1980490)`↗`
- #1980818: [Configure DHCP for VLAN](https://bugs.launchpad.net/bugs/1980818)`↗`
- #1981536: [volume group creation fails on md device - MAAS 3.2](https://bugs.launchpad.net/bugs/1981536)`↗`
- #1981560: [upgrade from 3.1 to 3.2 using debian packages missing steps](https://bugs.launchpad.net/bugs/1981560)`↗`
- #1982984: [reverse-proxy service is not displayed for region controller](https://bugs.launchpad.net/bugs/1982984)`↗`
- #1929478: [Commissioning fails with binary data in IPMI Lan_Conf_Security_Keys](https://bugs.launchpad.net/bugs/1929478)`↗`
- #1982208: [agent.yaml.example is missing when maas is installed via deb package](https://bugs.launchpad.net/bugs/1982208)`↗`
- #1982846: [Missing update_interface method on controller websocket handler](https://bugs.launchpad.net/bugs/1982846)`↗`

Please see the release notes for the 3.2 release, below, for a summary of 3.2 features.
	
** MAAS 3.2 release notes

We are happy to announce that MAAS 3.2 is now available.

** New MAAS 3.2 features

MAAS 3.2 provides several new features, as well as the usual cadre of bug fixes.

*** Improved performance

As part of the MAAS 3.2 development effort, we have taken steps to improve the performance of machine listings.  To date, we have measured the speed of listing a large number (100-1000) of machines via the REST API to be 32% faster, on average.  During the next cycle, we will be actively working to improve MAAS performance for other operations (such as search).

*** Better Redfish support

MAAS has previously supported the Redfish protocol for some time, but as an option, preferring IPMI over all others if a choice of protocol was possible.  In contrast, MAAS 3.2 supports Redfish as a BMC protocol by preferring Redfish over IPMI, provided that:

- The BMC has a Redfish Host Interface enabled
- That host interface can be accessed by the MAAS host

MAAS already supports Redfish, but with MAAS 3.2 we’re trying to auto-detect Redfish and use it if it's available.

You may know that Redfish is an alternative to the IPMI protocol for connecting with machine BMCs.  It provides additional features above and beyond those provided by IPMI.  Eventually, Redfish should supplant IPMI as the default BMC interface.

If the machine uses either IPMI or Redfish for its BMC, the ephemeral environment will automatically detect it, create a separate user for MAAS and configure the machine, so that MAAS may check and control the machine’s power status. Note that the name of the user that MAAS creates in the BMC is controlled by the `maas_auto_ipmi_user` config setting, both for IPMI and Redfish; nothing has changed in this regard with MAAS 3.2.

You can check whether or not a machine can communicate via Redfish, with the command: 

```nohighlight
dmidecode -t 42
```

If the machine has been enlisted by MAAS, you can also check the output of the `30-maas-01-bmc-config` commissioning script to discover this.

*** MAAS native TLS

MAAS 3.2 provides [native TLS](/t/how-to-enable-maas-native-tls/5116#heading--about-maas-native-tls). MAAS now has built-in TLS support for communicating with the UI and API over HTTPS. This eliminates the need to deploy a separate TLS-terminating reverse-proxy solution in front of MAAS to provide secure access to API and UI.  Note that you can still set up an HA proxy if you are using multiple controllers.

*** Hardware sync for deployed machines

MAAS 3.2 allows you to [sync hardware changes for deployed machines](https://maas.io/docs/how-to-customise-machines#heading--how-to-enable-hardware-sync-on-a-machine)`↗`.  You can see real-time updates to storage, etc., for a running machine.  This feature requires a special parameter be set prior to deployment.  Coupled with the existing ability to commission deployed machines, MAAS 3.2 moves a step closer to real-time reconfiguration of active, deployed, bare-metal.

*** Expanded tagging capability
 
MAAS 3.2 provides greatly [expanded tagging capability](/t/how-to-tag-machines/5928#heading--automatic-tags).  You can auto-apply tags to machines that match a custom XPath expression. Setting up an automatic tag lets you recognise special hardware characteristics and settings, e.g., the gpu passthrough.

*** More new features

MAAS 3.2 rounds out the feature set with a few more items:

- [Support for observability (O11y) in MAAS](/t/how-to-set-up-maas-metrics/5204): MAAS now supports integration with FOSS Observability stacks.

- [Ability for user to specify IPMI cipher suite](/t/power-management-reference/5246): You can explicitly select which cipher suite to use when interacting with a BMC.

- Roll-out of our new tabbed Reader Adaptive Documentation (incremental across the release cycle): We've eliminated the top menus; each page now contains information for all versions, selectable by dropdowns above the relevant sections.

** How to install MAAS 3.2

MAAS 3.2 can be installed fresh from snaps (recommended) with:

```
sudo snap install --channel=3.2 maas
```

MAAS 3.2 can be installed from packages by adding the `ppa:maas/3.2` PPA:

```
sudo add-apt-repository ppa:maas/3.2
sudo apt update
sudo apt install maas
```

You can then install MAAS 3.2 fresh (recommended) with:

```
sudo apt-get -y install maas
```

Or, if you prefer to upgrade, you can:

```
sudo apt upgrade maas
```

At this point, proceed with a normal installation.

** MAAS 3.2.7 bug fixes

The following bugs have been fixed in MAAS 3.2.7:

 - [1989974](https://bugs.launchpad.net/maas/+bug/1989974): rackd fails on CIS-hardened machine with "Failed to update and/or record network interface configuration: Expecting value: line 1 column 1 (char 0)"
 - [1938296](https://bugs.launchpad.net/maas/+bug/1938296): MAAS 3.0 incorrectly calculates the amount of free space on drive		High	Alberto Donato 	Fix Released
 - [1982866](https://bugs.launchpad.net/maas/+bug/1982866):	MAAS Breaks historical custom images		High	Christian Grabowski 	Fix Released
 - [1988759](https://bugs.launchpad.net/maas/+bug/1988759):	Provisioning LXD vmhost fails		High	Alberto Donato 	Fix Released
 - [1990014](https://bugs.launchpad.net/maas/+bug/1990014):	regiond.conf "debug_http: true" causes image downloads from regiond to fail with 500 error code		High	Anton Troyanov 	Fix Released
 - [1992185](https://bugs.launchpad.net/maas/+bug/1992185):	unable to deploy a machine with vmhost if a bond interface was created		High	Alberto Donato 	Fix Released
 - [1993152](https://bugs.launchpad.net/maas/+bug/1993152):	Updating a VM host through API unset tags		High	Alberto Donato 	Fix Released
 - [1993289](https://bugs.launchpad.net/maas/+bug/1993289):	Pod storage pool path can't be blank		High	Alberto Donato 	Fix Released
 - [1992330](https://bugs.launchpad.net/maas/+bug/1992330):	Use the rack controller IP as DNS when relaying DHCP		Medium	Björn Tillenius 	Fix Released
 - [1993618](https://bugs.launchpad.net/maas/+bug/1993618):	Web UI redirection policy can invalidate HAProxy and/or TLS setup		Medium	Anton Troyanov 	Fix Released
 - [1994945](https://bugs.launchpad.net/maas/+bug/1994945):	Failure to create ephemeral VM when no architectures are found on the VM host		Medium	Igor Brovtsin 	Fix Released
 - [1996419](https://bugs.launchpad.net/maas/+bug/1996419):	renaming a DNS record to a previous name fails with error: list.remove(x): x not in list		Medium	 	Fix Released
 - [1996997](https://bugs.launchpad.net/maas/+bug/1996997):	LXD resources fails on a Raspberry Pi with no Ethernet

** Bugs fixed in MAAS 3.2

Here is the breakdown of bugs fixed across the MAAS 3.2 release:

- [MAAS 3.2.1](https://launchpad.net/maas/+milestone/3.2.1)`↗`
- [MAAS 3.2](https://launchpad.net/maas/3.2/3.2.0)`↗`
- [MAAS 3.2 RC 2](https://launchpad.net/maas/3.2/3.2.0-rc2)`↗`
- [MAAS 3.2 RC 1](https://launchpad.net/maas/+milestone/3.2.0-rc1)`↗`
- [MAAS 3.2 Beta 6](https://launchpad.net/maas/3.2/3.2.0-beta6)`↗`
- [MAAS 3.2 Beta 5](https://launchpad.net/maas/3.2/3.2.0-beta5)`↗`
- [MAAS 3.2 Beta 4](https://launchpad.net/maas/3.2/3.2.0-beta4)`↗`
- [MAAS 3.2 Beta 3](https://launchpad.net/maas/3.2/3.2.0-beta3)`↗`
- [MAAS 3.2 Beta 2](https://launchpad.net/maas/+milestone/3.2.0-beta2)`↗`
- [MAAS 3.2 Beta 1](https://launchpad.net/maas/3.2/3.2.0-beta1)`↗`
 
** Known issues for MAAS 3.2

The following known issues exist for MAAS 3.2:

<a href="#heading--Cannot-update-controller/device-tags-via-WebSocket-API">*** id="heading--Cannot-update-controller/device-tags-via-WebSocket-API">Cannot update controller/device tags via WebSocket API

If you attempt to update a list of tags of a device with an automatic tag, you get an error: "Cannot add tag tag-name to node because it has a definition".

If you attempt to manually make the same API request, but send a list of tags with the automatic tag filtered out, the automatic tag will be removed from the device.

** Release notes for other MAAS versions

Here are release notes for other relatively recent MAAS versions:

- [MAAS 3.1](/t/what-is-new-with-maas-3-1/5964)
- [MAAS 3.0](/t/what-is-new-with-maas-3-0/5963)
- [MAAS 2.9](/t/what-is-new-with-maas-2-9/5961)
- [MAAS 2.8](/t/what-is-new-with-maas-2-8/5994)
- [MAAS 2.7](/t/what-is-new-with-maas-2-7/5993)

* What is new with MAAS 3.3

MAAS 3.3 is a concerted effort to improve MAAS on multiple fronts, including a large number of bug fixes. 

** Cumulative summary of MAAS 3.3 features

New features created for MAAS 3.3 include:

- [Ansible playbooks for HA MAAS, PostgreSQL, and other MAAS configurations](#heading--ansible-playbooks): [Ansible](https://www.redhat.com/en/technologies/management/ansible/what-is-ansible)`↗` [playbooks](https://docs.ansible.com/ansible/latest/getting_started/get_started_playbook.html)`↗` are now available for MAAS, making it easy to automate routine setup and configuration of MAAS.

- [Improved machine list filtering](#heading--Improved-machine-list-filtering): MAAS 3.3 enhances the presentation and filtering of the machine list, with a shorter wait to start filtering and a wider range of filter choices.

- [Integration of Vault for credential storage](#heading--vault-integration): MAAS 3.3 allows you to use [Hashicorp Vault](https://www.vaultproject.io/)`↗` to protect your secrets, if you wish.

Improved capabilities include the following:

- [Native support for 22.04 LTS and core22](#heading--22-04-support): We've removed the requirement to use snaps on 22.04 (Jammy Jellyfish); you now can load MAAS 3.3 on 22.04 using packages.

- [UI performance improvements for large machine counts](#heading--UI-performance-improvements): We've improved the performance of the UI machine list for large (>10000 machines) MAAS instances.  The machine list now goes live just a few seconds after the first visible page loads, with the rest of the list loading in background.

- [Enhanced MIB support for Windows OS images](#heading--Enhanced-MIB-support-for-Windows-OS-images): The [procedure](/t/how-to-build-custom-images/5104#heading--custom-windows-images) for creating custom Windows OS images has been thoroughly updated and verified.

Greatly expanded documentation sections include:

- [MAAS configuration settings reference](#heading--maas-config-settings-ref): There is now one reference page that addresses all MAAS settings in one place.  Other references throughout the document are preserved for now.

- [Improved MAAS event documentation](#heading--Improved-MAAS-event-documentation): MAAS event documentation has been expanded to include [much better explanations](/t/understanding-maas-events/6373) of MAAS events, including many examples.

- [Improved MAAS audit event documentation](#heading--Improved-MAAS-audit-event-documentation): MAAS audit event documentation has been greatly expanded to include [much better explanations](/t/understanding-maas-audit-events/6372) of MAAS audit events, including many examples and use cases.

Several forward-looking improvements are included as well:

- Reliability improvements for simultaneous machine deployments

- The first phase of [Nvidia DPU](https://www.nvidia.com/en-us/networking/products/data-processing-unit/)`↗` support

- Shifting the MAAS API documentation toward [OpenAPI standards](https://www.openapis.org/)`↗`

- Shifting the MAAS documentation toward the [Diátaxis](https://diataxis.fr/)`↗` style of documentation

These will be documented later in blog posts.


This release also includes well over one-hundred [bug fixes](#heading--MAAS-3.3-bug-list).  Read on to catch up with what we've done so far this cycle.

** How to install MAAS 3.3

MAAS will run on just about any modern hardware configuration, even a development laptop.  If you're not sure whether your target server will handle MAAS, [you can always double-check](/t/maas-installation-requirements/6233).

[note]
**NOTE** that PostgreSQL 12 is deprecated with the release of MAAS 3.3, in favour of PostgreSQL 14. Support for PostgreSQL 12 will be discontinued in MAAS 3.4.
[/note]

*** How to do a fresh snap install of MAAS 3.3

To install MAAS 3.3 from a snap, simply enter the following:

    $ sudo snap install --channel=3.3 maas

After entering your password, the snap will download and install from the 3.3 channel.

*** How to upgrade from an earlier snap version to MAAS 3.3

Maybe instead of a fresh install, you want to upgrade from a earlier snap version to the 3.3 snap, and you are using a `region+rack` configuration, use this command:

    $ sudo snap refresh --channel=3.3 maas

After entering your password, the snap will refresh from the 3.3 candidate channel.  You will **not** need to re-initialise MAAS.

If you are using a multi-node maas deployment with separate regions and racks, you should first run the upgrade command above for rack nodes, then for region nodes.

*** How to initialise MAAS 3.3 snap for a test or POC environment

You can initialise MAAS as a compact version for testing.  To achieve this, we provide a separate snap, called `maas-test-db`, which contains a PostgreSQL database for use in testing and evaluating MAAS.   The following instructions will help you take advantage of this test configuration.

Once MAAS is installed, you can use the `--help` flag with `maas init` to get relevant instructions:
 
    $ sudo maas init --help
    usage: maas init [-h] {region+rack,region,rack} . . .

    Initialise MAAS in the specified run mode.

    optional arguments:
      -h, --help            show this help message and exit

    run modes:
      {region+rack,region,rack}
        region+rack         Both region and rack controllers
        region              Region controller only
        rack                Rack controller only

    When installing region or rack+region modes, MAAS needs a
    PostgreSQL database to connect to.

    If you want to set up PostgreSQL for a non-production deployment on
    this machine, and configure it for use with MAAS, you can install
    the maas-test-db snap before running 'maas init':
        sudo snap install maas-test-db
        sudo maas init region+rack --database-uri maas-test-db:///

We'll quickly walk through these instructions to confirm your understanding.  First, install the `maas-test-db` snap:
 
    sudo snap install maas-test-db

Note that this step installs a a running PostgreSQL and a MAAS-ready database instantiation.  When it's done, you can double check with a built-in PostgreSQL shell:

    $ sudo maas-test-db.psql
    psql (12.4)
    Type "help" for help.

    postgres=# \l

This will produce a list of databases, one of which will be `maasdb`, owned by `maas`.  Note that this database is still empty because MAAS is not yet initialised and, hence, is not yet using the database.  Once this is done, you can run the `maas init` command:

    sudo maas init region+rack --database-uri maas-test-db:///

After running for a moment, the command will prompt you for a MAAS URL; typically, you can use the default:
 
    MAAS URL [default=http://10.45.222.159:5240/MAAS]:

When you've entered a suitable URL, or accepted the default, the following prompt will appear:
 
    MAAS has been set up.

    If you want to configure external authentication or use
    MAAS with Canonical RBAC, please run

      sudo maas configauth

    To create admins when not using external authentication, run

      sudo maas createadmin

Let's assume you just want a local testing user named `admin`:

    $ sudo maas createadmin
    Username: admin
    Password: ******
    Again: ******
    Email: admin@example.com
    Import SSH keys [] (lp:user-id or gh:user-id): gh:yourusername

At this point, MAAS is basically set up and running.  You can confirm this with `sudo maas status`.  If you need an API key, you can obtain this with `sudo maas apikey --username yourusername`.  Now you will be able to test and evaluate MAAS by going to the URL you entered or accepted above and entering your `admin` username and password.

*** Initialise MAAS for a production configuration

To install MAAS in a production configuration, you need to setup PostgreSQL, as described below.

**** Setting up PostgreSQL from scratch

To set up PostgreSQL, even if it's running on a different machine, you can use the following procedure:

1. You will need to install PostgreSQL on the machine where you want to keep the database.  This can be the same machine as the MAAS region/rack controllers or a totally separate machine.  If PostgreSQL (version 10 or better) is already running on your target machine, you can skip this step. To install PostgreSQL, run these commands:

        sudo apt update -y
        sudo apt install -y postgresql

2. You want to make sure you have a suitable PostgreSQL user, which can be accomplished with the following command, where `$MAAS_DBUSER` is your desired database username, and `$MAAS_DBPASS` is the intended password for that username.  Note that if you're executing this step in a LXD container (as root, which is the default), you may get a minor error, but the operation will still complete correctly.

        sudo -u postgres psql -c "CREATE USER \"$MAAS_DBUSER\" WITH ENCRYPTED PASSWORD '$MAAS_DBPASS'"

3. Create the MAAS database with the following command, where `$MAAS_DBNAME` is your desired name for the MAAS database (typically known as `maas`). Again, if you're executing this step in a LXD container as root, you can ignore the minor error that results.

        sudo -u postgres createdb -O "$MAAS_DBUSER" "$MAAS_DBNAME"

4. Edit `/etc/postgresql/14/main/pg_hba.conf` and add a line for the newly created database, replacing the variables with actual  names. You can limit access to a specific network by using a different CIDR than `0/0`.

        host    $MAAS_DBNAME    $MAAS_DBUSER    0/0     md5

5. You can then initialise MAAS via the following command:

        sudo maas init region+rack --database-uri "postgres://$MAAS_DBUSER:$MAAS_DBPASS@$HOSTNAME/$MAAS_DBNAME"

[note] You should use `localhost` for `$HOSTNAME` if you're running PostgreSQL on the same box as MAAS.[/note]

Don't worry; if you leave out any of the database parameters, you'll be prompted for those details.

*** How to do a fresh install of MAAS 3.3 from packages

MAAS 3.3 from packages runs on 22.04 LTS only.  The recommended way to set up an initial MAAS environment is to put everything on one machine:

``` bash
sudo apt-add-repository ppa:maas/3.3
sudo apt update
sudo apt-get -y install maas
```

Executing this command leads you to a list of dependent packages to be installed, and a summary prompt that lets you choose whether to continue with the install. Choosing "Y" proceeds with a standard <code>apt</code> package install.

****>Distributed environment</h4> 

<p>For a more distributed environment, you can place the region controller on one machine:</p>

``` bash
sudo apt install maas-region-controller
```

and the rack controller on another:

``` bash
sudo apt install maas-rack-controller
sudo maas-rack register
```

These two steps will lead you through two similar <code>apt</code> install sequences.

*** How to upgrade from 3.2 or lower to MAAS 3.3

If you are running MAAS 3.2 or lower, you can upgrade directly to MAAS 3.3. You must first make sure that the target system is running Ubuntu 22.04 LTS by executing the following command:

```nohighlight
lsb_release -a
```
The response should look something like this:

```nohighlight
Distributor ID:	Ubuntu
Description:	Ubuntu xx.yy
Release:	xx.yy
Codename:	$RELEASE_NAME
```

The required “xx.yy” required for MAAS 3.3 is “22.04,” code-named “jammy”.

If you are currently running Ubuntu focal 20.04 LTS, you can upgrade to jammy 22.04 LTS with the following procedure:

Upgrade the release:

```nohighlight
sudo do-release-upgrade --allow-third-party
```

Accept the defaults for any questions asked by the upgrade script.

Reboot the machine when requested.

Check whether the upgrade was successful:

```nohighlight
lsb_release -a
```

A successful upgrade should respond with output similar to the following:

```nohighlight
Distributor ID:	Ubuntu
Description:	Ubuntu 20.04(.nn) LTS
Release:	20.04
Codename:	focal
```

If you’re upgrading from MAAS version 2.8 or lower to version 3.3: While the following procedures should work, note that they are untested. Use at your own risk. Start by making a verifiable backup; see step 1, below.

Back up your MAAS server completely; the tools and media are left entirely to your discretion. Just be sure that you can definitely restore your previous configuration, should this procedure fail to work correctly.

Add the MAAS 3.3 PPA to your repository list with the following command, ignoring any apparent error messages:

```nohighlight
sudo apt-add-repository ppa:maas/3.3
```

Run the release upgrade like this, answering any questions with the given default values:

```nohighlight
sudo do-release-upgrade --allow-third-party
```

Check whether your upgrade has been successful by entering:

```nohighlight
lsb_release -a
```

If the ugprade was successful, this command should yield output similar to the following:

```nohighlight
No LSB modules are available.
Distributor ID:	Ubuntu
Description:	Ubuntu 20.04(.nn) LTS
Release:	20.04
Codename:	focal
```

Check your running MAAS install (by looking at the information on the bottom of the machine list) to make sure you’re running the 3.3 release.

If this didn’t work, you will need to restore from the backup you made in step 1, and consider obtaining separate hardware to install MAAS 3.3.

** Ansible playbooks for HA MAAS, PostgreSQL, and other MAAS configurations

[Ansible](https://www.redhat.com/en/technologies/management/ansible/what-is-ansible)`↗` [playbooks](https://docs.ansible.com/ansible/latest/getting_started/get_started_playbook.html)`↗` are now available for MAAS.  These extended YAML files automate various routine aspects of MAAS setup and configuration.  

*** Ten words or less

Automate the drudgery of installing and configuring MAAS with Ansible.

*** Background explanations on Ansible

Ansible is more than scripting; it's a fully-featured automation tool that allows you to group modules (binaries or snippets of code) into specific tasks.  These tasks can be combined to build plays, and plays can be combined to build playbooks.  The idea is to abstract each level so you're not trying to keep track of which lines of Bash code you need to cut and past in between others.  

With MAAS 3.3, playbooks are available to:

- install and configure a MAAS region on a targeted host; running the playbook on hosts with a MAAS region will upgrade it.
- install and configure a MAAS rack.
- setup the postgres primary role.
- setup the postgres secondary role.

MAAS Playbooks are available from a [repository](https://github.com/maas/MAAS-ansible-playbook)`↗`.  They will eventually be available through Ansible Galaxy.

There is also a set of groups that will automate setting up specific sections of MAAS.  For example, there is a PostgreSQL group that sets up the primary and secondary PostgreSQL roles, bypassing the need to run both playbooks individually.

After installing ansible, running each of the playbooks on a blank machine will have a fresh install of MAAS ready to go. For example, running the region+rack will setup a region+rack on the host machine.

*** How you can use Ansible playbooks

In general terms, you can run any of the MAAS Ansible plays with a command of this form:

```nohighlight
ansible-playbook -i hosts \
--extra_vars \
"maas_version=$MAAS_VERSION 
maas_postgres_password=$MAAS_PG_PASSWORD 
maas_postgres_replication_password=$MAAS_PG_REP_PASSWORD 
maas_installation_type=<deb|snap> 
maas_url=$MAAS_URL" \
./site.yaml
```

A command of this form will run all of the plays below (i.e., the entire playbook). If you want to run the tasks for one particular role (or roles), you can use the form --tags <target role(s)> to limit which parts of the MAAS Ansible playbook run. Consult the Ansible documentation for more details on additional options and command structure.

[note]
There's some extra good news here:  These Ansible playbooks were built and tested with MAAS 3.2, so you can use them with all variants of MAAS 3.2.
[/note]

For example, suppose you want to install a MAAS region controller.  As an operator, you want want to install a MAAS region controller onto a given host using Ansible. To accomplish this, you need only:

- set a maas_region_controller role on a given host,
- run the region controller playbook,
- and find the newly-configured region controller present on that host .

To set the `maas_region_controller` role, you add that role to your Inventory file in the form of either an INI or a YAML file, where each role is followed by the addresses of each host to attach the role to. The example below attaches the region controller role to a host running on 10.10.0.20 with the user `ubuntu`:

```nohighlight
INI:

[maas_region_controller]
10.10.0.20 ansible_user=ubuntu
YAML:

all:
  maas_region_controller:
    hosts:
      10.10.0.20:
        ansible_user: ubuntu
```

When running the playbook for a host with the `maas_region_controller` role, the playbook installs the MAAS region controller. The documented ansible variable `maas_installation_type` provides the user with the ability to set whether it’s a deb installation or a snap installation, along with additional variables for MAAS version, snap channel and/or PPA.

The default installation is a snap. A successful run of the playbook should give the operator an accessible and ready MAAS instance.  The playbook uses an Ansible variable to determine what version of MAAS to deploy. The playbook won’t execute (i.e “skipped” in the context of Ansible) if the Ubuntu version is incompatible with the version and install method. The Region Controller tasks should be able to execute on multiple hosts in a single execution if the target is an Ansible Group rather than a single host.  The newly-installed region controller should be accessible at the specified host ip address, as though the controller had been installed manually.

Read the [Ansible playbooks reference](/t/how-to-spin-up-maas-with-ansible/6367) document to learn more about the feature and the additional playbooks that are available.

<!--
** Integration of Vault for credential storage

MAAS deals with a number of secrets (user password, certificates and keys, API tokens, …), currently stored in the database -- which is insecure by default. This configuration may not meet everyone's security requirements or regulations. For this reason, we've integrated MAAS with Hashicorp Vault, a well-established solution for secure, centralised credential storage.

You can read the [MAAS Vault reference](/t/maas-vault-reference/6368) documentation to learn more. -->

** Improved machine list filtering

MAAS 3.3 dramatically reduces the latency associated with refreshing large machine lists.

*** Ten words or less

You can filter machines mere seconds after one page loads.

*** How list filtering is improved

[note]
**NOTE** that this feature is still in development, so some of the feature-set described in this section may not be fully operational yet.  As always, we reserve the right to change this feature-set until the final release of MAAS 3.3. These release notes will be updated as the feature develops.
[/note]

MAAS 3.3 enhances the way you can filter the machine list, in two ways:

1. You may begin filtering within a very short time after the first page of the machine list loads, even if you have more than 10,000 machines in the list.  

2. You have a wider range of filter choices, as described in the table below.

Note that with this version of MAAS, matching machine counts have been removed from the filter list for better performance.

*** More filter parameters have been added

The following table describes the expanded filter set for the MAAS machine list:

- Items marked "Dyn" are dynamic, populated based on existing data, that is, the "Tags" filter only shows tags that currently exist.  
- Items which are not dynamic present the entire range of possible values, regardless of whether that value currently exists in MAAS; for example, all machine status values are available to be filtered, even if no machines currently have that status.
- Items marked "Grp" can be used to group machines, instead of the default machine status.
- Items marked "Man" must be manually entered, i.e., they are not in the UI filter dropdown, but can be entered in the "Search" box if properly formatted (as in the examples given).

See [How to search MAAS](/t/how-to-find-machines/5192) for more details on how to use these parameters.


| Parameter (bold) w/example           | Shows nodes...                   | Dyn | Grp | Man |
|--------------------------------------|----------------------------------|-----|-----|-----|
| **arch**:(=architecture)             | with "architecture"              |     | Grp |     |
| arch:(!=architecture)                | NOT with "architecture"          | Dyn |     |     |
| **zone**:(=zone-name)                | in "zone-name"                   | Dyn | Grp |     |
| zone:(!=zone-name)                   | NOT in "zone-name"               | Dyn |     |     |
| **pool**:(=resource-pool)            | in "resource-pool"               | Dyn | Grp |     |
| pool:(!=resource-pool)               | NOT in "resource-pool"           | Dyn |     |     |
| **pod**:(=pod-name)                  | with "pod-name"                  | Dyn | Grp |     |
| pod:(!=pod-name)                     | NOT with "pod-name"              | Dyn |     |     |
| **pod_type**:(=pod-type)             | with power type "pod-type"       | Dyn | Grp | Man |
| pod_type:(!=pod-type)                | NOT with power type "pod-type"   | Dyn |     | Man |
| **domain**:(=domain-name)            | with "domain-name"               | Dyn | Grp | Man |
| domain:(!=domain-name)               | NOT with "domain-name"           | Dyn |     | Man |
| **status**:(=op-status)              | having "op-status"               |     | Grp |     |
| status:(!=op-status)                 | NOT having "op-status"           | Dyn |     |     |
| **owner**:(=user)                    | owned by "user"                  | Dyn | Grp |     |
| owner:(!=user)                       | NOT owned by "user"              | Dyn |     |     |
| **power_state**:(=power-state)       | having "power-state"             |     | Grp | Man |
| power_state:(!=power-state)          | NOT having "power-state"         | Dyn |     | Man |
| **tags**:(=tag-name)                 | with tag "tag-name"              | Dyn |     |     |
| tags:(!=tag-name)                    | NOT with tag "tag-name"          | Dyn |     |     |
| **fabrics**:(=fabric-name)           | in "fabric-name"                 | Dyn |     |     |
| fabrics:(!=fabric-name)              | NOT in "fabric-name"             | Dyn |     |     |
| **fabric_classes**:(=fabric-class)   | in "fabric-class"                | Dyn |     | Man |
| fabric_classes:(!=fabric-class)      | NOT in "fabric-class"            | Dyn |     | Man |
| **fabric_name**:(=fabric-name)       | in "boot-interface-fabric"       | Dyn |     | Man |
| fabric_name:(!=fabric-name)          | NOT in "boot-interface-fabric"   | Dyn |     | Man |
| **subnets**:(=subnet-name)           | attached to "subnet-name"        | Dyn |     |     |
| subnets:(!=subnet-name)              | Not attached to "subnet-name"    | Dyn |     |     |
| **link_speed**:(link-speed)          | having "link-speed"              | Dyn |     | Man |
| link_speed:(!link-speed)             | NOT having "link-speed"          | Dyn |     | Man |
| **vlans**:(=vlan-name)               | attached to "vlan-name"          | Dyn |     |     |
| vlans:(!=vlan-name)                  | NOT attached to "vlan-name"      | Dyn |     |     |
| **storage**:(storage-MB)             | having "storage-MB"              | Dyn |     | Man |
| **total_storage**:(total-stg-MB)     | having "total-stg-MB"            | Dyn |     | Man |
| total_storage:(!total-stg-MB)        | NOT having "total-stg-MB"        | Dyn |     | Man |
| **cpu_count**:(cpu-count)            | having "cpu-count"               | Dyn |     | Man |
| cpu_count:(!cpu-count)               | NOT having "cpu-count"           | Dyn |     | Man |
| **mem**:(ram-in-MB)                  | having "ram-in-MB"               | Dyn |     | Man |
| mem:(!ram-in-MB)                     | NOT having "ram-in-MB"           | Dyn |     | Man |
| **mac_address**:(=MAC)               | having MAC address "MAC"         | Dyn |     | Man |
| mac_address:(!=MAC)                  | NOT having                       | Dyn |     | Man |
| **agent_name**:(=agent-name)         | Include nodes with agent-name    | Dyn |     | Man |
| agent_name:(!=agent-name)            | Exclude nodes with agent-name    | Dyn |     | Man |
| **cpu_speed**:(cpu-speed-GHz)        | CPU speed                        | Dyn |     | Man |
| cpu_speed:(!cpu-speed-GHz)           | CPU speed                        | Dyn |     | Man |
| **osystem**:(=os-name)               | The OS of the desired node       | Dyn |     | Man |
| osystem:(!=os-name)                  | OS to ignore                     | Dyn |     | Man |
| **distro_series**:(=distro-name)     | Include nodes using distro       | Dyn |     | Man |
| distro_series:(!=distro-name)        | Exclude ndoes using distro       | Dyn |     | Man |
| **ip_addresses**:(=ip-address)       | Node's IP address                | Dyn |     | Man |
| ip_addresses:(!=ip-address)          | IP address to ignore             | Dyn |     | Man |
| **spaces**:(=space-name)             | Node's spaces                    | Dyn |     |     |
| spaces:(!=space-name)                | Node's spaces                    | Dyn |     |     |
| **workloads**:(=annotation-text)     | Node's workload annotations      | Dyn |     |     |
| workloads:(!=annotation-text)        | Node's workload annotations      | Dyn |     |     |
| **physical_disk_count**:(disk-count) | Physical disk Count              | Dyn |     | Man |
| physical_disk_count:(!disk-count)    | Physical disk Count              | Dyn |     | Man |
| **pxe_mac**:(=PXE-MAC)               | Boot interface MAC address       | Dyn |     | Man |
| pxe_mac:(!=PXE-MAC)                  | Boot interface MAC address       | Dyn |     | Man |
| **fqdn**:(=fqdn-value)               | Node FQDN                        | Dyn |     | Man |
| fqdn:(!=fqdn-value)                  | Node FQDN                        | Dyn |     | Man |
| **simple_status**:(=status-val)      | Include nodes with simple-status | Dyn |     | Man |
| simple_status:(!=status-val)         | Exclude nodes with simple-status | Dyn |     | Man |
| **devices**:(=)                      | Devices                          | Dyn |     | Man |
| **interfaces**:(=)                   | Interfaces                       | Dyn |     | Man |
| **parent**:(=)                       | Parent node                      | Dyn | Grp | Man |

** Native support for 22.04 LTS and core22

MAAS can now be installed as a PPA, directly on Ubuntu 22.04, without the need to use snaps.

*** Ten words or less

MAAS packages now run on Ubuntu 22.04, aka Jammy Jellyfish.

<a href="#heading--Notes-on-22.04-LTS-MAAS-packages">*** id="heading--Notes-on-22.04-LTS-MAAS-packages">Notes on 22.04 LTS MAAS packages

MAAS users want to install MAAS on a 22.04 LTS system via deb packages, as well as upgrade machines currently running MAAS on Ubuntu 20.04 LTS to 22.04 LTS.  With the advent of MAAS 3.3, we have created an appropriate PPA with all required dependencies.  This PPA can be directly installed on Ubuntu 22.04, Jammy Jellyfish, with no requirement to use snaps.

Note that the upgrade procedure will require a release upgrade from previous Ubuntu versions to Ubuntu 22.04.  Also note that, with this version of MAAS, PostgreSQL 12 is deprecated and should be upgraded to PostgreSQL 14.  The [installation guide](/t/how-to-install-maas/5128) provides the necessary details.

<!--
** Reliability improvements for simultaneous machine deployments

MAAS 3.3 brings some behind-the-scenes performance improvements to the product.  When you deploy many machines, you expect them all deploy reliably, with IPs allocated in bulk, and no DNS delays or RPC timeouts.  Within our development system, we've added system tests and metrics to track any failures or latency when large numbers of machines are being deployed.  We've then used these new data to lower the failure rate and reduce latency in those situations.

** The first phase of Nvidia DPU support

Long-term, we know that MAAS administrators want to enlist and use DPUs with MAAS.  The Nvidia DPU is a complex machine with a tremendous amount of capability, so this cycle, we made the first steps toward supporting them.  Details will follow in a forward-looking blog post sometime after the MAAS 3.3 release.
-->

** UI performance improvements

We wanted to improve the performance of the machine list page for large (>10000 machines) MAASes, and allow users to search and filter machines as quickly as possible. 

*** Ten words or less

We're working on making large machine lists load in background.

*** Background performance work

In MAAS 3.2 and earlier, machine search and filter requires that all machines be fetched by the UI client before it becomes usable. For smaller MAASes this may not be an issue, but when considering MAASes with 1000 machines or more this can make the user wait an unacceptably long time before they can search and filter.  With the release of MAAS 3.3, when a MAAS UI user wants to find a particular machine, they do not have to wait for all their machines data to load before they can start searching. The user can start searching for machines within a short time after the visible page of the machine list has fully loaded on the UI screen.  See [Improved machine list filtering](#heading--Improved-machine-list-filtering), in these release notes, for details on the enhanced filtering capabilities that were included in this work.

** Enhanced MIB support for Windows OS images

The [procedure](/t/how-to-build-custom-images/5104#heading--custom-windows-images) for creating custom Windows OS images has been thoroughly updated and verified.

*** Ten words or less

MAAS custom Windows images now support most releases and options.

*** What has been added to Windows custom images

Specifically, MIB now supports a much wider range of Windows images.  Previously, only 2012 and 2106 Windows versions were supported with MIB.  Now the list is much longer, bringing deployable MAAS versions up to date with the current Windows releases:

 - win2008r2
 - win2008hvr2
 - win2012
 - win2012hv
 - win2012r2
 - win2012hvr2
 - win2016
 - win2016-core
 - win2016hv
 - win2016dc
 - win2016dc-core
 - win2019
 - win2019-core
 - win2019dc
 - win2019dc-core
 - win10ent
 - win10ent-eval
 - win2022
 - win2022-core

There are also special instructions for using both UEFI and BIOS bootloaders, as well as instructions for using LXD containers with custom-built Windows images.  

Finally, MIB has been extended to accept a much wider range of options for windows builds.  Some of the new Windows-specific options include:

 - --windows-iso: path to the Windows ISO image.
 - --windows-edition: identifier for the Windows edition/option being installed (see above).
 - --windows-license-key: Windows license key (required with non-evaluation editions)
 - --windows-language: Windows installation language (default: en-US)
 - --windows-updates: download and install Windows Updates (requires internet access; might require a larger --disk-size option)
 - --windows-drivers: path to directory with Windows drivers to be installed (requires internet access; uses the Windows Driver Kit, by default)
 - --driver-store: combined with --windows-drivers, uses the Windows Driver Store to install drivers early into Windows Setup and image (does not require internet access; does not use the Windows Driver Kit).

Some news Windows-specific platform options include:

 - --uefi: use UEFI partition layout and firmware
 - --virtio: use paravirtualized VirtIO SCSI and VirtIO NET devices (instead of emulated devices) for installation (requires --windows-drivers)
 - --disk-size: specify the (virtual) disk size for Windows setup (must be larger for --windows-updates; increases deployment/copy-to-disk time, and is expanded to physical disk size during deployment)

This update should make it much simpler to use custom-built Windows images with MAAS.


** Shifting the MAAS API documentation to OpenAPI standards

MAAS API User want to experience the MAAS API in a more standard way, along the lines of the OpenAPI definition.  MAAS 3.3 begins this process by providing most of the MAAS API functionality in a discover-able form.  You should now be able to easily retrieve human-readable service documentation and API definitions using standard methods.  Consult [the API documentation](https://maas.io/docs/api)`↗` for details.

** MAAS configuration settings reference

MAAS 3.3 documentation consolidates configuration settings in one article, in addition to their other mentions throughout the documentation set.

*** Ten words or less

"Settings" now has its own page, and some new options.

*** What is new about this update

MAAS configuration settings are scattered in various (generally relevant) places throughout the documentation, but there has never been one reference page that addresses all settings in one place.  MAAS 3.3 remedies this by adding the [Configuration settings reference](/t/how-to-change-maas-settings/6347).

A minor new feature added with MAAS 3.3 is MAAS site identity, which enables some new configuration parameters:

- MAAS name: The “* MAAS name” is a text box that sets the text which appears at the bottom of every MAAS screen, in front of the version descriptor.

- MAAS name emoji: You may also paste a suitable emoji in front of the MAAS name to help identify it.

- MAAS theme main colour: You may also help identify your MAAS instance by changing the colour of the top bar; several colour choices are available.

These enhancements were made available to assist users who have more than one instance (e.g., production and staging), and have issues with operations accidentally making changes to the wrong instance.

** Improved MAAS event documentation

MAAS event documentation has been expanded to include [much better explanations](/t/understanding-maas-events/6373) of MAAS events, including many examples.

*** Ten words or less

We've finally documented MAAS events, making them easier to decode.

*** Understanding MAAS events

Events are state changes that happen to MAAS elements, caused by MAAS itself, an external agent, or a users. Understanding events is an essential debugging skill.  But events appear in three different places in MAAS, each presentation providing slightly different information.  These screens are usually dense and hard to search.

In this major documentation update, we've standardised on the MAAS CLI events query command as the best way to review, filter, and summarise events.  We've summarised the six main event types:

 - INFO: the default, used if no level= is specified; shows INFO and ERROR events. A typical INFO event is “Ready”, indicating that a machine has reached the “Ready” state.

 - CRITICAL: critical MAAS failures; shows only CRITICAL events. These events usually represent severe error conditions that should be immediately remedied.

 - ERROR: MAAS errors; shows only ERROR events. Typical ERROR events include such things as power on/off failures, commissioning timeouts, and image import failures.

 - WARNING: failures which may or may not affect MAAS performance; shows WARNING and ERROR events. A typical warning event, for example, might include the inability to find and boot a machine.

 - DEBUG: information which would help debug MAAS behaviour; shows DEBUG and INFO events. Typical DEBUG events involve routine image import activities, for example.

 - AUDIT: information which helps determine settings and user actions in MAAS; shows only AUDIT events. They are covered in more detail elsewhere.

In addition, the new document explains how these event types tend to overlap when queried.  We've also provide detailed instructions on how to use the most common filters:

 - hostname: Only events relating to the node with the matching hostname will be returned. This can be specified multiple times to get events relating to more than one node.

 - mac_address: Only nodes with matching MAC addresses will be returned. Note that MAC address is not part of the standard output, so you’d need to look it up elsewhere.

 - id: Only nodes with matching system IDs will be returned. This corresponds to the node parameter in the JSON listing, not the id parameter there, which is a serial event number.

 - zone: Only nodes in the zone will be returned. Note that zones are not part of the standard output, so you’d need to look these up elsewhere.

 - level: The event level to capture. You can choose from AUDIT, CRITICAL, DEBUG, ERROR, INFO, or WARNING. The default is INFO.

 - limit: Number of events to return. The default is 100, the maximum in one command is 1000.

 - before: Defines an event id to start returning older events. This is the “id” part of the JSON, not the system ID or “node”. Note that before and after cannot be used together, as the results are unpredictable.

 - after: Defines an event id to start returning newer events. This is the “id” part of the JSON, not the system ID or “node”. Note that before and after cannot be used together, as the results are unpredictable.

Since the MAAS CLI returns JSON -- which is hard to humans to parse -- we've included some exemplary `jq` predicates of the form:

```nohighlight
maas $PROFILE events query limit=20 \
| jq -r '(["USERNAME","NODE","HOSTNAME","LEVEL","DATE","TYPE","EVENT"] | 
(., map(length*"-"))),
(.events[] | [.username,.node,.hostname,.level,.created,.type,.description]) 
| @tsv' | column -t -s$'\t'
```

And finally, we provided some detailed usage examples.  For instance, we walked a MAAS machine called `fun-zebra` through the following states:

 - Commissioning
 - Allocation
 - Deployment
 - Releasing
 - Testing (with a premature manual abort)
 - Rescue mode

We used this example command:

```nohighlight
 maas $PROFILE events query level=INFO hostname=fun-zebra limit=1000 | jq -r '(["USERNAME","NODE","HOSTNAME","LEVEL","DATE","TYPE","EVENT"] | (., map(length*"-"))),(.events[] | [.username,.node,.hostname,.level,.created,.type,.description]) | @tsv' | column -t -s$'\t'
```

This gave us a reasonably thorough report of what happened to the machine:

```nohighlight
USERNAME  NODE    HOSTNAME   LEVEL  DATE                        TYPE                   EVENT
--------  ----    --------   -----  ----                        ----                   -----
unknown   bk7mg8  fun-zebra  INFO   Thu, 29 Sep. 2022 21:29:53  Exited rescue mode     
unknown   bk7mg8  fun-zebra  INFO   Thu, 29 Sep. 2022 21:29:52  Powering off           
unknown   bk7mg8  fun-zebra  INFO   Thu, 29 Sep. 2022 21:28:58  Rescue mode            
unknown   bk7mg8  fun-zebra  INFO   Thu, 29 Sep. 2022 21:27:18  Loading ephemeral      
unknown   bk7mg8  fun-zebra  INFO   Thu, 29 Sep. 2022 21:26:40  Performing PXE boot    
unknown   bk7mg8  fun-zebra  INFO   Thu, 29 Sep. 2022 21:26:23  Power cycling          
unknown   bk7mg8  fun-zebra  INFO   Thu, 29 Sep. 2022 21:26:23  Entering rescue mode   
unknown   bk7mg8  fun-zebra  INFO   Thu, 29 Sep. 2022 21:26:14  Powering off           
unknown   bk7mg8  fun-zebra  INFO   Thu, 29 Sep. 2022 21:26:14  Aborted testing        
unknown   bk7mg8  fun-zebra  INFO   Thu, 29 Sep. 2022 21:24:08  Performing PXE boot    
unknown   bk7mg8  fun-zebra  INFO   Thu, 29 Sep. 2022 21:23:51  Powering on            
unknown   bk7mg8  fun-zebra  INFO   Thu, 29 Sep. 2022 21:23:51  Testing                
unknown   bk7mg8  fun-zebra  INFO   Thu, 29 Sep. 2022 21:23:38  Released               
unknown   bk7mg8  fun-zebra  INFO   Thu, 29 Sep. 2022 21:23:37  Powering off           
unknown   bk7mg8  fun-zebra  INFO   Thu, 29 Sep. 2022 21:23:37  Releasing              
unknown   bk7mg8  fun-zebra  INFO   Thu, 29 Sep. 2022 21:22:41  Deployed               
unknown   bk7mg8  fun-zebra  INFO   Thu, 29 Sep. 2022 21:21:49  Rebooting              
unknown   bk7mg8  fun-zebra  INFO   Thu, 29 Sep. 2022 21:18:42  Configuring OS         
unknown   bk7mg8  fun-zebra  INFO   Thu, 29 Sep. 2022 21:17:42  Installing OS          
unknown   bk7mg8  fun-zebra  INFO   Thu, 29 Sep. 2022 21:17:30  Configuring storage    
unknown   bk7mg8  fun-zebra  INFO   Thu, 29 Sep. 2022 21:15:31  Loading ephemeral      
unknown   bk7mg8  fun-zebra  INFO   Thu, 29 Sep. 2022 21:14:48  Performing PXE boot    
unknown   bk7mg8  fun-zebra  INFO   Thu, 29 Sep. 2022 21:14:31  Powering on            
unknown   bk7mg8  fun-zebra  INFO   Thu, 29 Sep. 2022 21:14:27  Deploying              
unknown   bk7mg8  fun-zebra  INFO   Thu, 29 Sep. 2022 20:04:17  Ready                  
unknown   bk7mg8  fun-zebra  INFO   Thu, 29 Sep. 2022 20:04:07  Running test           smartctl-validate on sda
unknown   bk7mg8  fun-zebra  INFO   Thu, 29 Sep. 2022 20:01:27  Gathering information  
unknown   bk7mg8  fun-zebra  INFO   Thu, 29 Sep. 2022 20:01:10  Loading ephemeral      
unknown   bk7mg8  fun-zebra  INFO   Thu, 29 Sep. 2022 20:00:35  Performing PXE boot    
unknown   bk7mg8  fun-zebra  INFO   Thu, 29 Sep. 2022 20:00:16  Powering on            
unknown   bk7mg8  fun-zebra  INFO   Thu, 29 Sep. 2022 20:00:16  Commissioning          
```

Additional examples and techniques are provided as part of this new documentation.

** Improved MAAS audit event documentation

MAAS audit event documentation has been greatly expanded to include [much better explanations](/t/understanding-maas-audit-events/6372) of MAAS audit events, including [detailed examples of how to reconstruct machine life-cycles](/t/how-to-work-with-audit-event-logs/5987#heading--How-to-audit-a-machines-life-cycle-with-audit-events) in the updated version of "[How to work with audit event logs](/t/how-to-work-with-audit-event-logs/5987)".

*** Ten words or less

We've finally offered details about how you should audit MAAS.

*** Understanding how audit events explain MAAS internal operations

There's probably no limit to what you can figure out if you use audit events properly.  The problems are: (1) a lot goes on in MAAS, and (2) you need more than just the explicit audit events to get a clear picture of what's happening.  We've tried to address this by taking a deeper look at the auditing process (not just the events).  

As you may know, an audit event is just a [MAAS event](/t/understanding-maas-events/6373) tagged with `AUDIT`. It generally captures changes to the MAAS configuration and machine states. These events provide valuable oversight of user actions and automated updates -- and their effects -- especially when multiple users are interacting with multiple machines.  

*** Viewing events

Audit events are examined using the MAAS CLI with the `level=AUDIT` parameter set:

```nohighlight
$ maas $PROFILE events query level=AUDIT
```

You'll probably get better results by appending a `jq` filter, to prettify the output:

```nohighlight
$ maas $PROFILE events query level=AUDIT after=0 limit=20 \
| jq -r '(["USERNAME","HOSTNAME","DATE","EVENT"] | 
(., map(length*"-"))),
(.events[] | [.username,.hostname,.created,.description]) 
| @tsv' | column -t -s$'\t'
```

By itself, such a command might produce output similar to this:

```nohighlight
USERNAME  HOSTNAME     DATE                        EVENT
--------  --------     ----                        -----
unknown   valued-moth  Thu, 21 Apr. 2022 19:45:14  pci device 2 was updated on node 8wmfx3
unknown   valued-moth  Thu, 21 Apr. 2022 19:45:14  pci device 1 was updated on node 8wmfx3
unknown   valued-moth  Thu, 21 Apr. 2022 19:45:14  pci device 1 was updated on node 8wmfx3
unknown   valued-moth  Thu, 21 Apr. 2022 19:45:14  pci device 1 was updated on node 8wmfx3
unknown   valued-moth  Thu, 21 Apr. 2022 19:45:14  pci device 1 was updated on node 8wmfx3
unknown   valued-moth  Thu, 21 Apr. 2022 19:45:14  pci device 1 was updated on node 8wmfx3
unknown   valued-moth  Thu, 21 Apr. 2022 19:45:14  pci device 1 was updated on node 8wmfx3
unknown   valued-moth  Thu, 21 Apr. 2022 19:45:14  pci device 1 was updated on node 8wmfx3
unknown   valued-moth  Thu, 21 Apr. 2022 19:45:14  pci device 1 was updated on node 8wmfx3
unknown   valued-moth  Thu, 21 Apr. 2022 19:45:14  pci device 0 was updated on node 8wmfx3
unknown   valued-moth  Thu, 21 Apr. 2022 19:45:14  block device sda was updated on node 8wmfx3
unknown   valued-moth  Thu, 21 Apr. 2022 19:45:14  interface enp5s0 was updated on node 8wmfx3
unknown   valued-moth  Thu, 21 Apr. 2022 19:45:14  0 bytes of memory was removed on node 8wmfx3
admin     valued-moth  Thu, 21 Apr. 2022 19:36:48  Started deploying 'valued-moth'.
admin     valued-moth  Thu, 21 Apr. 2022 19:36:21  Acquired 'valued-moth'.
admin     unknown      Thu, 21 Apr. 2022 19:21:46  Updated configuration setting 'completed_intro' to 'True'.
admin     unknown      Thu, 21 Apr. 2022 19:20:49  Updated configuration setting 'upstream_dns' to '8.8.8.8'.
admin     unknown      Thu, 21 Apr. 2022 19:20:49  Updated configuration setting 'maas_name' to 'neuromancer'.
admin     unknown      Thu, 21 Apr. 2022 19:20:47  Updated configuration setting 'http_proxy' to ''.
admin     unknown      Thu, 21 Apr. 2022 19:20:24  Logged in admin.
```

You can, of course, use the [various event filters](/t/understanding-maas-events/6373#heading--filter-parameters) with `level=AUDIT` to further restrict your output.

*** The meaning of audit events

Later on in the documentation, we walk through a sample of audit events and demonstrate how to interpret and use them.  This includes detailed examples of various audit event queries, walking through real-world examples to answer questions like:

1. Who deployed `comic-muskox`? 

2. What happened to `sweet-urchin`?

3. Why is `fleet-calf` in rescue mode?

4. Where did these changes come from in `setup.sh`?

5. What caused `ruling-bobcat` to be marked as broken?

6. Who's responsible for the DHCP snippet called `foo`?

*** Auditing with finesse

As part of the updates to our "[How to work with audit event logs](/t/how-to-work-with-audit-event-logs/5987)", we've tried to offer you some finesse in reconstructing machine life-cycles.  We've shown how to combine various levels of MAAS event queries with standard command line utilities to produce clear audit trails such as this one:

```nohighlight
418606  ERROR    Marking node broken               Wed, 17 Nov. 2021 00:02:52  A Physical Interface requires a MAC address.
418607  DEBUG    Node changed status               Wed, 17 Nov. 2021 00:02:52  From 'New' to 'Broken'
418608  DEBUG    Marking node fixed                Wed, 17 Nov. 2021 00:04:24  
418609  DEBUG    Node changed status               Wed, 17 Nov. 2021 00:04:24  From 'Broken' to 'Ready'
418613  DEBUG    User acquiring node               Wed, 17 Nov. 2021 00:04:51  (admin)
418614  DEBUG    Node changed status               Wed, 17 Nov. 2021 00:04:51  From 'Ready' to 'Allocated' (to admin)
418615  DEBUG    User starting deployment          Wed, 17 Nov. 2021 00:04:51  (admin)
418616  DEBUG    Node changed status               Wed, 17 Nov. 2021 00:04:51  From 'Allocated' to 'Deploying'
418617  INFO     Deploying                         Wed, 17 Nov. 2021 00:04:51  
418618  AUDIT    Node                              Wed, 17 Nov. 2021 00:04:51  Started deploying 'ruling-bobcat'.
418619  INFO     Powering on                       Wed, 17 Nov. 2021 00:04:55  
418625  ERROR    Marking node failed               Wed, 17 Nov. 2021 00:05:32  Power on for the node failed: Failed talking to node's BMC: Failed to power pbpncx. BMC never transitioned from off to on.
418626  DEBUG    Node changed status               Wed, 17 Nov. 2021 00:05:32  From 'Deploying' to 'Failed deployment'
418627  ERROR    Failed to power on node           Wed, 17 Nov. 2021 00:05:32  Power on for the node failed: Failed talking to node's BMC: Failed to power pbpncx. BMC never transitioned from off to on.
```

In this case, we managed to recognise, rather quickly, that no physical interface had been defined for `ruling-bobcat`, hence deployment fails because MAAS can't communicate with the node's BMC.  There are many other issues you can recognise with careful use of MAAS events to audit machine behaviours.  We welcome your feedback on this new documentation endeavour.

<a href="#heading--MAAS-3.3-bug-list">** id="heading--MAAS-3.3-bug-list">MAAS 3.3 bug list

The following sections enumerate the bugs we've fixed in MAAS 3.3.

*** MAAS 3.3 Beta 1 bug list

So far in MAAS 3.3, we've fixed well over 100 bugs:

- [1762673](https://bugs.launchpad.net/bugs/1762673)`↗`: maas insists on running the proxy, even when it's disabled
- [1802505](https://bugs.launchpad.net/bugs/1802505)`↗`: [ui][2.4][2.5] maas ignores ttl parameter for address records
- [1806707](https://bugs.launchpad.net/bugs/1806707)`↗`: [2.5] Composing a VM with an interface attached to a (macvtap) network (on a KVM host NIC that is not a bridge) resulted in disconnect interface
- [1811109](https://bugs.launchpad.net/bugs/1811109)`↗`: [2.5, UI, RBAC] Normal users can unmount the root file system, but not remount it
- [1818004](https://bugs.launchpad.net/bugs/1818004)`↗`: Slow response in the UI
- [1822640](https://bugs.launchpad.net/bugs/1822640)`↗`: [websocket, UI] Admins should be able to change ownership of resources over the UI
- [1822840](https://bugs.launchpad.net/bugs/1822840)`↗`: [UI, feature] Add ability to edit/delete (manage) tags over the UI
- [1825255](https://bugs.launchpad.net/bugs/1825255)`↗`: TestPostgresListenerService test fails erroneously in CI
- [1826011](https://bugs.launchpad.net/bugs/1826011)`↗`: [UI] Compose machine from pod misaligned message
- [1826789](https://bugs.launchpad.net/bugs/1826789)`↗`: stress-ng-cpu-long times out in bionic
- [1826967](https://bugs.launchpad.net/bugs/1826967)`↗`: Exiting rescue mode shows 'loading ephemeral'
- [1833545](https://bugs.launchpad.net/bugs/1833545)`↗`: After removing a controller rackd still tries to update DNS
- [1840049](https://bugs.launchpad.net/bugs/1840049)`↗`: [UI] When changing configuration of an Interface, one has to enter the config twice
- [1852360](https://bugs.launchpad.net/bugs/1852360)`↗`: Validate network configuration button selects all network scripts that accept an interface parameter
- [1863395](https://bugs.launchpad.net/bugs/1863395)`↗`: 2.6.2 Unable to change power type to manual via UI
- [1871605](https://bugs.launchpad.net/bugs/1871605)`↗`: Updating controller name shouldn't be allowed in the UI
- [1874355](https://bugs.launchpad.net/bugs/1874355)`↗`: Controller details page not updated to match machine details page designs
- [1881948](https://bugs.launchpad.net/bugs/1881948)`↗`: IPv6 address for power control fails
- [1882633](https://bugs.launchpad.net/bugs/1882633)`↗`: Logical volume size is required
- [1883232](https://bugs.launchpad.net/bugs/1883232)`↗`: UI: UI application cached after upgrade
- [1890262](https://bugs.launchpad.net/bugs/1890262)`↗`: UI: Error message with a custom image URL doesn't clear
- [1893226](https://bugs.launchpad.net/bugs/1893226)`↗`: Machine-specific minimal commissioning kernel resets to MAAS wide default
- [1893670](https://bugs.launchpad.net/bugs/1893670)`↗`: UI: display bios_boot_mode in the web UI
- 1898131: IP address gets hidden, instead of subnet when window is resized
- [1905636](https://bugs.launchpad.net/bugs/1905636)`↗`: UI: CentOS 7 is the default over CentOS 8
- [1909348](https://bugs.launchpad.net/bugs/1909348)`↗`: MAAS 2.9.0 DNS zone remove @ labels impossible
- [1913800](https://bugs.launchpad.net/bugs/1913800)`↗`: PCI and USB information missing from controllers page
- [1918963](https://bugs.launchpad.net/bugs/1918963)`↗`: Controllers page out of sync with nodes
- [1918978](https://bugs.launchpad.net/bugs/1918978)`↗`: doesn't detect the subarchitecture xgene-uboot for a HP m400 cartridge
- [1927748](https://bugs.launchpad.net/bugs/1927748)`↗`: Need additional form inputs for DHCP Snippets associated with ipranges
- [1929478](https://bugs.launchpad.net/bugs/1929478)`↗`: Commissioning fails with binary data in IPMI Lan_Conf_Security_Keys
- [1929973](https://bugs.launchpad.net/bugs/1929973)`↗`: "Controllers have different installation sources." is not shown on the controllers page
- [1931654](https://bugs.launchpad.net/bugs/1931654)`↗`: domain.set_default error handling just returns id
- [1933408](https://bugs.launchpad.net/bugs/1933408)`↗`: Setting discovery parent returns cryptic error
- [1938296](https://bugs.launchpad.net/bugs/1938296)`↗`: MAAS 3.0 incorrectly calculates the amount of free space on drive
- [1940909](https://bugs.launchpad.net/bugs/1940909)`↗`: UI: Allow to create a machine as deployed from the UI
- [1951229](https://bugs.launchpad.net/bugs/1951229)`↗`: CLI: Uninformative errors when adding non-existent tags to machines
- [1955671](https://bugs.launchpad.net/bugs/1955671)`↗`: support for rocky linux UEFI
- [1956766](https://bugs.launchpad.net/bugs/1956766)`↗`: UI: Unable to deploy CentOS7 - centos/focal not a supported combination
- [1958817](https://bugs.launchpad.net/bugs/1958817)`↗`: Cannot delete a tag from multiple machines in a bulk with Web UI
- [1959856](https://bugs.launchpad.net/bugs/1959856)`↗`: newly added tags in UI don't show until refresh/page change
- [1960571](https://bugs.launchpad.net/bugs/1960571)`↗`: Domain name should be checked for duplicate against maas_internal_domain
- [1961627](https://bugs.launchpad.net/bugs/1961627)`↗`: confusing UI to add the first network space
- [1964024](https://bugs.launchpad.net/bugs/1964024)`↗`: smartctl-validate test runs even when explicitly removed from commissioning step
- [1965172](https://bugs.launchpad.net/bugs/1965172)`↗`: [3.1] Setting interface into unconfigured does not reset auto-assign IP mode
- [1967577](https://bugs.launchpad.net/bugs/1967577)`↗`: commissioning fails w/ 3.2-beta1: "please fill out the field"
- [1970803](https://bugs.launchpad.net/bugs/1970803)`↗`: CLI event filters give extraneous results with more than one filter
- [1971152](https://bugs.launchpad.net/bugs/1971152)`↗`: Authentication broken with MASS using Candid
- [1973236](https://bugs.launchpad.net/bugs/1973236)`↗`: MAAS reports failure to detect storage that it already detected
- [1973617](https://bugs.launchpad.net/bugs/1973617)`↗`: refresh a lxd KVM host resources after it was added
- [1976194](https://bugs.launchpad.net/bugs/1976194)`↗`: init rack can't find secrets
- [1976196](https://bugs.launchpad.net/bugs/1976196)`↗`: Controller WebSocket handler contains unimplemented methods
- [1977820](https://bugs.launchpad.net/bugs/1977820)`↗`: Some tests are skipped due to the "perf" filtering
- [1977822](https://bugs.launchpad.net/bugs/1977822)`↗`: ERROR: Redfish 'Redfish' object has no attribute '_get_network_interface'
- [1977864](https://bugs.launchpad.net/bugs/1977864)`↗`: 30-maas-01-bmc-config: ERROR: Redfish string indices must be integers
- [1977866](https://bugs.launchpad.net/bugs/1977866)`↗`: 30-maas-01-bmc-config: ERROR: 169.254.95.120/16 has host bits set
- [1977895](https://bugs.launchpad.net/bugs/1977895)`↗`: Certificate metadata missing from controller websocket model
- [1977942](https://bugs.launchpad.net/bugs/1977942)`↗`: 30-maas-01-bmc-config: ERROR: Redfish 'Redfish' object has no attribute '_bmc_config'
- [1977951](https://bugs.launchpad.net/bugs/1977951)`↗`: 30-maas-01-bmc-config: ERROR: Redfish nonnumeric port: 'None'
- [1978024](https://bugs.launchpad.net/bugs/1978024)`↗`: maas auto-creates interface name for docker bridge on controller, which breaks bind
- [1978037](https://bugs.launchpad.net/bugs/1978037)`↗`: Drop legacy /l/ UI prefix
- [1978072](https://bugs.launchpad.net/bugs/1978072)`↗`: 30-maas-01-bmc-config: ERROR: Redfish 'dict' object has no attribute 'split'
- [1978121](https://bugs.launchpad.net/bugs/1978121)`↗`: 30-maas-01-bmc-config: ERROR: ERROR: Unable to add BMC user!
- [1978154](https://bugs.launchpad.net/bugs/1978154)`↗`: MAAS 3.1 introduces breaking changes for custom centos7 images
- [1978922](https://bugs.launchpad.net/bugs/1978922)`↗`: MAAS 3.1 - Missing button "Create datastore" in VMFS7 storage layout
- [1979039](https://bugs.launchpad.net/bugs/1979039)`↗`: TLS certificates are not recognised by CLI maas <profile> boot-resources create action
- [1979256](https://bugs.launchpad.net/bugs/1979256)`↗`: Add config option for UI theme
- [1979316](https://bugs.launchpad.net/bugs/1979316)`↗`: UI stuck at the initial configuration page
- [1979317](https://bugs.launchpad.net/bugs/1979317)`↗`: Initial configuration form doesn't allow proxy URL with hostname
- [1980347](https://bugs.launchpad.net/bugs/1980347)`↗`: MAAS snap fails to parse supervisor STOPPING state
- [1980436](https://bugs.launchpad.net/bugs/1980436)`↗`: MAAS CLI with anonymous access fails when TLS is enabled
- [1980490](https://bugs.launchpad.net/bugs/1980490)`↗`: MAAS regiond IPC crash due to a machine-resources binary crash when parsing some VPDs
- [1980818](https://bugs.launchpad.net/bugs/1980818)`↗`: Configure DHCP for VLAN
- [1980846](https://bugs.launchpad.net/bugs/1980846)`↗`: IP Address tooltip on Machines page blocks access to everything underneath and doesnt disappear until mouse-off
- [1981536](https://bugs.launchpad.net/bugs/1981536)`↗`: volume group creation fails on md device - MAAS 3.2
- [1981560](https://bugs.launchpad.net/bugs/1981560)`↗`: upgrade from 3.1 to 3.2 using debian packages missing steps
- [1982208](https://bugs.launchpad.net/bugs/1982208)`↗`: agent.yaml.example is missing when maas is installed via deb package
- [1982315](https://bugs.launchpad.net/bugs/1982315)`↗`: MAAS not sending correct metadata_url
- [1982328](https://bugs.launchpad.net/bugs/1982328)`↗`: update docstring to include informative not found change
- [1982846](https://bugs.launchpad.net/bugs/1982846)`↗`: Missing update_interface method on controller websocket handler
- [1982866](https://bugs.launchpad.net/bugs/1982866)`↗`: MAAS Breaks historical custom images
- [1982984](https://bugs.launchpad.net/bugs/1982984)`↗`: reverse-proxy service is not displayed for region controller
- [1983624](https://bugs.launchpad.net/bugs/1983624)`↗`: Fresh MAAS 3.2 install failed to find controller
- [1984141](https://bugs.launchpad.net/bugs/1984141)`↗`: duplicate tag results in failed deployment for KVM host
- [1984852](https://bugs.launchpad.net/bugs/1984852)`↗`: machine.filter_options returns empty, duplicate and mis-typed options
- [1984994](https://bugs.launchpad.net/bugs/1984994)`↗`: machine.list fails for some group_key values
- [1985741](https://bugs.launchpad.net/bugs/1985741)`↗`: Commissioning script 'maas-kernel-cmdline' fails with bonded interfaces
- [1986372](https://bugs.launchpad.net/bugs/1986372)`↗`: UI: Setting Default minimum kernel version for commissioning blocks deployments
- [1987874](https://bugs.launchpad.net/bugs/1987874)`↗`: interface.update_ip_addresses raise an Exception when exsits multiple StaticIPAddress
- [1988543](https://bugs.launchpad.net/bugs/1988543)`↗`: VM Discovery fails, resulting in " Error: An architecture is required." when composing a LXD VM
- [1988759](https://bugs.launchpad.net/bugs/1988759)`↗`: Provisioning LXD vmhost fails
- [1988769](https://bugs.launchpad.net/bugs/1988769)`↗`: The ppc64 machine in our lab fails during commissioning
- [1988874](https://bugs.launchpad.net/bugs/1988874)`↗`: Release command is failing for ppc64 machine in our lab
- [1989949](https://bugs.launchpad.net/bugs/1989949)`↗`: provisioningserver TestGetSourceAddress.test_returns_none_if_no_route_found sometimes fails locally
- [1989970](https://bugs.launchpad.net/bugs/1989970)`↗`: Can't enlist machines on subnets with DNS set
- [1989974](https://bugs.launchpad.net/bugs/1989974)`↗`: rackd fails on CIS-hardened machine with "Failed to update and/or record network interface configuration: Expecting value: line 1 column 1 (char 0)"
- [1990014](https://bugs.launchpad.net/bugs/1990014)`↗`: regiond.conf "debug_http: true" causes image downloads from regiond to fail with 500 error code
- [1990649](https://bugs.launchpad.net/bugs/1990649)`↗`: Kernel parameters form resets to previous value after save
- [1990873](https://bugs.launchpad.net/bugs/1990873)`↗`: TestKeys - test_get_launchpad_crashes_for_user_not_found
- [1991106](https://bugs.launchpad.net/bugs/1991106)`↗`: vCenter password field text is visible in settings
- [1991210](https://bugs.launchpad.net/bugs/1991210)`↗`: Color theme resets with page reload
- [1991229](https://bugs.launchpad.net/bugs/1991229)`↗`: Selecting all machines in a state in the UI causes traceback in backend
- [1991372](https://bugs.launchpad.net/bugs/1991372)`↗`: websocket config update notifications are no longer sent
- [1991410](https://bugs.launchpad.net/bugs/1991410)`↗`: wildcard DNS entry is not allowed
- [1991792](https://bugs.launchpad.net/bugs/1991792)`↗`: machine.action clone does not accept filter
- [1991795](https://bugs.launchpad.net/bugs/1991795)`↗`: machine.action does not always throw errors for failed machines
- [1992332](https://bugs.launchpad.net/bugs/1992332)`↗`: websocket machine.list parent group label should return hostname
- [1992686](https://bugs.launchpad.net/bugs/1992686)`↗`: MAAS 3.3 alpha missing two existing filters
- [1992975](https://bugs.launchpad.net/bugs/1992975)`↗`: Grouping by parents fails if there's more than one page

More bug-fixes are planned for later 3.3 releases.

*** MAAS 3.3 Beta 2 bug list

- [1990289](https://bugs.launchpad.net/bugs/1990289)`↗`: allocate call with system_id can allocate a new machine
- [1991784](https://bugs.launchpad.net/bugs/1991784)`↗`: [needs-packaging] GL Excess
- [1992185](https://bugs.launchpad.net/bugs/1992185)`↗`: unable to deploy a machine with vmhost if a bond interface was created
- [1992494](https://bugs.launchpad.net/bugs/1992494)`↗`: Jammy KVM host support
- [1992791](https://bugs.launchpad.net/bugs/1992791)`↗`: Info icons appear/disappear based on checked options in subnet page
- [1993289](https://bugs.launchpad.net/bugs/1993289)`↗`: Pod storage pool path can't be blank

*** MAAS 3.3 Beta 3 bug list

- [1835271](https://bugs.launchpad.net/bugs/1835271)`↗`: Ephemeral deployment keeps cloud-inits autogenerated netplan config
- [1843268](https://bugs.launchpad.net/bugs/1843268)`↗`: maas become unresponsive with maasserver_notification stuck at concurrent update
- [1886045](https://bugs.launchpad.net/bugs/1886045)`↗`: Error message mentions Pods when trying to release a machine
- [1886850](https://bugs.launchpad.net/bugs/1886850)`↗`: Encrypt the BMC credentials
- [1937138](https://bugs.launchpad.net/bugs/1937138)`↗`: Calling mark_intro_complete doesn't respond correctly
- [1955709](https://bugs.launchpad.net/bugs/1955709)`↗`: Metadata field may_reboot not working correctly in 20.04
- [1988229](https://bugs.launchpad.net/bugs/1988229)`↗`: dhcp snippet create fails when dhcp subnet is relayed regression
- [1990383](https://bugs.launchpad.net/bugs/1990383)`↗`: Link subnet on new machine
- [1992330](https://bugs.launchpad.net/bugs/1992330)`↗`: Use the rack controller IP as DNS when relaying DHCP
- 1993032: maas_hardware_sync creds are readable to local users on deployed OS and can give a super user access to MAAS itself
- [1993152](https://bugs.launchpad.net/bugs/1993152)`↗`: Updating a VM host through API unset tags
- [1994899](https://bugs.launchpad.net/bugs/1994899)`↗`: MAAS cannot mark "broken" VMs as fixed without recommissioning
- [1995397](https://bugs.launchpad.net/bugs/1995397)`↗`: Sentry blocked by CORS
- [1995624](https://bugs.launchpad.net/bugs/1995624)`↗`: suppressing script results no longer available on machine listing
- [1996065](https://bugs.launchpad.net/bugs/1996065)`↗`: CLI errors when redirecting the output to a file
- [1996074](https://bugs.launchpad.net/bugs/1996074)`↗`: Machine details stuck at "Loading" for machines with no disks
- [1996419](https://bugs.launchpad.net/bugs/1996419)`↗`: renaming a DNS record to a previous name fails with error: list.remove(x): x not in list
- [1996935](https://bugs.launchpad.net/bugs/1996935)`↗`: agent.yaml.example is missing when maas is installed via snap
- [1997190](https://bugs.launchpad.net/bugs/1997190)`↗`: Power parameters access attempt from non-db thread 
- [1997191](https://bugs.launchpad.net/bugs/1997191)`↗`: Uncaught exception when configuring DNS
- [1997281](https://bugs.launchpad.net/bugs/1997281)`↗`: machine.count fails for new filter options
- [1997599](https://bugs.launchpad.net/bugs/1997599)`↗`: Losing LXD certificate 
	
*** MAAS 3.3 RC1 bug list

- [1997975](https://bugs.launchpad.net/maas/+bug/1997975)`↗`: Update grafana_agent/agent.yaml.example

*** MAAS 3.3 RC3 bug list

- [1990172](https://bugs.launchpad.net/maas/+bug/1990172)`↗`: "20-maas-03-machine-resources" commissioning script improperly reports a Pass when the test fails 

*** MAAS 3.3.0 bug list

- [2003888](https://bugs.launchpad.net/maas/+bug/2003888)`↗`: Grouped machine list view: Inconsistent display when machine state changes

** Release notes for other MAAS versions

Here are release notes for other relatively recent MAAS versions:

- [MAAS 3.2](https://maas.io/docs/what-is-new-with-maas-3-2)`↗`
- [MAAS 3.1](https://maas.io/docs/what-is-new-with-maas-3-1)`↗`
- [MAAS 3.0](https://maas.io/docs/what-is-new-with-maas-3-0)`↗`
- [MAAS 2.9](https://maas.io/docs/what-is-new-with-maas-2-9)`↗`
- [MAAS 2.8](https://maas.io/docs/what-is-new-with-maas-2-8)`↗`
- [MAAS 2.7](https://maas.io/docs/what-is-new-with-maas-2-7)`↗`

* What is new with MAAS

Here you will find release notes for:

- [The current version of MAAS](#heading--current-maas-release-notes)
- [Other MAAS versions](#heading--other-maas-versions)

** MAAS 3.3.2 has been released

We are happy to announce that MAAS 3.3.2 has been release with the following bug fixes:

- [1990867](https://bugs.launchpad.net/maas/+bug/1990867)	TestImportBootImages - test_update_last_image_sync_end_to_end_import_not_performed
- [1990872](https://bugs.launchpad.net/maas/+bug/1990872)	Flaky test: TestClusterClient - test_registerRackWithRegion_end_to_end
- [2011822](https://bugs.launchpad.net/maas/+bug/2011822)	Reverse DNS resolution fails for some machines
- [2012139](https://bugs.launchpad.net/maas/+bug/2012139)	maas commands occasionally fail with NO_CERTIFICATE_OR_CRL_FOUND when TLS is enabled
- [1986590](https://bugs.launchpad.net/maas/+bug/1986590)	maas-cli from PPA errors out with traceback - ModuleNotFoundError: No module named 'provisioningserver'

** MAAS 3.3.1 has been released

We are happy to announce that MAAS 3.3.1 has been released with the following bug fixes:

- [1773150](https://bugs.launchpad.net/maas/+bug/1773150) smartctl verify fails due to Unicode in Disk Vendor Name
- [1993618](https://bugs.launchpad.net/maas/+bug/1993618) Web UI redirection policy can invalidate HAProxy and/or TLS setup
- [1996997](https://bugs.launchpad.net/maas/+bug/1996997) LXD resources fails on a Raspberry Pi with no Ethernet
- [2003310](https://bugs.launchpad.net/maas/+bug/2003310) Refresh scripts are not re-run if they pass, but fail to report the results to the region
- [2008275](https://bugs.launchpad.net/maas/+bug/2008275) Intel AMT support is broken in MAAS 3.3.0
- [2009137](https://bugs.launchpad.net/maas/+bug/2009137) MAAS OpenApi Schema missing parameters
- [2009140](https://bugs.launchpad.net/maas/+bug/2009140) MAAS OpenApi Schema cutoff variable names
- [2009186](https://bugs.launchpad.net/maas/+bug/2009186) CLI results in connection timed out when behind haproxy and 5240 is blocked
- [2009805](https://bugs.launchpad.net/maas/+bug/2009805) machine deploy install_kvm=True fails

** MAAS 3.3 has been released

We are happy to announce that MAAS 3.3 has been released, with [one additional bug fix](#heading--MAAS-3-3-bug-list).  MAAS 3.3 is a concerted effort to improve MAAS on multiple fronts, including a large number of bug fixes. 

** Cumulative summary of MAAS 3.3 features

New features created for MAAS 3.3 include:

- [Ansible playbooks for HA MAAS, PostgreSQL, and other MAAS configurations](#heading--ansible-playbooks): [Ansible](https://www.redhat.com/en/technologies/management/ansible/what-is-ansible)`↗` [playbooks](https://docs.ansible.com/ansible/latest/getting_started/get_started_playbook.html)`↗` are now available for MAAS, making it easy to automate routine setup and configuration of MAAS.

- [Improved machine list filtering](#heading--Improved-machine-list-filtering): MAAS 3.3 enhances the presentation and filtering of the machine list, with a shorter wait to start filtering and a wider range of filter choices.

- [Integration of Vault for credential storage](#heading--vault-integration): MAAS 3.3 allows you to use [Hashicorp Vault](https://www.vaultproject.io/)`↗` to protect your secrets, if you wish.

Improved capabilities include the following:

- [Native support for 22.04 LTS and core22](#heading--22-04-support): We've removed the requirement to use snaps on 22.04 (Jammy Jellyfish); you now can load MAAS 3.3 on 22.04 using packages.

- [UI performance improvements for large machine counts](#heading--UI-performance-improvements): We've improved the performance of the UI machine list for large (>10000 machines) MAAS instances.  The machine list now goes live just a few seconds after the first visible page loads, with the rest of the list loading in background.

- [Enhanced MIB support for Windows OS images](#heading--Enhanced-MIB-support-for-Windows-OS-images): The [procedure](/t/how-to-build-custom-images/5104#heading--custom-windows-images) for creating custom Windows OS images has been thoroughly updated and verified.

Greatly expanded documentation sections include:

- [MAAS configuration settings reference](#heading--maas-config-settings-ref): There is now one reference page that addresses all MAAS settings in one place.  Other references throughout the document are preserved for now.

- [Improved MAAS event documentation](#heading--Improved-MAAS-event-documentation): MAAS event documentation has been expanded to include [much better explanations](/t/understanding-maas-events/6373) of MAAS events, including many examples.

- [Improved MAAS audit event documentation](#heading--Improved-MAAS-audit-event-documentation): MAAS audit event documentation has been greatly expanded to include [much better explanations](/t/understanding-maas-audit-events/6372) of MAAS audit events, including many examples and use cases.

Several forward-looking improvements are included as well:

- Reliability improvements for simultaneous machine deployments

- The first phase of [Nvidia DPU](https://www.nvidia.com/en-us/networking/products/data-processing-unit/)`↗` support

- Shifting the MAAS API documentation toward [OpenAPI standards](https://www.openapis.org/)`↗`

These will be documented later in blog posts.


This release also includes well over one-hundred [bug fixes](#heading--MAAS-3.3-bug-list).  Read on to catch up with what we've done so far this cycle.

** How to install MAAS 3.3

MAAS will run on just about any modern hardware configuration, even a development laptop.  If you're not sure whether your target server will handle MAAS, [you can always double-check](/t/maas-installation-requirements/6233).

[note]
**NOTE** that PostgreSQL 12 is deprecated with the release of MAAS 3.3, in favour of PostgreSQL 14. Support for PostgreSQL 12 will be discontinued in MAAS 3.4.  Also note, though, that Postgres 14 does not run on Focal 20.04 LTS.
[/note]

*** How to do a fresh snap install of MAAS 3.3

To install MAAS 3.3 from a snap, simply enter the following:

    $ sudo snap install --channel=3.3 maas

After entering your password, the snap will download and install from the 3.3 channel.

*** How to upgrade from an earlier snap version to MAAS 3.3

Maybe instead of a fresh install, you want to upgrade from a earlier snap version to the 3.3 snap, and you are using a `region+rack` configuration, use this command:

    $ sudo snap refresh --channel=3.3 maas

After entering your password, the snap will refresh from the 3.3 candidate channel.  You will **not** need to re-initialise MAAS.

If you are using a multi-node maas deployment with separate regions and racks, you should first run the upgrade command above for rack nodes, then for region nodes.

*** How to initialise MAAS 3.3 snap for a test or POC environment

You can initialise MAAS as a compact version for testing.  To achieve this, we provide a separate snap, called `maas-test-db`, which contains a PostgreSQL database for use in testing and evaluating MAAS.   The following instructions will help you take advantage of this test configuration.

Once MAAS is installed, you can use the `--help` flag with `maas init` to get relevant instructions:
 
    $ sudo maas init --help
    usage: maas init [-h] {region+rack,region,rack} . . .

    Initialise MAAS in the specified run mode.

    optional arguments:
      -h, --help            show this help message and exit

    run modes:
      {region+rack,region,rack}
        region+rack         Both region and rack controllers
        region              Region controller only
        rack                Rack controller only

    When installing region or rack+region modes, MAAS needs a
    PostgreSQL database to connect to.

    If you want to set up PostgreSQL for a non-production deployment on
    this machine, and configure it for use with MAAS, you can install
    the maas-test-db snap before running 'maas init':
        sudo snap install maas-test-db
        sudo maas init region+rack --database-uri maas-test-db:///

We'll quickly walk through these instructions to confirm your understanding.  First, install the `maas-test-db` snap:
 
    sudo snap install maas-test-db

Note that this step installs a a running PostgreSQL and a MAAS-ready database instantiation.  When it's done, you can double check with a built-in PostgreSQL shell:

    $ sudo maas-test-db.psql
    psql (12.4)
    Type "help" for help.

    postgres=# \l

This will produce a list of databases, one of which will be `maasdb`, owned by `maas`.  Note that this database is still empty because MAAS is not yet initialised and, hence, is not yet using the database.  Once this is done, you can run the `maas init` command:

    sudo maas init region+rack --database-uri maas-test-db:///

After running for a moment, the command will prompt you for a MAAS URL; typically, you can use the default:
 
    MAAS URL [default=http://10.45.222.159:5240/MAAS]:

When you've entered a suitable URL, or accepted the default, the following prompt will appear:
 
    MAAS has been set up.

    If you want to configure external authentication or use
    MAAS with Canonical RBAC, please run

      sudo maas configauth

    To create admins when not using external authentication, run

      sudo maas createadmin

Let's assume you just want a local testing user named `admin`:

    $ sudo maas createadmin
    Username: admin
    Password: ******
    Again: ******
    Email: admin@example.com
    Import SSH keys [] (lp:user-id or gh:user-id): gh:yourusername

At this point, MAAS is basically set up and running.  You can confirm this with `sudo maas status`.  If you need an API key, you can obtain this with `sudo maas apikey --username yourusername`.  Now you will be able to test and evaluate MAAS by going to the URL you entered or accepted above and entering your `admin` username and password.

*** Initialise MAAS for a production configuration

To install MAAS in a production configuration, you need to setup PostgreSQL, as described below.

**** Setting up PostgreSQL from scratch

To set up PostgreSQL, even if it's running on a different machine, you can use the following procedure:

1. You will need to install PostgreSQL on the machine where you want to keep the database.  This can be the same machine as the MAAS region/rack controllers or a totally separate machine.  If PostgreSQL (version 14) is already running on your target machine, you can skip this step. To install PostgreSQL, run these commands:

        sudo apt update
        sudo apt install -y postgresql

2. You want to make sure you have a suitable PostgreSQL user, which can be accomplished with the following command, where `$MAAS_DBUSER` is your desired database username, and `$MAAS_DBPASS` is the intended password for that username.  Note that if you're executing this step in a LXD container (as root, which is the default), you may get a minor error, but the operation will still complete correctly.

        sudo -u postgres psql -c "CREATE USER \"$MAAS_DBUSER\" WITH ENCRYPTED PASSWORD '$MAAS_DBPASS'"

3. Create the MAAS database with the following command, where `$MAAS_DBNAME` is your desired name for the MAAS database (typically known as `maas`). Again, if you're executing this step in a LXD container as root, you can ignore the minor error that results.

        sudo -u postgres createdb -O "$MAAS_DBUSER" "$MAAS_DBNAME"

4. Edit `/etc/postgresql/14/main/pg_hba.conf` and add a line for the newly created database, replacing the variables with actual  names. You can limit access to a specific network by using a different CIDR than `0/0`.

        host    $MAAS_DBNAME    $MAAS_DBUSER    0/0     md5

5. You can then initialise MAAS via the following command:

        sudo maas init region+rack --database-uri "postgres://$MAAS_DBUSER:$MAAS_DBPASS@$HOSTNAME/$MAAS_DBNAME"

[note] You should use `localhost` for `$HOSTNAME` if you're running PostgreSQL on the same box as MAAS.[/note]

Don't worry; if you leave out any of the database parameters, you'll be prompted for those details.

*** How to do a fresh install of MAAS 3.3 from packages

MAAS 3.3 from packages runs on 22.04 LTS only.  The recommended way to set up an initial MAAS environment is to put everything on one machine:

``` bash
sudo apt-add-repository ppa:maas/3.3
sudo apt update
sudo apt-get -y install maas
```

Executing this command leads you to a list of dependent packages to be installed, and a summary prompt that lets you choose whether to continue with the install. Choosing "Y" proceeds with a standard <code>apt</code> package install.

****>Distributed environment</h4> 

<p>For a more distributed environment, you can place the region controller on one machine:</p>

``` bash
sudo apt install maas-region-controller
```

and the rack controller on another:

``` bash
sudo apt install maas-rack-controller
sudo maas-rack register
```

These two steps will lead you through two similar <code>apt</code> install sequences.

*** How to upgrade from 3.2 or lower to MAAS 3.3

If you are running MAAS 3.2 or lower, you can upgrade directly to MAAS 3.3. You must first make sure that the target system is running Ubuntu 22.04 LTS by executing the following command:

```nohighlight
lsb_release -a
```
The response should look something like this:

```nohighlight
Distributor ID:	Ubuntu
Description:	Ubuntu xx.yy
Release:	xx.yy
Codename:	$RELEASE_NAME
```

The required “xx.yy” required for MAAS 3.3 is “22.04,” code-named “jammy”.

If you are currently running Ubuntu focal 20.04 LTS, you can upgrade to jammy 22.04 LTS with the following procedure:

Upgrade the release:

```nohighlight
sudo do-release-upgrade --allow-third-party
```

Accept the defaults for any questions asked by the upgrade script.

Reboot the machine when requested.

Check whether the upgrade was successful:

```nohighlight
lsb_release -a
```

A successful upgrade should respond with output similar to the following:

```nohighlight
Distributor ID:	Ubuntu
Description:	Ubuntu 20.04(.nn) LTS
Release:	20.04
Codename:	focal
```

If you’re upgrading from MAAS version 2.8 or lower to version 3.3: While the following procedures should work, note that they are untested. Use at your own risk. Start by making a verifiable backup; see step 1, below.

Back up your MAAS server completely; the tools and media are left entirely to your discretion. Just be sure that you can definitely restore your previous configuration, should this procedure fail to work correctly.

Add the MAAS 3.3 PPA to your repository list with the following command, ignoring any apparent error messages:

```nohighlight
sudo apt-add-repository ppa:maas/3.3
```

Run the release upgrade like this, answering any questions with the given default values:

```nohighlight
sudo do-release-upgrade --allow-third-party
```

Check whether your upgrade has been successful by entering:

```nohighlight
lsb_release -a
```

If the ugprade was successful, this command should yield output similar to the following:

```nohighlight
No LSB modules are available.
Distributor ID:	Ubuntu
Description:	Ubuntu 20.04(.nn) LTS
Release:	20.04
Codename:	focal
```

Check your running MAAS install (by looking at the information on the bottom of the machine list) to make sure you’re running the 3.3 release.

If this didn’t work, you will need to restore from the backup you made in step 1, and consider obtaining separate hardware to install MAAS 3.3.

** Ansible playbooks for HA MAAS, PostgreSQL, and other MAAS configurations

[Ansible](https://www.redhat.com/en/technologies/management/ansible/what-is-ansible)`↗` [playbooks](https://docs.ansible.com/ansible/latest/getting_started/get_started_playbook.html)`↗` are now available for MAAS.  These extended YAML files automate various routine aspects of MAAS setup and configuration.  

*** Ten words or less

Automate the drudgery of installing and configuring MAAS with Ansible.

*** Background explanations on Ansible

Ansible is more than scripting; it's a fully-featured automation tool that allows you to group modules (binaries or snippets of code) into specific tasks.  These tasks can be combined to build plays, and plays can be combined to build playbooks.  The idea is to abstract each level so you're not trying to keep track of which lines of Bash code you need to cut and past in between others.  

With MAAS 3.3, playbooks are available to:

- install and configure a MAAS region on a targeted host; running the playbook on hosts with a MAAS region will upgrade it.
- install and configure a MAAS rack.
- setup the postgres primary role.
- setup the postgres secondary role.

MAAS Playbooks are available from a [repository](https://github.com/maas/MAAS-ansible-playbook)`↗`.  They will eventually be available through Ansible Galaxy.

There is also a set of groups that will automate setting up specific sections of MAAS.  For example, there is a PostgreSQL group that sets up the primary and secondary PostgreSQL roles, bypassing the need to run both playbooks individually.

After installing ansible, running each of the playbooks on a blank machine will have a fresh install of MAAS ready to go. For example, running the region+rack will setup a region+rack on the host machine.

*** How you can use Ansible playbooks

In general terms, you can run any of the MAAS Ansible plays with a command of this form:

```nohighlight
ansible-playbook -i hosts \
--extra_vars \
"maas_version=$MAAS_VERSION 
maas_postgres_password=$MAAS_PG_PASSWORD 
maas_postgres_replication_password=$MAAS_PG_REP_PASSWORD 
maas_installation_type=<deb|snap> 
maas_url=$MAAS_URL" \
./site.yaml
```

A command of this form will run all of the plays below (i.e., the entire playbook). If you want to run the tasks for one particular role (or roles), you can use the form --tags <target role(s)> to limit which parts of the MAAS Ansible playbook run. Consult the Ansible documentation for more details on additional options and command structure.

[note]
There's some extra good news here:  These Ansible playbooks were built and tested with MAAS 3.2, so you can use them with all variants of MAAS 3.2.
[/note]

For example, suppose you want to install a MAAS region controller.  As an operator, you want want to install a MAAS region controller onto a given host using Ansible. To accomplish this, you need only:

- set a maas_region_controller role on a given host,
- run the region controller playbook,
- and find the newly-configured region controller present on that host .

To set the `maas_region_controller` role, you add that role to your Inventory file in the form of either an INI or a YAML file, where each role is followed by the addresses of each host to attach the role to. The example below attaches the region controller role to a host running on 10.10.0.20 with the user `ubuntu`:

```nohighlight
INI:

[maas_region_controller]
10.10.0.20 ansible_user=ubuntu
YAML:

all:
  maas_region_controller:
    hosts:
      10.10.0.20:
        ansible_user: ubuntu
```

When running the playbook for a host with the `maas_region_controller` role, the playbook installs the MAAS region controller. The documented ansible variable `maas_installation_type` provides the user with the ability to set whether it’s a deb installation or a snap installation, along with additional variables for MAAS version, snap channel and/or PPA.

The default installation is a snap. A successful run of the playbook should give the operator an accessible and ready MAAS instance.  The playbook uses an Ansible variable to determine what version of MAAS to deploy. The playbook won’t execute (i.e “skipped” in the context of Ansible) if the Ubuntu version is incompatible with the version and install method. The Region Controller tasks should be able to execute on multiple hosts in a single execution if the target is an Ansible Group rather than a single host.  The newly-installed region controller should be accessible at the specified host ip address, as though the controller had been installed manually.

Read the [Ansible playbooks reference](/t/how-to-spin-up-maas-with-ansible/6367) document to learn more about the feature and the additional playbooks that are available.

<!--
** Integration of Vault for credential storage

MAAS deals with a number of secrets (user password, certificates and keys, API tokens, …), currently stored in the database -- which is insecure by default. This configuration may not meet everyone's security requirements or regulations. For this reason, we've integrated MAAS with Hashicorp Vault, a well-established solution for secure, centralised credential storage.

You can read the [MAAS Vault reference](/t/maas-vault-reference/6368) documentation to learn more. -->

** Improved machine list filtering

MAAS 3.3 dramatically reduces the latency associated with refreshing large machine lists.

*** Ten words or less

You can filter machines mere seconds after one page loads.

*** How list filtering is improved

[note]
**NOTE** that this feature is still in development, so some of the feature-set described in this section may not be fully operational yet.  As always, we reserve the right to change this feature-set until the final release of MAAS 3.3. These release notes will be updated as the feature develops.
[/note]

MAAS 3.3 enhances the way you can filter the machine list, in two ways:

1. You may begin filtering within a very short time after the first page of the machine list loads, even if you have more than 10,000 machines in the list.  

2. You have a wider range of filter choices, as described in the table below.

Note that with this version of MAAS, matching machine counts have been removed from the filter list for better performance.

*** More filter parameters have been added

The following table describes the expanded filter set for the MAAS machine list:

- Items marked "Dyn" are dynamic, populated based on existing data, that is, the "Tags" filter only shows tags that currently exist.  
- Items which are not dynamic present the entire range of possible values, regardless of whether that value currently exists in MAAS; for example, all machine status values are available to be filtered, even if no machines currently have that status.
- Items marked "Grp" can be used to group machines, instead of the default machine status.
- Items marked "Man" must be manually entered, i.e., they are not in the UI filter dropdown, but can be entered in the "Search" box if properly formatted (as in the examples given).

See [How to search MAAS](/t/how-to-find-machines/5192) for more details on how to use these parameters.


| Parameter (bold) w/example           | Shows nodes...                   | Dyn | Grp | Man |
|--------------------------------------|----------------------------------|-----|-----|-----|
| **arch**:(=architecture)             | with "architecture"              |     | Grp |     |
| arch:(!=architecture)                | NOT with "architecture"          | Dyn |     |     |
| **zone**:(=zone-name)                | in "zone-name"                   | Dyn | Grp |     |
| zone:(!=zone-name)                   | NOT in "zone-name"               | Dyn |     |     |
| **pool**:(=resource-pool)            | in "resource-pool"               | Dyn | Grp |     |
| pool:(!=resource-pool)               | NOT in "resource-pool"           | Dyn |     |     |
| **pod**:(=pod-name)                  | with "pod-name"                  | Dyn | Grp |     |
| pod:(!=pod-name)                     | NOT with "pod-name"              | Dyn |     |     |
| **pod_type**:(=pod-type)             | with power type "pod-type"       | Dyn | Grp | Man |
| pod_type:(!=pod-type)                | NOT with power type "pod-type"   | Dyn |     | Man |
| **domain**:(=domain-name)            | with "domain-name"               | Dyn | Grp | Man |
| domain:(!=domain-name)               | NOT with "domain-name"           | Dyn |     | Man |
| **status**:(=op-status)              | having "op-status"               |     | Grp |     |
| status:(!=op-status)                 | NOT having "op-status"           | Dyn |     |     |
| **owner**:(=user)                    | owned by "user"                  | Dyn | Grp |     |
| owner:(!=user)                       | NOT owned by "user"              | Dyn |     |     |
| **power_state**:(=power-state)       | having "power-state"             |     | Grp | Man |
| power_state:(!=power-state)          | NOT having "power-state"         | Dyn |     | Man |
| **tags**:(=tag-name)                 | with tag "tag-name"              | Dyn |     |     |
| tags:(!=tag-name)                    | NOT with tag "tag-name"          | Dyn |     |     |
| **fabrics**:(=fabric-name)           | in "fabric-name"                 | Dyn |     |     |
| fabrics:(!=fabric-name)              | NOT in "fabric-name"             | Dyn |     |     |
| **fabric_classes**:(=fabric-class)   | in "fabric-class"                | Dyn |     | Man |
| fabric_classes:(!=fabric-class)      | NOT in "fabric-class"            | Dyn |     | Man |
| **fabric_name**:(=fabric-name)       | in "boot-interface-fabric"       | Dyn |     | Man |
| fabric_name:(!=fabric-name)          | NOT in "boot-interface-fabric"   | Dyn |     | Man |
| **subnets**:(=subnet-name)           | attached to "subnet-name"        | Dyn |     |     |
| subnets:(!=subnet-name)              | Not attached to "subnet-name"    | Dyn |     |     |
| **link_speed**:(link-speed)          | having "link-speed"              | Dyn |     | Man |
| link_speed:(!link-speed)             | NOT having "link-speed"          | Dyn |     | Man |
| **vlans**:(=vlan-name)               | attached to "vlan-name"          | Dyn |     |     |
| vlans:(!=vlan-name)                  | NOT attached to "vlan-name"      | Dyn |     |     |
| **storage**:(storage-MB)             | having "storage-MB"              | Dyn |     | Man |
| **total_storage**:(total-stg-MB)     | having "total-stg-MB"            | Dyn |     | Man |
| total_storage:(!total-stg-MB)        | NOT having "total-stg-MB"        | Dyn |     | Man |
| **cpu_count**:(cpu-count)            | having "cpu-count"               | Dyn |     | Man |
| cpu_count:(!cpu-count)               | NOT having "cpu-count"           | Dyn |     | Man |
| **mem**:(ram-in-MB)                  | having "ram-in-MB"               | Dyn |     | Man |
| mem:(!ram-in-MB)                     | NOT having "ram-in-MB"           | Dyn |     | Man |
| **mac_address**:(=MAC)               | having MAC address "MAC"         | Dyn |     | Man |
| mac_address:(!=MAC)                  | NOT having                       | Dyn |     | Man |
| **agent_name**:(=agent-name)         | Include nodes with agent-name    | Dyn |     | Man |
| agent_name:(!=agent-name)            | Exclude nodes with agent-name    | Dyn |     | Man |
| **cpu_speed**:(cpu-speed-GHz)        | CPU speed                        | Dyn |     | Man |
| cpu_speed:(!cpu-speed-GHz)           | CPU speed                        | Dyn |     | Man |
| **osystem**:(=os-name)               | The OS of the desired node       | Dyn |     | Man |
| osystem:(!=os-name)                  | OS to ignore                     | Dyn |     | Man |
| **distro_series**:(=distro-name)     | Include nodes using distro       | Dyn |     | Man |
| distro_series:(!=distro-name)        | Exclude ndoes using distro       | Dyn |     | Man |
| **ip_addresses**:(=ip-address)       | Node's IP address                | Dyn |     | Man |
| ip_addresses:(!=ip-address)          | IP address to ignore             | Dyn |     | Man |
| **spaces**:(=space-name)             | Node's spaces                    | Dyn |     |     |
| spaces:(!=space-name)                | Node's spaces                    | Dyn |     |     |
| **workloads**:(=annotation-text)     | Node's workload annotations      | Dyn |     |     |
| workloads:(!=annotation-text)        | Node's workload annotations      | Dyn |     |     |
| **physical_disk_count**:(disk-count) | Physical disk Count              | Dyn |     | Man |
| physical_disk_count:(!disk-count)    | Physical disk Count              | Dyn |     | Man |
| **pxe_mac**:(=PXE-MAC)               | Boot interface MAC address       | Dyn |     | Man |
| pxe_mac:(!=PXE-MAC)                  | Boot interface MAC address       | Dyn |     | Man |
| **fqdn**:(=fqdn-value)               | Node FQDN                        | Dyn |     | Man |
| fqdn:(!=fqdn-value)                  | Node FQDN                        | Dyn |     | Man |
| **simple_status**:(=status-val)      | Include nodes with simple-status | Dyn |     | Man |
| simple_status:(!=status-val)         | Exclude nodes with simple-status | Dyn |     | Man |
| **devices**:(=)                      | Devices                          | Dyn |     | Man |
| **interfaces**:(=)                   | Interfaces                       | Dyn |     | Man |
| **parent**:(=)                       | Parent node                      | Dyn | Grp | Man |

** Native support for 22.04 LTS and core22

MAAS can now be installed as a PPA, directly on Ubuntu 22.04, without the need to use snaps.

*** Ten words or less

MAAS packages now run on Ubuntu 22.04, aka Jammy Jellyfish.

<a href="#heading--Notes-on-22.04-LTS-MAAS-packages">*** id="heading--Notes-on-22.04-LTS-MAAS-packages">Notes on 22.04 LTS MAAS packages

MAAS users want to install MAAS on a 22.04 LTS system via deb packages, as well as upgrade machines currently running MAAS on Ubuntu 20.04 LTS to 22.04 LTS.  With the advent of MAAS 3.3, we have created an appropriate PPA with all required dependencies.  This PPA can be directly installed on Ubuntu 22.04, Jammy Jellyfish, with no requirement to use snaps.

Note that the upgrade procedure will require a release upgrade from previous Ubuntu versions to Ubuntu 22.04.  Also note that, with this version of MAAS, PostgreSQL 12 is deprecated and should be upgraded to PostgreSQL 14.  The [installation guide](/t/how-to-install-maas/5128) provides the necessary details.

<!--
** Reliability improvements for simultaneous machine deployments

MAAS 3.3 brings some behind-the-scenes performance improvements to the product.  When you deploy many machines, you expect them all deploy reliably, with IPs allocated in bulk, and no DNS delays or RPC timeouts.  Within our development system, we've added system tests and metrics to track any failures or latency when large numbers of machines are being deployed.  We've then used these new data to lower the failure rate and reduce latency in those situations.

** The first phase of Nvidia DPU support

Long-term, we know that MAAS administrators want to enlist and use DPUs with MAAS.  The Nvidia DPU is a complex machine with a tremendous amount of capability, so this cycle, we made the first steps toward supporting them.  Details will follow in a forward-looking blog post sometime after the MAAS 3.3 release.
-->

** UI performance improvements

We wanted to improve the performance of the machine list page for large (>10000 machines) MAASes, and allow users to search and filter machines as quickly as possible. 

*** Ten words or less

We're working on making large machine lists load in background.

*** Background performance work

In MAAS 3.2 and earlier, machine search and filter requires that all machines be fetched by the UI client before it becomes usable. For smaller MAASes this may not be an issue, but when considering MAASes with 1000 machines or more this can make the user wait an unacceptably long time before they can search and filter.  With the release of MAAS 3.3, when a MAAS UI user wants to find a particular machine, they do not have to wait for all their machines data to load before they can start searching. The user can start searching for machines within a short time after the visible page of the machine list has fully loaded on the UI screen.  See [Improved machine list filtering](#heading--Improved-machine-list-filtering), in these release notes, for details on the enhanced filtering capabilities that were included in this work.

** Enhanced MIB support for Windows OS images

The [procedure](/t/how-to-build-custom-images/5104#heading--custom-windows-images) for creating custom Windows OS images has been thoroughly updated and verified.

*** Ten words or less

MAAS custom Windows images now support most releases and options.

*** What has been added to Windows custom images

Specifically, MIB now supports a much wider range of Windows images.  Previously, only 2012 and 2106 Windows versions were supported with MIB.  Now the list is much longer, bringing deployable MAAS versions up to date with the current Windows releases:

 - win2008r2
 - win2008hvr2
 - win2012
 - win2012hv
 - win2012r2
 - win2012hvr2
 - win2016
 - win2016-core
 - win2016hv
 - win2016dc
 - win2016dc-core
 - win2019
 - win2019-core
 - win2019dc
 - win2019dc-core
 - win10ent
 - win10ent-eval
 - win2022
 - win2022-core

There are also special instructions for using both UEFI and BIOS bootloaders, as well as instructions for using LXD containers with custom-built Windows images.  

Finally, MIB has been extended to accept a much wider range of options for windows builds.  Some of the new Windows-specific options include:

 - --windows-iso: path to the Windows ISO image.
 - --windows-edition: identifier for the Windows edition/option being installed (see above).
 - --windows-license-key: Windows license key (required with non-evaluation editions)
 - --windows-language: Windows installation language (default: en-US)
 - --windows-updates: download and install Windows Updates (requires internet access; might require a larger --disk-size option)
 - --windows-drivers: path to directory with Windows drivers to be installed (requires internet access; uses the Windows Driver Kit, by default)
 - --driver-store: combined with --windows-drivers, uses the Windows Driver Store to install drivers early into Windows Setup and image (does not require internet access; does not use the Windows Driver Kit).

Some news Windows-specific platform options include:

 - --uefi: use UEFI partition layout and firmware
 - --virtio: use paravirtualized VirtIO SCSI and VirtIO NET devices (instead of emulated devices) for installation (requires --windows-drivers)
 - --disk-size: specify the (virtual) disk size for Windows setup (must be larger for --windows-updates; increases deployment/copy-to-disk time, and is expanded to physical disk size during deployment)

This update should make it much simpler to use custom-built Windows images with MAAS.


** Shifting the MAAS API documentation to OpenAPI standards

MAAS API User want to experience the MAAS API in a more standard way, along the lines of the OpenAPI definition.  MAAS 3.3 begins this process by providing most of the MAAS API functionality in a discover-able form.  You should now be able to easily retrieve human-readable service documentation and API definitions using standard methods.  Consult [the API documentation](https://maas.io/docs/api)`↗` for details.

** MAAS configuration settings reference

MAAS 3.3 documentation consolidates configuration settings in one article, in addition to their other mentions throughout the documentation set.

*** Ten words or less

"Settings" now has its own page, and some new options.

*** What is new about this update

MAAS configuration settings are scattered in various (generally relevant) places throughout the documentation, but there has never been one reference page that addresses all settings in one place.  MAAS 3.3 remedies this by adding the [Configuration settings reference](/t/how-to-change-maas-settings/6347).

A minor new feature added with MAAS 3.3 is MAAS site identity, which enables some new configuration parameters:

- MAAS name: The “* MAAS name” is a text box that sets the text which appears at the bottom of every MAAS screen, in front of the version descriptor.

- MAAS name emoji: You may also paste a suitable emoji in front of the MAAS name to help identify it.

- MAAS theme main colour: You may also help identify your MAAS instance by changing the colour of the top bar; several colour choices are available.

These enhancements were made available to assist users who have more than one instance (e.g., production and staging), and have issues with operations accidentally making changes to the wrong instance.

** Improved MAAS event documentation

MAAS event documentation has been expanded to include [much better explanations](/t/understanding-maas-events/6373) of MAAS events, including many examples.

*** Ten words or less

We've finally documented MAAS events, making them easier to decode.

*** Understanding MAAS events

Events are state changes that happen to MAAS elements, caused by MAAS itself, an external agent, or a users. Understanding events is an essential debugging skill.  But events appear in three different places in MAAS, each presentation providing slightly different information.  These screens are usually dense and hard to search.

In this major documentation update, we've standardised on the MAAS CLI events query command as the best way to review, filter, and summarise events.  We've summarised the six main event types:

 - INFO: the default, used if no level= is specified; shows INFO and ERROR events. A typical INFO event is “Ready”, indicating that a machine has reached the “Ready” state.

 - CRITICAL: critical MAAS failures; shows only CRITICAL events. These events usually represent severe error conditions that should be immediately remedied.

 - ERROR: MAAS errors; shows only ERROR events. Typical ERROR events include such things as power on/off failures, commissioning timeouts, and image import failures.

 - WARNING: failures which may or may not affect MAAS performance; shows WARNING and ERROR events. A typical warning event, for example, might include the inability to find and boot a machine.

 - DEBUG: information which would help debug MAAS behaviour; shows DEBUG and INFO events. Typical DEBUG events involve routine image import activities, for example.

 - AUDIT: information which helps determine settings and user actions in MAAS; shows only AUDIT events. They are covered in more detail elsewhere.

In addition, the new document explains how these event types tend to overlap when queried.  We've also provide detailed instructions on how to use the most common filters:

 - hostname: Only events relating to the node with the matching hostname will be returned. This can be specified multiple times to get events relating to more than one node.

 - mac_address: Only nodes with matching MAC addresses will be returned. Note that MAC address is not part of the standard output, so you’d need to look it up elsewhere.

 - id: Only nodes with matching system IDs will be returned. This corresponds to the node parameter in the JSON listing, not the id parameter there, which is a serial event number.

 - zone: Only nodes in the zone will be returned. Note that zones are not part of the standard output, so you’d need to look these up elsewhere.

 - level: The event level to capture. You can choose from AUDIT, CRITICAL, DEBUG, ERROR, INFO, or WARNING. The default is INFO.

 - limit: Number of events to return. The default is 100, the maximum in one command is 1000.

 - before: Defines an event id to start returning older events. This is the “id” part of the JSON, not the system ID or “node”. Note that before and after cannot be used together, as the results are unpredictable.

 - after: Defines an event id to start returning newer events. This is the “id” part of the JSON, not the system ID or “node”. Note that before and after cannot be used together, as the results are unpredictable.

Since the MAAS CLI returns JSON -- which is hard to humans to parse -- we've included some exemplary `jq` predicates of the form:

```nohighlight
maas $PROFILE events query limit=20 \
| jq -r '(["USERNAME","NODE","HOSTNAME","LEVEL","DATE","TYPE","EVENT"] | 
(., map(length*"-"))),
(.events[] | [.username,.node,.hostname,.level,.created,.type,.description]) 
| @tsv' | column -t -s$'\t'
```

And finally, we provided some detailed usage examples.  For instance, we walked a MAAS machine called `fun-zebra` through the following states:

 - Commissioning
 - Allocation
 - Deployment
 - Releasing
 - Testing (with a premature manual abort)
 - Rescue mode

We used this example command:

```nohighlight
 maas $PROFILE events query level=INFO hostname=fun-zebra limit=1000 | jq -r '(["USERNAME","NODE","HOSTNAME","LEVEL","DATE","TYPE","EVENT"] | (., map(length*"-"))),(.events[] | [.username,.node,.hostname,.level,.created,.type,.description]) | @tsv' | column -t -s$'\t'
```

This gave us a reasonably thorough report of what happened to the machine:

```nohighlight
USERNAME  NODE    HOSTNAME   LEVEL  DATE                        TYPE                   EVENT
--------  ----    --------   -----  ----                        ----                   -----
unknown   bk7mg8  fun-zebra  INFO   Thu, 29 Sep. 2022 21:29:53  Exited rescue mode     
unknown   bk7mg8  fun-zebra  INFO   Thu, 29 Sep. 2022 21:29:52  Powering off           
unknown   bk7mg8  fun-zebra  INFO   Thu, 29 Sep. 2022 21:28:58  Rescue mode            
unknown   bk7mg8  fun-zebra  INFO   Thu, 29 Sep. 2022 21:27:18  Loading ephemeral      
unknown   bk7mg8  fun-zebra  INFO   Thu, 29 Sep. 2022 21:26:40  Performing PXE boot    
unknown   bk7mg8  fun-zebra  INFO   Thu, 29 Sep. 2022 21:26:23  Power cycling          
unknown   bk7mg8  fun-zebra  INFO   Thu, 29 Sep. 2022 21:26:23  Entering rescue mode   
unknown   bk7mg8  fun-zebra  INFO   Thu, 29 Sep. 2022 21:26:14  Powering off           
unknown   bk7mg8  fun-zebra  INFO   Thu, 29 Sep. 2022 21:26:14  Aborted testing        
unknown   bk7mg8  fun-zebra  INFO   Thu, 29 Sep. 2022 21:24:08  Performing PXE boot    
unknown   bk7mg8  fun-zebra  INFO   Thu, 29 Sep. 2022 21:23:51  Powering on            
unknown   bk7mg8  fun-zebra  INFO   Thu, 29 Sep. 2022 21:23:51  Testing                
unknown   bk7mg8  fun-zebra  INFO   Thu, 29 Sep. 2022 21:23:38  Released               
unknown   bk7mg8  fun-zebra  INFO   Thu, 29 Sep. 2022 21:23:37  Powering off           
unknown   bk7mg8  fun-zebra  INFO   Thu, 29 Sep. 2022 21:23:37  Releasing              
unknown   bk7mg8  fun-zebra  INFO   Thu, 29 Sep. 2022 21:22:41  Deployed               
unknown   bk7mg8  fun-zebra  INFO   Thu, 29 Sep. 2022 21:21:49  Rebooting              
unknown   bk7mg8  fun-zebra  INFO   Thu, 29 Sep. 2022 21:18:42  Configuring OS         
unknown   bk7mg8  fun-zebra  INFO   Thu, 29 Sep. 2022 21:17:42  Installing OS          
unknown   bk7mg8  fun-zebra  INFO   Thu, 29 Sep. 2022 21:17:30  Configuring storage    
unknown   bk7mg8  fun-zebra  INFO   Thu, 29 Sep. 2022 21:15:31  Loading ephemeral      
unknown   bk7mg8  fun-zebra  INFO   Thu, 29 Sep. 2022 21:14:48  Performing PXE boot    
unknown   bk7mg8  fun-zebra  INFO   Thu, 29 Sep. 2022 21:14:31  Powering on            
unknown   bk7mg8  fun-zebra  INFO   Thu, 29 Sep. 2022 21:14:27  Deploying              
unknown   bk7mg8  fun-zebra  INFO   Thu, 29 Sep. 2022 20:04:17  Ready                  
unknown   bk7mg8  fun-zebra  INFO   Thu, 29 Sep. 2022 20:04:07  Running test           smartctl-validate on sda
unknown   bk7mg8  fun-zebra  INFO   Thu, 29 Sep. 2022 20:01:27  Gathering information  
unknown   bk7mg8  fun-zebra  INFO   Thu, 29 Sep. 2022 20:01:10  Loading ephemeral      
unknown   bk7mg8  fun-zebra  INFO   Thu, 29 Sep. 2022 20:00:35  Performing PXE boot    
unknown   bk7mg8  fun-zebra  INFO   Thu, 29 Sep. 2022 20:00:16  Powering on            
unknown   bk7mg8  fun-zebra  INFO   Thu, 29 Sep. 2022 20:00:16  Commissioning          
```

Additional examples and techniques are provided as part of this new documentation.

** Improved MAAS audit event documentation

MAAS audit event documentation has been greatly expanded to include [much better explanations](/t/understanding-maas-audit-events/6372) of MAAS audit events, including [detailed examples of how to reconstruct machine life-cycles](/t/how-to-work-with-audit-event-logs/5987#heading--How-to-audit-a-machines-life-cycle-with-audit-events) in the updated version of "[How to work with audit event logs](/t/how-to-work-with-audit-event-logs/5987)".

*** Ten words or less

We've finally offered details about how you should audit MAAS.

*** Understanding how audit events explain MAAS internal operations

There's probably no limit to what you can figure out if you use audit events properly.  The problems are: (1) a lot goes on in MAAS, and (2) you need more than just the explicit audit events to get a clear picture of what's happening.  We've tried to address this by taking a deeper look at the auditing process (not just the events).  

As you may know, an audit event is just a [MAAS event](/t/understanding-maas-events/6373) tagged with `AUDIT`. It generally captures changes to the MAAS configuration and machine states. These events provide valuable oversight of user actions and automated updates -- and their effects -- especially when multiple users are interacting with multiple machines.  

*** Viewing events

Audit events are examined using the MAAS CLI with the `level=AUDIT` parameter set:

```nohighlight
$ maas $PROFILE events query level=AUDIT
```

You'll probably get better results by appending a `jq` filter, to prettify the output:

```nohighlight
$ maas $PROFILE events query level=AUDIT after=0 limit=20 \
| jq -r '(["USERNAME","HOSTNAME","DATE","EVENT"] | 
(., map(length*"-"))),
(.events[] | [.username,.hostname,.created,.description]) 
| @tsv' | column -t -s$'\t'
```

By itself, such a command might produce output similar to this:

```nohighlight
USERNAME  HOSTNAME     DATE                        EVENT
--------  --------     ----                        -----
unknown   valued-moth  Thu, 21 Apr. 2022 19:45:14  pci device 2 was updated on node 8wmfx3
unknown   valued-moth  Thu, 21 Apr. 2022 19:45:14  pci device 1 was updated on node 8wmfx3
unknown   valued-moth  Thu, 21 Apr. 2022 19:45:14  pci device 1 was updated on node 8wmfx3
unknown   valued-moth  Thu, 21 Apr. 2022 19:45:14  pci device 1 was updated on node 8wmfx3
unknown   valued-moth  Thu, 21 Apr. 2022 19:45:14  pci device 1 was updated on node 8wmfx3
unknown   valued-moth  Thu, 21 Apr. 2022 19:45:14  pci device 1 was updated on node 8wmfx3
unknown   valued-moth  Thu, 21 Apr. 2022 19:45:14  pci device 1 was updated on node 8wmfx3
unknown   valued-moth  Thu, 21 Apr. 2022 19:45:14  pci device 1 was updated on node 8wmfx3
unknown   valued-moth  Thu, 21 Apr. 2022 19:45:14  pci device 1 was updated on node 8wmfx3
unknown   valued-moth  Thu, 21 Apr. 2022 19:45:14  pci device 0 was updated on node 8wmfx3
unknown   valued-moth  Thu, 21 Apr. 2022 19:45:14  block device sda was updated on node 8wmfx3
unknown   valued-moth  Thu, 21 Apr. 2022 19:45:14  interface enp5s0 was updated on node 8wmfx3
unknown   valued-moth  Thu, 21 Apr. 2022 19:45:14  0 bytes of memory was removed on node 8wmfx3
admin     valued-moth  Thu, 21 Apr. 2022 19:36:48  Started deploying 'valued-moth'.
admin     valued-moth  Thu, 21 Apr. 2022 19:36:21  Acquired 'valued-moth'.
admin     unknown      Thu, 21 Apr. 2022 19:21:46  Updated configuration setting 'completed_intro' to 'True'.
admin     unknown      Thu, 21 Apr. 2022 19:20:49  Updated configuration setting 'upstream_dns' to '8.8.8.8'.
admin     unknown      Thu, 21 Apr. 2022 19:20:49  Updated configuration setting 'maas_name' to 'neuromancer'.
admin     unknown      Thu, 21 Apr. 2022 19:20:47  Updated configuration setting 'http_proxy' to ''.
admin     unknown      Thu, 21 Apr. 2022 19:20:24  Logged in admin.
```

You can, of course, use the [various event filters](/t/understanding-maas-events/6373#heading--filter-parameters) with `level=AUDIT` to further restrict your output.

*** The meaning of audit events

Later on in the documentation, we walk through a sample of audit events and demonstrate how to interpret and use them.  This includes detailed examples of various audit event queries, walking through real-world examples to answer questions like:

1. Who deployed `comic-muskox`? 

2. What happened to `sweet-urchin`?

3. Why is `fleet-calf` in rescue mode?

4. Where did these changes come from in `setup.sh`?

5. What caused `ruling-bobcat` to be marked as broken?

6. Who's responsible for the DHCP snippet called `foo`?

*** Auditing with finesse

As part of the updates to our "[How to work with audit event logs](/t/how-to-work-with-audit-event-logs/5987)", we've tried to offer you some finesse in reconstructing machine life-cycles.  We've shown how to combine various levels of MAAS event queries with standard command line utilities to produce clear audit trails such as this one:

```nohighlight
418606  ERROR    Marking node broken               Wed, 17 Nov. 2021 00:02:52  A Physical Interface requires a MAC address.
418607  DEBUG    Node changed status               Wed, 17 Nov. 2021 00:02:52  From 'New' to 'Broken'
418608  DEBUG    Marking node fixed                Wed, 17 Nov. 2021 00:04:24  
418609  DEBUG    Node changed status               Wed, 17 Nov. 2021 00:04:24  From 'Broken' to 'Ready'
418613  DEBUG    User acquiring node               Wed, 17 Nov. 2021 00:04:51  (admin)
418614  DEBUG    Node changed status               Wed, 17 Nov. 2021 00:04:51  From 'Ready' to 'Allocated' (to admin)
418615  DEBUG    User starting deployment          Wed, 17 Nov. 2021 00:04:51  (admin)
418616  DEBUG    Node changed status               Wed, 17 Nov. 2021 00:04:51  From 'Allocated' to 'Deploying'
418617  INFO     Deploying                         Wed, 17 Nov. 2021 00:04:51  
418618  AUDIT    Node                              Wed, 17 Nov. 2021 00:04:51  Started deploying 'ruling-bobcat'.
418619  INFO     Powering on                       Wed, 17 Nov. 2021 00:04:55  
418625  ERROR    Marking node failed               Wed, 17 Nov. 2021 00:05:32  Power on for the node failed: Failed talking to node's BMC: Failed to power pbpncx. BMC never transitioned from off to on.
418626  DEBUG    Node changed status               Wed, 17 Nov. 2021 00:05:32  From 'Deploying' to 'Failed deployment'
418627  ERROR    Failed to power on node           Wed, 17 Nov. 2021 00:05:32  Power on for the node failed: Failed talking to node's BMC: Failed to power pbpncx. BMC never transitioned from off to on.
```

In this case, we managed to recognise, rather quickly, that no physical interface had been defined for `ruling-bobcat`, hence deployment fails because MAAS can't communicate with the node's BMC.  There are many other issues you can recognise with careful use of MAAS events to audit machine behaviours.  We welcome your feedback on this new documentation endeavour.

<a href="#heading--MAAS-3.3-bug-list">** id="heading--MAAS-3.3-bug-list">MAAS 3.3 bug list

The following sections enumerate the bugs we've fixed in MAAS 3.3.

*** MAAS 3.3 Beta 1 bug list

So far in MAAS 3.3, we've fixed well over 100 bugs:

- [1762673](https://bugs.launchpad.net/bugs/1762673)`↗`: maas insists on running the proxy, even when it's disabled
- [1802505](https://bugs.launchpad.net/bugs/1802505)`↗`: [ui][2.4][2.5] maas ignores ttl parameter for address records
- [1806707](https://bugs.launchpad.net/bugs/1806707)`↗`: [2.5] Composing a VM with an interface attached to a (macvtap) network (on a KVM host NIC that is not a bridge) resulted in disconnect interface
- [1811109](https://bugs.launchpad.net/bugs/1811109)`↗`: [2.5, UI, RBAC] Normal users can unmount the root file system, but not remount it
- [1818004](https://bugs.launchpad.net/bugs/1818004)`↗`: Slow response in the UI
- [1822640](https://bugs.launchpad.net/bugs/1822640)`↗`: [websocket, UI] Admins should be able to change ownership of resources over the UI
- [1822840](https://bugs.launchpad.net/bugs/1822840)`↗`: [UI, feature] Add ability to edit/delete (manage) tags over the UI
- [1825255](https://bugs.launchpad.net/bugs/1825255)`↗`: TestPostgresListenerService test fails erroneously in CI
- [1826011](https://bugs.launchpad.net/bugs/1826011)`↗`: [UI] Compose machine from pod misaligned message
- [1826789](https://bugs.launchpad.net/bugs/1826789)`↗`: stress-ng-cpu-long times out in bionic
- [1826967](https://bugs.launchpad.net/bugs/1826967)`↗`: Exiting rescue mode shows 'loading ephemeral'
- [1833545](https://bugs.launchpad.net/bugs/1833545)`↗`: After removing a controller rackd still tries to update DNS
- [1840049](https://bugs.launchpad.net/bugs/1840049)`↗`: [UI] When changing configuration of an Interface, one has to enter the config twice
- [1852360](https://bugs.launchpad.net/bugs/1852360)`↗`: Validate network configuration button selects all network scripts that accept an interface parameter
- [1863395](https://bugs.launchpad.net/bugs/1863395)`↗`: 2.6.2 Unable to change power type to manual via UI
- [1871605](https://bugs.launchpad.net/bugs/1871605)`↗`: Updating controller name shouldn't be allowed in the UI
- [1874355](https://bugs.launchpad.net/bugs/1874355)`↗`: Controller details page not updated to match machine details page designs
- [1881948](https://bugs.launchpad.net/bugs/1881948)`↗`: IPv6 address for power control fails
- [1882633](https://bugs.launchpad.net/bugs/1882633)`↗`: Logical volume size is required
- [1883232](https://bugs.launchpad.net/bugs/1883232)`↗`: UI: UI application cached after upgrade
- [1890262](https://bugs.launchpad.net/bugs/1890262)`↗`: UI: Error message with a custom image URL doesn't clear
- [1893226](https://bugs.launchpad.net/bugs/1893226)`↗`: Machine-specific minimal commissioning kernel resets to MAAS wide default
- [1893670](https://bugs.launchpad.net/bugs/1893670)`↗`: UI: display bios_boot_mode in the web UI
- 1898131: IP address gets hidden, instead of subnet when window is resized
- [1905636](https://bugs.launchpad.net/bugs/1905636)`↗`: UI: CentOS 7 is the default over CentOS 8
- [1909348](https://bugs.launchpad.net/bugs/1909348)`↗`: MAAS 2.9.0 DNS zone remove @ labels impossible
- [1913800](https://bugs.launchpad.net/bugs/1913800)`↗`: PCI and USB information missing from controllers page
- [1918963](https://bugs.launchpad.net/bugs/1918963)`↗`: Controllers page out of sync with nodes
- [1918978](https://bugs.launchpad.net/bugs/1918978)`↗`: doesn't detect the subarchitecture xgene-uboot for a HP m400 cartridge
- [1927748](https://bugs.launchpad.net/bugs/1927748)`↗`: Need additional form inputs for DHCP Snippets associated with ipranges
- [1929478](https://bugs.launchpad.net/bugs/1929478)`↗`: Commissioning fails with binary data in IPMI Lan_Conf_Security_Keys
- [1929973](https://bugs.launchpad.net/bugs/1929973)`↗`: "Controllers have different installation sources." is not shown on the controllers page
- [1931654](https://bugs.launchpad.net/bugs/1931654)`↗`: domain.set_default error handling just returns id
- [1933408](https://bugs.launchpad.net/bugs/1933408)`↗`: Setting discovery parent returns cryptic error
- [1938296](https://bugs.launchpad.net/bugs/1938296)`↗`: MAAS 3.0 incorrectly calculates the amount of free space on drive
- [1940909](https://bugs.launchpad.net/bugs/1940909)`↗`: UI: Allow to create a machine as deployed from the UI
- [1951229](https://bugs.launchpad.net/bugs/1951229)`↗`: CLI: Uninformative errors when adding non-existent tags to machines
- [1955671](https://bugs.launchpad.net/bugs/1955671)`↗`: support for rocky linux UEFI
- [1956766](https://bugs.launchpad.net/bugs/1956766)`↗`: UI: Unable to deploy CentOS7 - centos/focal not a supported combination
- [1958817](https://bugs.launchpad.net/bugs/1958817)`↗`: Cannot delete a tag from multiple machines in a bulk with Web UI
- [1959856](https://bugs.launchpad.net/bugs/1959856)`↗`: newly added tags in UI don't show until refresh/page change
- [1960571](https://bugs.launchpad.net/bugs/1960571)`↗`: Domain name should be checked for duplicate against maas_internal_domain
- [1961627](https://bugs.launchpad.net/bugs/1961627)`↗`: confusing UI to add the first network space
- [1964024](https://bugs.launchpad.net/bugs/1964024)`↗`: smartctl-validate test runs even when explicitly removed from commissioning step
- [1965172](https://bugs.launchpad.net/bugs/1965172)`↗`: [3.1] Setting interface into unconfigured does not reset auto-assign IP mode
- [1967577](https://bugs.launchpad.net/bugs/1967577)`↗`: commissioning fails w/ 3.2-beta1: "please fill out the field"
- [1970803](https://bugs.launchpad.net/bugs/1970803)`↗`: CLI event filters give extraneous results with more than one filter
- [1971152](https://bugs.launchpad.net/bugs/1971152)`↗`: Authentication broken with MASS using Candid
- [1973236](https://bugs.launchpad.net/bugs/1973236)`↗`: MAAS reports failure to detect storage that it already detected
- [1973617](https://bugs.launchpad.net/bugs/1973617)`↗`: refresh a lxd KVM host resources after it was added
- [1976194](https://bugs.launchpad.net/bugs/1976194)`↗`: init rack can't find secrets
- [1976196](https://bugs.launchpad.net/bugs/1976196)`↗`: Controller WebSocket handler contains unimplemented methods
- [1977820](https://bugs.launchpad.net/bugs/1977820)`↗`: Some tests are skipped due to the "perf" filtering
- [1977822](https://bugs.launchpad.net/bugs/1977822)`↗`: ERROR: Redfish 'Redfish' object has no attribute '_get_network_interface'
- [1977864](https://bugs.launchpad.net/bugs/1977864)`↗`: 30-maas-01-bmc-config: ERROR: Redfish string indices must be integers
- [1977866](https://bugs.launchpad.net/bugs/1977866)`↗`: 30-maas-01-bmc-config: ERROR: 169.254.95.120/16 has host bits set
- [1977895](https://bugs.launchpad.net/bugs/1977895)`↗`: Certificate metadata missing from controller websocket model
- [1977942](https://bugs.launchpad.net/bugs/1977942)`↗`: 30-maas-01-bmc-config: ERROR: Redfish 'Redfish' object has no attribute '_bmc_config'
- [1977951](https://bugs.launchpad.net/bugs/1977951)`↗`: 30-maas-01-bmc-config: ERROR: Redfish nonnumeric port: 'None'
- [1978024](https://bugs.launchpad.net/bugs/1978024)`↗`: maas auto-creates interface name for docker bridge on controller, which breaks bind
- [1978037](https://bugs.launchpad.net/bugs/1978037)`↗`: Drop legacy /l/ UI prefix
- [1978072](https://bugs.launchpad.net/bugs/1978072)`↗`: 30-maas-01-bmc-config: ERROR: Redfish 'dict' object has no attribute 'split'
- [1978121](https://bugs.launchpad.net/bugs/1978121)`↗`: 30-maas-01-bmc-config: ERROR: ERROR: Unable to add BMC user!
- [1978154](https://bugs.launchpad.net/bugs/1978154)`↗`: MAAS 3.1 introduces breaking changes for custom centos7 images
- [1978922](https://bugs.launchpad.net/bugs/1978922)`↗`: MAAS 3.1 - Missing button "Create datastore" in VMFS7 storage layout
- [1979039](https://bugs.launchpad.net/bugs/1979039)`↗`: TLS certificates are not recognised by CLI maas <profile> boot-resources create action
- [1979256](https://bugs.launchpad.net/bugs/1979256)`↗`: Add config option for UI theme
- [1979316](https://bugs.launchpad.net/bugs/1979316)`↗`: UI stuck at the initial configuration page
- [1979317](https://bugs.launchpad.net/bugs/1979317)`↗`: Initial configuration form doesn't allow proxy URL with hostname
- [1980347](https://bugs.launchpad.net/bugs/1980347)`↗`: MAAS snap fails to parse supervisor STOPPING state
- [1980436](https://bugs.launchpad.net/bugs/1980436)`↗`: MAAS CLI with anonymous access fails when TLS is enabled
- [1980490](https://bugs.launchpad.net/bugs/1980490)`↗`: MAAS regiond IPC crash due to a machine-resources binary crash when parsing some VPDs
- [1980818](https://bugs.launchpad.net/bugs/1980818)`↗`: Configure DHCP for VLAN
- [1980846](https://bugs.launchpad.net/bugs/1980846)`↗`: IP Address tooltip on Machines page blocks access to everything underneath and doesnt disappear until mouse-off
- [1981536](https://bugs.launchpad.net/bugs/1981536)`↗`: volume group creation fails on md device - MAAS 3.2
- [1981560](https://bugs.launchpad.net/bugs/1981560)`↗`: upgrade from 3.1 to 3.2 using debian packages missing steps
- [1982208](https://bugs.launchpad.net/bugs/1982208)`↗`: agent.yaml.example is missing when maas is installed via deb package
- [1982315](https://bugs.launchpad.net/bugs/1982315)`↗`: MAAS not sending correct metadata_url
- [1982328](https://bugs.launchpad.net/bugs/1982328)`↗`: update docstring to include informative not found change
- [1982846](https://bugs.launchpad.net/bugs/1982846)`↗`: Missing update_interface method on controller websocket handler
- [1982866](https://bugs.launchpad.net/bugs/1982866)`↗`: MAAS Breaks historical custom images
- [1982984](https://bugs.launchpad.net/bugs/1982984)`↗`: reverse-proxy service is not displayed for region controller
- [1983624](https://bugs.launchpad.net/bugs/1983624)`↗`: Fresh MAAS 3.2 install failed to find controller
- [1984141](https://bugs.launchpad.net/bugs/1984141)`↗`: duplicate tag results in failed deployment for KVM host
- [1984852](https://bugs.launchpad.net/bugs/1984852)`↗`: machine.filter_options returns empty, duplicate and mis-typed options
- [1984994](https://bugs.launchpad.net/bugs/1984994)`↗`: machine.list fails for some group_key values
- [1985741](https://bugs.launchpad.net/bugs/1985741)`↗`: Commissioning script 'maas-kernel-cmdline' fails with bonded interfaces
- [1986372](https://bugs.launchpad.net/bugs/1986372)`↗`: UI: Setting Default minimum kernel version for commissioning blocks deployments
- [1987874](https://bugs.launchpad.net/bugs/1987874)`↗`: interface.update_ip_addresses raise an Exception when exsits multiple StaticIPAddress
- [1988543](https://bugs.launchpad.net/bugs/1988543)`↗`: VM Discovery fails, resulting in " Error: An architecture is required." when composing a LXD VM
- [1988759](https://bugs.launchpad.net/bugs/1988759)`↗`: Provisioning LXD vmhost fails
- [1988769](https://bugs.launchpad.net/bugs/1988769)`↗`: The ppc64 machine in our lab fails during commissioning
- [1988874](https://bugs.launchpad.net/bugs/1988874)`↗`: Release command is failing for ppc64 machine in our lab
- [1989949](https://bugs.launchpad.net/bugs/1989949)`↗`: provisioningserver TestGetSourceAddress.test_returns_none_if_no_route_found sometimes fails locally
- [1989970](https://bugs.launchpad.net/bugs/1989970)`↗`: Can't enlist machines on subnets with DNS set
- [1989974](https://bugs.launchpad.net/bugs/1989974)`↗`: rackd fails on CIS-hardened machine with "Failed to update and/or record network interface configuration: Expecting value: line 1 column 1 (char 0)"
- [1990014](https://bugs.launchpad.net/bugs/1990014)`↗`: regiond.conf "debug_http: true" causes image downloads from regiond to fail with 500 error code
- [1990649](https://bugs.launchpad.net/bugs/1990649)`↗`: Kernel parameters form resets to previous value after save
- [1990873](https://bugs.launchpad.net/bugs/1990873)`↗`: TestKeys - test_get_launchpad_crashes_for_user_not_found
- [1991106](https://bugs.launchpad.net/bugs/1991106)`↗`: vCenter password field text is visible in settings
- [1991210](https://bugs.launchpad.net/bugs/1991210)`↗`: Color theme resets with page reload
- [1991229](https://bugs.launchpad.net/bugs/1991229)`↗`: Selecting all machines in a state in the UI causes traceback in backend
- [1991372](https://bugs.launchpad.net/bugs/1991372)`↗`: websocket config update notifications are no longer sent
- [1991410](https://bugs.launchpad.net/bugs/1991410)`↗`: wildcard DNS entry is not allowed
- [1991792](https://bugs.launchpad.net/bugs/1991792)`↗`: machine.action clone does not accept filter
- [1991795](https://bugs.launchpad.net/bugs/1991795)`↗`: machine.action does not always throw errors for failed machines
- [1992332](https://bugs.launchpad.net/bugs/1992332)`↗`: websocket machine.list parent group label should return hostname
- [1992686](https://bugs.launchpad.net/bugs/1992686)`↗`: MAAS 3.3 alpha missing two existing filters
- [1992975](https://bugs.launchpad.net/bugs/1992975)`↗`: Grouping by parents fails if there's more than one page

More bug-fixes are planned for later 3.3 releases.

*** MAAS 3.3 Beta 2 bug list

- [1990289](https://bugs.launchpad.net/bugs/1990289)`↗`: allocate call with system_id can allocate a new machine
- [1991784](https://bugs.launchpad.net/bugs/1991784)`↗`: [needs-packaging] GL Excess
- [1992185](https://bugs.launchpad.net/bugs/1992185)`↗`: unable to deploy a machine with vmhost if a bond interface was created
- [1992494](https://bugs.launchpad.net/bugs/1992494)`↗`: Jammy KVM host support
- [1992791](https://bugs.launchpad.net/bugs/1992791)`↗`: Info icons appear/disappear based on checked options in subnet page
- [1993289](https://bugs.launchpad.net/bugs/1993289)`↗`: Pod storage pool path can't be blank

*** MAAS 3.3 Beta 3 bug list

- [1835271](https://bugs.launchpad.net/bugs/1835271)`↗`: Ephemeral deployment keeps cloud-inits autogenerated netplan config
- [1843268](https://bugs.launchpad.net/bugs/1843268)`↗`: maas become unresponsive with maasserver_notification stuck at concurrent update
- [1886045](https://bugs.launchpad.net/bugs/1886045)`↗`: Error message mentions Pods when trying to release a machine
- [1886850](https://bugs.launchpad.net/bugs/1886850)`↗`: Encrypt the BMC credentials
- [1937138](https://bugs.launchpad.net/bugs/1937138)`↗`: Calling mark_intro_complete doesn't respond correctly
- [1955709](https://bugs.launchpad.net/bugs/1955709)`↗`: Metadata field may_reboot not working correctly in 20.04
- [1988229](https://bugs.launchpad.net/bugs/1988229)`↗`: dhcp snippet create fails when dhcp subnet is relayed regression
- [1990383](https://bugs.launchpad.net/bugs/1990383)`↗`: Link subnet on new machine
- [1992330](https://bugs.launchpad.net/bugs/1992330)`↗`: Use the rack controller IP as DNS when relaying DHCP
- 1993032: maas_hardware_sync creds are readable to local users on deployed OS and can give a super user access to MAAS itself
- [1993152](https://bugs.launchpad.net/bugs/1993152)`↗`: Updating a VM host through API unset tags
- [1994899](https://bugs.launchpad.net/bugs/1994899)`↗`: MAAS cannot mark "broken" VMs as fixed without recommissioning
- [1995397](https://bugs.launchpad.net/bugs/1995397)`↗`: Sentry blocked by CORS
- [1995624](https://bugs.launchpad.net/bugs/1995624)`↗`: suppressing script results no longer available on machine listing
- [1996065](https://bugs.launchpad.net/bugs/1996065)`↗`: CLI errors when redirecting the output to a file
- [1996074](https://bugs.launchpad.net/bugs/1996074)`↗`: Machine details stuck at "Loading" for machines with no disks
- [1996419](https://bugs.launchpad.net/bugs/1996419)`↗`: renaming a DNS record to a previous name fails with error: list.remove(x): x not in list
- [1996935](https://bugs.launchpad.net/bugs/1996935)`↗`: agent.yaml.example is missing when maas is installed via snap
- [1997190](https://bugs.launchpad.net/bugs/1997190)`↗`: Power parameters access attempt from non-db thread 
- [1997191](https://bugs.launchpad.net/bugs/1997191)`↗`: Uncaught exception when configuring DNS
- [1997281](https://bugs.launchpad.net/bugs/1997281)`↗`: machine.count fails for new filter options
- [1997599](https://bugs.launchpad.net/bugs/1997599)`↗`: Losing LXD certificate 
	
*** MAAS 3.3 RC1 bug list

- [1997975](https://bugs.launchpad.net/maas/+bug/1997975)`↗`: Update grafana_agent/agent.yaml.example

*** MAAS 3.3 RC3 bug list

- [1990172](https://bugs.launchpad.net/maas/+bug/1990172)`↗`: "20-maas-03-machine-resources" commissioning script improperly reports a Pass when the test fails 

*** MAAS 3.3.0 bug list

- [2003888](https://bugs.launchpad.net/maas/+bug/2003888)`↗`: Grouped machine list view: Inconsistent display when machine state changes

** Release notes for other MAAS versions

Here are release notes for other relatively recent MAAS versions:

- [MAAS 3.2](https://maas.io/docs/what-is-new-with-maas-3-2)`↗`
- [MAAS 3.1](https://maas.io/docs/what-is-new-with-maas-3-1)`↗`
- [MAAS 3.0](https://maas.io/docs/what-is-new-with-maas-3-0)`↗`
- [MAAS 2.9](https://maas.io/docs/what-is-new-with-maas-2-9)`↗`
- [MAAS 2.8](https://maas.io/docs/what-is-new-with-maas-2-8)`↗`
- [MAAS 2.7](https://maas.io/docs/what-is-new-with-maas-2-7)`↗`
